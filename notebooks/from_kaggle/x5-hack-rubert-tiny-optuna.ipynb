{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"widgets":{"application/vnd.jupyter.widget-state+json":{"a5b0e7638d6d4c4ca0996d7b020e59d6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c9d7f068b3b1456b94729031bf250e94","IPY_MODEL_3a24111dace7456a9e37fbb60d04c38b","IPY_MODEL_35d94b59d14549b5b9d86553a348e420"],"layout":"IPY_MODEL_7479e69dcd50435e884e334cc0cf38ee"}},"c9d7f068b3b1456b94729031bf250e94":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_174e223ef4a945579f9eb7f4ae0e93e4","placeholder":"​","style":"IPY_MODEL_d79c62114fd142029255e0db1d52ed9a","value":"config.json: "}},"3a24111dace7456a9e37fbb60d04c38b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f96b1b42a7e44816852f51b36e39dd09","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_833566aa83484331b5c11dfdb7f21ee9","value":1}},"35d94b59d14549b5b9d86553a348e420":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_52e1f4410c4f4c3d8cc06c0157a03af7","placeholder":"​","style":"IPY_MODEL_74ccad71321d41cea860aaefec83502d","value":" 1.37k/? [00:00&lt;00:00, 38.3kB/s]"}},"7479e69dcd50435e884e334cc0cf38ee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"174e223ef4a945579f9eb7f4ae0e93e4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d79c62114fd142029255e0db1d52ed9a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f96b1b42a7e44816852f51b36e39dd09":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"833566aa83484331b5c11dfdb7f21ee9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"52e1f4410c4f4c3d8cc06c0157a03af7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"74ccad71321d41cea860aaefec83502d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5beb6f270e544ad3b5f2b9edd2605a5b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fc0a360c3f2a4aaeb70b1885824fa0e1","IPY_MODEL_f908bd7798154697a9ff049ae264d9fb","IPY_MODEL_4d28a06aede948a294baaf885731bb09"],"layout":"IPY_MODEL_64d1aa0086b84b87bc7a92e0c58eef1c"}},"fc0a360c3f2a4aaeb70b1885824fa0e1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_76a07ea51a1d4e8d80962661675ea36e","placeholder":"​","style":"IPY_MODEL_bafb54e447c648a5882908f01813707d","value":"pytorch_model.bin: 100%"}},"f908bd7798154697a9ff049ae264d9fb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f74a6c9f08894d0d9d1aefa18dc2ac17","max":2950830248,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7de1fa7fcd614b418298905d6ec4f797","value":2950830248}},"4d28a06aede948a294baaf885731bb09":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d75f429947524e71ab02959544ba3c61","placeholder":"​","style":"IPY_MODEL_854e9b2131e945f69fea27cbcea4c144","value":" 2.95G/2.95G [01:15&lt;00:00, 39.3MB/s]"}},"64d1aa0086b84b87bc7a92e0c58eef1c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"76a07ea51a1d4e8d80962661675ea36e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bafb54e447c648a5882908f01813707d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f74a6c9f08894d0d9d1aefa18dc2ac17":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7de1fa7fcd614b418298905d6ec4f797":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d75f429947524e71ab02959544ba3c61":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"854e9b2131e945f69fea27cbcea4c144":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bd16b83691fd460e8cd07c0dd51a8aa2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6a3cb4e7e20d461db1e9a97415ed9e7c","IPY_MODEL_b3bc82a5af874c27a57e117569aba3f0","IPY_MODEL_1a2717f60ca9484abbafb785df3faa46"],"layout":"IPY_MODEL_0ea90c0f92c4414c8f93a554a74cfab1"}},"6a3cb4e7e20d461db1e9a97415ed9e7c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9287aa5abe4746128c2eeb1a4547a61a","placeholder":"​","style":"IPY_MODEL_a1cdb359996b4942ad1b416fbd747e97","value":"model.safetensors: 100%"}},"b3bc82a5af874c27a57e117569aba3f0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c2efdb87c7844176991e0906f1f183dc","max":2950734664,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ae82b099a4f143e38d82feb76211b8fb","value":2950734664}},"1a2717f60ca9484abbafb785df3faa46":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_43fdcf9676544621b741aa7ad751f621","placeholder":"​","style":"IPY_MODEL_dc799160c3714d9087f92c71c5bf79e2","value":" 2.95G/2.95G [03:04&lt;00:00, 18.5MB/s]"}},"0ea90c0f92c4414c8f93a554a74cfab1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9287aa5abe4746128c2eeb1a4547a61a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a1cdb359996b4942ad1b416fbd747e97":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c2efdb87c7844176991e0906f1f183dc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ae82b099a4f143e38d82feb76211b8fb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"43fdcf9676544621b741aa7ad751f621":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dc799160c3714d9087f92c71c5bf79e2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"219043a57bd8424c8a3ccf0c1a54115c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_50329a055e564b429c0c2c3e81f08272","IPY_MODEL_7fd98312f8c94562b016b924a38efc80","IPY_MODEL_31657f0e1130493d9faa5511f648fa53"],"layout":"IPY_MODEL_4bc7b0ae0c4b4685bbcc941601390f78"}},"50329a055e564b429c0c2c3e81f08272":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d05042af89454f91ba8bdf52d85f11fd","placeholder":"​","style":"IPY_MODEL_57c2910ccdfb47eeb7c1645e82778573","value":"Map: 100%"}},"7fd98312f8c94562b016b924a38efc80":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f4ebf79726ab40eaa1ac7fc99e0a38ce","max":20438,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d1f3799d8ade4c3981ddce11e76a0fcd","value":20438}},"31657f0e1130493d9faa5511f648fa53":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0366d2f2ee7c4c4b9621c3afd5e75f23","placeholder":"​","style":"IPY_MODEL_4dbe44cb63504cc6b81778510201e2e3","value":" 20438/20438 [00:01&lt;00:00, 9777.39 examples/s]"}},"4bc7b0ae0c4b4685bbcc941601390f78":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d05042af89454f91ba8bdf52d85f11fd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"57c2910ccdfb47eeb7c1645e82778573":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f4ebf79726ab40eaa1ac7fc99e0a38ce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d1f3799d8ade4c3981ddce11e76a0fcd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0366d2f2ee7c4c4b9621c3afd5e75f23":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4dbe44cb63504cc6b81778510201e2e3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d76a2f0f26d7457cb26bebde30ed5e0e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c117ffca9f6b4146b305b558f7939847","IPY_MODEL_dc094b00dec74afb802ee7d6df12c1be","IPY_MODEL_374bb6ddc4a64823b2cfdd327864a953"],"layout":"IPY_MODEL_b32f39249a974519ba3b1fd956c53276"}},"c117ffca9f6b4146b305b558f7939847":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e1e6420c5c4b409980a2a951dad44dd8","placeholder":"​","style":"IPY_MODEL_87b888ce920d454dad4945fa26643fe8","value":"Map: 100%"}},"dc094b00dec74afb802ee7d6df12c1be":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_259f9f4db413414a823a1a3f241a98a7","max":6813,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5e340cbd6da544828ce8609fb500945d","value":6813}},"374bb6ddc4a64823b2cfdd327864a953":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_56093491341a4bf39bfe7b09fdd41e86","placeholder":"​","style":"IPY_MODEL_a25d768bb6cd45b18999ede839a8428c","value":" 6813/6813 [00:00&lt;00:00, 17425.11 examples/s]"}},"b32f39249a974519ba3b1fd956c53276":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e1e6420c5c4b409980a2a951dad44dd8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"87b888ce920d454dad4945fa26643fe8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"259f9f4db413414a823a1a3f241a98a7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5e340cbd6da544828ce8609fb500945d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"56093491341a4bf39bfe7b09fdd41e86":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a25d768bb6cd45b18999ede839a8428c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13177364,"sourceType":"datasetVersion","datasetId":8350299},{"sourceId":13178124,"sourceType":"datasetVersion","datasetId":8350857},{"sourceId":13178161,"sourceType":"datasetVersion","datasetId":8350889}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install optuna seqeval evaluate -q","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BJsYu1XI0I1c","outputId":"5de3e6d3-3882-4c99-b7e0-a03fdc9906c8","trusted":true,"execution":{"iopub.status.busy":"2025-09-26T17:49:19.825457Z","iopub.execute_input":"2025-09-26T17:49:19.826068Z","iopub.status.idle":"2025-09-26T17:49:29.069570Z","shell.execute_reply.started":"2025-09-26T17:49:19.826045Z","shell.execute_reply":"2025-09-26T17:49:29.068680Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport ast\nfrom tqdm import tqdm\nfrom collections import Counter\nimport datasets\nfrom datasets import (Dataset, Features, Sequence, Value, ClassLabel, load_dataset,\n                    load_from_disk, concatenate_datasets, DatasetDict)\nfrom sklearn.model_selection import KFold\nfrom transformers import (AutoTokenizer, AutoModel, AutoModelForTokenClassification,\n                         pipeline, TrainingArguments, Trainer,\n                         DataCollatorForTokenClassification, EarlyStoppingCallback)\nimport torch\nimport optuna\nfrom optuna.pruners import MedianPruner\nimport os\nos.environ['WANDB_DISABLED'] = 'true'\nimport pickle\nimport numpy as np\nimport seqeval\nimport evaluate\nfrom seqeval.metrics import classification_report, precision_score, recall_score, f1_score, accuracy_score\n\nseqeval = evaluate.load(\"seqeval\")","metadata":{"id":"_88EgnsXmkyw","trusted":true,"execution":{"iopub.status.busy":"2025-09-26T17:49:29.071198Z","iopub.execute_input":"2025-09-26T17:49:29.071546Z","iopub.status.idle":"2025-09-26T17:50:10.615242Z","shell.execute_reply.started":"2025-09-26T17:49:29.071520Z","shell.execute_reply":"2025-09-26T17:50:10.614729Z"}},"outputs":[{"name":"stderr","text":"2025-09-26 17:49:50.481839: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1758908990.854228      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1758908990.957147      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa705fdde7b94b6dbc38b82a004a743b"}},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"# from google.colab import drive\n# drive.mount('/content/drive', force_remount=True)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-0vpcj2O3kaP","outputId":"44dd6115-274c-45f8-d1dd-3d2ac5b596ad","trusted":true,"execution":{"iopub.status.busy":"2025-09-26T17:50:10.616082Z","iopub.execute_input":"2025-09-26T17:50:10.616608Z","iopub.status.idle":"2025-09-26T17:50:10.619921Z","shell.execute_reply.started":"2025-09-26T17:50:10.616589Z","shell.execute_reply":"2025-09-26T17:50:10.619123Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"TRAIN_PATH_COLAB = \"/content/drive/MyDrive/Hackatons/X5_ner_2025/train.csv\"\nSUBMISSION_PATH_COLAB = \"/content/drive/MyDrive/Hackatons/X5_ner_2025/submission.csv\"\nTRAIN_PATH_KAGGLE = \"/kaggle/input/x5-ner-train/train.csv\"\nTRAIN_AUGMENTED = \"/kaggle/input/train-augmented-col-perc/train_with_augmented_volume_percent.csv\"","metadata":{"id":"g1wqNoXHKQ4p","trusted":true,"execution":{"iopub.status.busy":"2025-09-26T17:50:10.621294Z","iopub.execute_input":"2025-09-26T17:50:10.621506Z","iopub.status.idle":"2025-09-26T17:50:10.650181Z","shell.execute_reply.started":"2025-09-26T17:50:10.621490Z","shell.execute_reply":"2025-09-26T17:50:10.649693Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## IN","metadata":{}},{"cell_type":"markdown","source":"## Подготовка датасета","metadata":{"id":"_mL7LN0QnulJ"}},{"cell_type":"code","source":"train_df_raw = pd.read_csv(TRAIN_AUGMENTED, sep=';')\ntrain_df_raw","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"ScdZI8e5nmIi","outputId":"bb2bf9a7-c117-47db-c321-516f2b9e51a0","trusted":true,"execution":{"iopub.status.busy":"2025-09-26T17:50:14.614995Z","iopub.execute_input":"2025-09-26T17:50:14.615270Z","iopub.status.idle":"2025-09-26T17:50:14.765157Z","shell.execute_reply.started":"2025-09-26T17:50:14.615249Z","shell.execute_reply":"2025-09-26T17:50:14.764393Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                       sample  \\\n0                          aa   \n1                        aala   \n2                      aarcca   \n3                        abon   \n4                        abso   \n...                       ...   \n27547  ветчина 300 гр нарезка   \n27548   кукуруза 400 г mikado   \n27549         кукуруза 340 гр   \n27550     хлеб 350 г 5 злаков   \n27551       хлеб 500 гр злаки   \n\n                                              annotation  \n0                                          [(0, 2, 'O')]  \n1                                          [(0, 4, 'O')]  \n2                                          [(0, 6, 'O')]  \n3                                          [(0, 4, 'O')]  \n4                                    [(0, 4, 'B-BRAND')]  \n...                                                  ...  \n27547  [(0, 7, 'B-TYPE'), (8, 11, 'B-VOLUME'), (12, 1...  \n27548  [(0, 8, 'B-TYPE'), (9, 12, 'B-VOLUME'), (13, 1...  \n27549  [(0, 8, 'B-TYPE'), (9, 12, 'B-VOLUME'), (13, 1...  \n27550  [(0, 4, 'B-TYPE'), (5, 8, 'B-VOLUME'), (9, 10,...  \n27551  [(0, 4, 'B-TYPE'), (5, 8, 'B-VOLUME'), (9, 11,...  \n\n[27552 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sample</th>\n      <th>annotation</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>aa</td>\n      <td>[(0, 2, 'O')]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>aala</td>\n      <td>[(0, 4, 'O')]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>aarcca</td>\n      <td>[(0, 6, 'O')]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>abon</td>\n      <td>[(0, 4, 'O')]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>abso</td>\n      <td>[(0, 4, 'B-BRAND')]</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>27547</th>\n      <td>ветчина 300 гр нарезка</td>\n      <td>[(0, 7, 'B-TYPE'), (8, 11, 'B-VOLUME'), (12, 1...</td>\n    </tr>\n    <tr>\n      <th>27548</th>\n      <td>кукуруза 400 г mikado</td>\n      <td>[(0, 8, 'B-TYPE'), (9, 12, 'B-VOLUME'), (13, 1...</td>\n    </tr>\n    <tr>\n      <th>27549</th>\n      <td>кукуруза 340 гр</td>\n      <td>[(0, 8, 'B-TYPE'), (9, 12, 'B-VOLUME'), (13, 1...</td>\n    </tr>\n    <tr>\n      <th>27550</th>\n      <td>хлеб 350 г 5 злаков</td>\n      <td>[(0, 4, 'B-TYPE'), (5, 8, 'B-VOLUME'), (9, 10,...</td>\n    </tr>\n    <tr>\n      <th>27551</th>\n      <td>хлеб 500 гр злаки</td>\n      <td>[(0, 4, 'B-TYPE'), (5, 8, 'B-VOLUME'), (9, 11,...</td>\n    </tr>\n  </tbody>\n</table>\n<p>27552 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"train_df_raw['annotation'] = train_df_raw['annotation'].str.replace(\"\\'0\\'\", \"O\")","metadata":{"id":"nDQgR80T2J1s","trusted":true,"execution":{"iopub.status.busy":"2025-09-26T17:50:16.300645Z","iopub.execute_input":"2025-09-26T17:50:16.301469Z","iopub.status.idle":"2025-09-26T17:50:16.315417Z","shell.execute_reply.started":"2025-09-26T17:50:16.301432Z","shell.execute_reply":"2025-09-26T17:50:16.314651Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"train_df_raw['annotation'] = train_df_raw['annotation'].apply(lambda x: ast.literal_eval(str(x)))","metadata":{"id":"mIms8O__rRtw","trusted":true,"execution":{"iopub.status.busy":"2025-09-26T17:50:16.785204Z","iopub.execute_input":"2025-09-26T17:50:16.785511Z","iopub.status.idle":"2025-09-26T17:50:17.584014Z","shell.execute_reply.started":"2025-09-26T17:50:16.785480Z","shell.execute_reply":"2025-09-26T17:50:17.583256Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"all_words = []\nall_tags = []\nfor i, row in tqdm(train_df_raw.iterrows()):\n    words_sample = []\n    entities_sample = []\n    for ent in row['annotation']:\n        word = row['sample'][ent[0]:ent[1]]\n        words_sample.append(word)\n        entities_sample.append(ent[2])\n    all_words.append(words_sample)\n    all_tags.append(entities_sample)\n    # print(words_sample, entities_sample)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"38zKIXDInmC_","outputId":"86b3c56b-e254-4997-cddd-532395265fb4","trusted":true,"execution":{"iopub.status.busy":"2025-09-26T17:50:17.687837Z","iopub.execute_input":"2025-09-26T17:50:17.688244Z","iopub.status.idle":"2025-09-26T17:50:18.739259Z","shell.execute_reply.started":"2025-09-26T17:50:17.688203Z","shell.execute_reply":"2025-09-26T17:50:18.738683Z"}},"outputs":[{"name":"stderr","text":"27552it [00:01, 26380.00it/s]\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"assert len(all_words) == len(all_tags), 'different lengths'","metadata":{"id":"TkGRmYpSpzpZ","trusted":true,"execution":{"iopub.status.busy":"2025-09-26T17:50:18.740189Z","iopub.execute_input":"2025-09-26T17:50:18.740468Z","iopub.status.idle":"2025-09-26T17:50:18.743917Z","shell.execute_reply.started":"2025-09-26T17:50:18.740449Z","shell.execute_reply":"2025-09-26T17:50:18.743395Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"lbls_in_dataset = [\n 'O',\n 'B-BRAND',\n 'B-PERCENT',\n 'B-TYPE',\n 'B-VOLUME',\n 'I-BRAND',\n 'I-PERCENT',\n 'I-TYPE',\n 'I-VOLUME']\nlabel2id = {v:i for i, v in enumerate(lbls_in_dataset)}\nid2label = {i:v for i, v in enumerate(lbls_in_dataset)}","metadata":{"id":"RCNtZyfzsmbq","trusted":true,"execution":{"iopub.status.busy":"2025-09-26T17:50:19.789025Z","iopub.execute_input":"2025-09-26T17:50:19.789701Z","iopub.status.idle":"2025-09-26T17:50:19.793388Z","shell.execute_reply.started":"2025-09-26T17:50:19.789678Z","shell.execute_reply":"2025-09-26T17:50:19.792715Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"features=Features(\n    {\n        \"id\": Value(dtype='int32', id=None),\n        \"tokens\": Sequence(feature=Value(dtype='string', id=None)),\n        \"ner_tags\": Sequence(feature=ClassLabel(num_classes=len(lbls_in_dataset), names=list(lbls_in_dataset)), id=None)\n    }\n)","metadata":{"id":"N16RjsvYrPvW","trusted":true,"execution":{"iopub.status.busy":"2025-09-26T17:50:20.342206Z","iopub.execute_input":"2025-09-26T17:50:20.342414Z","iopub.status.idle":"2025-09-26T17:50:20.346174Z","shell.execute_reply.started":"2025-09-26T17:50:20.342399Z","shell.execute_reply":"2025-09-26T17:50:20.345382Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"ds = Dataset.from_dict({\"id\": list(range(len(all_words))),\n                        \"tokens\": all_words,\n                        \"ner_tags\": all_tags},\n                       features=features)","metadata":{"id":"NtF5L20Nt7oo","trusted":true,"execution":{"iopub.status.busy":"2025-09-26T17:50:22.350173Z","iopub.execute_input":"2025-09-26T17:50:22.350456Z","iopub.status.idle":"2025-09-26T17:50:22.612898Z","shell.execute_reply.started":"2025-09-26T17:50:22.350437Z","shell.execute_reply":"2025-09-26T17:50:22.612393Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"ds_splitted = ds.train_test_split(\n    test_size=0.25, shuffle=True, seed=42,\n    # stratify_by_column='ner_tags'\n)","metadata":{"id":"UvKyy7ZAs-Oa"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Counter([x for l in ds['ner_tags'] for x in l])","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XK4Panavxkxj","outputId":"63860067-3615-4cef-80e4-f11219157ed6","trusted":true,"execution":{"iopub.status.busy":"2025-09-26T10:24:01.849246Z","iopub.execute_input":"2025-09-26T10:24:01.849541Z","iopub.status.idle":"2025-09-26T10:24:01.925664Z","shell.execute_reply.started":"2025-09-26T10:24:01.849518Z","shell.execute_reply":"2025-09-26T10:24:01.925014Z"}},"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"Counter({0: 5407,\n         1: 7252,\n         3: 24845,\n         5: 490,\n         7: 4703,\n         2: 180,\n         4: 207,\n         8: 163,\n         6: 15})"},"metadata":{}}],"execution_count":43},{"cell_type":"code","source":"Counter([x for l in ds_splitted['test']['ner_tags'] for x in l])","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vQJREIqBvNx_","outputId":"a09d7e3f-3dc1-4def-8c48-934e119871b1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Counter({3: 6129, 1: 1774, 7: 1133, 0: 1358, 5: 122, 2: 9, 4: 18, 8: 8, 6: 2})"]},"metadata":{},"execution_count":14}],"execution_count":null},{"cell_type":"markdown","source":"## Загрузка модели","metadata":{"id":"BA0HcZtox2QD"}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained('cointegrated/rubert-tiny2',\n                                          use_fast=True,\n                                          add_prefix_space=True)\n\n# model = AutoModelForTokenClassification.from_pretrained('/kaggle/input/rut5_large_ft_colab_2509/pytorch/default/1/ruT5_large_250925',\n#                                                         num_labels=len(lbls_in_dataset),\n#                                                         id2label=id2label,\n#                                                         label2id=label2id).to(\"cuda\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":168,"referenced_widgets":["a5b0e7638d6d4c4ca0996d7b020e59d6","c9d7f068b3b1456b94729031bf250e94","3a24111dace7456a9e37fbb60d04c38b","35d94b59d14549b5b9d86553a348e420","7479e69dcd50435e884e334cc0cf38ee","174e223ef4a945579f9eb7f4ae0e93e4","d79c62114fd142029255e0db1d52ed9a","f96b1b42a7e44816852f51b36e39dd09","833566aa83484331b5c11dfdb7f21ee9","52e1f4410c4f4c3d8cc06c0157a03af7","74ccad71321d41cea860aaefec83502d","5beb6f270e544ad3b5f2b9edd2605a5b","fc0a360c3f2a4aaeb70b1885824fa0e1","f908bd7798154697a9ff049ae264d9fb","4d28a06aede948a294baaf885731bb09","64d1aa0086b84b87bc7a92e0c58eef1c","76a07ea51a1d4e8d80962661675ea36e","bafb54e447c648a5882908f01813707d","f74a6c9f08894d0d9d1aefa18dc2ac17","7de1fa7fcd614b418298905d6ec4f797","d75f429947524e71ab02959544ba3c61","854e9b2131e945f69fea27cbcea4c144","bd16b83691fd460e8cd07c0dd51a8aa2","6a3cb4e7e20d461db1e9a97415ed9e7c","b3bc82a5af874c27a57e117569aba3f0","1a2717f60ca9484abbafb785df3faa46","0ea90c0f92c4414c8f93a554a74cfab1","9287aa5abe4746128c2eeb1a4547a61a","a1cdb359996b4942ad1b416fbd747e97","c2efdb87c7844176991e0906f1f183dc","ae82b099a4f143e38d82feb76211b8fb","43fdcf9676544621b741aa7ad751f621","dc799160c3714d9087f92c71c5bf79e2"]},"id":"UC9EFn5p22gO","outputId":"38141ac6-91bd-4ddd-b88d-98b751e02579","trusted":true,"execution":{"iopub.status.busy":"2025-09-26T17:50:25.906064Z","iopub.execute_input":"2025-09-26T17:50:25.906326Z","iopub.status.idle":"2025-09-26T17:50:26.759461Z","shell.execute_reply.started":"2025-09-26T17:50:25.906307Z","shell.execute_reply":"2025-09-26T17:50:26.758665Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/401 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b091efd92c154f0d8f3d1d6675df9533"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10bc0dc4448f4654ade33f5d80bd2aab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9602dcca25d477783a4b755733b4a76"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a8e92ad68524fe9b60986fcf25d9872"}},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"s = '''\nсироп топинамбура\n'''\nr = tokenizer(s)\n[tokenizer.decode(x) for x in r.input_ids][:5]","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UBYSXH1hx6ZS","outputId":"278b6768-1b75-473b-a8ca-9ccf3f53967b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['сироп', 'то', 'пина', 'м', 'бур']"]},"metadata":{},"execution_count":17}],"execution_count":null},{"cell_type":"markdown","source":"## Токенизация и подготовка","metadata":{"id":"0ur88vOsyenk"}},{"cell_type":"code","source":"def tokenize_and_align_labels(examples, tokenizer):\n    tokenized_inputs = tokenizer(examples['tokens'], truncation=True, is_split_into_words=True)\n\n    labels = []\n    for i, label in enumerate(examples['ner_tags']):\n        word_ids = tokenized_inputs.word_ids(batch_index=i)\n        previous_index = None\n        label_ids = []\n        for word_idx in word_ids:\n            if word_idx is None:\n                label_ids.append(-100)\n            else:\n                label_ids.append(label[word_idx])\n        labels.append(label_ids)\n\n    tokenized_inputs['labels'] = labels\n    return tokenized_inputs","metadata":{"id":"pLXEC3IAyTQ0","trusted":true,"execution":{"iopub.status.busy":"2025-09-26T17:50:29.052866Z","iopub.execute_input":"2025-09-26T17:50:29.053136Z","iopub.status.idle":"2025-09-26T17:50:29.057903Z","shell.execute_reply.started":"2025-09-26T17:50:29.053114Z","shell.execute_reply":"2025-09-26T17:50:29.057327Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"tokenized_ds_train = ds_splitted['train'].map(tokenize_and_align_labels,\n                                              batched=True,\n                                              fn_kwargs={'tokenizer': tokenizer})","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["219043a57bd8424c8a3ccf0c1a54115c","50329a055e564b429c0c2c3e81f08272","7fd98312f8c94562b016b924a38efc80","31657f0e1130493d9faa5511f648fa53","4bc7b0ae0c4b4685bbcc941601390f78","d05042af89454f91ba8bdf52d85f11fd","57c2910ccdfb47eeb7c1645e82778573","f4ebf79726ab40eaa1ac7fc99e0a38ce","d1f3799d8ade4c3981ddce11e76a0fcd","0366d2f2ee7c4c4b9621c3afd5e75f23","4dbe44cb63504cc6b81778510201e2e3"]},"id":"_ARzDgYBy2Ih","outputId":"7b2d53b0-9805-4855-be50-23a146ab19d2"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/20438 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"219043a57bd8424c8a3ccf0c1a54115c"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"]}],"execution_count":null},{"cell_type":"code","source":"tokenized_ds_test = ds_splitted['test'].map(tokenize_and_align_labels,\n                                              batched=True,\n                                              fn_kwargs={'tokenizer': tokenizer})","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["d76a2f0f26d7457cb26bebde30ed5e0e","c117ffca9f6b4146b305b558f7939847","dc094b00dec74afb802ee7d6df12c1be","374bb6ddc4a64823b2cfdd327864a953","b32f39249a974519ba3b1fd956c53276","e1e6420c5c4b409980a2a951dad44dd8","87b888ce920d454dad4945fa26643fe8","259f9f4db413414a823a1a3f241a98a7","5e340cbd6da544828ce8609fb500945d","56093491341a4bf39bfe7b09fdd41e86","a25d768bb6cd45b18999ede839a8428c"]},"id":"sKYNr2E6y5oO","outputId":"2c4f7688-e86f-461d-8958-8456ae21ff40"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/6813 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d76a2f0f26d7457cb26bebde30ed5e0e"}},"metadata":{}}],"execution_count":null},{"cell_type":"markdown","source":"## Обучение","metadata":{"id":"9uwZDDAX0DbU"}},{"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nprint(f\"Using device: {device}\")\n\nprint(\"Compiling model for faster training...\")\ntorch.set_float32_matmul_precision('high')\nmodel.to(device)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NqxjKTvAzVlL","outputId":"5c6abf53-21d3-4984-ef8e-7a882c4d4656"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda:0\n","Compiling model for faster training...\n"]},{"output_type":"execute_result","data":{"text/plain":["T5ForTokenClassification(\n","  (transformer): T5EncoderModel(\n","    (shared): Embedding(32128, 1024)\n","    (encoder): T5Stack(\n","      (embed_tokens): Embedding(32128, 1024)\n","      (block): ModuleList(\n","        (0): T5Block(\n","          (layer): ModuleList(\n","            (0): T5LayerSelfAttention(\n","              (SelfAttention): T5Attention(\n","                (q): Linear(in_features=1024, out_features=1024, bias=False)\n","                (k): Linear(in_features=1024, out_features=1024, bias=False)\n","                (v): Linear(in_features=1024, out_features=1024, bias=False)\n","                (o): Linear(in_features=1024, out_features=1024, bias=False)\n","                (relative_attention_bias): Embedding(32, 16)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (1): T5LayerFF(\n","              (DenseReluDense): T5DenseActDense(\n","                (wi): Linear(in_features=1024, out_features=4096, bias=False)\n","                (wo): Linear(in_features=4096, out_features=1024, bias=False)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","                (act): ReLU()\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","        (1-23): 23 x T5Block(\n","          (layer): ModuleList(\n","            (0): T5LayerSelfAttention(\n","              (SelfAttention): T5Attention(\n","                (q): Linear(in_features=1024, out_features=1024, bias=False)\n","                (k): Linear(in_features=1024, out_features=1024, bias=False)\n","                (v): Linear(in_features=1024, out_features=1024, bias=False)\n","                (o): Linear(in_features=1024, out_features=1024, bias=False)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (1): T5LayerFF(\n","              (DenseReluDense): T5DenseActDense(\n","                (wi): Linear(in_features=1024, out_features=4096, bias=False)\n","                (wo): Linear(in_features=4096, out_features=1024, bias=False)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","                (act): ReLU()\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","      (final_layer_norm): T5LayerNorm()\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","  )\n","  (dropout): Dropout(p=0.0, inplace=False)\n","  (classifier): Linear(in_features=1024, out_features=9, bias=True)\n",")"]},"metadata":{},"execution_count":21}],"execution_count":null},{"cell_type":"code","source":"# функция метрики\ndef compute_metrics_custom(p):\n    labels_list = list(id2label.values())\n    predictions, labels = p\n    if not os.path.exists('/content/test/p_trainer.pkl'):\n        os.makedirs('/content/test', exist_ok=True)\n        with open('/content/test/p_trainer.pkl', 'wb') as f:\n            pickle.dump(p, f)\n    predictions = np.argmax(predictions, axis=2)\n\n\n    true_predictions = [\n        [labels_list[p] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    true_labels = [\n        [labels_list[l] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n\n    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n    report_dict = classification_report(true_labels, true_predictions, digits=4, output_dict=True)\n    report = classification_report(true_labels, true_predictions, digits=4)\n    macro_f1 = report_dict[\"macro avg\"][\"f1-score\"]\n    print(\"=== seqeval classification_report ===\")\n    print(report)\n    CLASS_REPORT_PATH = '/content/logs/last_classification_report.txt'\n    try:\n        with open(CLASS_REPORT_PATH, \"w\", encoding=\"utf-8\") as f:\n            f.write(report)\n    except Exception as e:\n        print(f\"Warning: failed to write classification report to {CLASS_REPORT_PATH}: {e}\")\n\n    return {\n        \"f1_macro\": macro_f1,\n        \"precision\": results[\"overall_precision\"],\n        \"recall\": results[\"overall_recall\"],\n        \"f1\": results[\"overall_f1\"],\n        \"accuracy\": results[\"overall_accuracy\"],\n    }","metadata":{"id":"vYjFZW220FJS"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_collator = DataCollatorForTokenClassification(tokenizer)","metadata":{"id":"MMcW2Zs4EvvD"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import optuna.logging\noptuna.logging.set_verbosity(optuna.logging.INFO)\n\nbest_f1 = 0\n\n# GDRIVE_DIR = '/content/drive/MyDrive/Hackatons/X5_ner_2025/ruT5_large_250925'\n\ndef printer(s):\n    print('*'*150, end='\\n\\n')\n    print(s, end='\\n\\n')\n    print('*'*150, end='\\n\\n')","metadata":{"id":"0cdPNPNR3MqG"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"early_stop_cb = EarlyStoppingCallback(early_stopping_patience=2)","metadata":{"id":"mP5VlP__32Oi"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def objective(trial: optuna.Trial, model=model):\n    global best_f1\n    model = model\n    model_name = 'ruT5_large_250925_optuna_v1'\n    trial_check_dir = f\"./checkpoints_trial\"\n\n    os.makedirs(trial_check_dir, exist_ok=True)\n    args = TrainingArguments(\n        # f\"{model_name}-finetuned-ner\",\n        output_dir=trial_check_dir,\n        overwrite_output_dir=True,\n        eval_strategy = \"epoch\",\n        torch_compile=True,\n        # 64\\\n        per_device_train_batch_size=256,\n        per_device_eval_batch_size=256,\n        learning_rate=trial.suggest_loguniform('learning_rate', low=1e-5, high=5e-4),\n        weight_decay=trial.suggest_loguniform('weight_decay', 1e-4, 0.05),\n        num_train_epochs=trial.suggest_int('num_train_epochs', low = 3, high = 10),\n        seed=42,\n        data_seed=24,\n        gradient_accumulation_steps=2,\n        warmup_ratio=0.1,\n        report_to=None,\n        logging_dir=\"./logs\",\n        logging_steps=1,\n        load_best_model_at_end=True,\n        metric_for_best_model=\"eval_f1_macro\",\n        # greater_is_better=False,\n        save_total_limit=1,\n        save_strategy=\"epoch\",  # Changed to match evaluation_strategy\n    )\n\n    # early_stopping = EarlyStoppingCallback(\n    #     early_stopping_patience=1,  # Stop if F1 decreases for 1 consecutive epoch\n    #     early_stopping_threshold=0.001\n    # )\n\n    trainer = Trainer(\n        model,\n        args,\n        train_dataset=tokenized_ds_train,\n        eval_dataset=tokenized_ds_test,\n        # train_dataset=small_dataset_train,\n        # eval_dataset=small_dataset_test,\n        data_collator=data_collator,\n        compute_metrics=compute_metrics_custom,\n        tokenizer=tokenizer,\n        callbacks=[early_stop_cb],\n    )\n\n    trainer.train()\n\n    # Evaluate and save best model globally\n    eval_metrics = trainer.evaluate()\n    current_f1 = eval_metrics[\"eval_f1_macro\"]\n\n    if current_f1 > best_f1:\n        best_f1 = current_f1\n        trainer.save_model(\"./best_model\")\n        trainer.save_model(GDRIVE_DIR)\n        printer(f\"New best model saved with F1: {best_f1:.4f}\")\n    return current_f1","metadata":{"id":"o-sZkAfk3OMb"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"study = optuna.create_study(study_name='test_optuna', direction='maximize')\nstudy.optimize(func=objective, n_trials=5)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"gM3DK3Sj3XQA","outputId":"daddce65-aa09-4fb5-b986-9dd1e03b5745"},"outputs":[{"output_type":"stream","name":"stderr","text":["[I 2025-09-25 15:47:56,064] A new study created in memory with name: test_optuna\n","/tmp/ipython-input-992928850.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  learning_rate=trial.suggest_loguniform('learning_rate', low=1e-5, high=5e-4),\n","/tmp/ipython-input-992928850.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  weight_decay=trial.suggest_loguniform('weight_decay', 1e-4, 0.05),\n","The speedups for torchdynamo mostly come with GPU Ampere or higher and which is not detected here.\n","Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","/tmp/ipython-input-992928850.py:39: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n","W0925 15:48:54.194000 16270 torch/_inductor/utils.py:1436] [0/0] Not enough SMs to use max_autotune_gemm mode\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='360' max='360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [360/360 29:08, Epoch 9/9]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>F1 Macro</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.742300</td>\n","      <td>0.537095</td>\n","      <td>0.400311</td>\n","      <td>0.803884</td>\n","      <td>0.892531</td>\n","      <td>0.845891</td>\n","      <td>0.843418</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.362400</td>\n","      <td>0.358328</td>\n","      <td>0.450653</td>\n","      <td>0.905844</td>\n","      <td>0.943039</td>\n","      <td>0.924067</td>\n","      <td>0.915001</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.158600</td>\n","      <td>0.302577</td>\n","      <td>0.579675</td>\n","      <td>0.930021</td>\n","      <td>0.954029</td>\n","      <td>0.941872</td>\n","      <td>0.930194</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.158400</td>\n","      <td>0.325737</td>\n","      <td>0.720133</td>\n","      <td>0.916179</td>\n","      <td>0.951784</td>\n","      <td>0.933642</td>\n","      <td>0.924071</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.033900</td>\n","      <td>0.308182</td>\n","      <td>0.823673</td>\n","      <td>0.938384</td>\n","      <td>0.958659</td>\n","      <td>0.948413</td>\n","      <td>0.937526</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.041600</td>\n","      <td>0.379980</td>\n","      <td>0.770415</td>\n","      <td>0.939387</td>\n","      <td>0.954543</td>\n","      <td>0.946905</td>\n","      <td>0.936732</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.036000</td>\n","      <td>0.425532</td>\n","      <td>0.874428</td>\n","      <td>0.938848</td>\n","      <td>0.960670</td>\n","      <td>0.949634</td>\n","      <td>0.938206</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>0.014800</td>\n","      <td>0.478993</td>\n","      <td>0.876659</td>\n","      <td>0.943126</td>\n","      <td>0.959313</td>\n","      <td>0.951151</td>\n","      <td>0.938773</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>0.004200</td>\n","      <td>0.514241</td>\n","      <td>0.876177</td>\n","      <td>0.942816</td>\n","      <td>0.958425</td>\n","      <td>0.950557</td>\n","      <td>0.938093</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["=== seqeval classification_report ===\n","              precision    recall  f1-score   support\n","\n","       BRAND     0.6379    0.7954    0.7080      5043\n","     PERCENT     0.0000    0.0000    0.0000        14\n","        TYPE     0.8637    0.9250    0.8933     16297\n","      VOLUME     0.0000    0.0000    0.0000        29\n","\n","   micro avg     0.8039    0.8925    0.8459     21383\n","   macro avg     0.3754    0.4301    0.4003     21383\n","weighted avg     0.8087    0.8925    0.8478     21383\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["=== seqeval classification_report ===\n","              precision    recall  f1-score   support\n","\n","       BRAND     0.8601    0.8547    0.8574      5043\n","     PERCENT     0.0000    0.0000    0.0000        14\n","        TYPE     0.9191    0.9729    0.9452     16297\n","      VOLUME     0.0000    0.0000    0.0000        29\n","\n","   micro avg     0.9058    0.9430    0.9241     21383\n","   macro avg     0.4448    0.4569    0.4507     21383\n","weighted avg     0.9034    0.9430    0.9226     21383\n","\n","=== seqeval classification_report ===\n","              precision    recall  f1-score   support\n","\n","       BRAND     0.8873    0.9011    0.8941      5043\n","     PERCENT     1.0000    0.0714    0.1333        14\n","        TYPE     0.9439    0.9723    0.9579     16297\n","      VOLUME     0.3600    0.3103    0.3333        29\n","\n","   micro avg     0.9300    0.9540    0.9419     21383\n","   macro avg     0.7978    0.5638    0.5797     21383\n","weighted avg     0.9298    0.9540    0.9415     21383\n","\n","=== seqeval classification_report ===\n","              precision    recall  f1-score   support\n","\n","       BRAND     0.8240    0.9365    0.8767      5043\n","     PERCENT     0.8000    0.2857    0.4211        14\n","        TYPE     0.9487    0.9577    0.9532     16297\n","      VOLUME     0.6800    0.5862    0.6296        29\n","\n","   micro avg     0.9162    0.9518    0.9336     21383\n","   macro avg     0.8132    0.6915    0.7201     21383\n","weighted avg     0.9188    0.9518    0.9344     21383\n","\n","=== seqeval classification_report ===\n","              precision    recall  f1-score   support\n","\n","       BRAND     0.9110    0.9009    0.9059      5043\n","     PERCENT     0.7500    0.6429    0.6923        14\n","        TYPE     0.9467    0.9774    0.9618     16297\n","      VOLUME     0.9000    0.6207    0.7347        29\n","\n","   micro avg     0.9384    0.9587    0.9484     21383\n","   macro avg     0.8769    0.7855    0.8237     21383\n","weighted avg     0.9381    0.9587    0.9481     21383\n","\n","=== seqeval classification_report ===\n","              precision    recall  f1-score   support\n","\n","       BRAND     0.8843    0.9231    0.9033      5043\n","     PERCENT     0.7273    0.5714    0.6400        14\n","        TYPE     0.9576    0.9654    0.9615     16297\n","      VOLUME     0.6522    0.5172    0.5769        29\n","\n","   micro avg     0.9394    0.9545    0.9469     21383\n","   macro avg     0.8053    0.7443    0.7704     21383\n","weighted avg     0.9397    0.9545    0.9470     21383\n","\n","=== seqeval classification_report ===\n","              precision    recall  f1-score   support\n","\n","       BRAND     0.9047    0.9110    0.9078      5043\n","     PERCENT     0.9000    0.6429    0.7500        14\n","        TYPE     0.9493    0.9765    0.9627     16297\n","      VOLUME     0.8929    0.8621    0.8772        29\n","\n","   micro avg     0.9388    0.9607    0.9496     21383\n","   macro avg     0.9117    0.8481    0.8744     21383\n","weighted avg     0.9387    0.9607    0.9495     21383\n","\n","=== seqeval classification_report ===\n","              precision    recall  f1-score   support\n","\n","       BRAND     0.9096    0.9139    0.9118      5043\n","     PERCENT     0.9000    0.6429    0.7500        14\n","        TYPE     0.9535    0.9737    0.9635     16297\n","      VOLUME     0.8667    0.8966    0.8814        29\n","\n","   micro avg     0.9431    0.9593    0.9512     21383\n","   macro avg     0.9074    0.8568    0.8767     21383\n","weighted avg     0.9430    0.9593    0.9511     21383\n","\n","=== seqeval classification_report ===\n","              precision    recall  f1-score   support\n","\n","       BRAND     0.9070    0.9131    0.9101      5043\n","     PERCENT     0.9000    0.6429    0.7500        14\n","        TYPE     0.9539    0.9728    0.9633     16297\n","      VOLUME     0.8667    0.8966    0.8814        29\n","\n","   micro avg     0.9428    0.9584    0.9506     21383\n","   macro avg     0.9069    0.8563    0.8762     21383\n","weighted avg     0.9427    0.9584    0.9505     21383\n","\n"]},{"output_type":"stream","name":"stderr","text":["There were missing keys in the checkpoint model loaded: ['transformer.encoder.embed_tokens.weight'].\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='27' max='27' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [27/27 00:13]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["=== seqeval classification_report ===\n","              precision    recall  f1-score   support\n","\n","       BRAND     0.9096    0.9139    0.9118      5043\n","     PERCENT     0.9000    0.6429    0.7500        14\n","        TYPE     0.9535    0.9737    0.9635     16297\n","      VOLUME     0.8667    0.8966    0.8814        29\n","\n","   micro avg     0.9431    0.9593    0.9512     21383\n","   macro avg     0.9074    0.8568    0.8767     21383\n","weighted avg     0.9430    0.9593    0.9511     21383\n","\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-09-25 16:21:06,317] Trial 0 finished with value: 0.8766589668718043 and parameters: {'learning_rate': 0.0004683284028374025, 'weight_decay': 0.0020733902422178014, 'num_train_epochs': 9}. Best is trial 0 with value: 0.8766589668718043.\n","/tmp/ipython-input-992928850.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  learning_rate=trial.suggest_loguniform('learning_rate', low=1e-5, high=5e-4),\n","/tmp/ipython-input-992928850.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  weight_decay=trial.suggest_loguniform('weight_decay', 1e-4, 0.05),\n","The speedups for torchdynamo mostly come with GPU Ampere or higher and which is not detected here.\n","Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"]},{"output_type":"stream","name":"stdout","text":["******************************************************************************************************************************************************\n","\n","New best model saved with F1: 0.8767\n","\n","******************************************************************************************************************************************************\n","\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-992928850.py:39: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='280' max='280' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [280/280 20:31, Epoch 7/7]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>F1 Macro</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.002900</td>\n","      <td>0.546093</td>\n","      <td>0.872007</td>\n","      <td>0.943423</td>\n","      <td>0.959968</td>\n","      <td>0.951624</td>\n","      <td>0.939189</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.000900</td>\n","      <td>0.623692</td>\n","      <td>0.854504</td>\n","      <td>0.941767</td>\n","      <td>0.960529</td>\n","      <td>0.951056</td>\n","      <td>0.939076</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.000100</td>\n","      <td>0.644883</td>\n","      <td>0.872071</td>\n","      <td>0.942940</td>\n","      <td>0.960623</td>\n","      <td>0.951699</td>\n","      <td>0.939453</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.001000</td>\n","      <td>0.610403</td>\n","      <td>0.871778</td>\n","      <td>0.944058</td>\n","      <td>0.958893</td>\n","      <td>0.951418</td>\n","      <td>0.939302</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.000200</td>\n","      <td>0.615621</td>\n","      <td>0.872327</td>\n","      <td>0.943050</td>\n","      <td>0.961044</td>\n","      <td>0.951962</td>\n","      <td>0.940134</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.000200</td>\n","      <td>0.625413</td>\n","      <td>0.872305</td>\n","      <td>0.942818</td>\n","      <td>0.960763</td>\n","      <td>0.951706</td>\n","      <td>0.939869</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.000100</td>\n","      <td>0.628055</td>\n","      <td>0.872243</td>\n","      <td>0.942769</td>\n","      <td>0.960670</td>\n","      <td>0.951635</td>\n","      <td>0.939907</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["=== seqeval classification_report ===\n","              precision    recall  f1-score   support\n","\n","       BRAND     0.9143    0.9096    0.9119      5043\n","     PERCENT     0.9000    0.6429    0.7500        14\n","        TYPE     0.9523    0.9760    0.9640     16297\n","      VOLUME     0.8621    0.8621    0.8621        29\n","\n","   micro avg     0.9434    0.9600    0.9516     21383\n","   macro avg     0.9072    0.8476    0.8720     21383\n","weighted avg     0.9432    0.9600    0.9515     21383\n","\n","=== seqeval classification_report ===\n","              precision    recall  f1-score   support\n","\n","       BRAND     0.9089    0.9137    0.9113      5043\n","     PERCENT     0.8889    0.5714    0.6957        14\n","        TYPE     0.9520    0.9755    0.9636     16297\n","      VOLUME     0.8333    0.8621    0.8475        29\n","\n","   micro avg     0.9418    0.9605    0.9511     21383\n","   macro avg     0.8958    0.8307    0.8545     21383\n","weighted avg     0.9416    0.9605    0.9509     21383\n","\n","=== seqeval classification_report ===\n","              precision    recall  f1-score   support\n","\n","       BRAND     0.9100    0.9141    0.9121      5043\n","     PERCENT     0.9000    0.6429    0.7500        14\n","        TYPE     0.9531    0.9755    0.9642     16297\n","      VOLUME     0.8621    0.8621    0.8621        29\n","\n","   micro avg     0.9429    0.9606    0.9517     21383\n","   macro avg     0.9063    0.8486    0.8721     21383\n","weighted avg     0.9428    0.9606    0.9516     21383\n","\n","=== seqeval classification_report ===\n","              precision    recall  f1-score   support\n","\n","       BRAND     0.9113    0.9106    0.9109      5043\n","     PERCENT     0.9000    0.6429    0.7500        14\n","        TYPE     0.9541    0.9743    0.9641     16297\n","      VOLUME     0.8621    0.8621    0.8621        29\n","\n","   micro avg     0.9441    0.9589    0.9514     21383\n","   macro avg     0.9069    0.8474    0.8718     21383\n","weighted avg     0.9439    0.9589    0.9513     21383\n","\n","=== seqeval classification_report ===\n","              precision    recall  f1-score   support\n","\n","       BRAND     0.9085    0.9175    0.9130      5043\n","     PERCENT     0.9000    0.6429    0.7500        14\n","        TYPE     0.9538    0.9750    0.9643     16297\n","      VOLUME     0.8621    0.8621    0.8621        29\n","\n","   micro avg     0.9430    0.9610    0.9520     21383\n","   macro avg     0.9061    0.8494    0.8723     21383\n","weighted avg     0.9429    0.9610    0.9519     21383\n","\n","=== seqeval classification_report ===\n","              precision    recall  f1-score   support\n","\n","       BRAND     0.9109    0.9159    0.9134      5043\n","     PERCENT     0.9000    0.6429    0.7500        14\n","        TYPE     0.9527    0.9751    0.9638     16297\n","      VOLUME     0.8621    0.8621    0.8621        29\n","\n","   micro avg     0.9428    0.9608    0.9517     21383\n","   macro avg     0.9064    0.8490    0.8723     21383\n","weighted avg     0.9427    0.9608    0.9516     21383\n","\n","=== seqeval classification_report ===\n","              precision    recall  f1-score   support\n","\n","       BRAND     0.9100    0.9163    0.9132      5043\n","     PERCENT     0.9000    0.6429    0.7500        14\n","        TYPE     0.9529    0.9748    0.9638     16297\n","      VOLUME     0.8621    0.8621    0.8621        29\n","\n","   micro avg     0.9428    0.9607    0.9516     21383\n","   macro avg     0.9062    0.8490    0.8722     21383\n","weighted avg     0.9426    0.9607    0.9515     21383\n","\n"]},{"output_type":"stream","name":"stderr","text":["There were missing keys in the checkpoint model loaded: ['transformer.encoder.embed_tokens.weight'].\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='27' max='27' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [27/27 00:14]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2025-09-25 16:41:59,096] Trial 1 finished with value: 0.8723269404322582 and parameters: {'learning_rate': 1.6956919779303677e-05, 'weight_decay': 0.0004587134033578368, 'num_train_epochs': 7}. Best is trial 0 with value: 0.8766589668718043.\n","/tmp/ipython-input-992928850.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  learning_rate=trial.suggest_loguniform('learning_rate', low=1e-5, high=5e-4),\n","/tmp/ipython-input-992928850.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  weight_decay=trial.suggest_loguniform('weight_decay', 1e-4, 0.05),\n","The speedups for torchdynamo mostly come with GPU Ampere or higher and which is not detected here.\n","Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"]},{"output_type":"stream","name":"stdout","text":["=== seqeval classification_report ===\n","              precision    recall  f1-score   support\n","\n","       BRAND     0.9085    0.9175    0.9130      5043\n","     PERCENT     0.9000    0.6429    0.7500        14\n","        TYPE     0.9538    0.9750    0.9643     16297\n","      VOLUME     0.8621    0.8621    0.8621        29\n","\n","   micro avg     0.9430    0.9610    0.9520     21383\n","   macro avg     0.9061    0.8494    0.8723     21383\n","weighted avg     0.9429    0.9610    0.9519     21383\n","\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-992928850.py:39: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='200' max='360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [200/360 14:00 < 11:19, 0.24 it/s, Epoch 5/9]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>F1 Macro</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.000000</td>\n","      <td>1.073410</td>\n","      <td>0.871447</td>\n","      <td>0.943406</td>\n","      <td>0.958098</td>\n","      <td>0.950695</td>\n","      <td>0.938017</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.001500</td>\n","      <td>0.843991</td>\n","      <td>0.870619</td>\n","      <td>0.942651</td>\n","      <td>0.957022</td>\n","      <td>0.949782</td>\n","      <td>0.937866</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.000200</td>\n","      <td>1.071172</td>\n","      <td>0.892632</td>\n","      <td>0.940048</td>\n","      <td>0.962821</td>\n","      <td>0.951298</td>\n","      <td>0.939151</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.000000</td>\n","      <td>1.034390</td>\n","      <td>0.883332</td>\n","      <td>0.941954</td>\n","      <td>0.960015</td>\n","      <td>0.950899</td>\n","      <td>0.939000</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.000600</td>\n","      <td>0.740452</td>\n","      <td>0.887223</td>\n","      <td>0.941660</td>\n","      <td>0.959407</td>\n","      <td>0.950451</td>\n","      <td>0.939265</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["=== seqeval classification_report ===\n","              precision    recall  f1-score   support\n","\n","       BRAND     0.9087    0.9120    0.9103      5043\n","     PERCENT     0.9000    0.6429    0.7500        14\n","        TYPE     0.9541    0.9728    0.9634     16297\n","      VOLUME     0.8621    0.8621    0.8621        29\n","\n","   micro avg     0.9434    0.9581    0.9507     21383\n","   macro avg     0.9062    0.8474    0.8714     21383\n","weighted avg     0.9433    0.9581    0.9506     21383\n","\n","=== seqeval classification_report ===\n","              precision    recall  f1-score   support\n","\n","       BRAND     0.9020    0.9177    0.9098      5043\n","     PERCENT     1.0000    0.6429    0.7826        14\n","        TYPE     0.9554    0.9697    0.9625     16297\n","      VOLUME     0.8276    0.8276    0.8276        29\n","\n","   micro avg     0.9427    0.9570    0.9498     21383\n","   macro avg     0.9212    0.8395    0.8706     21383\n","weighted avg     0.9427    0.9570    0.9498     21383\n","\n","=== seqeval classification_report ===\n","              precision    recall  f1-score   support\n","\n","       BRAND     0.9099    0.9129    0.9114      5043\n","     PERCENT     1.0000    0.7143    0.8333        14\n","        TYPE     0.9492    0.9786    0.9637     16297\n","      VOLUME     0.8621    0.8621    0.8621        29\n","\n","   micro avg     0.9400    0.9628    0.9513     21383\n","   macro avg     0.9303    0.8670    0.8926     21383\n","weighted avg     0.9399    0.9628    0.9512     21383\n","\n","=== seqeval classification_report ===\n","              precision    recall  f1-score   support\n","\n","       BRAND     0.9045    0.9149    0.9097      5043\n","     PERCENT     1.0000    0.6429    0.7826        14\n","        TYPE     0.9535    0.9744    0.9638     16297\n","      VOLUME     0.8929    0.8621    0.8772        29\n","\n","   micro avg     0.9420    0.9600    0.9509     21383\n","   macro avg     0.9377    0.8486    0.8833     21383\n","weighted avg     0.9419    0.9600    0.9508     21383\n","\n","=== seqeval classification_report ===\n","              precision    recall  f1-score   support\n","\n","       BRAND     0.8981    0.9227    0.9102      5043\n","     PERCENT     1.0000    0.7143    0.8333        14\n","        TYPE     0.9554    0.9712    0.9632     16297\n","      VOLUME     0.8571    0.8276    0.8421        29\n","\n","   micro avg     0.9417    0.9594    0.9505     21383\n","   macro avg     0.9277    0.8589    0.8872     21383\n","weighted avg     0.9418    0.9594    0.9505     21383\n","\n"]},{"output_type":"stream","name":"stderr","text":["There were missing keys in the checkpoint model loaded: ['transformer.encoder.embed_tokens.weight'].\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='27' max='27' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [27/27 00:14]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["=== seqeval classification_report ===\n","              precision    recall  f1-score   support\n","\n","       BRAND     0.9099    0.9129    0.9114      5043\n","     PERCENT     1.0000    0.7143    0.8333        14\n","        TYPE     0.9492    0.9786    0.9637     16297\n","      VOLUME     0.8621    0.8621    0.8621        29\n","\n","   micro avg     0.9400    0.9628    0.9513     21383\n","   macro avg     0.9303    0.8670    0.8926     21383\n","weighted avg     0.9399    0.9628    0.9512     21383\n","\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-09-25 16:56:43,952] Trial 2 finished with value: 0.8926324166140165 and parameters: {'learning_rate': 6.44944805049113e-05, 'weight_decay': 0.004066034568680406, 'num_train_epochs': 9}. Best is trial 2 with value: 0.8926324166140165.\n","/tmp/ipython-input-992928850.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  learning_rate=trial.suggest_loguniform('learning_rate', low=1e-5, high=5e-4),\n","/tmp/ipython-input-992928850.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  weight_decay=trial.suggest_loguniform('weight_decay', 1e-4, 0.05),\n","The speedups for torchdynamo mostly come with GPU Ampere or higher and which is not detected here.\n","Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"]},{"output_type":"stream","name":"stdout","text":["******************************************************************************************************************************************************\n","\n","New best model saved with F1: 0.8926\n","\n","******************************************************************************************************************************************************\n","\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-992928850.py:39: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='120' max='360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [120/360 08:45 < 17:48, 0.22 it/s, Epoch 3/9]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>F1 Macro</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.137000</td>\n","      <td>0.558386</td>\n","      <td>0.858002</td>\n","      <td>0.936880</td>\n","      <td>0.950288</td>\n","      <td>0.943536</td>\n","      <td>0.931932</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.031900</td>\n","      <td>0.395097</td>\n","      <td>0.795494</td>\n","      <td>0.938484</td>\n","      <td>0.951036</td>\n","      <td>0.944718</td>\n","      <td>0.931970</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.008900</td>\n","      <td>0.435523</td>\n","      <td>0.804443</td>\n","      <td>0.941481</td>\n","      <td>0.952532</td>\n","      <td>0.946974</td>\n","      <td>0.934805</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["=== seqeval classification_report ===\n","              precision    recall  f1-score   support\n","\n","       BRAND     0.8983    0.8937    0.8960      5043\n","     PERCENT     0.9000    0.6429    0.7500        14\n","        TYPE     0.9487    0.9683    0.9584     16297\n","      VOLUME     0.8276    0.8276    0.8276        29\n","\n","   micro avg     0.9369    0.9503    0.9435     21383\n","   macro avg     0.8937    0.8331    0.8580     21383\n","weighted avg     0.9366    0.9503    0.9434     21383\n","\n","=== seqeval classification_report ===\n","              precision    recall  f1-score   support\n","\n","       BRAND     0.8909    0.9100    0.9003      5043\n","     PERCENT     0.8333    0.7143    0.7692        14\n","        TYPE     0.9537    0.9648    0.9592     16297\n","      VOLUME     0.7222    0.4483    0.5532        29\n","\n","   micro avg     0.9385    0.9510    0.9447     21383\n","   macro avg     0.8500    0.7593    0.7955     21383\n","weighted avg     0.9385    0.9510    0.9447     21383\n","\n","=== seqeval classification_report ===\n","              precision    recall  f1-score   support\n","\n","       BRAND     0.9095    0.8949    0.9021      5043\n","     PERCENT     0.7333    0.7857    0.7586        14\n","        TYPE     0.9514    0.9713    0.9613     16297\n","      VOLUME     0.7778    0.4828    0.5957        29\n","\n","   micro avg     0.9415    0.9525    0.9470     21383\n","   macro avg     0.8430    0.7837    0.8044     21383\n","weighted avg     0.9411    0.9525    0.9467     21383\n","\n"]},{"output_type":"stream","name":"stderr","text":["There were missing keys in the checkpoint model loaded: ['transformer.encoder.embed_tokens.weight'].\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='27' max='27' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [27/27 00:14]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2025-09-25 17:05:50,476] Trial 3 finished with value: 0.8580016656503588 and parameters: {'learning_rate': 0.00048392903640124505, 'weight_decay': 0.0038169848188184135, 'num_train_epochs': 9}. Best is trial 2 with value: 0.8926324166140165.\n","/tmp/ipython-input-992928850.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  learning_rate=trial.suggest_loguniform('learning_rate', low=1e-5, high=5e-4),\n","/tmp/ipython-input-992928850.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  weight_decay=trial.suggest_loguniform('weight_decay', 1e-4, 0.05),\n","The speedups for torchdynamo mostly come with GPU Ampere or higher and which is not detected here.\n","Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"]},{"output_type":"stream","name":"stdout","text":["=== seqeval classification_report ===\n","              precision    recall  f1-score   support\n","\n","       BRAND     0.8983    0.8937    0.8960      5043\n","     PERCENT     0.9000    0.6429    0.7500        14\n","        TYPE     0.9487    0.9683    0.9584     16297\n","      VOLUME     0.8276    0.8276    0.8276        29\n","\n","   micro avg     0.9369    0.9503    0.9435     21383\n","   macro avg     0.8937    0.8331    0.8580     21383\n","weighted avg     0.9366    0.9503    0.9434     21383\n","\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-992928850.py:39: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [120/120 08:29, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>F1 Macro</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.009600</td>\n","      <td>0.497267</td>\n","      <td>0.858961</td>\n","      <td>0.939611</td>\n","      <td>0.957583</td>\n","      <td>0.948512</td>\n","      <td>0.936846</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.001700</td>\n","      <td>0.623256</td>\n","      <td>0.883508</td>\n","      <td>0.939863</td>\n","      <td>0.956741</td>\n","      <td>0.948227</td>\n","      <td>0.936392</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.000200</td>\n","      <td>0.633704</td>\n","      <td>0.879540</td>\n","      <td>0.940540</td>\n","      <td>0.958706</td>\n","      <td>0.949536</td>\n","      <td>0.937866</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["=== seqeval classification_report ===\n","              precision    recall  f1-score   support\n","\n","       BRAND     0.9097    0.9014    0.9056      5043\n","     PERCENT     1.0000    0.6429    0.7826        14\n","        TYPE     0.9487    0.9756    0.9619     16297\n","      VOLUME     0.8148    0.7586    0.7857        29\n","\n","   micro avg     0.9396    0.9576    0.9485     21383\n","   macro avg     0.9183    0.8196    0.8590     21383\n","weighted avg     0.9394    0.9576    0.9483     21383\n","\n","=== seqeval classification_report ===\n","              precision    recall  f1-score   support\n","\n","       BRAND     0.9054    0.9032    0.9043      5043\n","     PERCENT     1.0000    0.6429    0.7826        14\n","        TYPE     0.9504    0.9736    0.9619     16297\n","      VOLUME     0.8438    0.9310    0.8852        29\n","\n","   micro avg     0.9399    0.9567    0.9482     21383\n","   macro avg     0.9249    0.8627    0.8835     21383\n","weighted avg     0.9397    0.9567    0.9481     21383\n","\n","=== seqeval classification_report ===\n","              precision    recall  f1-score   support\n","\n","       BRAND     0.9074    0.9040    0.9057      5043\n","     PERCENT     1.0000    0.6429    0.7826        14\n","        TYPE     0.9506    0.9760    0.9632     16297\n","      VOLUME     0.8387    0.8966    0.8667        29\n","\n","   micro avg     0.9405    0.9587    0.9495     21383\n","   macro avg     0.9242    0.8549    0.8795     21383\n","weighted avg     0.9403    0.9587    0.9494     21383\n","\n"]},{"output_type":"stream","name":"stderr","text":["There were missing keys in the checkpoint model loaded: ['transformer.encoder.embed_tokens.weight'].\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='27' max='27' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [27/27 00:14]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2025-09-25 17:14:40,644] Trial 4 finished with value: 0.8835080653340189 and parameters: {'learning_rate': 0.00012973534241957156, 'weight_decay': 0.0006306410098490446, 'num_train_epochs': 3}. Best is trial 2 with value: 0.8926324166140165.\n"]},{"output_type":"stream","name":"stdout","text":["=== seqeval classification_report ===\n","              precision    recall  f1-score   support\n","\n","       BRAND     0.9054    0.9032    0.9043      5043\n","     PERCENT     1.0000    0.6429    0.7826        14\n","        TYPE     0.9504    0.9736    0.9619     16297\n","      VOLUME     0.8438    0.9310    0.8852        29\n","\n","   micro avg     0.9399    0.9567    0.9482     21383\n","   macro avg     0.9249    0.8627    0.8835     21383\n","weighted avg     0.9397    0.9567    0.9481     21383\n","\n"]}],"execution_count":null},{"cell_type":"code","source":"best_trial = study.best_trial\nprinter(f\"Final best F1: {best_trial.value}\")\nprinter(f\"Final params: {best_trial.params}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zFMzaJHRFzka","outputId":"cb94c27c-0ccf-4eb1-e259-e3599810e9e6"},"outputs":[{"output_type":"stream","name":"stdout","text":["******************************************************************************************************************************************************\n","\n","Final best F1: 0.8926324166140165\n","\n","******************************************************************************************************************************************************\n","\n","******************************************************************************************************************************************************\n","\n","Final params: {'learning_rate': 6.44944805049113e-05, 'weight_decay': 0.004066034568680406, 'num_train_epochs': 9}\n","\n","******************************************************************************************************************************************************\n","\n"]}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"2zVG89pKFzhz"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## тест","metadata":{"id":"5Zr5wuGcCKj-"}},{"cell_type":"code","source":"df_test = pd.read_csv('submission.csv', sep=';', usecols=['sample'])","metadata":{"id":"7vMnAC-kGaWC"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_model_path = '/content/drive/MyDrive/Hackatons/X5_ner_2025/ruT5_large_250925'\nfinal_tokenizer = AutoTokenizer.from_pretrained(best_model_path, use_fast=True, add_prefix_space=True)\nfinal_model = AutoModelForTokenClassification.from_pretrained(best_model_path)\n","metadata":{"id":"d-lZjXh2CLlY"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_model.to('cuda')","metadata":{"id":"Km6LgmUEdlaL","colab":{"base_uri":"https://localhost:8080/"},"outputId":"16cac88a-b60d-4f0c-ec4d-d31e9aa215c7"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["T5ForTokenClassification(\n","  (transformer): T5EncoderModel(\n","    (shared): Embedding(32128, 1024)\n","    (encoder): T5Stack(\n","      (embed_tokens): Embedding(32128, 1024)\n","      (block): ModuleList(\n","        (0): T5Block(\n","          (layer): ModuleList(\n","            (0): T5LayerSelfAttention(\n","              (SelfAttention): T5Attention(\n","                (q): Linear(in_features=1024, out_features=1024, bias=False)\n","                (k): Linear(in_features=1024, out_features=1024, bias=False)\n","                (v): Linear(in_features=1024, out_features=1024, bias=False)\n","                (o): Linear(in_features=1024, out_features=1024, bias=False)\n","                (relative_attention_bias): Embedding(32, 16)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (1): T5LayerFF(\n","              (DenseReluDense): T5DenseActDense(\n","                (wi): Linear(in_features=1024, out_features=4096, bias=False)\n","                (wo): Linear(in_features=4096, out_features=1024, bias=False)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","                (act): ReLU()\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","        (1-23): 23 x T5Block(\n","          (layer): ModuleList(\n","            (0): T5LayerSelfAttention(\n","              (SelfAttention): T5Attention(\n","                (q): Linear(in_features=1024, out_features=1024, bias=False)\n","                (k): Linear(in_features=1024, out_features=1024, bias=False)\n","                (v): Linear(in_features=1024, out_features=1024, bias=False)\n","                (o): Linear(in_features=1024, out_features=1024, bias=False)\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (1): T5LayerFF(\n","              (DenseReluDense): T5DenseActDense(\n","                (wi): Linear(in_features=1024, out_features=4096, bias=False)\n","                (wo): Linear(in_features=4096, out_features=1024, bias=False)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","                (act): ReLU()\n","              )\n","              (layer_norm): T5LayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","      (final_layer_norm): T5LayerNorm()\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","  )\n","  (dropout): Dropout(p=0.0, inplace=False)\n","  (classifier): Linear(in_features=1024, out_features=9, bias=True)\n",")"]},"metadata":{},"execution_count":35}],"execution_count":null},{"cell_type":"code","source":"token_classifier = pipeline(\n    \"token-classification\", model=final_model, aggregation_strategy=\"first\", tokenizer=final_tokenizer\n)\nfor s in samples:\n    print(f'sample: {s}')\n    res = token_classifier(s)\n    for i, r in enumerate(res):\n        # print('Entity: '+ r['entity_group'] + '   Word: ' + r['word'])\n        print('Entity: '+ r['entity_group'] + '   Word: ' + r['word'] + '. Probs:  ' + str(round(r['score'], 4)))\n    print('#'*40)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":228},"id":"GhB6AHvk446v","outputId":"68d19b89-5595-4d00-e8a2-f595c33f76af"},"outputs":[{"output_type":"stream","name":"stderr","text":["Device set to use cuda:0\n"]},{"output_type":"error","ename":"NameError","evalue":"name 'samples' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-926685419.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"token-classification\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfinal_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maggregation_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"first\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfinal_tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m )\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'sample: {s}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'samples' is not defined"]}],"execution_count":null},{"cell_type":"code","source":"s = 'очиститель для унитаза'\nres = token_classifier(s)\nfor i, r in enumerate(res):\n    # print('Entity: '+ r['entity_group'] + '   Word: ' + r['word'])\n    print('Entity: '+ r['entity_group'] + '   Word: ' + r['word'] + '. Probs:  ' + str(round(r['score'], 4)))\nprint(res)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M0gOiZCtH7ya","outputId":"2c9727b7-1033-4149-cc37-f92a10719607"},"outputs":[{"output_type":"stream","name":"stdout","text":["Entity: TYPE   Word: очиститель. Probs:  0.9998\n","[{'entity_group': 'TYPE', 'score': np.float32(0.9997584), 'word': 'очиститель', 'start': 0, 'end': 10}]\n"]}],"execution_count":null},{"cell_type":"code","source":"lbls_in_dataset = [\n 'O',\n 'B-BRAND',\n 'B-PERCENT',\n 'B-TYPE',\n 'B-VOLUME',\n 'I-BRAND',\n 'I-PERCENT',\n 'I-TYPE',\n 'I-VOLUME']\nlabel2id = {v:i for i, v in enumerate(lbls_in_dataset)}\nid2label = {i:v for i, v in enumerate(lbls_in_dataset)}","metadata":{"id":"PiJyCZtGJY6T"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nimport re\n\ndef predict_all_entities(text: str, model, tokenizer, id2label, device=None, debug=False):\n    \"\"\"\n    Word-level inference with original character spans (start_idx, end_idx, ENTITY).\n    Returns a list of tuples for each word (including 'O').\n    \"\"\"\n\n    model.eval()\n    if device is None:\n        device = next(model.parameters()).device\n\n    # --- find words and their char spans in original text ---\n    words = []\n    spans = []\n    for match in re.finditer(r\"\\S+\", text):\n        words.append(match.group())\n        spans.append(match.span())  # (start_idx, end_idx)\n\n    # encode with word-level info\n    enc = tokenizer(\n        words,\n        is_split_into_words=True,\n        return_tensors=\"pt\",\n        truncation=True\n    )\n\n    input_ids = enc[\"input_ids\"].to(device)\n    attention_mask = enc[\"attention_mask\"].to(device)\n    word_ids = enc.word_ids(batch_index=0)\n\n    with torch.no_grad():\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        logits = outputs.logits[0]               # (seq_len, num_labels)\n        probs = F.softmax(logits, dim=-1)        # (seq_len, num_labels)\n    # print()\n    # print(probs)\n    results = []\n    prev_word_idx = None\n    # print(word_ids)\n    for token_idx, word_idx in enumerate(word_ids):\n        if debug:\n            print(token_idx, word_idx, logits[token_idx])\n        if word_idx is None:\n            prev_word_idx = None\n            continue\n\n        # only take first subtoken per word\n        if word_idx != prev_word_idx:\n            label_id = int(torch.argmax(logits[token_idx]).cpu().numpy())\n            label = id2label[label_id]\n\n            start_idx, end_idx = spans[word_idx]\n            results.append((start_idx, end_idx, label))\n\n        prev_word_idx = word_idx\n\n    return results\n\n\n# -------------------------\n# Example usage\n# -------------------------\ns = \"сыр натура сливочный\"\n\n# Example id2label (replace with your mapping)\n# id2label = {0: \"O\", 1: \"B-TYPE\", 2: \"I-TYPE\", ...}\n\nres = predict_all_entities(s, trained_model, tokenizer, id2label)\nprint(res)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":193},"id":"K3XfLW2HILpe","outputId":"cd2e872b-90af-4e54-dd90-bc2560e56421","trusted":true,"execution":{"iopub.status.busy":"2025-09-26T14:45:07.058964Z","iopub.execute_input":"2025-09-26T14:45:07.059338Z","iopub.status.idle":"2025-09-26T14:45:07.130463Z","shell.execute_reply.started":"2025-09-26T14:45:07.059316Z","shell.execute_reply":"2025-09-26T14:45:07.129880Z"}},"outputs":[{"name":"stdout","text":"[(0, 3, 'B-TYPE'), (4, 10, 'B-BRAND'), (11, 20, 'I-BRAND')]\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"annotations = []\nfor s in tqdm(df_test['sample'].tolist()):\n    r = predict_all_entities(s, final_model, final_tokenizer, id2label)\n    annotations.append(r)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZiNM_xC_Kr2z","outputId":"bc43b4cd-6c32-4004-b46f-cf4a097f4c8e"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 5000/5000 [01:59<00:00, 41.73it/s]\n"]}],"execution_count":null},{"cell_type":"code","source":"df_test['annotation'] = annotations","metadata":{"id":"kXvAYjGsK53U"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test.sample(20)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":677},"id":"lJnBzEdQO-UA","outputId":"d86136c7-7a3a-4a2e-a7d8-bf13f1128e27"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                sample                           annotation\n","1501            куркум                     [(0, 6, B-TYPE)]\n","2586            вялены                     [(0, 6, B-TYPE)]\n","2653         exponetto                    [(0, 9, B-BRAND)]\n","1055  моцарелла шарики   [(0, 9, B-TYPE), (10, 16, I-TYPE)]\n","705       галеты petra   [(0, 6, B-TYPE), (7, 12, B-BRAND)]\n","106            ванилик                     [(0, 7, B-TYPE)]\n","589           нектар j    [(0, 6, B-TYPE), (7, 8, B-BRAND)]\n","2468               тнк                          [(0, 3, O)]\n","2413       рыбный фарш    [(0, 6, B-TYPE), (7, 11, I-TYPE)]\n","1600  сгущенные молочн   [(0, 9, B-TYPE), (10, 16, I-TYPE)]\n","2464          вереники                     [(0, 8, B-TYPE)]\n","228              пончо                     [(0, 5, B-TYPE)]\n","915              ол йс         [(0, 2, B-BRAND), (3, 5, O)]\n","794            йогуртп                     [(0, 7, B-TYPE)]\n","3021          пнмидоры                     [(0, 8, B-TYPE)]\n","3543           семечко                     [(0, 7, B-TYPE)]\n","1073     разрыхлительн                    [(0, 13, B-TYPE)]\n","3351            слифки                     [(0, 6, B-TYPE)]\n","1744  доя мытья посуды  [(0, 3, O), (4, 9, O), (10, 16, O)]\n","1084      круассан лим   [(0, 8, B-TYPE), (9, 12, B-BRAND)]"],"text/html":["\n","  <div id=\"df-ad773860-a83a-4851-8079-d74780bad119\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sample</th>\n","      <th>annotation</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1501</th>\n","      <td>куркум</td>\n","      <td>[(0, 6, B-TYPE)]</td>\n","    </tr>\n","    <tr>\n","      <th>2586</th>\n","      <td>вялены</td>\n","      <td>[(0, 6, B-TYPE)]</td>\n","    </tr>\n","    <tr>\n","      <th>2653</th>\n","      <td>exponetto</td>\n","      <td>[(0, 9, B-BRAND)]</td>\n","    </tr>\n","    <tr>\n","      <th>1055</th>\n","      <td>моцарелла шарики</td>\n","      <td>[(0, 9, B-TYPE), (10, 16, I-TYPE)]</td>\n","    </tr>\n","    <tr>\n","      <th>705</th>\n","      <td>галеты petra</td>\n","      <td>[(0, 6, B-TYPE), (7, 12, B-BRAND)]</td>\n","    </tr>\n","    <tr>\n","      <th>106</th>\n","      <td>ванилик</td>\n","      <td>[(0, 7, B-TYPE)]</td>\n","    </tr>\n","    <tr>\n","      <th>589</th>\n","      <td>нектар j</td>\n","      <td>[(0, 6, B-TYPE), (7, 8, B-BRAND)]</td>\n","    </tr>\n","    <tr>\n","      <th>2468</th>\n","      <td>тнк</td>\n","      <td>[(0, 3, O)]</td>\n","    </tr>\n","    <tr>\n","      <th>2413</th>\n","      <td>рыбный фарш</td>\n","      <td>[(0, 6, B-TYPE), (7, 11, I-TYPE)]</td>\n","    </tr>\n","    <tr>\n","      <th>1600</th>\n","      <td>сгущенные молочн</td>\n","      <td>[(0, 9, B-TYPE), (10, 16, I-TYPE)]</td>\n","    </tr>\n","    <tr>\n","      <th>2464</th>\n","      <td>вереники</td>\n","      <td>[(0, 8, B-TYPE)]</td>\n","    </tr>\n","    <tr>\n","      <th>228</th>\n","      <td>пончо</td>\n","      <td>[(0, 5, B-TYPE)]</td>\n","    </tr>\n","    <tr>\n","      <th>915</th>\n","      <td>ол йс</td>\n","      <td>[(0, 2, B-BRAND), (3, 5, O)]</td>\n","    </tr>\n","    <tr>\n","      <th>794</th>\n","      <td>йогуртп</td>\n","      <td>[(0, 7, B-TYPE)]</td>\n","    </tr>\n","    <tr>\n","      <th>3021</th>\n","      <td>пнмидоры</td>\n","      <td>[(0, 8, B-TYPE)]</td>\n","    </tr>\n","    <tr>\n","      <th>3543</th>\n","      <td>семечко</td>\n","      <td>[(0, 7, B-TYPE)]</td>\n","    </tr>\n","    <tr>\n","      <th>1073</th>\n","      <td>разрыхлительн</td>\n","      <td>[(0, 13, B-TYPE)]</td>\n","    </tr>\n","    <tr>\n","      <th>3351</th>\n","      <td>слифки</td>\n","      <td>[(0, 6, B-TYPE)]</td>\n","    </tr>\n","    <tr>\n","      <th>1744</th>\n","      <td>доя мытья посуды</td>\n","      <td>[(0, 3, O), (4, 9, O), (10, 16, O)]</td>\n","    </tr>\n","    <tr>\n","      <th>1084</th>\n","      <td>круассан лим</td>\n","      <td>[(0, 8, B-TYPE), (9, 12, B-BRAND)]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ad773860-a83a-4851-8079-d74780bad119')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-ad773860-a83a-4851-8079-d74780bad119 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-ad773860-a83a-4851-8079-d74780bad119');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-047a93af-a29c-4008-848f-bb771052a08e\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-047a93af-a29c-4008-848f-bb771052a08e')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-047a93af-a29c-4008-848f-bb771052a08e button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"df_test\",\n  \"rows\": 20,\n  \"fields\": [\n    {\n      \"column\": \"sample\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 20,\n        \"samples\": [\n          \"\\u043a\\u0443\\u0440\\u043a\\u0443\\u043c\",\n          \"\\u0441\\u043b\\u0438\\u0444\\u043a\\u0438\",\n          \"\\u0441\\u0435\\u043c\\u0435\\u0447\\u043a\\u043e\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"annotation\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":40}],"execution_count":null},{"cell_type":"code","source":"df_test.to_csv('submission_ruT5_large_250925.csv', index=False, sep=';')","metadata":{"id":"jNY15xLhO_ma"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## k-fold","metadata":{"id":"A34migI_LXaj"}},{"cell_type":"code","source":"data_collator = DataCollatorForTokenClassification(tokenizer)","metadata":{"id":"lzrIpRnUMdjN","trusted":true,"execution":{"iopub.status.busy":"2025-09-26T17:50:56.036649Z","iopub.execute_input":"2025-09-26T17:50:56.036926Z","iopub.status.idle":"2025-09-26T17:50:56.040658Z","shell.execute_reply.started":"2025-09-26T17:50:56.036903Z","shell.execute_reply":"2025-09-26T17:50:56.039836Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"ds_tokenized = ds.map(tokenize_and_align_labels,\n                                          batched=True,\n                                          fn_kwargs={'tokenizer': tokenizer})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# функция метрики\ndef compute_metrics_custom(p):\n    labels_list = list(id2label.values())\n    predictions, labels = p\n    if not os.path.exists('/content/test/p_trainer.pkl'):\n        os.makedirs('/content/test', exist_ok=True)\n        with open('/content/test/p_trainer.pkl', 'wb') as f:\n            pickle.dump(p, f)\n    predictions = np.argmax(predictions, axis=2)\n\n\n    true_predictions = [\n        [labels_list[p] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    true_labels = [\n        [labels_list[l] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n\n    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n    report_dict = classification_report(true_labels, true_predictions, digits=4, output_dict=True)\n    report = classification_report(true_labels, true_predictions, digits=4)\n    macro_f1 = report_dict[\"macro avg\"][\"f1-score\"]\n    print(\"=== seqeval classification_report ===\")\n    print(report)\n    CLASS_REPORT_PATH = '/content/logs/last_classification_report.txt'\n    try:\n        with open(CLASS_REPORT_PATH, \"w\", encoding=\"utf-8\") as f:\n            f.write(report)\n    except Exception as e:\n        print(f\"Warning: failed to write classification report to {CLASS_REPORT_PATH}: {e}\")\n\n    return {\n        \"f1_macro\": macro_f1,\n        \"precision\": results[\"overall_precision\"],\n        \"recall\": results[\"overall_recall\"],\n        \"f1\": results[\"overall_f1\"],\n        \"accuracy\": results[\"overall_accuracy\"],\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T17:50:56.550702Z","iopub.execute_input":"2025-09-26T17:50:56.551340Z","iopub.status.idle":"2025-09-26T17:50:56.558583Z","shell.execute_reply.started":"2025-09-26T17:50:56.551253Z","shell.execute_reply":"2025-09-26T17:50:56.557961Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"early_stop_cb = EarlyStoppingCallback(early_stopping_patience=2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Assuming these are defined globally:\n# - full_dataset: your full Hugging Face Dataset (not pre-split)\n# - tokenizer, data_collator, compute_metrics_custom\n# - num_labels, id2label, label2id\n# - early_stop_cb (if used)\n# - GDRIVE_DIR, printer\n\ndef objective(trial: optuna.Trial):\n    # Suggest hyperparameters\n    learning_rate = trial.suggest_float('learning_rate', 1e-5, 5e-4, log=True)\n    weight_decay = trial.suggest_float('weight_decay', 1e-4, 0.05, log=True)\n    num_train_epochs = trial.suggest_int('num_train_epochs', 3, 10)\n    \n    # 5-Fold CV setup\n    kfold = KFold(n_splits=3, shuffle=True, random_state=42)\n    fold_f1_scores = []\n\n\n\n    # Get indices for all folds\n    all_splits = list(kfold.split(ds_tokenized))\n\n    for fold, (train_idx, val_idx) in enumerate(all_splits):\n        # Create train/val datasets for this fold\n        train_ds = ds_tokenized.select(train_idx)\n        val_ds = ds_tokenized.select(val_idx)\n\n        # ✅ CRITICAL: Load FRESH model for each fold\n        model = AutoModelForTokenClassification.from_pretrained(\n            'ai-forever/ruT5-large',  # or better: switch to ruBERT!\n            num_labels=len(lbls_in_dataset),\n            id2label=id2label,\n            label2id=label2id\n        ).to(\"cuda\")\n\n        trial_check_dir = f\"./checkpoints_trial_{trial.number}_fold_{fold}\"\n        os.makedirs(trial_check_dir, exist_ok=True)\n\n        args = TrainingArguments(\n            output_dir=trial_check_dir,\n            overwrite_output_dir=True,\n            eval_strategy=\"epoch\",\n            torch_compile=True,\n            per_device_train_batch_size=256,        # Reduced from 256 (T5-large is huge!)\n            per_device_eval_batch_size=256,\n            learning_rate=learning_rate,\n            weight_decay=weight_decay,\n            num_train_epochs=num_train_epochs,\n            seed=42 + fold,                        # Different seed per fold\n            data_seed=24 + fold,\n            gradient_accumulation_steps=1,         # To compensate for smaller batch\n            warmup_ratio=0.1,\n            report_to=None,\n            logging_dir=\"./logs\",\n            logging_steps=1,\n            load_best_model_at_end=True,\n            metric_for_best_model=\"eval_f1_macro\",\n            save_total_limit=1,\n            save_strategy=\"epoch\",\n            # Disable logging to speed up CV\n            disable_tqdm=True,\n        )\n\n        trainer = Trainer(\n            model=model,\n            args=args,\n            train_dataset=train_ds,\n            eval_dataset=val_ds,\n            data_collator=data_collator,\n            compute_metrics=compute_metrics_custom,\n            tokenizer=tokenizer,\n            callbacks=[early_stop_cb] if 'early_stop_cb' in globals() else [],\n        )\n\n        trainer.train()\n\n        # Evaluate on validation fold\n        eval_metrics = trainer.evaluate()\n        fold_f1 = eval_metrics[\"eval_f1_macro\"]\n        fold_f1_scores.append(fold_f1)\n\n        # Report intermediate value to Optuna for pruning\n        trial.report(fold_f1, step=fold)\n\n        # Pruning: check if trial should be pruned\n        if trial.should_prune():\n            # Cleanup before pruning\n            del model, trainer\n            torch.cuda.empty_cache()\n            raise optuna.TrialPruned()\n\n        # # Cleanup to save memory\n        # del model, trainer\n        # torch.cuda.empty_cache()\n\n    # Return average F1 across all 5 folds\n    avg_f1 = np.mean(fold_f1_scores)\n    return avg_f1\n\n\n# Create study with MedianPruner\nstudy = optuna.create_study(\n    study_name='test_optuna_3fold',\n    direction='maximize',\n    pruner=MedianPruner(\n        n_startup_trials=2,    # First 3 trials not pruned\n        n_warmup_steps=1,      # Wait for 2 folds before pruning\n        interval_steps=1       # Check every fold\n    )\n)\n\n# Run optimization\nstudy.optimize(objective, n_trials=10)  # Reduced trials due to 5x cost\n\n# After optimization, print best results\nprint(\"Best trial:\")\nprint(f\"  Value: {study.best_value:.4f}\")\nprint(\"  Params:\")\nfor key, value in study.best_params.items():\n    print(f\"    {key}: {value}\")","metadata":{"id":"HZD3Go7CLSNn","trusted":true,"execution":{"iopub.status.busy":"2025-09-26T17:53:41.241892Z","iopub.execute_input":"2025-09-26T17:53:41.242509Z","iopub.status.idle":"2025-09-26T18:24:46.357184Z","shell.execute_reply.started":"2025-09-26T17:53:41.242483Z","shell.execute_reply":"2025-09-26T18:24:46.356421Z"}},"outputs":[{"name":"stderr","text":"[I 2025-09-26 17:53:41,250] A new study created in memory with name: test_optuna_5fold\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/27552 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be2f0d6256ce4604bc5e6b91a6b751ba"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nThe speedups for torchdynamo mostly come with GPU Ampere or higher and which is not detected here.\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/tmp/ipykernel_36/3364797558.py:66: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 2.265, 'grad_norm': 8.001017570495605, 'learning_rate': 0.0, 'epoch': 0.022727272727272728}\n{'loss': 2.2583, 'grad_norm': 8.146414756774902, 'learning_rate': 8.570529762207097e-06, 'epoch': 0.045454545454545456}\n{'loss': 2.2364, 'grad_norm': 8.015944480895996, 'learning_rate': 1.7141059524414193e-05, 'epoch': 0.06818181818181818}\n{'loss': 2.203, 'grad_norm': 8.202024459838867, 'learning_rate': 2.571158928662129e-05, 'epoch': 0.09090909090909091}\n{'loss': 2.1436, 'grad_norm': 7.650907039642334, 'learning_rate': 3.428211904882839e-05, 'epoch': 0.11363636363636363}\n{'loss': 2.0474, 'grad_norm': 7.481686115264893, 'learning_rate': 4.285264881103548e-05, 'epoch': 0.13636363636363635}\n{'loss': 1.972, 'grad_norm': 6.931197166442871, 'learning_rate': 5.142317857324258e-05, 'epoch': 0.1590909090909091}\n{'loss': 1.845, 'grad_norm': 6.843026638031006, 'learning_rate': 5.9993708335449675e-05, 'epoch': 0.18181818181818182}\n{'loss': 1.7292, 'grad_norm': 6.0464653968811035, 'learning_rate': 6.856423809765677e-05, 'epoch': 0.20454545454545456}\n{'loss': 1.5937, 'grad_norm': 5.495540142059326, 'learning_rate': 7.713476785986388e-05, 'epoch': 0.22727272727272727}\n{'loss': 1.4376, 'grad_norm': 4.900632858276367, 'learning_rate': 8.570529762207096e-05, 'epoch': 0.25}\n{'loss': 1.3285, 'grad_norm': 3.875337600708008, 'learning_rate': 9.427582738427806e-05, 'epoch': 0.2727272727272727}\n{'loss': 1.2514, 'grad_norm': 2.71905255317688, 'learning_rate': 0.00010284635714648515, 'epoch': 0.29545454545454547}\n{'loss': 1.2088, 'grad_norm': 1.9630746841430664, 'learning_rate': 0.00011141688690869226, 'epoch': 0.3181818181818182}\n{'loss': 1.1768, 'grad_norm': 1.8902199268341064, 'learning_rate': 0.00011998741667089935, 'epoch': 0.3409090909090909}\n{'loss': 1.1691, 'grad_norm': 2.2957658767700195, 'learning_rate': 0.00012855794643310643, 'epoch': 0.36363636363636365}\n{'loss': 1.078, 'grad_norm': 2.0012118816375732, 'learning_rate': 0.00013712847619531355, 'epoch': 0.38636363636363635}\n{'loss': 1.0281, 'grad_norm': 1.5150527954101562, 'learning_rate': 0.00014569900595752064, 'epoch': 0.4090909090909091}\n{'loss': 1.0182, 'grad_norm': 1.2547595500946045, 'learning_rate': 0.00015426953571972776, 'epoch': 0.4318181818181818}\n{'loss': 0.9637, 'grad_norm': 1.2894450426101685, 'learning_rate': 0.00016284006548193485, 'epoch': 0.45454545454545453}\n{'loss': 0.9869, 'grad_norm': 2.069836139678955, 'learning_rate': 0.00017141059524414191, 'epoch': 0.4772727272727273}\n{'loss': 0.9124, 'grad_norm': 2.433983325958252, 'learning_rate': 0.00017998112500634903, 'epoch': 0.5}\n{'loss': 0.9133, 'grad_norm': 1.7921518087387085, 'learning_rate': 0.00018855165476855612, 'epoch': 0.5227272727272727}\n{'loss': 0.8566, 'grad_norm': 1.50333571434021, 'learning_rate': 0.00019712218453076322, 'epoch': 0.5454545454545454}\n{'loss': 0.8543, 'grad_norm': 1.0181010961532593, 'learning_rate': 0.0002056927142929703, 'epoch': 0.5681818181818182}\n{'loss': 0.8114, 'grad_norm': 1.5972018241882324, 'learning_rate': 0.00021426324405517743, 'epoch': 0.5909090909090909}\n{'loss': 0.7036, 'grad_norm': 1.2342400550842285, 'learning_rate': 0.00022283377381738452, 'epoch': 0.6136363636363636}\n{'loss': 0.6873, 'grad_norm': 1.008567214012146, 'learning_rate': 0.0002314043035795916, 'epoch': 0.6363636363636364}\n{'loss': 0.6923, 'grad_norm': 0.6996092796325684, 'learning_rate': 0.0002399748333417987, 'epoch': 0.6590909090909091}\n{'loss': 0.6351, 'grad_norm': 1.2129548788070679, 'learning_rate': 0.00024854536310400577, 'epoch': 0.6818181818181818}\n{'loss': 0.6111, 'grad_norm': 1.006269097328186, 'learning_rate': 0.00025711589286621286, 'epoch': 0.7045454545454546}\n{'loss': 0.6073, 'grad_norm': 0.7629260420799255, 'learning_rate': 0.00026568642262842, 'epoch': 0.7272727272727273}\n{'loss': 0.6032, 'grad_norm': 1.332740306854248, 'learning_rate': 0.0002742569523906271, 'epoch': 0.75}\n{'loss': 0.613, 'grad_norm': 1.3385664224624634, 'learning_rate': 0.0002828274821528342, 'epoch': 0.7727272727272727}\n{'loss': 0.5606, 'grad_norm': 1.1699857711791992, 'learning_rate': 0.0002913980119150413, 'epoch': 0.7954545454545454}\n{'loss': 0.531, 'grad_norm': 0.9939770698547363, 'learning_rate': 0.00029996854167724837, 'epoch': 0.8181818181818182}\n{'loss': 0.5426, 'grad_norm': 1.4748196601867676, 'learning_rate': 0.0003085390714394555, 'epoch': 0.8409090909090909}\n{'loss': 0.5009, 'grad_norm': 1.0278830528259277, 'learning_rate': 0.0003171096012016626, 'epoch': 0.8636363636363636}\n{'loss': 0.5195, 'grad_norm': 1.1668493747711182, 'learning_rate': 0.0003256801309638697, 'epoch': 0.8863636363636364}\n{'loss': 0.4877, 'grad_norm': 1.391707420349121, 'learning_rate': 0.00033425066072607674, 'epoch': 0.9090909090909091}\n{'loss': 0.5427, 'grad_norm': 1.0887371301651, 'learning_rate': 0.00034282119048828383, 'epoch': 0.9318181818181818}\n{'loss': 0.4707, 'grad_norm': 1.1774754524230957, 'learning_rate': 0.0003513917202504909, 'epoch': 0.9545454545454546}\n{'loss': 0.461, 'grad_norm': 2.055708885192871, 'learning_rate': 0.00035996225001269806, 'epoch': 0.9772727272727273}\n{'loss': 0.3912, 'grad_norm': 2.7955429553985596, 'learning_rate': 0.00036853277977490516, 'epoch': 1.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.7835    0.6808    0.7285      3142\n     PERCENT     0.4183    0.9697    0.5845        66\n        TYPE     0.8752    0.9535    0.9127     11415\n      VOLUME     0.0714    0.0143    0.0238        70\n\n   micro avg     0.8536    0.8908    0.8718     14693\n   macro avg     0.5371    0.6546    0.5624     14693\nweighted avg     0.8497    0.8908    0.8676     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.43036121129989624, 'eval_f1_macro': 0.5623731948946242, 'eval_precision': 0.8535837735602948, 'eval_recall': 0.8907643095351528, 'eval_f1': 0.8717777925797643, 'eval_accuracy': 0.8695723147241267, 'eval_runtime': 1.4915, 'eval_samples_per_second': 3694.867, 'eval_steps_per_second': 7.375, 'epoch': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.3915, 'grad_norm': 1.3266241550445557, 'learning_rate': 0.00037710330953711225, 'epoch': 1.0227272727272727}\n{'loss': 0.4297, 'grad_norm': 2.044633626937866, 'learning_rate': 0.00037615102845242257, 'epoch': 1.0454545454545454}\n{'loss': 0.4461, 'grad_norm': 1.8021128177642822, 'learning_rate': 0.0003751987473677329, 'epoch': 1.0681818181818181}\n{'loss': 0.3656, 'grad_norm': 1.2012099027633667, 'learning_rate': 0.0003742464662830432, 'epoch': 1.0909090909090908}\n{'loss': 0.3444, 'grad_norm': 1.399692177772522, 'learning_rate': 0.00037329418519835354, 'epoch': 1.1136363636363635}\n{'loss': 0.4014, 'grad_norm': 2.1424505710601807, 'learning_rate': 0.00037234190411366386, 'epoch': 1.1363636363636362}\n{'loss': 0.3916, 'grad_norm': 0.9843030571937561, 'learning_rate': 0.0003713896230289742, 'epoch': 1.1590909090909092}\n{'loss': 0.3652, 'grad_norm': 1.4784042835235596, 'learning_rate': 0.0003704373419442845, 'epoch': 1.1818181818181819}\n{'loss': 0.3622, 'grad_norm': 1.7220035791397095, 'learning_rate': 0.00036948506085959483, 'epoch': 1.2045454545454546}\n{'loss': 0.3877, 'grad_norm': 2.1251866817474365, 'learning_rate': 0.00036853277977490516, 'epoch': 1.2272727272727273}\n{'loss': 0.3606, 'grad_norm': 1.9258557558059692, 'learning_rate': 0.0003675804986902155, 'epoch': 1.25}\n{'loss': 0.372, 'grad_norm': 0.8877275586128235, 'learning_rate': 0.0003666282176055258, 'epoch': 1.2727272727272727}\n{'loss': 0.3599, 'grad_norm': 2.280600070953369, 'learning_rate': 0.0003656759365208361, 'epoch': 1.2954545454545454}\n{'loss': 0.3071, 'grad_norm': 2.047292470932007, 'learning_rate': 0.00036472365543614645, 'epoch': 1.3181818181818181}\n{'loss': 0.3435, 'grad_norm': 1.4384474754333496, 'learning_rate': 0.00036377137435145677, 'epoch': 1.3409090909090908}\n{'loss': 0.3515, 'grad_norm': 1.0624479055404663, 'learning_rate': 0.0003628190932667671, 'epoch': 1.3636363636363638}\n{'loss': 0.367, 'grad_norm': 1.1646627187728882, 'learning_rate': 0.0003618668121820774, 'epoch': 1.3863636363636362}\n{'loss': 0.3037, 'grad_norm': 1.7390460968017578, 'learning_rate': 0.00036091453109738774, 'epoch': 1.4090909090909092}\n{'loss': 0.3277, 'grad_norm': 1.4500221014022827, 'learning_rate': 0.00035996225001269806, 'epoch': 1.4318181818181819}\n{'loss': 0.3325, 'grad_norm': 0.8886391520500183, 'learning_rate': 0.0003590099689280084, 'epoch': 1.4545454545454546}\n{'loss': 0.2835, 'grad_norm': 1.890364408493042, 'learning_rate': 0.0003580576878433187, 'epoch': 1.4772727272727273}\n{'loss': 0.3045, 'grad_norm': 1.380210280418396, 'learning_rate': 0.00035710540675862903, 'epoch': 1.5}\n{'loss': 0.3641, 'grad_norm': 1.4832979440689087, 'learning_rate': 0.00035615312567393936, 'epoch': 1.5227272727272727}\n{'loss': 0.2828, 'grad_norm': 0.8184820413589478, 'learning_rate': 0.0003552008445892497, 'epoch': 1.5454545454545454}\n{'loss': 0.2649, 'grad_norm': 1.6109998226165771, 'learning_rate': 0.00035424856350456, 'epoch': 1.5681818181818183}\n{'loss': 0.2761, 'grad_norm': 2.2006406784057617, 'learning_rate': 0.0003532962824198703, 'epoch': 1.5909090909090908}\n{'loss': 0.2802, 'grad_norm': 1.2711995840072632, 'learning_rate': 0.00035234400133518065, 'epoch': 1.6136363636363638}\n{'loss': 0.3122, 'grad_norm': 1.006182074546814, 'learning_rate': 0.0003513917202504909, 'epoch': 1.6363636363636362}\n{'loss': 0.2666, 'grad_norm': 1.5065741539001465, 'learning_rate': 0.0003504394391658013, 'epoch': 1.6590909090909092}\n{'loss': 0.3122, 'grad_norm': 1.0850436687469482, 'learning_rate': 0.0003494871580811116, 'epoch': 1.6818181818181817}\n{'loss': 0.2753, 'grad_norm': 1.7704013586044312, 'learning_rate': 0.0003485348769964219, 'epoch': 1.7045454545454546}\n{'loss': 0.2658, 'grad_norm': 1.0672968626022339, 'learning_rate': 0.00034758259591173227, 'epoch': 1.7272727272727273}\n{'loss': 0.3575, 'grad_norm': 1.6405152082443237, 'learning_rate': 0.0003466303148270426, 'epoch': 1.75}\n{'loss': 0.3195, 'grad_norm': 1.5282323360443115, 'learning_rate': 0.00034567803374235286, 'epoch': 1.7727272727272727}\n{'loss': 0.2842, 'grad_norm': 1.5626734495162964, 'learning_rate': 0.00034472575265766324, 'epoch': 1.7954545454545454}\n{'loss': 0.2872, 'grad_norm': 1.5069562196731567, 'learning_rate': 0.00034377347157297356, 'epoch': 1.8181818181818183}\n{'loss': 0.2746, 'grad_norm': 1.3747544288635254, 'learning_rate': 0.00034282119048828383, 'epoch': 1.8409090909090908}\n{'loss': 0.309, 'grad_norm': 2.064072847366333, 'learning_rate': 0.0003418689094035942, 'epoch': 1.8636363636363638}\n{'loss': 0.2342, 'grad_norm': 1.404049038887024, 'learning_rate': 0.00034091662831890453, 'epoch': 1.8863636363636362}\n{'loss': 0.3183, 'grad_norm': 0.9239153265953064, 'learning_rate': 0.0003399643472342148, 'epoch': 1.9090909090909092}\n{'loss': 0.2182, 'grad_norm': 0.867889404296875, 'learning_rate': 0.0003390120661495252, 'epoch': 1.9318181818181817}\n{'loss': 0.3281, 'grad_norm': 0.9002161622047424, 'learning_rate': 0.0003380597850648355, 'epoch': 1.9545454545454546}\n{'loss': 0.2538, 'grad_norm': 0.7416172027587891, 'learning_rate': 0.00033710750398014577, 'epoch': 1.9772727272727273}\n{'loss': 0.4398, 'grad_norm': 3.5096652507781982, 'learning_rate': 0.00033615522289545614, 'epoch': 2.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8542    0.8450    0.8496      3142\n     PERCENT     0.6889    0.9394    0.7949        66\n        TYPE     0.9384    0.9616    0.9499     11415\n      VOLUME     0.6250    0.5000    0.5556        70\n\n   micro avg     0.9183    0.9344    0.9263     14693\n   macro avg     0.7766    0.8115    0.7875     14693\nweighted avg     0.9178    0.9344    0.9259     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.27481138706207275, 'eval_f1_macro': 0.7874808770668584, 'eval_precision': 0.9182663366998863, 'eval_recall': 0.9343905261008644, 'eval_f1': 0.9262582647416003, 'eval_accuracy': 0.9170203504189792, 'eval_runtime': 1.5389, 'eval_samples_per_second': 3581.062, 'eval_steps_per_second': 7.148, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.2183, 'grad_norm': 1.0755908489227295, 'learning_rate': 0.0003352029418107664, 'epoch': 2.022727272727273}\n{'loss': 0.2242, 'grad_norm': 0.9474616050720215, 'learning_rate': 0.00033425066072607674, 'epoch': 2.0454545454545454}\n{'loss': 0.2211, 'grad_norm': 0.8302925229072571, 'learning_rate': 0.0003332983796413871, 'epoch': 2.0681818181818183}\n{'loss': 0.1699, 'grad_norm': 0.9115373492240906, 'learning_rate': 0.0003323460985566974, 'epoch': 2.090909090909091}\n{'loss': 0.1523, 'grad_norm': 0.5720126032829285, 'learning_rate': 0.0003313938174720077, 'epoch': 2.1136363636363638}\n{'loss': 0.1494, 'grad_norm': 0.8474003672599792, 'learning_rate': 0.0003304415363873181, 'epoch': 2.1363636363636362}\n{'loss': 0.1728, 'grad_norm': 0.7105135917663574, 'learning_rate': 0.00032948925530262835, 'epoch': 2.159090909090909}\n{'loss': 0.1945, 'grad_norm': 0.8926301002502441, 'learning_rate': 0.00032853697421793873, 'epoch': 2.1818181818181817}\n{'loss': 0.19, 'grad_norm': 0.7999191284179688, 'learning_rate': 0.00032758469313324905, 'epoch': 2.2045454545454546}\n{'loss': 0.1974, 'grad_norm': 0.6867371797561646, 'learning_rate': 0.0003266324120485593, 'epoch': 2.227272727272727}\n{'loss': 0.1855, 'grad_norm': 0.9520118832588196, 'learning_rate': 0.0003256801309638697, 'epoch': 2.25}\n{'loss': 0.1549, 'grad_norm': 0.7756453156471252, 'learning_rate': 0.00032472784987918, 'epoch': 2.2727272727272725}\n{'loss': 0.2049, 'grad_norm': 1.0066972970962524, 'learning_rate': 0.0003237755687944903, 'epoch': 2.2954545454545454}\n{'loss': 0.1919, 'grad_norm': 0.9478017091751099, 'learning_rate': 0.00032282328770980067, 'epoch': 2.3181818181818183}\n{'loss': 0.1752, 'grad_norm': 0.7637032866477966, 'learning_rate': 0.00032187100662511094, 'epoch': 2.340909090909091}\n{'loss': 0.1984, 'grad_norm': 1.4259796142578125, 'learning_rate': 0.00032091872554042126, 'epoch': 2.3636363636363638}\n{'loss': 0.1886, 'grad_norm': 1.0047016143798828, 'learning_rate': 0.00031996644445573164, 'epoch': 2.3863636363636362}\n{'loss': 0.2422, 'grad_norm': 1.1613879203796387, 'learning_rate': 0.0003190141633710419, 'epoch': 2.409090909090909}\n{'loss': 0.1566, 'grad_norm': 1.048750877380371, 'learning_rate': 0.00031806188228635223, 'epoch': 2.4318181818181817}\n{'loss': 0.1843, 'grad_norm': 0.7634931802749634, 'learning_rate': 0.0003171096012016626, 'epoch': 2.4545454545454546}\n{'loss': 0.1994, 'grad_norm': 1.8055516481399536, 'learning_rate': 0.0003161573201169729, 'epoch': 2.4772727272727275}\n{'loss': 0.1634, 'grad_norm': 1.5971691608428955, 'learning_rate': 0.0003152050390322832, 'epoch': 2.5}\n{'loss': 0.2371, 'grad_norm': 1.6179158687591553, 'learning_rate': 0.0003142527579475936, 'epoch': 2.5227272727272725}\n{'loss': 0.1867, 'grad_norm': 0.6878541707992554, 'learning_rate': 0.00031330047686290385, 'epoch': 2.5454545454545454}\n{'loss': 0.2018, 'grad_norm': 1.2467026710510254, 'learning_rate': 0.00031234819577821417, 'epoch': 2.5681818181818183}\n{'loss': 0.2295, 'grad_norm': 2.199267864227295, 'learning_rate': 0.00031139591469352455, 'epoch': 2.590909090909091}\n{'loss': 0.1353, 'grad_norm': 0.7965973615646362, 'learning_rate': 0.0003104436336088348, 'epoch': 2.6136363636363638}\n{'loss': 0.1649, 'grad_norm': 1.0702693462371826, 'learning_rate': 0.00030949135252414514, 'epoch': 2.6363636363636362}\n{'loss': 0.1724, 'grad_norm': 2.032761335372925, 'learning_rate': 0.0003085390714394555, 'epoch': 2.659090909090909}\n{'loss': 0.1568, 'grad_norm': 1.9653791189193726, 'learning_rate': 0.0003075867903547658, 'epoch': 2.6818181818181817}\n{'loss': 0.2062, 'grad_norm': 1.357242226600647, 'learning_rate': 0.0003066345092700761, 'epoch': 2.7045454545454546}\n{'loss': 0.1526, 'grad_norm': 0.8984622955322266, 'learning_rate': 0.00030568222818538643, 'epoch': 2.7272727272727275}\n{'loss': 0.1627, 'grad_norm': 1.1406681537628174, 'learning_rate': 0.00030472994710069675, 'epoch': 2.75}\n{'loss': 0.1663, 'grad_norm': 0.7753964066505432, 'learning_rate': 0.0003037776660160071, 'epoch': 2.7727272727272725}\n{'loss': 0.2269, 'grad_norm': 1.5126757621765137, 'learning_rate': 0.0003028253849313174, 'epoch': 2.7954545454545454}\n{'loss': 0.1904, 'grad_norm': 0.7355353832244873, 'learning_rate': 0.0003018731038466277, 'epoch': 2.8181818181818183}\n{'loss': 0.1999, 'grad_norm': 1.549868106842041, 'learning_rate': 0.00030092082276193805, 'epoch': 2.840909090909091}\n{'loss': 0.1875, 'grad_norm': 1.6397027969360352, 'learning_rate': 0.00029996854167724837, 'epoch': 2.8636363636363638}\n{'loss': 0.1555, 'grad_norm': 1.3280967473983765, 'learning_rate': 0.0002990162605925587, 'epoch': 2.8863636363636362}\n{'loss': 0.1851, 'grad_norm': 1.1385645866394043, 'learning_rate': 0.000298063979507869, 'epoch': 2.909090909090909}\n{'loss': 0.1834, 'grad_norm': 1.2957825660705566, 'learning_rate': 0.00029711169842317934, 'epoch': 2.9318181818181817}\n{'loss': 0.2299, 'grad_norm': 2.060818910598755, 'learning_rate': 0.00029615941733848966, 'epoch': 2.9545454545454546}\n{'loss': 0.218, 'grad_norm': 1.3610055446624756, 'learning_rate': 0.0002952071362538, 'epoch': 2.9772727272727275}\n{'loss': 0.0836, 'grad_norm': 4.658990859985352, 'learning_rate': 0.0002942548551691103, 'epoch': 3.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8798    0.8806    0.8802      3142\n     PERCENT     0.8986    0.9394    0.9185        66\n        TYPE     0.9457    0.9710    0.9582     11415\n      VOLUME     0.9130    0.9000    0.9065        70\n\n   micro avg     0.9315    0.9512    0.9413     14693\n   macro avg     0.9093    0.9228    0.9159     14693\nweighted avg     0.9313    0.9512    0.9411     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.23605656623840332, 'eval_f1_macro': 0.9158560603443224, 'eval_precision': 0.9315470239285476, 'eval_recall': 0.9512012522970121, 'eval_f1': 0.9412715517241379, 'eval_accuracy': 0.9309500489715965, 'eval_runtime': 1.9009, 'eval_samples_per_second': 2899.115, 'eval_steps_per_second': 5.787, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.1282, 'grad_norm': 0.6027787327766418, 'learning_rate': 0.00029330257408442063, 'epoch': 3.022727272727273}\n{'loss': 0.112, 'grad_norm': 0.5973909497261047, 'learning_rate': 0.00029235029299973095, 'epoch': 3.0454545454545454}\n{'loss': 0.1141, 'grad_norm': 0.8067646026611328, 'learning_rate': 0.0002913980119150413, 'epoch': 3.0681818181818183}\n{'loss': 0.1136, 'grad_norm': 0.6542567014694214, 'learning_rate': 0.0002904457308303516, 'epoch': 3.090909090909091}\n{'loss': 0.1238, 'grad_norm': 0.7830553650856018, 'learning_rate': 0.0002894934497456619, 'epoch': 3.1136363636363638}\n{'loss': 0.1075, 'grad_norm': 0.7971400618553162, 'learning_rate': 0.00028854116866097225, 'epoch': 3.1363636363636362}\n{'loss': 0.1057, 'grad_norm': 0.8667889833450317, 'learning_rate': 0.00028758888757628257, 'epoch': 3.159090909090909}\n{'loss': 0.1209, 'grad_norm': 0.7557263970375061, 'learning_rate': 0.0002866366064915929, 'epoch': 3.1818181818181817}\n{'loss': 0.1477, 'grad_norm': 0.7399359345436096, 'learning_rate': 0.0002856843254069032, 'epoch': 3.2045454545454546}\n{'loss': 0.0922, 'grad_norm': 0.6109102368354797, 'learning_rate': 0.00028473204432221354, 'epoch': 3.227272727272727}\n{'loss': 0.1287, 'grad_norm': 0.6763461828231812, 'learning_rate': 0.00028377976323752386, 'epoch': 3.25}\n{'loss': 0.1074, 'grad_norm': 0.9412378072738647, 'learning_rate': 0.0002828274821528342, 'epoch': 3.2727272727272725}\n{'loss': 0.1398, 'grad_norm': 1.3037039041519165, 'learning_rate': 0.0002818752010681445, 'epoch': 3.2954545454545454}\n{'loss': 0.1197, 'grad_norm': 1.3130548000335693, 'learning_rate': 0.00028092291998345483, 'epoch': 3.3181818181818183}\n{'loss': 0.1103, 'grad_norm': 1.4396333694458008, 'learning_rate': 0.00027997063889876516, 'epoch': 3.340909090909091}\n{'loss': 0.1126, 'grad_norm': 0.8733338117599487, 'learning_rate': 0.0002790183578140755, 'epoch': 3.3636363636363638}\n{'loss': 0.0975, 'grad_norm': 1.3389079570770264, 'learning_rate': 0.0002780660767293858, 'epoch': 3.3863636363636362}\n{'loss': 0.1296, 'grad_norm': 1.877274751663208, 'learning_rate': 0.0002771137956446961, 'epoch': 3.409090909090909}\n{'loss': 0.0906, 'grad_norm': 0.7015224099159241, 'learning_rate': 0.00027616151456000645, 'epoch': 3.4318181818181817}\n{'loss': 0.1621, 'grad_norm': 1.3268259763717651, 'learning_rate': 0.00027520923347531677, 'epoch': 3.4545454545454546}\n{'loss': 0.1074, 'grad_norm': 0.8146901726722717, 'learning_rate': 0.0002742569523906271, 'epoch': 3.4772727272727275}\n{'loss': 0.132, 'grad_norm': 1.554161548614502, 'learning_rate': 0.0002733046713059374, 'epoch': 3.5}\n{'loss': 0.1379, 'grad_norm': 0.7138600945472717, 'learning_rate': 0.00027235239022124774, 'epoch': 3.5227272727272725}\n{'loss': 0.0894, 'grad_norm': 0.725382387638092, 'learning_rate': 0.00027140010913655806, 'epoch': 3.5454545454545454}\n{'loss': 0.0903, 'grad_norm': 1.0074717998504639, 'learning_rate': 0.0002704478280518684, 'epoch': 3.5681818181818183}\n{'loss': 0.1446, 'grad_norm': 0.9405069351196289, 'learning_rate': 0.0002694955469671787, 'epoch': 3.590909090909091}\n{'loss': 0.1036, 'grad_norm': 0.5903359651565552, 'learning_rate': 0.00026854326588248903, 'epoch': 3.6136363636363638}\n{'loss': 0.088, 'grad_norm': 0.7582515478134155, 'learning_rate': 0.00026759098479779936, 'epoch': 3.6363636363636362}\n{'loss': 0.0879, 'grad_norm': 0.7878310084342957, 'learning_rate': 0.0002666387037131097, 'epoch': 3.659090909090909}\n{'loss': 0.0981, 'grad_norm': 0.8363181948661804, 'learning_rate': 0.00026568642262842, 'epoch': 3.6818181818181817}\n{'loss': 0.0977, 'grad_norm': 0.8142485618591309, 'learning_rate': 0.0002647341415437303, 'epoch': 3.7045454545454546}\n{'loss': 0.1165, 'grad_norm': 1.269092321395874, 'learning_rate': 0.00026378186045904065, 'epoch': 3.7272727272727275}\n{'loss': 0.0815, 'grad_norm': 0.7733877301216125, 'learning_rate': 0.00026282957937435097, 'epoch': 3.75}\n{'loss': 0.1268, 'grad_norm': 0.775213897228241, 'learning_rate': 0.0002618772982896613, 'epoch': 3.7727272727272725}\n{'loss': 0.1171, 'grad_norm': 1.1375174522399902, 'learning_rate': 0.0002609250172049716, 'epoch': 3.7954545454545454}\n{'loss': 0.1119, 'grad_norm': 1.1233088970184326, 'learning_rate': 0.00025997273612028194, 'epoch': 3.8181818181818183}\n{'loss': 0.1076, 'grad_norm': 1.023527979850769, 'learning_rate': 0.00025902045503559226, 'epoch': 3.840909090909091}\n{'loss': 0.1572, 'grad_norm': 0.8872900605201721, 'learning_rate': 0.0002580681739509026, 'epoch': 3.8636363636363638}\n{'loss': 0.1135, 'grad_norm': 1.3305158615112305, 'learning_rate': 0.00025711589286621286, 'epoch': 3.8863636363636362}\n{'loss': 0.1432, 'grad_norm': 2.237621545791626, 'learning_rate': 0.00025616361178152323, 'epoch': 3.909090909090909}\n{'loss': 0.1188, 'grad_norm': 2.169867515563965, 'learning_rate': 0.00025521133069683356, 'epoch': 3.9318181818181817}\n{'loss': 0.1426, 'grad_norm': 0.9930147528648376, 'learning_rate': 0.0002542590496121438, 'epoch': 3.9545454545454546}\n{'loss': 0.1523, 'grad_norm': 0.8925800919532776, 'learning_rate': 0.0002533067685274542, 'epoch': 3.9772727272727275}\n{'loss': 0.3406, 'grad_norm': 5.419142246246338, 'learning_rate': 0.0002523544874427645, 'epoch': 4.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8893    0.8870    0.8881      3142\n     PERCENT     0.9531    0.9242    0.9385        66\n        TYPE     0.9477    0.9693    0.9584     11415\n      VOLUME     0.9155    0.9286    0.9220        70\n\n   micro avg     0.9353    0.9513    0.9432     14693\n   macro avg     0.9264    0.9273    0.9267     14693\nweighted avg     0.9351    0.9513    0.9431     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.24124912917613983, 'eval_f1_macro': 0.9267436807914773, 'eval_precision': 0.9352960856473737, 'eval_recall': 0.9513373715374668, 'eval_f1': 0.9432485322896281, 'eval_accuracy': 0.9324191968658179, 'eval_runtime': 1.568, 'eval_samples_per_second': 3514.697, 'eval_steps_per_second': 7.015, 'epoch': 4.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.0793, 'grad_norm': 0.5665311217308044, 'learning_rate': 0.0002514022063580748, 'epoch': 4.0227272727272725}\n{'loss': 0.0751, 'grad_norm': 0.6360889673233032, 'learning_rate': 0.0002504499252733852, 'epoch': 4.045454545454546}\n{'loss': 0.0922, 'grad_norm': 0.7732618451118469, 'learning_rate': 0.0002494976441886955, 'epoch': 4.068181818181818}\n{'loss': 0.0816, 'grad_norm': 0.5988957285881042, 'learning_rate': 0.00024854536310400577, 'epoch': 4.090909090909091}\n{'loss': 0.0984, 'grad_norm': 1.5051323175430298, 'learning_rate': 0.00024759308201931614, 'epoch': 4.113636363636363}\n{'loss': 0.0893, 'grad_norm': 0.5755395889282227, 'learning_rate': 0.00024664080093462647, 'epoch': 4.136363636363637}\n{'loss': 0.0772, 'grad_norm': 0.9371432662010193, 'learning_rate': 0.00024568851984993673, 'epoch': 4.159090909090909}\n{'loss': 0.0924, 'grad_norm': 0.9269899725914001, 'learning_rate': 0.0002447362387652471, 'epoch': 4.181818181818182}\n{'loss': 0.0688, 'grad_norm': 0.6012246012687683, 'learning_rate': 0.00024378395768055744, 'epoch': 4.204545454545454}\n{'loss': 0.0844, 'grad_norm': 1.0379341840744019, 'learning_rate': 0.00024283167659586773, 'epoch': 4.2272727272727275}\n{'loss': 0.113, 'grad_norm': 2.278963565826416, 'learning_rate': 0.00024187939551117805, 'epoch': 4.25}\n{'loss': 0.0826, 'grad_norm': 1.151555061340332, 'learning_rate': 0.00024092711442648835, 'epoch': 4.2727272727272725}\n{'loss': 0.0674, 'grad_norm': 0.6750203967094421, 'learning_rate': 0.0002399748333417987, 'epoch': 4.295454545454546}\n{'loss': 0.0605, 'grad_norm': 0.5480443239212036, 'learning_rate': 0.00023902255225710902, 'epoch': 4.318181818181818}\n{'loss': 0.0866, 'grad_norm': 1.0797353982925415, 'learning_rate': 0.00023807027117241932, 'epoch': 4.340909090909091}\n{'loss': 0.0776, 'grad_norm': 1.26889967918396, 'learning_rate': 0.00023711799008772967, 'epoch': 4.363636363636363}\n{'loss': 0.0796, 'grad_norm': 0.9979124069213867, 'learning_rate': 0.00023616570900304002, 'epoch': 4.386363636363637}\n{'loss': 0.0557, 'grad_norm': 0.6146214008331299, 'learning_rate': 0.0002352134279183503, 'epoch': 4.409090909090909}\n{'loss': 0.0952, 'grad_norm': 1.3923819065093994, 'learning_rate': 0.00023426114683366064, 'epoch': 4.431818181818182}\n{'loss': 0.0681, 'grad_norm': 0.809845507144928, 'learning_rate': 0.000233308865748971, 'epoch': 4.454545454545454}\n{'loss': 0.0778, 'grad_norm': 1.068137526512146, 'learning_rate': 0.00023235658466428129, 'epoch': 4.4772727272727275}\n{'loss': 0.0805, 'grad_norm': 0.8636427521705627, 'learning_rate': 0.0002314043035795916, 'epoch': 4.5}\n{'loss': 0.0931, 'grad_norm': 1.2374666929244995, 'learning_rate': 0.00023045202249490196, 'epoch': 4.5227272727272725}\n{'loss': 0.0632, 'grad_norm': 1.1450084447860718, 'learning_rate': 0.00022949974141021226, 'epoch': 4.545454545454545}\n{'loss': 0.1056, 'grad_norm': 1.0342968702316284, 'learning_rate': 0.00022854746032552258, 'epoch': 4.568181818181818}\n{'loss': 0.0896, 'grad_norm': 0.7785072922706604, 'learning_rate': 0.00022759517924083287, 'epoch': 4.590909090909091}\n{'loss': 0.0834, 'grad_norm': 1.1089624166488647, 'learning_rate': 0.00022664289815614323, 'epoch': 4.613636363636363}\n{'loss': 0.1059, 'grad_norm': 0.8733707666397095, 'learning_rate': 0.00022569061707145355, 'epoch': 4.636363636363637}\n{'loss': 0.0734, 'grad_norm': 0.9574121236801147, 'learning_rate': 0.00022473833598676384, 'epoch': 4.659090909090909}\n{'loss': 0.0527, 'grad_norm': 0.5426391959190369, 'learning_rate': 0.0002237860549020742, 'epoch': 4.681818181818182}\n{'loss': 0.0527, 'grad_norm': 1.347515344619751, 'learning_rate': 0.00022283377381738452, 'epoch': 4.704545454545455}\n{'loss': 0.0782, 'grad_norm': 0.6134058237075806, 'learning_rate': 0.00022188149273269481, 'epoch': 4.7272727272727275}\n{'loss': 0.0688, 'grad_norm': 0.5294644236564636, 'learning_rate': 0.00022092921164800516, 'epoch': 4.75}\n{'loss': 0.0605, 'grad_norm': 0.7604065537452698, 'learning_rate': 0.0002199769305633155, 'epoch': 4.7727272727272725}\n{'loss': 0.0729, 'grad_norm': 0.8622555136680603, 'learning_rate': 0.00021902464947862578, 'epoch': 4.795454545454545}\n{'loss': 0.0711, 'grad_norm': 0.714697539806366, 'learning_rate': 0.00021807236839393613, 'epoch': 4.818181818181818}\n{'loss': 0.0713, 'grad_norm': 1.0046032667160034, 'learning_rate': 0.00021712008730924646, 'epoch': 4.840909090909091}\n{'loss': 0.0978, 'grad_norm': 0.8399959206581116, 'learning_rate': 0.00021616780622455675, 'epoch': 4.863636363636363}\n{'loss': 0.1176, 'grad_norm': 0.789323627948761, 'learning_rate': 0.0002152155251398671, 'epoch': 4.886363636363637}\n{'loss': 0.0626, 'grad_norm': 0.8173129558563232, 'learning_rate': 0.00021426324405517743, 'epoch': 4.909090909090909}\n{'loss': 0.0713, 'grad_norm': 0.785362958908081, 'learning_rate': 0.00021331096297048772, 'epoch': 4.931818181818182}\n{'loss': 0.085, 'grad_norm': 1.1270248889923096, 'learning_rate': 0.00021235868188579807, 'epoch': 4.954545454545455}\n{'loss': 0.0718, 'grad_norm': 1.1906640529632568, 'learning_rate': 0.00021140640080110837, 'epoch': 4.9772727272727275}\n{'loss': 0.0841, 'grad_norm': 3.3998515605926514, 'learning_rate': 0.0002104541197164187, 'epoch': 5.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8767    0.8988    0.8876      3142\n     PERCENT     0.8971    0.9242    0.9104        66\n        TYPE     0.9540    0.9593    0.9566     11415\n      VOLUME     0.9306    0.9571    0.9437        70\n\n   micro avg     0.9369    0.9462    0.9415     14693\n   macro avg     0.9146    0.9349    0.9246     14693\nweighted avg     0.9371    0.9462    0.9416     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.27065712213516235, 'eval_f1_macro': 0.9245914122415311, 'eval_precision': 0.9368555832603275, 'eval_recall': 0.9461648404001906, 'eval_f1': 0.9414872003250712, 'eval_accuracy': 0.9305147458918271, 'eval_runtime': 1.5683, 'eval_samples_per_second': 3514.009, 'eval_steps_per_second': 7.014, 'epoch': 5.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.061, 'grad_norm': 0.696844220161438, 'learning_rate': 0.00020950183863172904, 'epoch': 5.0227272727272725}\n{'loss': 0.0668, 'grad_norm': 1.183020830154419, 'learning_rate': 0.00020854955754703934, 'epoch': 5.045454545454546}\n{'loss': 0.0712, 'grad_norm': 0.5849869251251221, 'learning_rate': 0.00020759727646234966, 'epoch': 5.068181818181818}\n{'loss': 0.0498, 'grad_norm': 0.7295658588409424, 'learning_rate': 0.00020664499537766, 'epoch': 5.090909090909091}\n{'loss': 0.059, 'grad_norm': 0.6994723677635193, 'learning_rate': 0.0002056927142929703, 'epoch': 5.113636363636363}\n{'loss': 0.0702, 'grad_norm': 0.8442686200141907, 'learning_rate': 0.00020474043320828063, 'epoch': 5.136363636363637}\n{'loss': 0.0436, 'grad_norm': 0.5479183793067932, 'learning_rate': 0.00020378815212359098, 'epoch': 5.159090909090909}\n{'loss': 0.0628, 'grad_norm': 0.9441989064216614, 'learning_rate': 0.00020283587103890128, 'epoch': 5.181818181818182}\n{'loss': 0.0405, 'grad_norm': 0.6623688340187073, 'learning_rate': 0.0002018835899542116, 'epoch': 5.204545454545454}\n{'loss': 0.0544, 'grad_norm': 1.2994916439056396, 'learning_rate': 0.00020093130886952195, 'epoch': 5.2272727272727275}\n{'loss': 0.0583, 'grad_norm': 0.8420743346214294, 'learning_rate': 0.00019997902778483225, 'epoch': 5.25}\n{'loss': 0.0518, 'grad_norm': 0.7670789361000061, 'learning_rate': 0.00019902674670014257, 'epoch': 5.2727272727272725}\n{'loss': 0.0745, 'grad_norm': 0.8229972124099731, 'learning_rate': 0.00019807446561545292, 'epoch': 5.295454545454546}\n{'loss': 0.059, 'grad_norm': 0.6475308537483215, 'learning_rate': 0.00019712218453076322, 'epoch': 5.318181818181818}\n{'loss': 0.0764, 'grad_norm': 0.5501797795295715, 'learning_rate': 0.00019616990344607354, 'epoch': 5.340909090909091}\n{'loss': 0.0479, 'grad_norm': 0.8159408569335938, 'learning_rate': 0.00019521762236138383, 'epoch': 5.363636363636363}\n{'loss': 0.0606, 'grad_norm': 0.8851704001426697, 'learning_rate': 0.00019426534127669419, 'epoch': 5.386363636363637}\n{'loss': 0.0703, 'grad_norm': 0.9361596703529358, 'learning_rate': 0.0001933130601920045, 'epoch': 5.409090909090909}\n{'loss': 0.0404, 'grad_norm': 0.8004188537597656, 'learning_rate': 0.0001923607791073148, 'epoch': 5.431818181818182}\n{'loss': 0.0641, 'grad_norm': 0.8723959922790527, 'learning_rate': 0.00019140849802262515, 'epoch': 5.454545454545454}\n{'loss': 0.072, 'grad_norm': 1.3201017379760742, 'learning_rate': 0.00019045621693793548, 'epoch': 5.4772727272727275}\n{'loss': 0.037, 'grad_norm': 0.5135698318481445, 'learning_rate': 0.00018950393585324577, 'epoch': 5.5}\n{'loss': 0.0615, 'grad_norm': 1.0539321899414062, 'learning_rate': 0.00018855165476855612, 'epoch': 5.5227272727272725}\n{'loss': 0.0344, 'grad_norm': 0.8016236424446106, 'learning_rate': 0.00018759937368386645, 'epoch': 5.545454545454545}\n{'loss': 0.0399, 'grad_norm': 0.6969701051712036, 'learning_rate': 0.00018664709259917677, 'epoch': 5.568181818181818}\n{'loss': 0.0725, 'grad_norm': 1.0452353954315186, 'learning_rate': 0.0001856948115144871, 'epoch': 5.590909090909091}\n{'loss': 0.067, 'grad_norm': 1.286448359489441, 'learning_rate': 0.00018474253042979742, 'epoch': 5.613636363636363}\n{'loss': 0.0829, 'grad_norm': 0.9211083054542542, 'learning_rate': 0.00018379024934510774, 'epoch': 5.636363636363637}\n{'loss': 0.0529, 'grad_norm': 0.6512417197227478, 'learning_rate': 0.00018283796826041806, 'epoch': 5.659090909090909}\n{'loss': 0.0578, 'grad_norm': 0.7464086413383484, 'learning_rate': 0.00018188568717572839, 'epoch': 5.681818181818182}\n{'loss': 0.054, 'grad_norm': 0.6237043142318726, 'learning_rate': 0.0001809334060910387, 'epoch': 5.704545454545455}\n{'loss': 0.069, 'grad_norm': 0.7656354904174805, 'learning_rate': 0.00017998112500634903, 'epoch': 5.7272727272727275}\n{'loss': 0.0602, 'grad_norm': 0.9505595564842224, 'learning_rate': 0.00017902884392165936, 'epoch': 5.75}\n{'loss': 0.0685, 'grad_norm': 1.1346399784088135, 'learning_rate': 0.00017807656283696968, 'epoch': 5.7727272727272725}\n{'loss': 0.0782, 'grad_norm': 1.499958872795105, 'learning_rate': 0.00017712428175228, 'epoch': 5.795454545454545}\n{'loss': 0.0667, 'grad_norm': 0.8335992693901062, 'learning_rate': 0.00017617200066759033, 'epoch': 5.818181818181818}\n{'loss': 0.0483, 'grad_norm': 0.8511818647384644, 'learning_rate': 0.00017521971958290065, 'epoch': 5.840909090909091}\n{'loss': 0.0624, 'grad_norm': 0.8185830116271973, 'learning_rate': 0.00017426743849821094, 'epoch': 5.863636363636363}\n{'loss': 0.0836, 'grad_norm': 0.9754956364631653, 'learning_rate': 0.0001733151574135213, 'epoch': 5.886363636363637}\n{'loss': 0.0678, 'grad_norm': 0.8478068709373474, 'learning_rate': 0.00017236287632883162, 'epoch': 5.909090909090909}\n{'loss': 0.0507, 'grad_norm': 1.193485975265503, 'learning_rate': 0.00017141059524414191, 'epoch': 5.931818181818182}\n{'loss': 0.067, 'grad_norm': 0.7576127648353577, 'learning_rate': 0.00017045831415945226, 'epoch': 5.954545454545455}\n{'loss': 0.0772, 'grad_norm': 0.7972515225410461, 'learning_rate': 0.0001695060330747626, 'epoch': 5.9772727272727275}\n{'loss': 0.0101, 'grad_norm': 0.5220274925231934, 'learning_rate': 0.00016855375199007288, 'epoch': 6.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8938    0.8816    0.8877      3142\n     PERCENT     0.8857    0.9394    0.9118        66\n        TYPE     0.9513    0.9634    0.9573     11415\n      VOLUME     0.9155    0.9286    0.9220        70\n\n   micro avg     0.9388    0.9456    0.9422     14693\n   macro avg     0.9116    0.9282    0.9197     14693\nweighted avg     0.9385    0.9456    0.9420     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.2854701280593872, 'eval_f1_macro': 0.9196825481889723, 'eval_precision': 0.9387837837837838, 'eval_recall': 0.945620363438372, 'eval_f1': 0.9421896721255891, 'eval_accuracy': 0.9299706170421156, 'eval_runtime': 1.5059, 'eval_samples_per_second': 3659.608, 'eval_steps_per_second': 7.305, 'epoch': 6.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.0402, 'grad_norm': 0.9781439900398254, 'learning_rate': 0.0001676014709053832, 'epoch': 6.0227272727272725}\n{'loss': 0.0503, 'grad_norm': 0.9234228134155273, 'learning_rate': 0.00016664918982069356, 'epoch': 6.045454545454546}\n{'loss': 0.0465, 'grad_norm': 0.9552072286605835, 'learning_rate': 0.00016569690873600385, 'epoch': 6.068181818181818}\n{'loss': 0.059, 'grad_norm': 0.7400855422019958, 'learning_rate': 0.00016474462765131418, 'epoch': 6.090909090909091}\n{'loss': 0.0539, 'grad_norm': 0.777160108089447, 'learning_rate': 0.00016379234656662453, 'epoch': 6.113636363636363}\n{'loss': 0.0451, 'grad_norm': 0.6584670543670654, 'learning_rate': 0.00016284006548193485, 'epoch': 6.136363636363637}\n{'loss': 0.045, 'grad_norm': 0.6014907360076904, 'learning_rate': 0.00016188778439724515, 'epoch': 6.159090909090909}\n{'loss': 0.0423, 'grad_norm': 0.6131636500358582, 'learning_rate': 0.00016093550331255547, 'epoch': 6.181818181818182}\n{'loss': 0.0357, 'grad_norm': 0.4802744686603546, 'learning_rate': 0.00015998322222786582, 'epoch': 6.204545454545454}\n{'loss': 0.0347, 'grad_norm': 0.6834214925765991, 'learning_rate': 0.00015903094114317611, 'epoch': 6.2272727272727275}\n{'loss': 0.0362, 'grad_norm': 1.1818017959594727, 'learning_rate': 0.00015807866005848644, 'epoch': 6.25}\n{'loss': 0.0758, 'grad_norm': 0.7420693635940552, 'learning_rate': 0.0001571263789737968, 'epoch': 6.2727272727272725}\n{'loss': 0.0527, 'grad_norm': 0.629496157169342, 'learning_rate': 0.00015617409788910708, 'epoch': 6.295454545454546}\n{'loss': 0.0427, 'grad_norm': 0.7710106372833252, 'learning_rate': 0.0001552218168044174, 'epoch': 6.318181818181818}\n{'loss': 0.0632, 'grad_norm': 1.1684119701385498, 'learning_rate': 0.00015426953571972776, 'epoch': 6.340909090909091}\n{'loss': 0.0348, 'grad_norm': 0.3944353461265564, 'learning_rate': 0.00015331725463503805, 'epoch': 6.363636363636363}\n{'loss': 0.038, 'grad_norm': 0.6347470879554749, 'learning_rate': 0.00015236497355034838, 'epoch': 6.386363636363637}\n{'loss': 0.0445, 'grad_norm': 0.6033290028572083, 'learning_rate': 0.0001514126924656587, 'epoch': 6.409090909090909}\n{'loss': 0.0375, 'grad_norm': 0.5783697366714478, 'learning_rate': 0.00015046041138096902, 'epoch': 6.431818181818182}\n{'loss': 0.0285, 'grad_norm': 0.6317901015281677, 'learning_rate': 0.00014950813029627935, 'epoch': 6.454545454545454}\n{'loss': 0.0609, 'grad_norm': 1.134364366531372, 'learning_rate': 0.00014855584921158967, 'epoch': 6.4772727272727275}\n{'loss': 0.0271, 'grad_norm': 0.6440700888633728, 'learning_rate': 0.0001476035681269, 'epoch': 6.5}\n{'loss': 0.0547, 'grad_norm': 0.6963779926300049, 'learning_rate': 0.00014665128704221032, 'epoch': 6.5227272727272725}\n{'loss': 0.0307, 'grad_norm': 0.6571716070175171, 'learning_rate': 0.00014569900595752064, 'epoch': 6.545454545454545}\n{'loss': 0.0607, 'grad_norm': 0.9097073078155518, 'learning_rate': 0.00014474672487283096, 'epoch': 6.568181818181818}\n{'loss': 0.06, 'grad_norm': 0.8531294465065002, 'learning_rate': 0.00014379444378814129, 'epoch': 6.590909090909091}\n{'loss': 0.0354, 'grad_norm': 0.7374951839447021, 'learning_rate': 0.0001428421627034516, 'epoch': 6.613636363636363}\n{'loss': 0.0518, 'grad_norm': 0.788582444190979, 'learning_rate': 0.00014188988161876193, 'epoch': 6.636363636363637}\n{'loss': 0.0468, 'grad_norm': 0.9132254719734192, 'learning_rate': 0.00014093760053407225, 'epoch': 6.659090909090909}\n{'loss': 0.0407, 'grad_norm': 0.760590136051178, 'learning_rate': 0.00013998531944938258, 'epoch': 6.681818181818182}\n{'loss': 0.0311, 'grad_norm': 0.42266035079956055, 'learning_rate': 0.0001390330383646929, 'epoch': 6.704545454545455}\n{'loss': 0.037, 'grad_norm': 0.47803857922554016, 'learning_rate': 0.00013808075728000322, 'epoch': 6.7272727272727275}\n{'loss': 0.0465, 'grad_norm': 1.0937834978103638, 'learning_rate': 0.00013712847619531355, 'epoch': 6.75}\n{'loss': 0.0539, 'grad_norm': 0.759456992149353, 'learning_rate': 0.00013617619511062387, 'epoch': 6.7727272727272725}\n{'loss': 0.0636, 'grad_norm': 1.0536633729934692, 'learning_rate': 0.0001352239140259342, 'epoch': 6.795454545454545}\n{'loss': 0.052, 'grad_norm': 0.6234291791915894, 'learning_rate': 0.00013427163294124452, 'epoch': 6.818181818181818}\n{'loss': 0.0437, 'grad_norm': 0.5240302681922913, 'learning_rate': 0.00013331935185655484, 'epoch': 6.840909090909091}\n{'loss': 0.0478, 'grad_norm': 1.282777190208435, 'learning_rate': 0.00013236707077186516, 'epoch': 6.863636363636363}\n{'loss': 0.0579, 'grad_norm': 0.6869136095046997, 'learning_rate': 0.00013141478968717549, 'epoch': 6.886363636363637}\n{'loss': 0.0439, 'grad_norm': 0.5467973351478577, 'learning_rate': 0.0001304625086024858, 'epoch': 6.909090909090909}\n{'loss': 0.041, 'grad_norm': 0.6837835311889648, 'learning_rate': 0.00012951022751779613, 'epoch': 6.931818181818182}\n{'loss': 0.0572, 'grad_norm': 0.8128502368927002, 'learning_rate': 0.00012855794643310643, 'epoch': 6.954545454545455}\n{'loss': 0.0515, 'grad_norm': 0.8625847697257996, 'learning_rate': 0.00012760566534841678, 'epoch': 6.9772727272727275}\n{'loss': 0.0091, 'grad_norm': 0.5714216232299805, 'learning_rate': 0.0001266533842637271, 'epoch': 7.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8713    0.9010    0.8859      3142\n     PERCENT     0.8986    0.9394    0.9185        66\n        TYPE     0.9531    0.9606    0.9568     11415\n      VOLUME     0.9143    0.9143    0.9143        70\n\n   micro avg     0.9349    0.9475    0.9412     14693\n   macro avg     0.9093    0.9288    0.9189     14693\nweighted avg     0.9352    0.9475    0.9413     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.31166309118270874, 'eval_f1_macro': 0.9188964016469159, 'eval_precision': 0.9348643567015847, 'eval_recall': 0.9475260328047369, 'eval_f1': 0.9411526111205002, 'eval_accuracy': 0.9293720753074328, 'eval_runtime': 1.4835, 'eval_samples_per_second': 3714.911, 'eval_steps_per_second': 7.415, 'epoch': 7.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.0324, 'grad_norm': 0.6708340048789978, 'learning_rate': 0.0001257011031790374, 'epoch': 7.0227272727272725}\n{'loss': 0.0361, 'grad_norm': 0.5050141215324402, 'learning_rate': 0.00012474882209434775, 'epoch': 7.045454545454546}\n{'loss': 0.0461, 'grad_norm': 0.5186808109283447, 'learning_rate': 0.00012379654100965807, 'epoch': 7.068181818181818}\n{'loss': 0.045, 'grad_norm': 0.7085420489311218, 'learning_rate': 0.00012284425992496837, 'epoch': 7.090909090909091}\n{'loss': 0.0386, 'grad_norm': 0.8146044611930847, 'learning_rate': 0.00012189197884027872, 'epoch': 7.113636363636363}\n{'loss': 0.0248, 'grad_norm': 0.6152453422546387, 'learning_rate': 0.00012093969775558903, 'epoch': 7.136363636363637}\n{'loss': 0.0269, 'grad_norm': 0.4255545735359192, 'learning_rate': 0.00011998741667089935, 'epoch': 7.159090909090909}\n{'loss': 0.0231, 'grad_norm': 0.5032510161399841, 'learning_rate': 0.00011903513558620966, 'epoch': 7.181818181818182}\n{'loss': 0.0475, 'grad_norm': 0.8755000829696655, 'learning_rate': 0.00011808285450152001, 'epoch': 7.204545454545454}\n{'loss': 0.0428, 'grad_norm': 0.6191189289093018, 'learning_rate': 0.00011713057341683032, 'epoch': 7.2272727272727275}\n{'loss': 0.0359, 'grad_norm': 0.7519473433494568, 'learning_rate': 0.00011617829233214064, 'epoch': 7.25}\n{'loss': 0.0517, 'grad_norm': 0.8020862936973572, 'learning_rate': 0.00011522601124745098, 'epoch': 7.2727272727272725}\n{'loss': 0.0353, 'grad_norm': 0.6167593598365784, 'learning_rate': 0.00011427373016276129, 'epoch': 7.295454545454546}\n{'loss': 0.0426, 'grad_norm': 0.7976933121681213, 'learning_rate': 0.00011332144907807161, 'epoch': 7.318181818181818}\n{'loss': 0.0376, 'grad_norm': 0.5909169912338257, 'learning_rate': 0.00011236916799338192, 'epoch': 7.340909090909091}\n{'loss': 0.0529, 'grad_norm': 0.7325011491775513, 'learning_rate': 0.00011141688690869226, 'epoch': 7.363636363636363}\n{'loss': 0.0412, 'grad_norm': 0.7965472936630249, 'learning_rate': 0.00011046460582400258, 'epoch': 7.386363636363637}\n{'loss': 0.0527, 'grad_norm': 0.9463018774986267, 'learning_rate': 0.00010951232473931289, 'epoch': 7.409090909090909}\n{'loss': 0.0298, 'grad_norm': 0.7941649556159973, 'learning_rate': 0.00010856004365462323, 'epoch': 7.431818181818182}\n{'loss': 0.0335, 'grad_norm': 0.6900964379310608, 'learning_rate': 0.00010760776256993355, 'epoch': 7.454545454545454}\n{'loss': 0.0206, 'grad_norm': 0.4408281743526459, 'learning_rate': 0.00010665548148524386, 'epoch': 7.4772727272727275}\n{'loss': 0.0274, 'grad_norm': 0.7072586417198181, 'learning_rate': 0.00010570320040055418, 'epoch': 7.5}\n{'loss': 0.0477, 'grad_norm': 0.5032645463943481, 'learning_rate': 0.00010475091931586452, 'epoch': 7.5227272727272725}\n{'loss': 0.0459, 'grad_norm': 1.1121519804000854, 'learning_rate': 0.00010379863823117483, 'epoch': 7.545454545454545}\n{'loss': 0.0402, 'grad_norm': 0.7595964074134827, 'learning_rate': 0.00010284635714648515, 'epoch': 7.568181818181818}\n{'loss': 0.0272, 'grad_norm': 0.6412326693534851, 'learning_rate': 0.00010189407606179549, 'epoch': 7.590909090909091}\n{'loss': 0.0438, 'grad_norm': 0.6587435007095337, 'learning_rate': 0.0001009417949771058, 'epoch': 7.613636363636363}\n{'loss': 0.0211, 'grad_norm': 0.5124600529670715, 'learning_rate': 9.998951389241612e-05, 'epoch': 7.636363636363637}\n{'loss': 0.0368, 'grad_norm': 0.6887394189834595, 'learning_rate': 9.903723280772646e-05, 'epoch': 7.659090909090909}\n{'loss': 0.0208, 'grad_norm': 0.4498266875743866, 'learning_rate': 9.808495172303677e-05, 'epoch': 7.681818181818182}\n{'loss': 0.0376, 'grad_norm': 0.7182118892669678, 'learning_rate': 9.713267063834709e-05, 'epoch': 7.704545454545455}\n{'loss': 0.0231, 'grad_norm': 0.552560031414032, 'learning_rate': 9.61803895536574e-05, 'epoch': 7.7272727272727275}\n{'loss': 0.0234, 'grad_norm': 0.5623637437820435, 'learning_rate': 9.522810846896774e-05, 'epoch': 7.75}\n{'loss': 0.053, 'grad_norm': 0.8251316547393799, 'learning_rate': 9.427582738427806e-05, 'epoch': 7.7727272727272725}\n{'loss': 0.038, 'grad_norm': 0.7593976259231567, 'learning_rate': 9.332354629958839e-05, 'epoch': 7.795454545454545}\n{'loss': 0.0259, 'grad_norm': 0.6280046105384827, 'learning_rate': 9.237126521489871e-05, 'epoch': 7.818181818181818}\n{'loss': 0.0326, 'grad_norm': 0.661921501159668, 'learning_rate': 9.141898413020903e-05, 'epoch': 7.840909090909091}\n{'loss': 0.045, 'grad_norm': 0.453343003988266, 'learning_rate': 9.046670304551935e-05, 'epoch': 7.863636363636363}\n{'loss': 0.0259, 'grad_norm': 0.5854523777961731, 'learning_rate': 8.951442196082968e-05, 'epoch': 7.886363636363637}\n{'loss': 0.0249, 'grad_norm': 0.44076040387153625, 'learning_rate': 8.856214087614e-05, 'epoch': 7.909090909090909}\n{'loss': 0.0443, 'grad_norm': 1.1166452169418335, 'learning_rate': 8.760985979145032e-05, 'epoch': 7.931818181818182}\n{'loss': 0.0399, 'grad_norm': 0.7248950004577637, 'learning_rate': 8.665757870676065e-05, 'epoch': 7.954545454545455}\n{'loss': 0.0529, 'grad_norm': 0.9963213205337524, 'learning_rate': 8.570529762207096e-05, 'epoch': 7.9772727272727275}\n{'loss': 0.0061, 'grad_norm': 0.6488953828811646, 'learning_rate': 8.47530165373813e-05, 'epoch': 8.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8877    0.8956    0.8916      3142\n     PERCENT     0.9130    0.9545    0.9333        66\n        TYPE     0.9522    0.9639    0.9580     11415\n      VOLUME     0.9429    0.9429    0.9429        70\n\n   micro avg     0.9382    0.9492    0.9437     14693\n   macro avg     0.9240    0.9392    0.9315     14693\nweighted avg     0.9382    0.9492    0.9437     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.3184155821800232, 'eval_f1_macro': 0.9314644182782239, 'eval_precision': 0.9382400430570506, 'eval_recall': 0.9491594636901927, 'eval_f1': 0.9436681665933621, 'eval_accuracy': 0.9313853520513657, 'eval_runtime': 1.9305, 'eval_samples_per_second': 2854.731, 'eval_steps_per_second': 5.698, 'epoch': 8.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.0226, 'grad_norm': 0.4530157446861267, 'learning_rate': 8.38007354526916e-05, 'epoch': 8.022727272727273}\n{'loss': 0.0351, 'grad_norm': 0.8263549208641052, 'learning_rate': 8.284845436800193e-05, 'epoch': 8.045454545454545}\n{'loss': 0.0281, 'grad_norm': 1.09102201461792, 'learning_rate': 8.189617328331226e-05, 'epoch': 8.068181818181818}\n{'loss': 0.0529, 'grad_norm': 0.9396995902061462, 'learning_rate': 8.094389219862257e-05, 'epoch': 8.090909090909092}\n{'loss': 0.0259, 'grad_norm': 0.6732105016708374, 'learning_rate': 7.999161111393291e-05, 'epoch': 8.113636363636363}\n{'loss': 0.0406, 'grad_norm': 0.7873594760894775, 'learning_rate': 7.903933002924322e-05, 'epoch': 8.136363636363637}\n{'loss': 0.0249, 'grad_norm': 1.2573771476745605, 'learning_rate': 7.808704894455354e-05, 'epoch': 8.159090909090908}\n{'loss': 0.0359, 'grad_norm': 0.9189605712890625, 'learning_rate': 7.713476785986388e-05, 'epoch': 8.181818181818182}\n{'loss': 0.031, 'grad_norm': 0.6025394797325134, 'learning_rate': 7.618248677517419e-05, 'epoch': 8.204545454545455}\n{'loss': 0.0266, 'grad_norm': 0.7794604301452637, 'learning_rate': 7.523020569048451e-05, 'epoch': 8.227272727272727}\n{'loss': 0.0296, 'grad_norm': 0.7081425189971924, 'learning_rate': 7.427792460579483e-05, 'epoch': 8.25}\n{'loss': 0.0202, 'grad_norm': 0.36532706022262573, 'learning_rate': 7.332564352110516e-05, 'epoch': 8.272727272727273}\n{'loss': 0.0419, 'grad_norm': 0.7130240797996521, 'learning_rate': 7.237336243641548e-05, 'epoch': 8.295454545454545}\n{'loss': 0.0339, 'grad_norm': 0.6402568221092224, 'learning_rate': 7.14210813517258e-05, 'epoch': 8.318181818181818}\n{'loss': 0.0282, 'grad_norm': 0.7636922001838684, 'learning_rate': 7.046880026703613e-05, 'epoch': 8.340909090909092}\n{'loss': 0.0304, 'grad_norm': 1.057486653327942, 'learning_rate': 6.951651918234645e-05, 'epoch': 8.363636363636363}\n{'loss': 0.0109, 'grad_norm': 0.28252363204956055, 'learning_rate': 6.856423809765677e-05, 'epoch': 8.386363636363637}\n{'loss': 0.0199, 'grad_norm': 0.4028630256652832, 'learning_rate': 6.76119570129671e-05, 'epoch': 8.409090909090908}\n{'loss': 0.0295, 'grad_norm': 0.4911702275276184, 'learning_rate': 6.665967592827742e-05, 'epoch': 8.431818181818182}\n{'loss': 0.0228, 'grad_norm': 0.49751588702201843, 'learning_rate': 6.570739484358774e-05, 'epoch': 8.454545454545455}\n{'loss': 0.0274, 'grad_norm': 0.8605141639709473, 'learning_rate': 6.475511375889807e-05, 'epoch': 8.477272727272727}\n{'loss': 0.0183, 'grad_norm': 0.4703028202056885, 'learning_rate': 6.380283267420839e-05, 'epoch': 8.5}\n{'loss': 0.0268, 'grad_norm': 0.5400142073631287, 'learning_rate': 6.28505515895187e-05, 'epoch': 8.522727272727273}\n{'loss': 0.042, 'grad_norm': 0.5011182427406311, 'learning_rate': 6.189827050482904e-05, 'epoch': 8.545454545454545}\n{'loss': 0.0454, 'grad_norm': 0.636526882648468, 'learning_rate': 6.094598942013936e-05, 'epoch': 8.568181818181818}\n{'loss': 0.0347, 'grad_norm': 0.39592501521110535, 'learning_rate': 5.9993708335449675e-05, 'epoch': 8.590909090909092}\n{'loss': 0.0245, 'grad_norm': 0.5368036031723022, 'learning_rate': 5.9041427250760005e-05, 'epoch': 8.613636363636363}\n{'loss': 0.0268, 'grad_norm': 0.6350366473197937, 'learning_rate': 5.808914616607032e-05, 'epoch': 8.636363636363637}\n{'loss': 0.0285, 'grad_norm': 0.892418622970581, 'learning_rate': 5.7136865081380645e-05, 'epoch': 8.659090909090908}\n{'loss': 0.0478, 'grad_norm': 0.5402267575263977, 'learning_rate': 5.618458399669096e-05, 'epoch': 8.681818181818182}\n{'loss': 0.0214, 'grad_norm': 0.4381004869937897, 'learning_rate': 5.523230291200129e-05, 'epoch': 8.704545454545455}\n{'loss': 0.0344, 'grad_norm': 0.7872346639633179, 'learning_rate': 5.4280021827311614e-05, 'epoch': 8.727272727272727}\n{'loss': 0.0457, 'grad_norm': 0.7669004797935486, 'learning_rate': 5.332774074262193e-05, 'epoch': 8.75}\n{'loss': 0.0453, 'grad_norm': 0.5629024505615234, 'learning_rate': 5.237545965793226e-05, 'epoch': 8.772727272727273}\n{'loss': 0.0383, 'grad_norm': 0.5564406514167786, 'learning_rate': 5.142317857324258e-05, 'epoch': 8.795454545454545}\n{'loss': 0.0197, 'grad_norm': 0.5445935726165771, 'learning_rate': 5.04708974885529e-05, 'epoch': 8.818181818181818}\n{'loss': 0.0482, 'grad_norm': 0.6670262217521667, 'learning_rate': 4.951861640386323e-05, 'epoch': 8.840909090909092}\n{'loss': 0.0269, 'grad_norm': 0.6129674911499023, 'learning_rate': 4.8566335319173546e-05, 'epoch': 8.863636363636363}\n{'loss': 0.0357, 'grad_norm': 0.6555424928665161, 'learning_rate': 4.761405423448387e-05, 'epoch': 8.886363636363637}\n{'loss': 0.0234, 'grad_norm': 1.028170108795166, 'learning_rate': 4.666177314979419e-05, 'epoch': 8.909090909090908}\n{'loss': 0.0333, 'grad_norm': 0.6040155291557312, 'learning_rate': 4.5709492065104516e-05, 'epoch': 8.931818181818182}\n{'loss': 0.0384, 'grad_norm': 0.5673290491104126, 'learning_rate': 4.475721098041484e-05, 'epoch': 8.954545454545455}\n{'loss': 0.0159, 'grad_norm': 0.3366406261920929, 'learning_rate': 4.380492989572516e-05, 'epoch': 8.977272727272727}\n{'loss': 0.0041, 'grad_norm': 0.32320907711982727, 'learning_rate': 4.285264881103548e-05, 'epoch': 9.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8732    0.9032    0.8880      3142\n     PERCENT     0.9265    0.9545    0.9403        66\n        TYPE     0.9526    0.9645    0.9585     11415\n      VOLUME     0.9296    0.9429    0.9362        70\n\n   micro avg     0.9351    0.9513    0.9431     14693\n   macro avg     0.9205    0.9413    0.9307     14693\nweighted avg     0.9354    0.9513    0.9432     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.3287074565887451, 'eval_f1_macro': 0.9307425552118769, 'eval_precision': 0.9351040342543654, 'eval_recall': 0.9512693119172395, 'eval_f1': 0.9431174089068824, 'eval_accuracy': 0.9317662422461639, 'eval_runtime': 1.5364, 'eval_samples_per_second': 3587.006, 'eval_steps_per_second': 7.16, 'epoch': 9.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.0386, 'grad_norm': 0.9939125776290894, 'learning_rate': 4.19003677263458e-05, 'epoch': 9.022727272727273}\n{'loss': 0.0246, 'grad_norm': 0.4742502272129059, 'learning_rate': 4.094808664165613e-05, 'epoch': 9.045454545454545}\n{'loss': 0.018, 'grad_norm': 0.43369215726852417, 'learning_rate': 3.9995805556966455e-05, 'epoch': 9.068181818181818}\n{'loss': 0.0223, 'grad_norm': 0.4259803891181946, 'learning_rate': 3.904352447227677e-05, 'epoch': 9.090909090909092}\n{'loss': 0.0274, 'grad_norm': 0.39183509349823, 'learning_rate': 3.8091243387587094e-05, 'epoch': 9.113636363636363}\n{'loss': 0.0166, 'grad_norm': 0.6597934365272522, 'learning_rate': 3.713896230289742e-05, 'epoch': 9.136363636363637}\n{'loss': 0.0437, 'grad_norm': 0.6148281693458557, 'learning_rate': 3.618668121820774e-05, 'epoch': 9.159090909090908}\n{'loss': 0.0281, 'grad_norm': 0.6082212328910828, 'learning_rate': 3.5234400133518064e-05, 'epoch': 9.181818181818182}\n{'loss': 0.0208, 'grad_norm': 0.7495087385177612, 'learning_rate': 3.428211904882839e-05, 'epoch': 9.204545454545455}\n{'loss': 0.0286, 'grad_norm': 0.5973825454711914, 'learning_rate': 3.332983796413871e-05, 'epoch': 9.227272727272727}\n{'loss': 0.0185, 'grad_norm': 0.5685213208198547, 'learning_rate': 3.237755687944903e-05, 'epoch': 9.25}\n{'loss': 0.0239, 'grad_norm': 0.3667941689491272, 'learning_rate': 3.142527579475935e-05, 'epoch': 9.272727272727273}\n{'loss': 0.0469, 'grad_norm': 1.1765265464782715, 'learning_rate': 3.047299471006968e-05, 'epoch': 9.295454545454545}\n{'loss': 0.0332, 'grad_norm': 0.3896133005619049, 'learning_rate': 2.9520713625380003e-05, 'epoch': 9.318181818181818}\n{'loss': 0.0258, 'grad_norm': 0.6386693716049194, 'learning_rate': 2.8568432540690322e-05, 'epoch': 9.340909090909092}\n{'loss': 0.0226, 'grad_norm': 0.7526205778121948, 'learning_rate': 2.7616151456000645e-05, 'epoch': 9.363636363636363}\n{'loss': 0.0202, 'grad_norm': 0.4750438928604126, 'learning_rate': 2.6663870371310965e-05, 'epoch': 9.386363636363637}\n{'loss': 0.032, 'grad_norm': 0.7333115935325623, 'learning_rate': 2.571158928662129e-05, 'epoch': 9.409090909090908}\n{'loss': 0.0182, 'grad_norm': 0.44192755222320557, 'learning_rate': 2.4759308201931615e-05, 'epoch': 9.431818181818182}\n{'loss': 0.0192, 'grad_norm': 0.5786036849021912, 'learning_rate': 2.3807027117241935e-05, 'epoch': 9.454545454545455}\n{'loss': 0.0247, 'grad_norm': 0.492947518825531, 'learning_rate': 2.2854746032552258e-05, 'epoch': 9.477272727272727}\n{'loss': 0.0339, 'grad_norm': 0.45651790499687195, 'learning_rate': 2.190246494786258e-05, 'epoch': 9.5}\n{'loss': 0.0465, 'grad_norm': 0.7369938492774963, 'learning_rate': 2.09501838631729e-05, 'epoch': 9.522727272727273}\n{'loss': 0.0169, 'grad_norm': 0.7252989411354065, 'learning_rate': 1.9997902778483227e-05, 'epoch': 9.545454545454545}\n{'loss': 0.0258, 'grad_norm': 0.4255337417125702, 'learning_rate': 1.9045621693793547e-05, 'epoch': 9.568181818181818}\n{'loss': 0.0277, 'grad_norm': 0.3948947489261627, 'learning_rate': 1.809334060910387e-05, 'epoch': 9.590909090909092}\n{'loss': 0.0227, 'grad_norm': 0.5035586357116699, 'learning_rate': 1.7141059524414193e-05, 'epoch': 9.613636363636363}\n{'loss': 0.0365, 'grad_norm': 0.7151075601577759, 'learning_rate': 1.6188778439724517e-05, 'epoch': 9.636363636363637}\n{'loss': 0.0169, 'grad_norm': 0.380420446395874, 'learning_rate': 1.523649735503484e-05, 'epoch': 9.659090909090908}\n{'loss': 0.0375, 'grad_norm': 0.593920111656189, 'learning_rate': 1.4284216270345161e-05, 'epoch': 9.681818181818182}\n{'loss': 0.0212, 'grad_norm': 0.7592674493789673, 'learning_rate': 1.3331935185655483e-05, 'epoch': 9.704545454545455}\n{'loss': 0.0363, 'grad_norm': 0.6695815324783325, 'learning_rate': 1.2379654100965807e-05, 'epoch': 9.727272727272727}\n{'loss': 0.0473, 'grad_norm': 1.0868866443634033, 'learning_rate': 1.1427373016276129e-05, 'epoch': 9.75}\n{'loss': 0.0345, 'grad_norm': 0.7631973624229431, 'learning_rate': 1.047509193158645e-05, 'epoch': 9.772727272727273}\n{'loss': 0.031, 'grad_norm': 0.5737376809120178, 'learning_rate': 9.522810846896774e-06, 'epoch': 9.795454545454545}\n{'loss': 0.0385, 'grad_norm': 0.4445171058177948, 'learning_rate': 8.570529762207097e-06, 'epoch': 9.818181818181818}\n{'loss': 0.0214, 'grad_norm': 0.49120932817459106, 'learning_rate': 7.61824867751742e-06, 'epoch': 9.840909090909092}\n{'loss': 0.0182, 'grad_norm': 0.44342631101608276, 'learning_rate': 6.665967592827741e-06, 'epoch': 9.863636363636363}\n{'loss': 0.0276, 'grad_norm': 0.425004243850708, 'learning_rate': 5.7136865081380645e-06, 'epoch': 9.886363636363637}\n{'loss': 0.0275, 'grad_norm': 0.8807634115219116, 'learning_rate': 4.761405423448387e-06, 'epoch': 9.909090909090908}\n{'loss': 0.0283, 'grad_norm': 0.7288313508033752, 'learning_rate': 3.80912433875871e-06, 'epoch': 9.931818181818182}\n{'loss': 0.0292, 'grad_norm': 0.8670858144760132, 'learning_rate': 2.8568432540690322e-06, 'epoch': 9.954545454545455}\n{'loss': 0.0139, 'grad_norm': 0.3468814194202423, 'learning_rate': 1.904562169379355e-06, 'epoch': 9.977272727272727}\n{'loss': 0.0058, 'grad_norm': 0.6808091998100281, 'learning_rate': 9.522810846896775e-07, 'epoch': 10.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8776    0.9013    0.8893      3142\n     PERCENT     0.9254    0.9394    0.9323        66\n        TYPE     0.9523    0.9644    0.9583     11415\n      VOLUME     0.9306    0.9571    0.9437        70\n\n   micro avg     0.9360    0.9508    0.9433     14693\n   macro avg     0.9215    0.9406    0.9309     14693\nweighted avg     0.9361    0.9508    0.9434     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.33269092440605164, 'eval_f1_macro': 0.9309116026995946, 'eval_precision': 0.935950690071017, 'eval_recall': 0.9507928945756483, 'eval_f1': 0.9433134136871604, 'eval_accuracy': 0.9318206551311351, 'eval_runtime': 1.4881, 'eval_samples_per_second': 3703.387, 'eval_steps_per_second': 7.392, 'epoch': 10.0}\n{'train_runtime': 66.39, 'train_samples_per_second': 3319.93, 'train_steps_per_second': 6.628, 'train_loss': 0.19928909980120477, 'epoch': 10.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8877    0.8956    0.8916      3142\n     PERCENT     0.9130    0.9545    0.9333        66\n        TYPE     0.9522    0.9639    0.9580     11415\n      VOLUME     0.9429    0.9429    0.9429        70\n\n   micro avg     0.9382    0.9492    0.9437     14693\n   macro avg     0.9240    0.9392    0.9315     14693\nweighted avg     0.9382    0.9492    0.9437     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.3184155821800232, 'eval_f1_macro': 0.9314644182782239, 'eval_precision': 0.9382400430570506, 'eval_recall': 0.9491594636901927, 'eval_f1': 0.9436681665933621, 'eval_accuracy': 0.9313853520513657, 'eval_runtime': 1.5402, 'eval_samples_per_second': 3578.019, 'eval_steps_per_second': 7.142, 'epoch': 10.0}\n","output_type":"stream"},{"name":"stderr","text":"The speedups for torchdynamo mostly come with GPU Ampere or higher and which is not detected here.\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/tmp/ipykernel_36/3364797558.py:66: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 2.2318, 'grad_norm': 7.58394193649292, 'learning_rate': 0.0, 'epoch': 0.022727272727272728}\n{'loss': 2.2298, 'grad_norm': 7.743690490722656, 'learning_rate': 8.570529762207097e-06, 'epoch': 0.045454545454545456}\n{'loss': 2.2094, 'grad_norm': 7.745577335357666, 'learning_rate': 1.7141059524414193e-05, 'epoch': 0.06818181818181818}\n{'loss': 2.1476, 'grad_norm': 7.399015426635742, 'learning_rate': 2.571158928662129e-05, 'epoch': 0.09090909090909091}\n{'loss': 2.1101, 'grad_norm': 7.272665023803711, 'learning_rate': 3.428211904882839e-05, 'epoch': 0.11363636363636363}\n{'loss': 2.015, 'grad_norm': 7.301682472229004, 'learning_rate': 4.285264881103548e-05, 'epoch': 0.13636363636363635}\n{'loss': 1.9366, 'grad_norm': 6.8502349853515625, 'learning_rate': 5.142317857324258e-05, 'epoch': 0.1590909090909091}\n{'loss': 1.8491, 'grad_norm': 6.114405155181885, 'learning_rate': 5.9993708335449675e-05, 'epoch': 0.18181818181818182}\n{'loss': 1.6942, 'grad_norm': 6.268580913543701, 'learning_rate': 6.856423809765677e-05, 'epoch': 0.20454545454545456}\n{'loss': 1.5932, 'grad_norm': 5.207488059997559, 'learning_rate': 7.713476785986388e-05, 'epoch': 0.22727272727272727}\n{'loss': 1.426, 'grad_norm': 4.733641147613525, 'learning_rate': 8.570529762207096e-05, 'epoch': 0.25}\n{'loss': 1.393, 'grad_norm': 3.3809027671813965, 'learning_rate': 9.427582738427806e-05, 'epoch': 0.2727272727272727}\n{'loss': 1.2884, 'grad_norm': 2.692594051361084, 'learning_rate': 0.00010284635714648515, 'epoch': 0.29545454545454547}\n{'loss': 1.1665, 'grad_norm': 2.0884315967559814, 'learning_rate': 0.00011141688690869226, 'epoch': 0.3181818181818182}\n{'loss': 1.2471, 'grad_norm': 2.352147102355957, 'learning_rate': 0.00011998741667089935, 'epoch': 0.3409090909090909}\n{'loss': 1.1712, 'grad_norm': 2.4430882930755615, 'learning_rate': 0.00012855794643310643, 'epoch': 0.36363636363636365}\n{'loss': 1.11, 'grad_norm': 2.463275671005249, 'learning_rate': 0.00013712847619531355, 'epoch': 0.38636363636363635}\n{'loss': 1.0926, 'grad_norm': 2.2996020317077637, 'learning_rate': 0.00014569900595752064, 'epoch': 0.4090909090909091}\n{'loss': 1.0354, 'grad_norm': 1.582688331604004, 'learning_rate': 0.00015426953571972776, 'epoch': 0.4318181818181818}\n{'loss': 1.0146, 'grad_norm': 1.1767786741256714, 'learning_rate': 0.00016284006548193485, 'epoch': 0.45454545454545453}\n{'loss': 0.9725, 'grad_norm': 1.6603591442108154, 'learning_rate': 0.00017141059524414191, 'epoch': 0.4772727272727273}\n{'loss': 0.9127, 'grad_norm': 2.5407931804656982, 'learning_rate': 0.00017998112500634903, 'epoch': 0.5}\n{'loss': 0.9188, 'grad_norm': 2.751678228378296, 'learning_rate': 0.00018855165476855612, 'epoch': 0.5227272727272727}\n{'loss': 0.9224, 'grad_norm': 1.7747166156768799, 'learning_rate': 0.00019712218453076322, 'epoch': 0.5454545454545454}\n{'loss': 0.8374, 'grad_norm': 1.229239583015442, 'learning_rate': 0.0002056927142929703, 'epoch': 0.5681818181818182}\n{'loss': 0.7461, 'grad_norm': 1.2128297090530396, 'learning_rate': 0.00021426324405517743, 'epoch': 0.5909090909090909}\n{'loss': 0.8076, 'grad_norm': 1.483148455619812, 'learning_rate': 0.00022283377381738452, 'epoch': 0.6136363636363636}\n{'loss': 0.709, 'grad_norm': 1.4203541278839111, 'learning_rate': 0.0002314043035795916, 'epoch': 0.6363636363636364}\n{'loss': 0.7401, 'grad_norm': 1.0850319862365723, 'learning_rate': 0.0002399748333417987, 'epoch': 0.6590909090909091}\n{'loss': 0.6966, 'grad_norm': 1.534541368484497, 'learning_rate': 0.00024854536310400577, 'epoch': 0.6818181818181818}\n{'loss': 0.6412, 'grad_norm': 1.596371054649353, 'learning_rate': 0.00025711589286621286, 'epoch': 0.7045454545454546}\n{'loss': 0.6636, 'grad_norm': 1.3906221389770508, 'learning_rate': 0.00026568642262842, 'epoch': 0.7272727272727273}\n{'loss': 0.5819, 'grad_norm': 0.6389135718345642, 'learning_rate': 0.0002742569523906271, 'epoch': 0.75}\n{'loss': 0.6046, 'grad_norm': 1.3818010091781616, 'learning_rate': 0.0002828274821528342, 'epoch': 0.7727272727272727}\n{'loss': 0.5627, 'grad_norm': 1.945827603340149, 'learning_rate': 0.0002913980119150413, 'epoch': 0.7954545454545454}\n{'loss': 0.5283, 'grad_norm': 1.1938064098358154, 'learning_rate': 0.00029996854167724837, 'epoch': 0.8181818181818182}\n{'loss': 0.5432, 'grad_norm': 2.5822205543518066, 'learning_rate': 0.0003085390714394555, 'epoch': 0.8409090909090909}\n{'loss': 0.569, 'grad_norm': 2.3250420093536377, 'learning_rate': 0.0003171096012016626, 'epoch': 0.8636363636363636}\n{'loss': 0.521, 'grad_norm': 2.6303818225860596, 'learning_rate': 0.0003256801309638697, 'epoch': 0.8863636363636364}\n{'loss': 0.527, 'grad_norm': 2.024815797805786, 'learning_rate': 0.00033425066072607674, 'epoch': 0.9090909090909091}\n{'loss': 0.5696, 'grad_norm': 2.036431074142456, 'learning_rate': 0.00034282119048828383, 'epoch': 0.9318181818181818}\n{'loss': 0.5118, 'grad_norm': 1.1597398519515991, 'learning_rate': 0.0003513917202504909, 'epoch': 0.9545454545454546}\n{'loss': 0.4813, 'grad_norm': 1.4758785963058472, 'learning_rate': 0.00035996225001269806, 'epoch': 0.9772727272727273}\n{'loss': 0.3256, 'grad_norm': 3.2977023124694824, 'learning_rate': 0.00036853277977490516, 'epoch': 1.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.7803    0.6959    0.7357      3404\n     PERCENT     0.6250    0.9155    0.7429        71\n        TYPE     0.8689    0.9511    0.9081     11194\n      VOLUME     0.0000    0.0000    0.0000        56\n\n   micro avg     0.8496    0.8884    0.8685     14725\n   macro avg     0.5685    0.6406    0.5967     14725\nweighted avg     0.8439    0.8884    0.8640     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.4441682994365692, 'eval_f1_macro': 0.596677145781547, 'eval_precision': 0.8495810872247841, 'eval_recall': 0.8883531409168082, 'eval_f1': 0.8685346258548569, 'eval_accuracy': 0.865592281971167, 'eval_runtime': 1.5315, 'eval_samples_per_second': 3598.378, 'eval_steps_per_second': 7.182, 'epoch': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.4039, 'grad_norm': 1.3026583194732666, 'learning_rate': 0.00037710330953711225, 'epoch': 1.0227272727272727}\n{'loss': 0.3772, 'grad_norm': 1.048647165298462, 'learning_rate': 0.00037615102845242257, 'epoch': 1.0454545454545454}\n{'loss': 0.3925, 'grad_norm': 1.5314258337020874, 'learning_rate': 0.0003751987473677329, 'epoch': 1.0681818181818181}\n{'loss': 0.3669, 'grad_norm': 1.5253944396972656, 'learning_rate': 0.0003742464662830432, 'epoch': 1.0909090909090908}\n{'loss': 0.4105, 'grad_norm': 0.9645980000495911, 'learning_rate': 0.00037329418519835354, 'epoch': 1.1136363636363635}\n{'loss': 0.3119, 'grad_norm': 1.423688530921936, 'learning_rate': 0.00037234190411366386, 'epoch': 1.1363636363636362}\n{'loss': 0.3752, 'grad_norm': 1.4468616247177124, 'learning_rate': 0.0003713896230289742, 'epoch': 1.1590909090909092}\n{'loss': 0.3942, 'grad_norm': 0.7219982743263245, 'learning_rate': 0.0003704373419442845, 'epoch': 1.1818181818181819}\n{'loss': 0.3568, 'grad_norm': 0.9058952331542969, 'learning_rate': 0.00036948506085959483, 'epoch': 1.2045454545454546}\n{'loss': 0.3885, 'grad_norm': 1.3202780485153198, 'learning_rate': 0.00036853277977490516, 'epoch': 1.2272727272727273}\n{'loss': 0.3888, 'grad_norm': 1.2125155925750732, 'learning_rate': 0.0003675804986902155, 'epoch': 1.25}\n{'loss': 0.3405, 'grad_norm': 2.034796714782715, 'learning_rate': 0.0003666282176055258, 'epoch': 1.2727272727272727}\n{'loss': 0.2777, 'grad_norm': 1.1634695529937744, 'learning_rate': 0.0003656759365208361, 'epoch': 1.2954545454545454}\n{'loss': 0.3195, 'grad_norm': 0.8584941625595093, 'learning_rate': 0.00036472365543614645, 'epoch': 1.3181818181818181}\n{'loss': 0.3563, 'grad_norm': 1.0039573907852173, 'learning_rate': 0.00036377137435145677, 'epoch': 1.3409090909090908}\n{'loss': 0.3729, 'grad_norm': 1.5271462202072144, 'learning_rate': 0.0003628190932667671, 'epoch': 1.3636363636363638}\n{'loss': 0.2879, 'grad_norm': 0.7250851392745972, 'learning_rate': 0.0003618668121820774, 'epoch': 1.3863636363636362}\n{'loss': 0.3004, 'grad_norm': 1.38655686378479, 'learning_rate': 0.00036091453109738774, 'epoch': 1.4090909090909092}\n{'loss': 0.3401, 'grad_norm': 0.8666107654571533, 'learning_rate': 0.00035996225001269806, 'epoch': 1.4318181818181819}\n{'loss': 0.321, 'grad_norm': 1.4202423095703125, 'learning_rate': 0.0003590099689280084, 'epoch': 1.4545454545454546}\n{'loss': 0.3143, 'grad_norm': 0.8370862603187561, 'learning_rate': 0.0003580576878433187, 'epoch': 1.4772727272727273}\n{'loss': 0.3107, 'grad_norm': 0.7273327708244324, 'learning_rate': 0.00035710540675862903, 'epoch': 1.5}\n{'loss': 0.3209, 'grad_norm': 0.9655554890632629, 'learning_rate': 0.00035615312567393936, 'epoch': 1.5227272727272727}\n{'loss': 0.2513, 'grad_norm': 1.15486478805542, 'learning_rate': 0.0003552008445892497, 'epoch': 1.5454545454545454}\n{'loss': 0.3104, 'grad_norm': 0.9102619290351868, 'learning_rate': 0.00035424856350456, 'epoch': 1.5681818181818183}\n{'loss': 0.2956, 'grad_norm': 2.030017137527466, 'learning_rate': 0.0003532962824198703, 'epoch': 1.5909090909090908}\n{'loss': 0.2755, 'grad_norm': 1.1785200834274292, 'learning_rate': 0.00035234400133518065, 'epoch': 1.6136363636363638}\n{'loss': 0.2998, 'grad_norm': 0.6568906307220459, 'learning_rate': 0.0003513917202504909, 'epoch': 1.6363636363636362}\n{'loss': 0.2875, 'grad_norm': 1.251279354095459, 'learning_rate': 0.0003504394391658013, 'epoch': 1.6590909090909092}\n{'loss': 0.2985, 'grad_norm': 1.4660989046096802, 'learning_rate': 0.0003494871580811116, 'epoch': 1.6818181818181817}\n{'loss': 0.2751, 'grad_norm': 0.9293326735496521, 'learning_rate': 0.0003485348769964219, 'epoch': 1.7045454545454546}\n{'loss': 0.3213, 'grad_norm': 1.1909159421920776, 'learning_rate': 0.00034758259591173227, 'epoch': 1.7272727272727273}\n{'loss': 0.2521, 'grad_norm': 1.2772718667984009, 'learning_rate': 0.0003466303148270426, 'epoch': 1.75}\n{'loss': 0.2182, 'grad_norm': 0.914893388748169, 'learning_rate': 0.00034567803374235286, 'epoch': 1.7727272727272727}\n{'loss': 0.2878, 'grad_norm': 0.7979947924613953, 'learning_rate': 0.00034472575265766324, 'epoch': 1.7954545454545454}\n{'loss': 0.2996, 'grad_norm': 0.698005735874176, 'learning_rate': 0.00034377347157297356, 'epoch': 1.8181818181818183}\n{'loss': 0.317, 'grad_norm': 0.7859145402908325, 'learning_rate': 0.00034282119048828383, 'epoch': 1.8409090909090908}\n{'loss': 0.2564, 'grad_norm': 0.8840851187705994, 'learning_rate': 0.0003418689094035942, 'epoch': 1.8636363636363638}\n{'loss': 0.2788, 'grad_norm': 0.7660755515098572, 'learning_rate': 0.00034091662831890453, 'epoch': 1.8863636363636362}\n{'loss': 0.2987, 'grad_norm': 0.9643272161483765, 'learning_rate': 0.0003399643472342148, 'epoch': 1.9090909090909092}\n{'loss': 0.2258, 'grad_norm': 1.1107118129730225, 'learning_rate': 0.0003390120661495252, 'epoch': 1.9318181818181817}\n{'loss': 0.2681, 'grad_norm': 1.3188896179199219, 'learning_rate': 0.0003380597850648355, 'epoch': 1.9545454545454546}\n{'loss': 0.2449, 'grad_norm': 0.9157400727272034, 'learning_rate': 0.00033710750398014577, 'epoch': 1.9772727272727273}\n{'loss': 0.1956, 'grad_norm': 4.197817802429199, 'learning_rate': 0.00033615522289545614, 'epoch': 2.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8745    0.8519    0.8631      3404\n     PERCENT     0.9178    0.9437    0.9306        71\n        TYPE     0.9272    0.9719    0.9490     11194\n      VOLUME     0.7843    0.7143    0.7477        56\n\n   micro avg     0.9152    0.9430    0.9289     14725\n   macro avg     0.8760    0.8704    0.8726     14725\nweighted avg     0.9145    0.9430    0.9283     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.27260062098503113, 'eval_f1_macro': 0.8725816067193963, 'eval_precision': 0.9151782772029262, 'eval_recall': 0.9430220713073005, 'eval_f1': 0.928891564653154, 'eval_accuracy': 0.921394507482322, 'eval_runtime': 1.4793, 'eval_samples_per_second': 3725.518, 'eval_steps_per_second': 7.436, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.1796, 'grad_norm': 0.7262886762619019, 'learning_rate': 0.0003352029418107664, 'epoch': 2.022727272727273}\n{'loss': 0.1958, 'grad_norm': 0.9041571021080017, 'learning_rate': 0.00033425066072607674, 'epoch': 2.0454545454545454}\n{'loss': 0.1678, 'grad_norm': 0.988410234451294, 'learning_rate': 0.0003332983796413871, 'epoch': 2.0681818181818183}\n{'loss': 0.2147, 'grad_norm': 1.0165387392044067, 'learning_rate': 0.0003323460985566974, 'epoch': 2.090909090909091}\n{'loss': 0.1991, 'grad_norm': 0.8240340948104858, 'learning_rate': 0.0003313938174720077, 'epoch': 2.1136363636363638}\n{'loss': 0.1916, 'grad_norm': 0.9717041850090027, 'learning_rate': 0.0003304415363873181, 'epoch': 2.1363636363636362}\n{'loss': 0.1738, 'grad_norm': 1.0555167198181152, 'learning_rate': 0.00032948925530262835, 'epoch': 2.159090909090909}\n{'loss': 0.2163, 'grad_norm': 0.7241308689117432, 'learning_rate': 0.00032853697421793873, 'epoch': 2.1818181818181817}\n{'loss': 0.1999, 'grad_norm': 1.8053702116012573, 'learning_rate': 0.00032758469313324905, 'epoch': 2.2045454545454546}\n{'loss': 0.1595, 'grad_norm': 0.9184411764144897, 'learning_rate': 0.0003266324120485593, 'epoch': 2.227272727272727}\n{'loss': 0.2308, 'grad_norm': 2.26745343208313, 'learning_rate': 0.0003256801309638697, 'epoch': 2.25}\n{'loss': 0.2, 'grad_norm': 1.2325704097747803, 'learning_rate': 0.00032472784987918, 'epoch': 2.2727272727272725}\n{'loss': 0.1627, 'grad_norm': 0.974912166595459, 'learning_rate': 0.0003237755687944903, 'epoch': 2.2954545454545454}\n{'loss': 0.156, 'grad_norm': 0.7923110723495483, 'learning_rate': 0.00032282328770980067, 'epoch': 2.3181818181818183}\n{'loss': 0.1493, 'grad_norm': 0.6596285700798035, 'learning_rate': 0.00032187100662511094, 'epoch': 2.340909090909091}\n{'loss': 0.1766, 'grad_norm': 0.6820521354675293, 'learning_rate': 0.00032091872554042126, 'epoch': 2.3636363636363638}\n{'loss': 0.1929, 'grad_norm': 1.25638747215271, 'learning_rate': 0.00031996644445573164, 'epoch': 2.3863636363636362}\n{'loss': 0.1757, 'grad_norm': 0.8900747299194336, 'learning_rate': 0.0003190141633710419, 'epoch': 2.409090909090909}\n{'loss': 0.1595, 'grad_norm': 1.0410773754119873, 'learning_rate': 0.00031806188228635223, 'epoch': 2.4318181818181817}\n{'loss': 0.1432, 'grad_norm': 0.7009444236755371, 'learning_rate': 0.0003171096012016626, 'epoch': 2.4545454545454546}\n{'loss': 0.1722, 'grad_norm': 1.6010726690292358, 'learning_rate': 0.0003161573201169729, 'epoch': 2.4772727272727275}\n{'loss': 0.1583, 'grad_norm': 0.797822892665863, 'learning_rate': 0.0003152050390322832, 'epoch': 2.5}\n{'loss': 0.1807, 'grad_norm': 1.1290537118911743, 'learning_rate': 0.0003142527579475936, 'epoch': 2.5227272727272725}\n{'loss': 0.1654, 'grad_norm': 1.2827807664871216, 'learning_rate': 0.00031330047686290385, 'epoch': 2.5454545454545454}\n{'loss': 0.2064, 'grad_norm': 0.8799974322319031, 'learning_rate': 0.00031234819577821417, 'epoch': 2.5681818181818183}\n{'loss': 0.1417, 'grad_norm': 1.1165099143981934, 'learning_rate': 0.00031139591469352455, 'epoch': 2.590909090909091}\n{'loss': 0.2343, 'grad_norm': 1.5845915079116821, 'learning_rate': 0.0003104436336088348, 'epoch': 2.6136363636363638}\n{'loss': 0.142, 'grad_norm': 0.9251284599304199, 'learning_rate': 0.00030949135252414514, 'epoch': 2.6363636363636362}\n{'loss': 0.1436, 'grad_norm': 1.292269229888916, 'learning_rate': 0.0003085390714394555, 'epoch': 2.659090909090909}\n{'loss': 0.146, 'grad_norm': 0.8367771506309509, 'learning_rate': 0.0003075867903547658, 'epoch': 2.6818181818181817}\n{'loss': 0.1857, 'grad_norm': 1.214647650718689, 'learning_rate': 0.0003066345092700761, 'epoch': 2.7045454545454546}\n{'loss': 0.136, 'grad_norm': 0.6245550513267517, 'learning_rate': 0.00030568222818538643, 'epoch': 2.7272727272727275}\n{'loss': 0.2095, 'grad_norm': 1.1015915870666504, 'learning_rate': 0.00030472994710069675, 'epoch': 2.75}\n{'loss': 0.1936, 'grad_norm': 1.0099272727966309, 'learning_rate': 0.0003037776660160071, 'epoch': 2.7727272727272725}\n{'loss': 0.1661, 'grad_norm': 0.7944049835205078, 'learning_rate': 0.0003028253849313174, 'epoch': 2.7954545454545454}\n{'loss': 0.1496, 'grad_norm': 0.6429041624069214, 'learning_rate': 0.0003018731038466277, 'epoch': 2.8181818181818183}\n{'loss': 0.1606, 'grad_norm': 1.1229512691497803, 'learning_rate': 0.00030092082276193805, 'epoch': 2.840909090909091}\n{'loss': 0.174, 'grad_norm': 1.0186787843704224, 'learning_rate': 0.00029996854167724837, 'epoch': 2.8636363636363638}\n{'loss': 0.1725, 'grad_norm': 0.9082627892494202, 'learning_rate': 0.0002990162605925587, 'epoch': 2.8863636363636362}\n{'loss': 0.1452, 'grad_norm': 0.904066801071167, 'learning_rate': 0.000298063979507869, 'epoch': 2.909090909090909}\n{'loss': 0.1672, 'grad_norm': 1.622809886932373, 'learning_rate': 0.00029711169842317934, 'epoch': 2.9318181818181817}\n{'loss': 0.1518, 'grad_norm': 0.7195020318031311, 'learning_rate': 0.00029615941733848966, 'epoch': 2.9545454545454546}\n{'loss': 0.1616, 'grad_norm': 1.1369357109069824, 'learning_rate': 0.0002952071362538, 'epoch': 2.9772727272727275}\n{'loss': 0.0734, 'grad_norm': 1.0897451639175415, 'learning_rate': 0.0002942548551691103, 'epoch': 3.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.9093    0.8393    0.8729      3404\n     PERCENT     0.9437    0.9437    0.9437        71\n        TYPE     0.9265    0.9780    0.9516     11194\n      VOLUME     0.7843    0.7143    0.7477        56\n\n   micro avg     0.9225    0.9448    0.9335     14725\n   macro avg     0.8910    0.8688    0.8790     14725\nweighted avg     0.9221    0.9448    0.9326     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.2825150787830353, 'eval_f1_macro': 0.8789528176680496, 'eval_precision': 0.9225464190981433, 'eval_recall': 0.9447877758913412, 'eval_f1': 0.9335346418386177, 'eval_accuracy': 0.9255056734089788, 'eval_runtime': 1.548, 'eval_samples_per_second': 3560.055, 'eval_steps_per_second': 7.106, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.1145, 'grad_norm': 1.2896358966827393, 'learning_rate': 0.00029330257408442063, 'epoch': 3.022727272727273}\n{'loss': 0.1276, 'grad_norm': 1.426113486289978, 'learning_rate': 0.00029235029299973095, 'epoch': 3.0454545454545454}\n{'loss': 0.1369, 'grad_norm': 1.4674595594406128, 'learning_rate': 0.0002913980119150413, 'epoch': 3.0681818181818183}\n{'loss': 0.0794, 'grad_norm': 0.6421156525611877, 'learning_rate': 0.0002904457308303516, 'epoch': 3.090909090909091}\n{'loss': 0.1043, 'grad_norm': 0.9685506224632263, 'learning_rate': 0.0002894934497456619, 'epoch': 3.1136363636363638}\n{'loss': 0.1406, 'grad_norm': 0.9207014441490173, 'learning_rate': 0.00028854116866097225, 'epoch': 3.1363636363636362}\n{'loss': 0.1288, 'grad_norm': 0.8292953372001648, 'learning_rate': 0.00028758888757628257, 'epoch': 3.159090909090909}\n{'loss': 0.1182, 'grad_norm': 1.1183719635009766, 'learning_rate': 0.0002866366064915929, 'epoch': 3.1818181818181817}\n{'loss': 0.1013, 'grad_norm': 0.6291089057922363, 'learning_rate': 0.0002856843254069032, 'epoch': 3.2045454545454546}\n{'loss': 0.0891, 'grad_norm': 0.6652045845985413, 'learning_rate': 0.00028473204432221354, 'epoch': 3.227272727272727}\n{'loss': 0.0912, 'grad_norm': 0.7869036197662354, 'learning_rate': 0.00028377976323752386, 'epoch': 3.25}\n{'loss': 0.1031, 'grad_norm': 0.6620797514915466, 'learning_rate': 0.0002828274821528342, 'epoch': 3.2727272727272725}\n{'loss': 0.1186, 'grad_norm': 1.409469485282898, 'learning_rate': 0.0002818752010681445, 'epoch': 3.2954545454545454}\n{'loss': 0.1228, 'grad_norm': 1.7109043598175049, 'learning_rate': 0.00028092291998345483, 'epoch': 3.3181818181818183}\n{'loss': 0.0816, 'grad_norm': 0.7800083756446838, 'learning_rate': 0.00027997063889876516, 'epoch': 3.340909090909091}\n{'loss': 0.1317, 'grad_norm': 0.877293586730957, 'learning_rate': 0.0002790183578140755, 'epoch': 3.3636363636363638}\n{'loss': 0.1228, 'grad_norm': 1.2432153224945068, 'learning_rate': 0.0002780660767293858, 'epoch': 3.3863636363636362}\n{'loss': 0.1044, 'grad_norm': 1.0151969194412231, 'learning_rate': 0.0002771137956446961, 'epoch': 3.409090909090909}\n{'loss': 0.1358, 'grad_norm': 1.5950911045074463, 'learning_rate': 0.00027616151456000645, 'epoch': 3.4318181818181817}\n{'loss': 0.1339, 'grad_norm': 2.4608981609344482, 'learning_rate': 0.00027520923347531677, 'epoch': 3.4545454545454546}\n{'loss': 0.1101, 'grad_norm': 1.3974720239639282, 'learning_rate': 0.0002742569523906271, 'epoch': 3.4772727272727275}\n{'loss': 0.1018, 'grad_norm': 1.3210208415985107, 'learning_rate': 0.0002733046713059374, 'epoch': 3.5}\n{'loss': 0.0963, 'grad_norm': 0.7442231774330139, 'learning_rate': 0.00027235239022124774, 'epoch': 3.5227272727272725}\n{'loss': 0.1425, 'grad_norm': 2.078705310821533, 'learning_rate': 0.00027140010913655806, 'epoch': 3.5454545454545454}\n{'loss': 0.1229, 'grad_norm': 1.9073764085769653, 'learning_rate': 0.0002704478280518684, 'epoch': 3.5681818181818183}\n{'loss': 0.1535, 'grad_norm': 1.635848045349121, 'learning_rate': 0.0002694955469671787, 'epoch': 3.590909090909091}\n{'loss': 0.1249, 'grad_norm': 1.6160751581192017, 'learning_rate': 0.00026854326588248903, 'epoch': 3.6136363636363638}\n{'loss': 0.0883, 'grad_norm': 0.9005432724952698, 'learning_rate': 0.00026759098479779936, 'epoch': 3.6363636363636362}\n{'loss': 0.1272, 'grad_norm': 1.1446213722229004, 'learning_rate': 0.0002666387037131097, 'epoch': 3.659090909090909}\n{'loss': 0.1199, 'grad_norm': 0.6810818314552307, 'learning_rate': 0.00026568642262842, 'epoch': 3.6818181818181817}\n{'loss': 0.1165, 'grad_norm': 1.1754406690597534, 'learning_rate': 0.0002647341415437303, 'epoch': 3.7045454545454546}\n{'loss': 0.1046, 'grad_norm': 1.4329992532730103, 'learning_rate': 0.00026378186045904065, 'epoch': 3.7272727272727275}\n{'loss': 0.1051, 'grad_norm': 1.2542048692703247, 'learning_rate': 0.00026282957937435097, 'epoch': 3.75}\n{'loss': 0.0967, 'grad_norm': 0.7507942318916321, 'learning_rate': 0.0002618772982896613, 'epoch': 3.7727272727272725}\n{'loss': 0.1085, 'grad_norm': 0.7184588313102722, 'learning_rate': 0.0002609250172049716, 'epoch': 3.7954545454545454}\n{'loss': 0.1313, 'grad_norm': 1.5185461044311523, 'learning_rate': 0.00025997273612028194, 'epoch': 3.8181818181818183}\n{'loss': 0.1131, 'grad_norm': 2.3157804012298584, 'learning_rate': 0.00025902045503559226, 'epoch': 3.840909090909091}\n{'loss': 0.1249, 'grad_norm': 1.5772709846496582, 'learning_rate': 0.0002580681739509026, 'epoch': 3.8636363636363638}\n{'loss': 0.1013, 'grad_norm': 1.154516339302063, 'learning_rate': 0.00025711589286621286, 'epoch': 3.8863636363636362}\n{'loss': 0.1028, 'grad_norm': 0.9301692247390747, 'learning_rate': 0.00025616361178152323, 'epoch': 3.909090909090909}\n{'loss': 0.0972, 'grad_norm': 1.3689292669296265, 'learning_rate': 0.00025521133069683356, 'epoch': 3.9318181818181817}\n{'loss': 0.1242, 'grad_norm': 0.8461504578590393, 'learning_rate': 0.0002542590496121438, 'epoch': 3.9545454545454546}\n{'loss': 0.1084, 'grad_norm': 0.8787109851837158, 'learning_rate': 0.0002533067685274542, 'epoch': 3.9772727272727275}\n{'loss': 0.0303, 'grad_norm': 1.2125333547592163, 'learning_rate': 0.0002523544874427645, 'epoch': 4.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.9011    0.8831    0.8920      3404\n     PERCENT     0.9429    0.9296    0.9362        71\n        TYPE     0.9383    0.9747    0.9561     11194\n      VOLUME     0.8148    0.7857    0.8000        56\n\n   micro avg     0.9296    0.9526    0.9410     14725\n   macro avg     0.8993    0.8933    0.8961     14725\nweighted avg     0.9292    0.9526    0.9406     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.26265206933021545, 'eval_f1_macro': 0.8960747696382488, 'eval_precision': 0.9296176022267877, 'eval_recall': 0.952597623089983, 'eval_f1': 0.9409673307841954, 'eval_accuracy': 0.9325220632571397, 'eval_runtime': 1.4416, 'eval_samples_per_second': 3822.818, 'eval_steps_per_second': 7.63, 'epoch': 4.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.0925, 'grad_norm': 1.1989483833312988, 'learning_rate': 0.0002514022063580748, 'epoch': 4.0227272727272725}\n{'loss': 0.0736, 'grad_norm': 0.6057141423225403, 'learning_rate': 0.0002504499252733852, 'epoch': 4.045454545454546}\n{'loss': 0.0689, 'grad_norm': 0.6093639731407166, 'learning_rate': 0.0002494976441886955, 'epoch': 4.068181818181818}\n{'loss': 0.0839, 'grad_norm': 1.3029230833053589, 'learning_rate': 0.00024854536310400577, 'epoch': 4.090909090909091}\n{'loss': 0.0568, 'grad_norm': 0.4968738853931427, 'learning_rate': 0.00024759308201931614, 'epoch': 4.113636363636363}\n{'loss': 0.0622, 'grad_norm': 0.8842708468437195, 'learning_rate': 0.00024664080093462647, 'epoch': 4.136363636363637}\n{'loss': 0.0983, 'grad_norm': 1.1064121723175049, 'learning_rate': 0.00024568851984993673, 'epoch': 4.159090909090909}\n{'loss': 0.0745, 'grad_norm': 0.6388077139854431, 'learning_rate': 0.0002447362387652471, 'epoch': 4.181818181818182}\n{'loss': 0.0894, 'grad_norm': 1.1890169382095337, 'learning_rate': 0.00024378395768055744, 'epoch': 4.204545454545454}\n{'loss': 0.0878, 'grad_norm': 1.0579637289047241, 'learning_rate': 0.00024283167659586773, 'epoch': 4.2272727272727275}\n{'loss': 0.0728, 'grad_norm': 0.7371494770050049, 'learning_rate': 0.00024187939551117805, 'epoch': 4.25}\n{'loss': 0.0642, 'grad_norm': 0.9350903630256653, 'learning_rate': 0.00024092711442648835, 'epoch': 4.2727272727272725}\n{'loss': 0.0688, 'grad_norm': 0.6334827542304993, 'learning_rate': 0.0002399748333417987, 'epoch': 4.295454545454546}\n{'loss': 0.0722, 'grad_norm': 0.6612814664840698, 'learning_rate': 0.00023902255225710902, 'epoch': 4.318181818181818}\n{'loss': 0.0574, 'grad_norm': 0.651398241519928, 'learning_rate': 0.00023807027117241932, 'epoch': 4.340909090909091}\n{'loss': 0.0838, 'grad_norm': 1.583122730255127, 'learning_rate': 0.00023711799008772967, 'epoch': 4.363636363636363}\n{'loss': 0.0767, 'grad_norm': 1.082067608833313, 'learning_rate': 0.00023616570900304002, 'epoch': 4.386363636363637}\n{'loss': 0.0597, 'grad_norm': 0.8801485896110535, 'learning_rate': 0.0002352134279183503, 'epoch': 4.409090909090909}\n{'loss': 0.0685, 'grad_norm': 0.9138464331626892, 'learning_rate': 0.00023426114683366064, 'epoch': 4.431818181818182}\n{'loss': 0.0718, 'grad_norm': 0.9913060665130615, 'learning_rate': 0.000233308865748971, 'epoch': 4.454545454545454}\n{'loss': 0.0728, 'grad_norm': 1.1812782287597656, 'learning_rate': 0.00023235658466428129, 'epoch': 4.4772727272727275}\n{'loss': 0.0685, 'grad_norm': 0.9330163598060608, 'learning_rate': 0.0002314043035795916, 'epoch': 4.5}\n{'loss': 0.0691, 'grad_norm': 0.7843260169029236, 'learning_rate': 0.00023045202249490196, 'epoch': 4.5227272727272725}\n{'loss': 0.0747, 'grad_norm': 0.8894676566123962, 'learning_rate': 0.00022949974141021226, 'epoch': 4.545454545454545}\n{'loss': 0.0845, 'grad_norm': 1.2029778957366943, 'learning_rate': 0.00022854746032552258, 'epoch': 4.568181818181818}\n{'loss': 0.1015, 'grad_norm': 1.679370641708374, 'learning_rate': 0.00022759517924083287, 'epoch': 4.590909090909091}\n{'loss': 0.0664, 'grad_norm': 1.0387108325958252, 'learning_rate': 0.00022664289815614323, 'epoch': 4.613636363636363}\n{'loss': 0.0874, 'grad_norm': 0.6123905181884766, 'learning_rate': 0.00022569061707145355, 'epoch': 4.636363636363637}\n{'loss': 0.0766, 'grad_norm': 1.1153186559677124, 'learning_rate': 0.00022473833598676384, 'epoch': 4.659090909090909}\n{'loss': 0.0927, 'grad_norm': 1.2808482646942139, 'learning_rate': 0.0002237860549020742, 'epoch': 4.681818181818182}\n{'loss': 0.0742, 'grad_norm': 1.323009729385376, 'learning_rate': 0.00022283377381738452, 'epoch': 4.704545454545455}\n{'loss': 0.0915, 'grad_norm': 0.8036078214645386, 'learning_rate': 0.00022188149273269481, 'epoch': 4.7272727272727275}\n{'loss': 0.1306, 'grad_norm': 1.5729901790618896, 'learning_rate': 0.00022092921164800516, 'epoch': 4.75}\n{'loss': 0.0635, 'grad_norm': 1.0722076892852783, 'learning_rate': 0.0002199769305633155, 'epoch': 4.7727272727272725}\n{'loss': 0.1021, 'grad_norm': 0.621651291847229, 'learning_rate': 0.00021902464947862578, 'epoch': 4.795454545454545}\n{'loss': 0.1156, 'grad_norm': 0.8837742209434509, 'learning_rate': 0.00021807236839393613, 'epoch': 4.818181818181818}\n{'loss': 0.0797, 'grad_norm': 0.6790855526924133, 'learning_rate': 0.00021712008730924646, 'epoch': 4.840909090909091}\n{'loss': 0.1044, 'grad_norm': 1.1647061109542847, 'learning_rate': 0.00021616780622455675, 'epoch': 4.863636363636363}\n{'loss': 0.0624, 'grad_norm': 0.754967451095581, 'learning_rate': 0.0002152155251398671, 'epoch': 4.886363636363637}\n{'loss': 0.1042, 'grad_norm': 0.7472183108329773, 'learning_rate': 0.00021426324405517743, 'epoch': 4.909090909090909}\n{'loss': 0.06, 'grad_norm': 0.6441171765327454, 'learning_rate': 0.00021331096297048772, 'epoch': 4.931818181818182}\n{'loss': 0.0655, 'grad_norm': 0.6000604629516602, 'learning_rate': 0.00021235868188579807, 'epoch': 4.954545454545455}\n{'loss': 0.1096, 'grad_norm': 0.8730925917625427, 'learning_rate': 0.00021140640080110837, 'epoch': 4.9772727272727275}\n{'loss': 0.014, 'grad_norm': 1.0119200944900513, 'learning_rate': 0.0002104541197164187, 'epoch': 5.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.9024    0.8749    0.8884      3404\n     PERCENT     0.9429    0.9296    0.9362        71\n        TYPE     0.9369    0.9729    0.9546     11194\n      VOLUME     0.8269    0.7679    0.7963        56\n\n   micro avg     0.9290    0.9493    0.9390     14725\n   macro avg     0.9023    0.8863    0.8939     14725\nweighted avg     0.9286    0.9493    0.9386     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.27978673577308655, 'eval_f1_macro': 0.8938721444628688, 'eval_precision': 0.9290176791173734, 'eval_recall': 0.9492699490662139, 'eval_f1': 0.9390346310167613, 'eval_accuracy': 0.9310420435235433, 'eval_runtime': 1.4412, 'eval_samples_per_second': 3823.78, 'eval_steps_per_second': 7.632, 'epoch': 5.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.0812, 'grad_norm': 1.1341980695724487, 'learning_rate': 0.00020950183863172904, 'epoch': 5.0227272727272725}\n{'loss': 0.056, 'grad_norm': 0.41596463322639465, 'learning_rate': 0.00020854955754703934, 'epoch': 5.045454545454546}\n{'loss': 0.0528, 'grad_norm': 0.9080910086631775, 'learning_rate': 0.00020759727646234966, 'epoch': 5.068181818181818}\n{'loss': 0.0572, 'grad_norm': 0.9346189498901367, 'learning_rate': 0.00020664499537766, 'epoch': 5.090909090909091}\n{'loss': 0.0849, 'grad_norm': 1.133792519569397, 'learning_rate': 0.0002056927142929703, 'epoch': 5.113636363636363}\n{'loss': 0.0631, 'grad_norm': 1.0963852405548096, 'learning_rate': 0.00020474043320828063, 'epoch': 5.136363636363637}\n{'loss': 0.083, 'grad_norm': 1.5048434734344482, 'learning_rate': 0.00020378815212359098, 'epoch': 5.159090909090909}\n{'loss': 0.0502, 'grad_norm': 1.2952960729599, 'learning_rate': 0.00020283587103890128, 'epoch': 5.181818181818182}\n{'loss': 0.0379, 'grad_norm': 0.47765421867370605, 'learning_rate': 0.0002018835899542116, 'epoch': 5.204545454545454}\n{'loss': 0.0624, 'grad_norm': 1.6449599266052246, 'learning_rate': 0.00020093130886952195, 'epoch': 5.2272727272727275}\n{'loss': 0.0617, 'grad_norm': 1.2733635902404785, 'learning_rate': 0.00019997902778483225, 'epoch': 5.25}\n{'loss': 0.0628, 'grad_norm': 0.9890872836112976, 'learning_rate': 0.00019902674670014257, 'epoch': 5.2727272727272725}\n{'loss': 0.0524, 'grad_norm': 0.9696663022041321, 'learning_rate': 0.00019807446561545292, 'epoch': 5.295454545454546}\n{'loss': 0.0656, 'grad_norm': 1.2501311302185059, 'learning_rate': 0.00019712218453076322, 'epoch': 5.318181818181818}\n{'loss': 0.08, 'grad_norm': 0.9077884554862976, 'learning_rate': 0.00019616990344607354, 'epoch': 5.340909090909091}\n{'loss': 0.0366, 'grad_norm': 0.5531972646713257, 'learning_rate': 0.00019521762236138383, 'epoch': 5.363636363636363}\n{'loss': 0.0569, 'grad_norm': 0.86557936668396, 'learning_rate': 0.00019426534127669419, 'epoch': 5.386363636363637}\n{'loss': 0.0684, 'grad_norm': 0.7972391247749329, 'learning_rate': 0.0001933130601920045, 'epoch': 5.409090909090909}\n{'loss': 0.0899, 'grad_norm': 1.2152535915374756, 'learning_rate': 0.0001923607791073148, 'epoch': 5.431818181818182}\n{'loss': 0.0477, 'grad_norm': 0.715671718120575, 'learning_rate': 0.00019140849802262515, 'epoch': 5.454545454545454}\n{'loss': 0.068, 'grad_norm': 1.518720030784607, 'learning_rate': 0.00019045621693793548, 'epoch': 5.4772727272727275}\n{'loss': 0.0465, 'grad_norm': 0.8954823613166809, 'learning_rate': 0.00018950393585324577, 'epoch': 5.5}\n{'loss': 0.0578, 'grad_norm': 0.8188990950584412, 'learning_rate': 0.00018855165476855612, 'epoch': 5.5227272727272725}\n{'loss': 0.0693, 'grad_norm': 0.9408881664276123, 'learning_rate': 0.00018759937368386645, 'epoch': 5.545454545454545}\n{'loss': 0.047, 'grad_norm': 0.49983659386634827, 'learning_rate': 0.00018664709259917677, 'epoch': 5.568181818181818}\n{'loss': 0.0441, 'grad_norm': 1.116541862487793, 'learning_rate': 0.0001856948115144871, 'epoch': 5.590909090909091}\n{'loss': 0.0393, 'grad_norm': 0.4468423128128052, 'learning_rate': 0.00018474253042979742, 'epoch': 5.613636363636363}\n{'loss': 0.0603, 'grad_norm': 0.795161783695221, 'learning_rate': 0.00018379024934510774, 'epoch': 5.636363636363637}\n{'loss': 0.043, 'grad_norm': 0.8037989735603333, 'learning_rate': 0.00018283796826041806, 'epoch': 5.659090909090909}\n{'loss': 0.0695, 'grad_norm': 0.9088999629020691, 'learning_rate': 0.00018188568717572839, 'epoch': 5.681818181818182}\n{'loss': 0.0478, 'grad_norm': 0.8377330899238586, 'learning_rate': 0.0001809334060910387, 'epoch': 5.704545454545455}\n{'loss': 0.0773, 'grad_norm': 0.8855360150337219, 'learning_rate': 0.00017998112500634903, 'epoch': 5.7272727272727275}\n{'loss': 0.0604, 'grad_norm': 1.1526098251342773, 'learning_rate': 0.00017902884392165936, 'epoch': 5.75}\n{'loss': 0.0447, 'grad_norm': 0.7563618421554565, 'learning_rate': 0.00017807656283696968, 'epoch': 5.7727272727272725}\n{'loss': 0.0455, 'grad_norm': 0.5934655666351318, 'learning_rate': 0.00017712428175228, 'epoch': 5.795454545454545}\n{'loss': 0.0447, 'grad_norm': 0.7117205262184143, 'learning_rate': 0.00017617200066759033, 'epoch': 5.818181818181818}\n{'loss': 0.0546, 'grad_norm': 0.6460011601448059, 'learning_rate': 0.00017521971958290065, 'epoch': 5.840909090909091}\n{'loss': 0.072, 'grad_norm': 1.025321125984192, 'learning_rate': 0.00017426743849821094, 'epoch': 5.863636363636363}\n{'loss': 0.0579, 'grad_norm': 1.1172806024551392, 'learning_rate': 0.0001733151574135213, 'epoch': 5.886363636363637}\n{'loss': 0.066, 'grad_norm': 0.7016804814338684, 'learning_rate': 0.00017236287632883162, 'epoch': 5.909090909090909}\n{'loss': 0.0814, 'grad_norm': 1.6703605651855469, 'learning_rate': 0.00017141059524414191, 'epoch': 5.931818181818182}\n{'loss': 0.0821, 'grad_norm': 1.212868571281433, 'learning_rate': 0.00017045831415945226, 'epoch': 5.954545454545455}\n{'loss': 0.0494, 'grad_norm': 0.9953438639640808, 'learning_rate': 0.0001695060330747626, 'epoch': 5.9772727272727275}\n{'loss': 0.009, 'grad_norm': 0.46840354800224304, 'learning_rate': 0.00016855375199007288, 'epoch': 6.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8811    0.8884    0.8847      3404\n     PERCENT     0.9412    0.9014    0.9209        71\n        TYPE     0.9488    0.9588    0.9538     11194\n      VOLUME     0.7895    0.8036    0.7965        56\n\n   micro avg     0.9325    0.9417    0.9371     14725\n   macro avg     0.8901    0.8880    0.8890     14725\nweighted avg     0.9325    0.9417    0.9371     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.303173303604126, 'eval_f1_macro': 0.8889603744551084, 'eval_precision': 0.9325442195171162, 'eval_recall': 0.9416638370118845, 'eval_f1': 0.9370818409136987, 'eval_accuracy': 0.9284657128761716, 'eval_runtime': 1.4329, 'eval_samples_per_second': 3846.155, 'eval_steps_per_second': 7.677, 'epoch': 6.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.0621, 'grad_norm': 1.1536192893981934, 'learning_rate': 0.0001676014709053832, 'epoch': 6.0227272727272725}\n{'loss': 0.0446, 'grad_norm': 0.5126515626907349, 'learning_rate': 0.00016664918982069356, 'epoch': 6.045454545454546}\n{'loss': 0.041, 'grad_norm': 1.1618359088897705, 'learning_rate': 0.00016569690873600385, 'epoch': 6.068181818181818}\n{'loss': 0.042, 'grad_norm': 0.8514069318771362, 'learning_rate': 0.00016474462765131418, 'epoch': 6.090909090909091}\n{'loss': 0.0561, 'grad_norm': 0.8650026321411133, 'learning_rate': 0.00016379234656662453, 'epoch': 6.113636363636363}\n{'loss': 0.0352, 'grad_norm': 0.7731946110725403, 'learning_rate': 0.00016284006548193485, 'epoch': 6.136363636363637}\n{'loss': 0.0401, 'grad_norm': 0.6465662121772766, 'learning_rate': 0.00016188778439724515, 'epoch': 6.159090909090909}\n{'loss': 0.0574, 'grad_norm': 1.3945890665054321, 'learning_rate': 0.00016093550331255547, 'epoch': 6.181818181818182}\n{'loss': 0.0531, 'grad_norm': 1.0749117136001587, 'learning_rate': 0.00015998322222786582, 'epoch': 6.204545454545454}\n{'loss': 0.0313, 'grad_norm': 0.7251322865486145, 'learning_rate': 0.00015903094114317611, 'epoch': 6.2272727272727275}\n{'loss': 0.0447, 'grad_norm': 0.562379777431488, 'learning_rate': 0.00015807866005848644, 'epoch': 6.25}\n{'loss': 0.0388, 'grad_norm': 0.5204746723175049, 'learning_rate': 0.0001571263789737968, 'epoch': 6.2727272727272725}\n{'loss': 0.0284, 'grad_norm': 0.5124817490577698, 'learning_rate': 0.00015617409788910708, 'epoch': 6.295454545454546}\n{'loss': 0.0354, 'grad_norm': 0.8210465312004089, 'learning_rate': 0.0001552218168044174, 'epoch': 6.318181818181818}\n{'loss': 0.0536, 'grad_norm': 0.632858157157898, 'learning_rate': 0.00015426953571972776, 'epoch': 6.340909090909091}\n{'loss': 0.0419, 'grad_norm': 0.7770037055015564, 'learning_rate': 0.00015331725463503805, 'epoch': 6.363636363636363}\n{'loss': 0.0415, 'grad_norm': 0.9786313772201538, 'learning_rate': 0.00015236497355034838, 'epoch': 6.386363636363637}\n{'loss': 0.0443, 'grad_norm': 0.5042067170143127, 'learning_rate': 0.0001514126924656587, 'epoch': 6.409090909090909}\n{'loss': 0.0384, 'grad_norm': 0.47464922070503235, 'learning_rate': 0.00015046041138096902, 'epoch': 6.431818181818182}\n{'loss': 0.071, 'grad_norm': 0.947887659072876, 'learning_rate': 0.00014950813029627935, 'epoch': 6.454545454545454}\n{'loss': 0.0525, 'grad_norm': 0.9864804148674011, 'learning_rate': 0.00014855584921158967, 'epoch': 6.4772727272727275}\n{'loss': 0.039, 'grad_norm': 0.9474749565124512, 'learning_rate': 0.0001476035681269, 'epoch': 6.5}\n{'loss': 0.0325, 'grad_norm': 0.36559513211250305, 'learning_rate': 0.00014665128704221032, 'epoch': 6.5227272727272725}\n{'loss': 0.0492, 'grad_norm': 0.8061442375183105, 'learning_rate': 0.00014569900595752064, 'epoch': 6.545454545454545}\n{'loss': 0.0347, 'grad_norm': 0.4981538653373718, 'learning_rate': 0.00014474672487283096, 'epoch': 6.568181818181818}\n{'loss': 0.0345, 'grad_norm': 0.6306036710739136, 'learning_rate': 0.00014379444378814129, 'epoch': 6.590909090909091}\n{'loss': 0.0337, 'grad_norm': 0.6136751174926758, 'learning_rate': 0.0001428421627034516, 'epoch': 6.613636363636363}\n{'loss': 0.0532, 'grad_norm': 1.0238010883331299, 'learning_rate': 0.00014188988161876193, 'epoch': 6.636363636363637}\n{'loss': 0.0511, 'grad_norm': 0.47958266735076904, 'learning_rate': 0.00014093760053407225, 'epoch': 6.659090909090909}\n{'loss': 0.0364, 'grad_norm': 0.603816032409668, 'learning_rate': 0.00013998531944938258, 'epoch': 6.681818181818182}\n{'loss': 0.0527, 'grad_norm': 0.577340304851532, 'learning_rate': 0.0001390330383646929, 'epoch': 6.704545454545455}\n{'loss': 0.0554, 'grad_norm': 0.8832253813743591, 'learning_rate': 0.00013808075728000322, 'epoch': 6.7272727272727275}\n{'loss': 0.0804, 'grad_norm': 0.9616824388504028, 'learning_rate': 0.00013712847619531355, 'epoch': 6.75}\n{'loss': 0.0496, 'grad_norm': 0.7870662808418274, 'learning_rate': 0.00013617619511062387, 'epoch': 6.7727272727272725}\n{'loss': 0.0404, 'grad_norm': 0.6153239011764526, 'learning_rate': 0.0001352239140259342, 'epoch': 6.795454545454545}\n{'loss': 0.0386, 'grad_norm': 0.8777799606323242, 'learning_rate': 0.00013427163294124452, 'epoch': 6.818181818181818}\n{'loss': 0.0456, 'grad_norm': 0.6418985724449158, 'learning_rate': 0.00013331935185655484, 'epoch': 6.840909090909091}\n{'loss': 0.0399, 'grad_norm': 0.5809282064437866, 'learning_rate': 0.00013236707077186516, 'epoch': 6.863636363636363}\n{'loss': 0.0394, 'grad_norm': 0.5897946357727051, 'learning_rate': 0.00013141478968717549, 'epoch': 6.886363636363637}\n{'loss': 0.0533, 'grad_norm': 0.8724234104156494, 'learning_rate': 0.0001304625086024858, 'epoch': 6.909090909090909}\n{'loss': 0.047, 'grad_norm': 0.8195860981941223, 'learning_rate': 0.00012951022751779613, 'epoch': 6.931818181818182}\n{'loss': 0.029, 'grad_norm': 0.3868277072906494, 'learning_rate': 0.00012855794643310643, 'epoch': 6.954545454545455}\n{'loss': 0.0505, 'grad_norm': 0.6869829893112183, 'learning_rate': 0.00012760566534841678, 'epoch': 6.9772727272727275}\n{'loss': 0.0069, 'grad_norm': 0.794227659702301, 'learning_rate': 0.0001266533842637271, 'epoch': 7.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.9028    0.8790    0.8907      3404\n     PERCENT     0.9429    0.9296    0.9362        71\n        TYPE     0.9435    0.9719    0.9575     11194\n      VOLUME     0.8627    0.7857    0.8224        56\n\n   micro avg     0.9342    0.9495    0.9418     14725\n   macro avg     0.9130    0.8915    0.9017     14725\nweighted avg     0.9338    0.9495    0.9414     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.3178223669528961, 'eval_f1_macro': 0.9017078224754324, 'eval_precision': 0.9342465753424658, 'eval_recall': 0.9494736842105264, 'eval_f1': 0.9417985853822837, 'eval_accuracy': 0.9320835388916296, 'eval_runtime': 1.4508, 'eval_samples_per_second': 3798.585, 'eval_steps_per_second': 7.582, 'epoch': 7.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.0283, 'grad_norm': 0.5458894371986389, 'learning_rate': 0.0001257011031790374, 'epoch': 7.0227272727272725}\n{'loss': 0.0332, 'grad_norm': 0.7201671004295349, 'learning_rate': 0.00012474882209434775, 'epoch': 7.045454545454546}\n{'loss': 0.0264, 'grad_norm': 0.528487503528595, 'learning_rate': 0.00012379654100965807, 'epoch': 7.068181818181818}\n{'loss': 0.0429, 'grad_norm': 0.46718907356262207, 'learning_rate': 0.00012284425992496837, 'epoch': 7.090909090909091}\n{'loss': 0.0375, 'grad_norm': 0.4092269837856293, 'learning_rate': 0.00012189197884027872, 'epoch': 7.113636363636363}\n{'loss': 0.0314, 'grad_norm': 0.7743521928787231, 'learning_rate': 0.00012093969775558903, 'epoch': 7.136363636363637}\n{'loss': 0.0375, 'grad_norm': 1.0606820583343506, 'learning_rate': 0.00011998741667089935, 'epoch': 7.159090909090909}\n{'loss': 0.033, 'grad_norm': 0.8424679040908813, 'learning_rate': 0.00011903513558620966, 'epoch': 7.181818181818182}\n{'loss': 0.0308, 'grad_norm': 0.7387948036193848, 'learning_rate': 0.00011808285450152001, 'epoch': 7.204545454545454}\n{'loss': 0.0415, 'grad_norm': 0.6160318851470947, 'learning_rate': 0.00011713057341683032, 'epoch': 7.2272727272727275}\n{'loss': 0.0216, 'grad_norm': 0.3707299828529358, 'learning_rate': 0.00011617829233214064, 'epoch': 7.25}\n{'loss': 0.029, 'grad_norm': 0.9220455288887024, 'learning_rate': 0.00011522601124745098, 'epoch': 7.2727272727272725}\n{'loss': 0.0518, 'grad_norm': 0.7523878812789917, 'learning_rate': 0.00011427373016276129, 'epoch': 7.295454545454546}\n{'loss': 0.044, 'grad_norm': 0.8407614827156067, 'learning_rate': 0.00011332144907807161, 'epoch': 7.318181818181818}\n{'loss': 0.0382, 'grad_norm': 0.6414490938186646, 'learning_rate': 0.00011236916799338192, 'epoch': 7.340909090909091}\n{'loss': 0.0415, 'grad_norm': 0.5891773700714111, 'learning_rate': 0.00011141688690869226, 'epoch': 7.363636363636363}\n{'loss': 0.0201, 'grad_norm': 0.36168089509010315, 'learning_rate': 0.00011046460582400258, 'epoch': 7.386363636363637}\n{'loss': 0.0227, 'grad_norm': 0.5725070834159851, 'learning_rate': 0.00010951232473931289, 'epoch': 7.409090909090909}\n{'loss': 0.0462, 'grad_norm': 0.6444472074508667, 'learning_rate': 0.00010856004365462323, 'epoch': 7.431818181818182}\n{'loss': 0.0192, 'grad_norm': 0.5899130702018738, 'learning_rate': 0.00010760776256993355, 'epoch': 7.454545454545454}\n{'loss': 0.0481, 'grad_norm': 0.594496488571167, 'learning_rate': 0.00010665548148524386, 'epoch': 7.4772727272727275}\n{'loss': 0.0446, 'grad_norm': 0.5566183924674988, 'learning_rate': 0.00010570320040055418, 'epoch': 7.5}\n{'loss': 0.0354, 'grad_norm': 0.7043643593788147, 'learning_rate': 0.00010475091931586452, 'epoch': 7.5227272727272725}\n{'loss': 0.0348, 'grad_norm': 0.9111629724502563, 'learning_rate': 0.00010379863823117483, 'epoch': 7.545454545454545}\n{'loss': 0.0325, 'grad_norm': 0.7371808290481567, 'learning_rate': 0.00010284635714648515, 'epoch': 7.568181818181818}\n{'loss': 0.0454, 'grad_norm': 0.8314304351806641, 'learning_rate': 0.00010189407606179549, 'epoch': 7.590909090909091}\n{'loss': 0.0346, 'grad_norm': 0.6230766177177429, 'learning_rate': 0.0001009417949771058, 'epoch': 7.613636363636363}\n{'loss': 0.0306, 'grad_norm': 0.6741204261779785, 'learning_rate': 9.998951389241612e-05, 'epoch': 7.636363636363637}\n{'loss': 0.0322, 'grad_norm': 0.5794732570648193, 'learning_rate': 9.903723280772646e-05, 'epoch': 7.659090909090909}\n{'loss': 0.0532, 'grad_norm': 0.9663182497024536, 'learning_rate': 9.808495172303677e-05, 'epoch': 7.681818181818182}\n{'loss': 0.0497, 'grad_norm': 0.674085795879364, 'learning_rate': 9.713267063834709e-05, 'epoch': 7.704545454545455}\n{'loss': 0.038, 'grad_norm': 1.6745187044143677, 'learning_rate': 9.61803895536574e-05, 'epoch': 7.7272727272727275}\n{'loss': 0.0218, 'grad_norm': 0.45680463314056396, 'learning_rate': 9.522810846896774e-05, 'epoch': 7.75}\n{'loss': 0.049, 'grad_norm': 0.8055655360221863, 'learning_rate': 9.427582738427806e-05, 'epoch': 7.7727272727272725}\n{'loss': 0.0341, 'grad_norm': 0.5431302189826965, 'learning_rate': 9.332354629958839e-05, 'epoch': 7.795454545454545}\n{'loss': 0.0285, 'grad_norm': 0.4997485876083374, 'learning_rate': 9.237126521489871e-05, 'epoch': 7.818181818181818}\n{'loss': 0.0298, 'grad_norm': 0.49848055839538574, 'learning_rate': 9.141898413020903e-05, 'epoch': 7.840909090909091}\n{'loss': 0.042, 'grad_norm': 0.6717859506607056, 'learning_rate': 9.046670304551935e-05, 'epoch': 7.863636363636363}\n{'loss': 0.0511, 'grad_norm': 0.6839860677719116, 'learning_rate': 8.951442196082968e-05, 'epoch': 7.886363636363637}\n{'loss': 0.0176, 'grad_norm': 0.6025487780570984, 'learning_rate': 8.856214087614e-05, 'epoch': 7.909090909090909}\n{'loss': 0.0389, 'grad_norm': 0.629978597164154, 'learning_rate': 8.760985979145032e-05, 'epoch': 7.931818181818182}\n{'loss': 0.0359, 'grad_norm': 0.7153287529945374, 'learning_rate': 8.665757870676065e-05, 'epoch': 7.954545454545455}\n{'loss': 0.0677, 'grad_norm': 0.8867976069450378, 'learning_rate': 8.570529762207096e-05, 'epoch': 7.9772727272727275}\n{'loss': 0.0401, 'grad_norm': 2.9996140003204346, 'learning_rate': 8.47530165373813e-05, 'epoch': 8.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8874    0.8934    0.8904      3404\n     PERCENT     0.9403    0.8873    0.9130        71\n        TYPE     0.9440    0.9705    0.9571     11194\n      VOLUME     0.8113    0.7679    0.7890        56\n\n   micro avg     0.9306    0.9515    0.9409     14725\n   macro avg     0.8957    0.8798    0.8874     14725\nweighted avg     0.9304    0.9515    0.9408     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.33463504910469055, 'eval_f1_macro': 0.8873603102900348, 'eval_precision': 0.9305924548352816, 'eval_recall': 0.9515110356536503, 'eval_f1': 0.9409354957859037, 'eval_accuracy': 0.9321931699830072, 'eval_runtime': 1.8334, 'eval_samples_per_second': 3005.817, 'eval_steps_per_second': 6.0, 'epoch': 8.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.0326, 'grad_norm': 0.6979623436927795, 'learning_rate': 8.38007354526916e-05, 'epoch': 8.022727272727273}\n{'loss': 0.0444, 'grad_norm': 0.7995193600654602, 'learning_rate': 8.284845436800193e-05, 'epoch': 8.045454545454545}\n{'loss': 0.0343, 'grad_norm': 0.5861100554466248, 'learning_rate': 8.189617328331226e-05, 'epoch': 8.068181818181818}\n{'loss': 0.025, 'grad_norm': 0.6672625541687012, 'learning_rate': 8.094389219862257e-05, 'epoch': 8.090909090909092}\n{'loss': 0.0261, 'grad_norm': 0.8873501420021057, 'learning_rate': 7.999161111393291e-05, 'epoch': 8.113636363636363}\n{'loss': 0.025, 'grad_norm': 0.552121639251709, 'learning_rate': 7.903933002924322e-05, 'epoch': 8.136363636363637}\n{'loss': 0.0276, 'grad_norm': 0.3888280689716339, 'learning_rate': 7.808704894455354e-05, 'epoch': 8.159090909090908}\n{'loss': 0.0111, 'grad_norm': 0.29438096284866333, 'learning_rate': 7.713476785986388e-05, 'epoch': 8.181818181818182}\n{'loss': 0.0293, 'grad_norm': 0.6609566807746887, 'learning_rate': 7.618248677517419e-05, 'epoch': 8.204545454545455}\n{'loss': 0.0466, 'grad_norm': 0.9213314056396484, 'learning_rate': 7.523020569048451e-05, 'epoch': 8.227272727272727}\n{'loss': 0.0327, 'grad_norm': 0.834313690662384, 'learning_rate': 7.427792460579483e-05, 'epoch': 8.25}\n{'loss': 0.0284, 'grad_norm': 0.597192645072937, 'learning_rate': 7.332564352110516e-05, 'epoch': 8.272727272727273}\n{'loss': 0.04, 'grad_norm': 0.6969497799873352, 'learning_rate': 7.237336243641548e-05, 'epoch': 8.295454545454545}\n{'loss': 0.0273, 'grad_norm': 0.6816952228546143, 'learning_rate': 7.14210813517258e-05, 'epoch': 8.318181818181818}\n{'loss': 0.0275, 'grad_norm': 0.8729815483093262, 'learning_rate': 7.046880026703613e-05, 'epoch': 8.340909090909092}\n{'loss': 0.0256, 'grad_norm': 0.4845539629459381, 'learning_rate': 6.951651918234645e-05, 'epoch': 8.363636363636363}\n{'loss': 0.0216, 'grad_norm': 0.6898738145828247, 'learning_rate': 6.856423809765677e-05, 'epoch': 8.386363636363637}\n{'loss': 0.0356, 'grad_norm': 0.8139654994010925, 'learning_rate': 6.76119570129671e-05, 'epoch': 8.409090909090908}\n{'loss': 0.0308, 'grad_norm': 0.5308534502983093, 'learning_rate': 6.665967592827742e-05, 'epoch': 8.431818181818182}\n{'loss': 0.0333, 'grad_norm': 0.44013506174087524, 'learning_rate': 6.570739484358774e-05, 'epoch': 8.454545454545455}\n{'loss': 0.0302, 'grad_norm': 0.38484346866607666, 'learning_rate': 6.475511375889807e-05, 'epoch': 8.477272727272727}\n{'loss': 0.0408, 'grad_norm': 0.83186936378479, 'learning_rate': 6.380283267420839e-05, 'epoch': 8.5}\n{'loss': 0.0449, 'grad_norm': 0.8811212778091431, 'learning_rate': 6.28505515895187e-05, 'epoch': 8.522727272727273}\n{'loss': 0.0201, 'grad_norm': 0.6515455842018127, 'learning_rate': 6.189827050482904e-05, 'epoch': 8.545454545454545}\n{'loss': 0.0367, 'grad_norm': 0.687621533870697, 'learning_rate': 6.094598942013936e-05, 'epoch': 8.568181818181818}\n{'loss': 0.0243, 'grad_norm': 0.4434847831726074, 'learning_rate': 5.9993708335449675e-05, 'epoch': 8.590909090909092}\n{'loss': 0.0268, 'grad_norm': 0.7875362634658813, 'learning_rate': 5.9041427250760005e-05, 'epoch': 8.613636363636363}\n{'loss': 0.0377, 'grad_norm': 1.1015688180923462, 'learning_rate': 5.808914616607032e-05, 'epoch': 8.636363636363637}\n{'loss': 0.0416, 'grad_norm': 0.7926039695739746, 'learning_rate': 5.7136865081380645e-05, 'epoch': 8.659090909090908}\n{'loss': 0.0178, 'grad_norm': 0.386269748210907, 'learning_rate': 5.618458399669096e-05, 'epoch': 8.681818181818182}\n{'loss': 0.0336, 'grad_norm': 0.6009215116500854, 'learning_rate': 5.523230291200129e-05, 'epoch': 8.704545454545455}\n{'loss': 0.0346, 'grad_norm': 0.7545189261436462, 'learning_rate': 5.4280021827311614e-05, 'epoch': 8.727272727272727}\n{'loss': 0.0382, 'grad_norm': 0.501533567905426, 'learning_rate': 5.332774074262193e-05, 'epoch': 8.75}\n{'loss': 0.0369, 'grad_norm': 0.2883753180503845, 'learning_rate': 5.237545965793226e-05, 'epoch': 8.772727272727273}\n{'loss': 0.0402, 'grad_norm': 0.7903833985328674, 'learning_rate': 5.142317857324258e-05, 'epoch': 8.795454545454545}\n{'loss': 0.0431, 'grad_norm': 0.7737163305282593, 'learning_rate': 5.04708974885529e-05, 'epoch': 8.818181818181818}\n{'loss': 0.0357, 'grad_norm': 0.6941847205162048, 'learning_rate': 4.951861640386323e-05, 'epoch': 8.840909090909092}\n{'loss': 0.0421, 'grad_norm': 0.8243739008903503, 'learning_rate': 4.8566335319173546e-05, 'epoch': 8.863636363636363}\n{'loss': 0.0279, 'grad_norm': 0.5117889642715454, 'learning_rate': 4.761405423448387e-05, 'epoch': 8.886363636363637}\n{'loss': 0.0309, 'grad_norm': 0.6216597557067871, 'learning_rate': 4.666177314979419e-05, 'epoch': 8.909090909090908}\n{'loss': 0.0199, 'grad_norm': 0.33355751633644104, 'learning_rate': 4.5709492065104516e-05, 'epoch': 8.931818181818182}\n{'loss': 0.0362, 'grad_norm': 1.1838083267211914, 'learning_rate': 4.475721098041484e-05, 'epoch': 8.954545454545455}\n{'loss': 0.0208, 'grad_norm': 0.4361269474029541, 'learning_rate': 4.380492989572516e-05, 'epoch': 8.977272727272727}\n{'loss': 0.0979, 'grad_norm': 4.804513931274414, 'learning_rate': 4.285264881103548e-05, 'epoch': 9.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8932    0.8869    0.8900      3404\n     PERCENT     0.9412    0.9014    0.9209        71\n        TYPE     0.9441    0.9703    0.9570     11194\n      VOLUME     0.8302    0.7857    0.8073        56\n\n   micro avg     0.9322    0.9499    0.9410     14725\n   macro avg     0.9022    0.8861    0.8938     14725\nweighted avg     0.9319    0.9499    0.9408     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.34564071893692017, 'eval_f1_macro': 0.8938096882616812, 'eval_precision': 0.9322225924691769, 'eval_recall': 0.9499490662139219, 'eval_f1': 0.9410023545240498, 'eval_accuracy': 0.9319190922545634, 'eval_runtime': 1.4512, 'eval_samples_per_second': 3797.624, 'eval_steps_per_second': 7.58, 'epoch': 9.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.0266, 'grad_norm': 0.4055612087249756, 'learning_rate': 4.19003677263458e-05, 'epoch': 9.022727272727273}\n{'loss': 0.029, 'grad_norm': 0.7043263912200928, 'learning_rate': 4.094808664165613e-05, 'epoch': 9.045454545454545}\n{'loss': 0.0251, 'grad_norm': 0.6450002789497375, 'learning_rate': 3.9995805556966455e-05, 'epoch': 9.068181818181818}\n{'loss': 0.0347, 'grad_norm': 1.0285621881484985, 'learning_rate': 3.904352447227677e-05, 'epoch': 9.090909090909092}\n{'loss': 0.0299, 'grad_norm': 0.599998950958252, 'learning_rate': 3.8091243387587094e-05, 'epoch': 9.113636363636363}\n{'loss': 0.0293, 'grad_norm': 0.5555079579353333, 'learning_rate': 3.713896230289742e-05, 'epoch': 9.136363636363637}\n{'loss': 0.0182, 'grad_norm': 0.6258923411369324, 'learning_rate': 3.618668121820774e-05, 'epoch': 9.159090909090908}\n{'loss': 0.0278, 'grad_norm': 0.4805312752723694, 'learning_rate': 3.5234400133518064e-05, 'epoch': 9.181818181818182}\n{'loss': 0.0418, 'grad_norm': 0.5303696393966675, 'learning_rate': 3.428211904882839e-05, 'epoch': 9.204545454545455}\n{'loss': 0.033, 'grad_norm': 0.8095331192016602, 'learning_rate': 3.332983796413871e-05, 'epoch': 9.227272727272727}\n{'loss': 0.0199, 'grad_norm': 0.60182785987854, 'learning_rate': 3.237755687944903e-05, 'epoch': 9.25}\n{'loss': 0.0299, 'grad_norm': 0.7026847004890442, 'learning_rate': 3.142527579475935e-05, 'epoch': 9.272727272727273}\n{'loss': 0.0387, 'grad_norm': 0.5269832611083984, 'learning_rate': 3.047299471006968e-05, 'epoch': 9.295454545454545}\n{'loss': 0.0294, 'grad_norm': 0.5800140500068665, 'learning_rate': 2.9520713625380003e-05, 'epoch': 9.318181818181818}\n{'loss': 0.0246, 'grad_norm': 0.5285787582397461, 'learning_rate': 2.8568432540690322e-05, 'epoch': 9.340909090909092}\n{'loss': 0.0152, 'grad_norm': 0.53631591796875, 'learning_rate': 2.7616151456000645e-05, 'epoch': 9.363636363636363}\n{'loss': 0.0237, 'grad_norm': 0.7017875909805298, 'learning_rate': 2.6663870371310965e-05, 'epoch': 9.386363636363637}\n{'loss': 0.0247, 'grad_norm': 0.5043150782585144, 'learning_rate': 2.571158928662129e-05, 'epoch': 9.409090909090908}\n{'loss': 0.0313, 'grad_norm': 0.5120247602462769, 'learning_rate': 2.4759308201931615e-05, 'epoch': 9.431818181818182}\n{'loss': 0.0313, 'grad_norm': 0.5555852055549622, 'learning_rate': 2.3807027117241935e-05, 'epoch': 9.454545454545455}\n{'loss': 0.0319, 'grad_norm': 0.8297238945960999, 'learning_rate': 2.2854746032552258e-05, 'epoch': 9.477272727272727}\n{'loss': 0.025, 'grad_norm': 0.5379306077957153, 'learning_rate': 2.190246494786258e-05, 'epoch': 9.5}\n{'loss': 0.0273, 'grad_norm': 0.9954738616943359, 'learning_rate': 2.09501838631729e-05, 'epoch': 9.522727272727273}\n{'loss': 0.0213, 'grad_norm': 0.3148437738418579, 'learning_rate': 1.9997902778483227e-05, 'epoch': 9.545454545454545}\n{'loss': 0.0478, 'grad_norm': 0.8350557088851929, 'learning_rate': 1.9045621693793547e-05, 'epoch': 9.568181818181818}\n{'loss': 0.0186, 'grad_norm': 0.34287893772125244, 'learning_rate': 1.809334060910387e-05, 'epoch': 9.590909090909092}\n{'loss': 0.023, 'grad_norm': 0.8616059422492981, 'learning_rate': 1.7141059524414193e-05, 'epoch': 9.613636363636363}\n{'loss': 0.0337, 'grad_norm': 0.5686317682266235, 'learning_rate': 1.6188778439724517e-05, 'epoch': 9.636363636363637}\n{'loss': 0.0385, 'grad_norm': 0.6764343976974487, 'learning_rate': 1.523649735503484e-05, 'epoch': 9.659090909090908}\n{'loss': 0.0323, 'grad_norm': 0.5858246088027954, 'learning_rate': 1.4284216270345161e-05, 'epoch': 9.681818181818182}\n{'loss': 0.0364, 'grad_norm': 0.5288658142089844, 'learning_rate': 1.3331935185655483e-05, 'epoch': 9.704545454545455}\n{'loss': 0.0201, 'grad_norm': 0.47324255108833313, 'learning_rate': 1.2379654100965807e-05, 'epoch': 9.727272727272727}\n{'loss': 0.0276, 'grad_norm': 0.4068472981452942, 'learning_rate': 1.1427373016276129e-05, 'epoch': 9.75}\n{'loss': 0.0225, 'grad_norm': 0.757698118686676, 'learning_rate': 1.047509193158645e-05, 'epoch': 9.772727272727273}\n{'loss': 0.044, 'grad_norm': 0.6275416016578674, 'learning_rate': 9.522810846896774e-06, 'epoch': 9.795454545454545}\n{'loss': 0.024, 'grad_norm': 0.41497424244880676, 'learning_rate': 8.570529762207097e-06, 'epoch': 9.818181818181818}\n{'loss': 0.0179, 'grad_norm': 0.4184158146381378, 'learning_rate': 7.61824867751742e-06, 'epoch': 9.840909090909092}\n{'loss': 0.0274, 'grad_norm': 0.7264857292175293, 'learning_rate': 6.665967592827741e-06, 'epoch': 9.863636363636363}\n{'loss': 0.0235, 'grad_norm': 0.46924924850463867, 'learning_rate': 5.7136865081380645e-06, 'epoch': 9.886363636363637}\n{'loss': 0.0226, 'grad_norm': 0.500999927520752, 'learning_rate': 4.761405423448387e-06, 'epoch': 9.909090909090908}\n{'loss': 0.0282, 'grad_norm': 0.4860459566116333, 'learning_rate': 3.80912433875871e-06, 'epoch': 9.931818181818182}\n{'loss': 0.0327, 'grad_norm': 0.7149813771247864, 'learning_rate': 2.8568432540690322e-06, 'epoch': 9.954545454545455}\n{'loss': 0.0246, 'grad_norm': 0.4694012701511383, 'learning_rate': 1.904562169379355e-06, 'epoch': 9.977272727272727}\n{'loss': 0.0036, 'grad_norm': 0.12182418256998062, 'learning_rate': 9.522810846896775e-07, 'epoch': 10.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8930    0.8901    0.8916      3404\n     PERCENT     0.9412    0.9014    0.9209        71\n        TYPE     0.9451    0.9684    0.9566     11194\n      VOLUME     0.8269    0.7679    0.7963        56\n\n   micro avg     0.9329    0.9492    0.9410     14725\n   macro avg     0.9015    0.8819    0.8913     14725\nweighted avg     0.9326    0.9492    0.9408     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.3433302938938141, 'eval_f1_macro': 0.8913281358214893, 'eval_precision': 0.932857238203297, 'eval_recall': 0.9492020373514432, 'eval_f1': 0.9409586643328396, 'eval_accuracy': 0.9318094611631859, 'eval_runtime': 1.4623, 'eval_samples_per_second': 3768.658, 'eval_steps_per_second': 7.522, 'epoch': 10.0}\n{'train_runtime': 62.0366, 'train_samples_per_second': 3552.902, 'train_steps_per_second': 7.093, 'train_loss': 0.19647529133256864, 'epoch': 10.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nThe speedups for torchdynamo mostly come with GPU Ampere or higher and which is not detected here.\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.9028    0.8790    0.8907      3404\n     PERCENT     0.9429    0.9296    0.9362        71\n        TYPE     0.9435    0.9719    0.9575     11194\n      VOLUME     0.8627    0.7857    0.8224        56\n\n   micro avg     0.9342    0.9495    0.9418     14725\n   macro avg     0.9130    0.8915    0.9017     14725\nweighted avg     0.9338    0.9495    0.9414     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.3178223669528961, 'eval_f1_macro': 0.9017078224754324, 'eval_precision': 0.9342465753424658, 'eval_recall': 0.9494736842105264, 'eval_f1': 0.9417985853822837, 'eval_accuracy': 0.9320835388916296, 'eval_runtime': 1.5293, 'eval_samples_per_second': 3603.527, 'eval_steps_per_second': 7.193, 'epoch': 10.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/3364797558.py:66: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 2.2572, 'grad_norm': 7.357028961181641, 'learning_rate': 0.0, 'epoch': 0.022727272727272728}\n{'loss': 2.2706, 'grad_norm': 7.434906005859375, 'learning_rate': 8.570529762207097e-06, 'epoch': 0.045454545454545456}\n{'loss': 2.2459, 'grad_norm': 7.381216526031494, 'learning_rate': 1.7141059524414193e-05, 'epoch': 0.06818181818181818}\n{'loss': 2.2133, 'grad_norm': 7.158947467803955, 'learning_rate': 2.571158928662129e-05, 'epoch': 0.09090909090909091}\n{'loss': 2.149, 'grad_norm': 7.108647346496582, 'learning_rate': 3.428211904882839e-05, 'epoch': 0.11363636363636363}\n{'loss': 2.0803, 'grad_norm': 7.393138885498047, 'learning_rate': 4.285264881103548e-05, 'epoch': 0.13636363636363635}\n{'loss': 1.988, 'grad_norm': 6.46140718460083, 'learning_rate': 5.142317857324258e-05, 'epoch': 0.1590909090909091}\n{'loss': 1.8801, 'grad_norm': 6.515856742858887, 'learning_rate': 5.9993708335449675e-05, 'epoch': 0.18181818181818182}\n{'loss': 1.7461, 'grad_norm': 6.090237617492676, 'learning_rate': 6.856423809765677e-05, 'epoch': 0.20454545454545456}\n{'loss': 1.6228, 'grad_norm': 5.5632829666137695, 'learning_rate': 7.713476785986388e-05, 'epoch': 0.22727272727272727}\n{'loss': 1.5318, 'grad_norm': 4.604071617126465, 'learning_rate': 8.570529762207096e-05, 'epoch': 0.25}\n{'loss': 1.4136, 'grad_norm': 3.7140424251556396, 'learning_rate': 9.427582738427806e-05, 'epoch': 0.2727272727272727}\n{'loss': 1.2889, 'grad_norm': 2.8226430416107178, 'learning_rate': 0.00010284635714648515, 'epoch': 0.29545454545454547}\n{'loss': 1.2661, 'grad_norm': 2.0849523544311523, 'learning_rate': 0.00011141688690869226, 'epoch': 0.3181818181818182}\n{'loss': 1.2562, 'grad_norm': 2.308619976043701, 'learning_rate': 0.00011998741667089935, 'epoch': 0.3409090909090909}\n{'loss': 1.1341, 'grad_norm': 2.094331741333008, 'learning_rate': 0.00012855794643310643, 'epoch': 0.36363636363636365}\n{'loss': 1.1338, 'grad_norm': 2.3448286056518555, 'learning_rate': 0.00013712847619531355, 'epoch': 0.38636363636363635}\n{'loss': 1.0389, 'grad_norm': 1.504726529121399, 'learning_rate': 0.00014569900595752064, 'epoch': 0.4090909090909091}\n{'loss': 1.0082, 'grad_norm': 1.106063961982727, 'learning_rate': 0.00015426953571972776, 'epoch': 0.4318181818181818}\n{'loss': 1.0315, 'grad_norm': 1.2993435859680176, 'learning_rate': 0.00016284006548193485, 'epoch': 0.45454545454545453}\n{'loss': 0.9638, 'grad_norm': 1.6715872287750244, 'learning_rate': 0.00017141059524414191, 'epoch': 0.4772727272727273}\n{'loss': 0.9333, 'grad_norm': 1.6763311624526978, 'learning_rate': 0.00017998112500634903, 'epoch': 0.5}\n{'loss': 0.8998, 'grad_norm': 1.7744967937469482, 'learning_rate': 0.00018855165476855612, 'epoch': 0.5227272727272727}\n{'loss': 0.8252, 'grad_norm': 1.5892658233642578, 'learning_rate': 0.00019712218453076322, 'epoch': 0.5454545454545454}\n{'loss': 0.7969, 'grad_norm': 1.0203372240066528, 'learning_rate': 0.0002056927142929703, 'epoch': 0.5681818181818182}\n{'loss': 0.8278, 'grad_norm': 1.2400301694869995, 'learning_rate': 0.00021426324405517743, 'epoch': 0.5909090909090909}\n{'loss': 0.8025, 'grad_norm': 1.0862468481063843, 'learning_rate': 0.00022283377381738452, 'epoch': 0.6136363636363636}\n{'loss': 0.7715, 'grad_norm': 0.7414030432701111, 'learning_rate': 0.0002314043035795916, 'epoch': 0.6363636363636364}\n{'loss': 0.6575, 'grad_norm': 1.8142770528793335, 'learning_rate': 0.0002399748333417987, 'epoch': 0.6590909090909091}\n{'loss': 0.6658, 'grad_norm': 0.9208345413208008, 'learning_rate': 0.00024854536310400577, 'epoch': 0.6818181818181818}\n{'loss': 0.666, 'grad_norm': 0.8254461884498596, 'learning_rate': 0.00025711589286621286, 'epoch': 0.7045454545454546}\n{'loss': 0.7078, 'grad_norm': 1.60720956325531, 'learning_rate': 0.00026568642262842, 'epoch': 0.7272727272727273}\n{'loss': 0.6209, 'grad_norm': 0.6631209850311279, 'learning_rate': 0.0002742569523906271, 'epoch': 0.75}\n{'loss': 0.5743, 'grad_norm': 0.8121837377548218, 'learning_rate': 0.0002828274821528342, 'epoch': 0.7727272727272727}\n{'loss': 0.6403, 'grad_norm': 0.7589470148086548, 'learning_rate': 0.0002913980119150413, 'epoch': 0.7954545454545454}\n{'loss': 0.5887, 'grad_norm': 1.1405525207519531, 'learning_rate': 0.00029996854167724837, 'epoch': 0.8181818181818182}\n{'loss': 0.4869, 'grad_norm': 0.9348194003105164, 'learning_rate': 0.0003085390714394555, 'epoch': 0.8409090909090909}\n{'loss': 0.5239, 'grad_norm': 0.9831450581550598, 'learning_rate': 0.0003171096012016626, 'epoch': 0.8636363636363636}\n{'loss': 0.5081, 'grad_norm': 1.131811261177063, 'learning_rate': 0.0003256801309638697, 'epoch': 0.8863636363636364}\n{'loss': 0.4553, 'grad_norm': 0.6266830563545227, 'learning_rate': 0.00033425066072607674, 'epoch': 0.9090909090909091}\n{'loss': 0.4589, 'grad_norm': 0.8430483937263489, 'learning_rate': 0.00034282119048828383, 'epoch': 0.9318181818181818}\n{'loss': 0.4417, 'grad_norm': 1.1050828695297241, 'learning_rate': 0.0003513917202504909, 'epoch': 0.9545454545454546}\n{'loss': 0.4685, 'grad_norm': 1.2183518409729004, 'learning_rate': 0.00035996225001269806, 'epoch': 0.9772727272727273}\n{'loss': 0.4066, 'grad_norm': 2.5051393508911133, 'learning_rate': 0.00036853277977490516, 'epoch': 1.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.7826    0.7548    0.7685      3311\n     PERCENT     0.4629    0.9419    0.6207        86\n        TYPE     0.8889    0.9479    0.9175     11299\n      VOLUME     1.0000    0.0476    0.0909        42\n\n   micro avg     0.8621    0.9019    0.8815     14738\n   macro avg     0.7836    0.6730    0.5994     14738\nweighted avg     0.8629    0.9019    0.8799     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.4022563695907593, 'eval_f1_macro': 0.5993778899037292, 'eval_precision': 0.8621092229861201, 'eval_recall': 0.9018862803636857, 'eval_f1': 0.8815492770924526, 'eval_accuracy': 0.878421944156057, 'eval_runtime': 1.4402, 'eval_samples_per_second': 3825.75, 'eval_steps_per_second': 7.638, 'epoch': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.3809, 'grad_norm': 1.5409178733825684, 'learning_rate': 0.00037710330953711225, 'epoch': 1.0227272727272727}\n{'loss': 0.4058, 'grad_norm': 1.476279616355896, 'learning_rate': 0.00037615102845242257, 'epoch': 1.0454545454545454}\n{'loss': 0.4007, 'grad_norm': 0.9998605847358704, 'learning_rate': 0.0003751987473677329, 'epoch': 1.0681818181818181}\n{'loss': 0.3742, 'grad_norm': 0.7594941854476929, 'learning_rate': 0.0003742464662830432, 'epoch': 1.0909090909090908}\n{'loss': 0.3872, 'grad_norm': 2.2018465995788574, 'learning_rate': 0.00037329418519835354, 'epoch': 1.1136363636363635}\n{'loss': 0.4294, 'grad_norm': 1.2780629396438599, 'learning_rate': 0.00037234190411366386, 'epoch': 1.1363636363636362}\n{'loss': 0.3169, 'grad_norm': 1.1989537477493286, 'learning_rate': 0.0003713896230289742, 'epoch': 1.1590909090909092}\n{'loss': 0.3583, 'grad_norm': 1.5838748216629028, 'learning_rate': 0.0003704373419442845, 'epoch': 1.1818181818181819}\n{'loss': 0.3529, 'grad_norm': 1.340747356414795, 'learning_rate': 0.00036948506085959483, 'epoch': 1.2045454545454546}\n{'loss': 0.4091, 'grad_norm': 0.8781798481941223, 'learning_rate': 0.00036853277977490516, 'epoch': 1.2272727272727273}\n{'loss': 0.3626, 'grad_norm': 0.8984783291816711, 'learning_rate': 0.0003675804986902155, 'epoch': 1.25}\n{'loss': 0.316, 'grad_norm': 0.9967251420021057, 'learning_rate': 0.0003666282176055258, 'epoch': 1.2727272727272727}\n{'loss': 0.3155, 'grad_norm': 1.1110622882843018, 'learning_rate': 0.0003656759365208361, 'epoch': 1.2954545454545454}\n{'loss': 0.3582, 'grad_norm': 0.7743876576423645, 'learning_rate': 0.00036472365543614645, 'epoch': 1.3181818181818181}\n{'loss': 0.2846, 'grad_norm': 1.553097128868103, 'learning_rate': 0.00036377137435145677, 'epoch': 1.3409090909090908}\n{'loss': 0.3647, 'grad_norm': 1.5552549362182617, 'learning_rate': 0.0003628190932667671, 'epoch': 1.3636363636363638}\n{'loss': 0.3254, 'grad_norm': 2.19389009475708, 'learning_rate': 0.0003618668121820774, 'epoch': 1.3863636363636362}\n{'loss': 0.3491, 'grad_norm': 1.109595537185669, 'learning_rate': 0.00036091453109738774, 'epoch': 1.4090909090909092}\n{'loss': 0.2957, 'grad_norm': 1.8188775777816772, 'learning_rate': 0.00035996225001269806, 'epoch': 1.4318181818181819}\n{'loss': 0.3156, 'grad_norm': 1.727252721786499, 'learning_rate': 0.0003590099689280084, 'epoch': 1.4545454545454546}\n{'loss': 0.2721, 'grad_norm': 1.1035289764404297, 'learning_rate': 0.0003580576878433187, 'epoch': 1.4772727272727273}\n{'loss': 0.3247, 'grad_norm': 0.7366515398025513, 'learning_rate': 0.00035710540675862903, 'epoch': 1.5}\n{'loss': 0.2936, 'grad_norm': 0.7732305526733398, 'learning_rate': 0.00035615312567393936, 'epoch': 1.5227272727272727}\n{'loss': 0.2744, 'grad_norm': 0.7845451831817627, 'learning_rate': 0.0003552008445892497, 'epoch': 1.5454545454545454}\n{'loss': 0.2543, 'grad_norm': 1.0446276664733887, 'learning_rate': 0.00035424856350456, 'epoch': 1.5681818181818183}\n{'loss': 0.2995, 'grad_norm': 0.8265702128410339, 'learning_rate': 0.0003532962824198703, 'epoch': 1.5909090909090908}\n{'loss': 0.2651, 'grad_norm': 1.1392115354537964, 'learning_rate': 0.00035234400133518065, 'epoch': 1.6136363636363638}\n{'loss': 0.3017, 'grad_norm': 0.9404561519622803, 'learning_rate': 0.0003513917202504909, 'epoch': 1.6363636363636362}\n{'loss': 0.2655, 'grad_norm': 1.3449620008468628, 'learning_rate': 0.0003504394391658013, 'epoch': 1.6590909090909092}\n{'loss': 0.3046, 'grad_norm': 1.060596227645874, 'learning_rate': 0.0003494871580811116, 'epoch': 1.6818181818181817}\n{'loss': 0.2897, 'grad_norm': 1.3787106275558472, 'learning_rate': 0.0003485348769964219, 'epoch': 1.7045454545454546}\n{'loss': 0.265, 'grad_norm': 0.9322254061698914, 'learning_rate': 0.00034758259591173227, 'epoch': 1.7272727272727273}\n{'loss': 0.2721, 'grad_norm': 1.2394084930419922, 'learning_rate': 0.0003466303148270426, 'epoch': 1.75}\n{'loss': 0.31, 'grad_norm': 1.3529975414276123, 'learning_rate': 0.00034567803374235286, 'epoch': 1.7727272727272727}\n{'loss': 0.226, 'grad_norm': 0.7827669382095337, 'learning_rate': 0.00034472575265766324, 'epoch': 1.7954545454545454}\n{'loss': 0.2796, 'grad_norm': 0.7107253670692444, 'learning_rate': 0.00034377347157297356, 'epoch': 1.8181818181818183}\n{'loss': 0.2341, 'grad_norm': 0.97066330909729, 'learning_rate': 0.00034282119048828383, 'epoch': 1.8409090909090908}\n{'loss': 0.2918, 'grad_norm': 0.762473464012146, 'learning_rate': 0.0003418689094035942, 'epoch': 1.8636363636363638}\n{'loss': 0.2699, 'grad_norm': 1.5689127445220947, 'learning_rate': 0.00034091662831890453, 'epoch': 1.8863636363636362}\n{'loss': 0.2708, 'grad_norm': 0.8511403203010559, 'learning_rate': 0.0003399643472342148, 'epoch': 1.9090909090909092}\n{'loss': 0.3061, 'grad_norm': 0.8318237662315369, 'learning_rate': 0.0003390120661495252, 'epoch': 1.9318181818181817}\n{'loss': 0.2742, 'grad_norm': 1.1766648292541504, 'learning_rate': 0.0003380597850648355, 'epoch': 1.9545454545454546}\n{'loss': 0.2374, 'grad_norm': 1.068446159362793, 'learning_rate': 0.00033710750398014577, 'epoch': 1.9772727272727273}\n{'loss': 0.2118, 'grad_norm': 4.465248107910156, 'learning_rate': 0.00033615522289545614, 'epoch': 2.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8493    0.8831    0.8659      3311\n     PERCENT     0.7547    0.9302    0.8333        86\n        TYPE     0.9419    0.9640    0.9528     11299\n      VOLUME     0.5000    0.4286    0.4615        42\n\n   micro avg     0.9185    0.9441    0.9311     14738\n   macro avg     0.7615    0.8015    0.7784     14738\nweighted avg     0.9187    0.9441    0.9312     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.250018835067749, 'eval_f1_macro': 0.7783837270351155, 'eval_precision': 0.9184764670935375, 'eval_recall': 0.9440901072058624, 'eval_f1': 0.93110717034162, 'eval_accuracy': 0.9240478662368177, 'eval_runtime': 1.4466, 'eval_samples_per_second': 3808.895, 'eval_steps_per_second': 7.604, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.182, 'grad_norm': 0.767128586769104, 'learning_rate': 0.0003352029418107664, 'epoch': 2.022727272727273}\n{'loss': 0.1819, 'grad_norm': 0.8533474206924438, 'learning_rate': 0.00033425066072607674, 'epoch': 2.0454545454545454}\n{'loss': 0.1993, 'grad_norm': 1.0961052179336548, 'learning_rate': 0.0003332983796413871, 'epoch': 2.0681818181818183}\n{'loss': 0.2172, 'grad_norm': 1.258623719215393, 'learning_rate': 0.0003323460985566974, 'epoch': 2.090909090909091}\n{'loss': 0.1824, 'grad_norm': 0.8636970520019531, 'learning_rate': 0.0003313938174720077, 'epoch': 2.1136363636363638}\n{'loss': 0.1341, 'grad_norm': 0.5416932106018066, 'learning_rate': 0.0003304415363873181, 'epoch': 2.1363636363636362}\n{'loss': 0.1739, 'grad_norm': 0.6779907941818237, 'learning_rate': 0.00032948925530262835, 'epoch': 2.159090909090909}\n{'loss': 0.1892, 'grad_norm': 0.8584607839584351, 'learning_rate': 0.00032853697421793873, 'epoch': 2.1818181818181817}\n{'loss': 0.2052, 'grad_norm': 1.1700665950775146, 'learning_rate': 0.00032758469313324905, 'epoch': 2.2045454545454546}\n{'loss': 0.2114, 'grad_norm': 0.810723066329956, 'learning_rate': 0.0003266324120485593, 'epoch': 2.227272727272727}\n{'loss': 0.2344, 'grad_norm': 2.502500534057617, 'learning_rate': 0.0003256801309638697, 'epoch': 2.25}\n{'loss': 0.1648, 'grad_norm': 2.255537748336792, 'learning_rate': 0.00032472784987918, 'epoch': 2.2727272727272725}\n{'loss': 0.2139, 'grad_norm': 1.429704189300537, 'learning_rate': 0.0003237755687944903, 'epoch': 2.2954545454545454}\n{'loss': 0.2055, 'grad_norm': 1.599241018295288, 'learning_rate': 0.00032282328770980067, 'epoch': 2.3181818181818183}\n{'loss': 0.1107, 'grad_norm': 0.7220579385757446, 'learning_rate': 0.00032187100662511094, 'epoch': 2.340909090909091}\n{'loss': 0.189, 'grad_norm': 1.0723267793655396, 'learning_rate': 0.00032091872554042126, 'epoch': 2.3636363636363638}\n{'loss': 0.1537, 'grad_norm': 0.7963499426841736, 'learning_rate': 0.00031996644445573164, 'epoch': 2.3863636363636362}\n{'loss': 0.2516, 'grad_norm': 1.7076278924942017, 'learning_rate': 0.0003190141633710419, 'epoch': 2.409090909090909}\n{'loss': 0.2031, 'grad_norm': 1.2327849864959717, 'learning_rate': 0.00031806188228635223, 'epoch': 2.4318181818181817}\n{'loss': 0.1251, 'grad_norm': 1.2667601108551025, 'learning_rate': 0.0003171096012016626, 'epoch': 2.4545454545454546}\n{'loss': 0.1777, 'grad_norm': 1.228053092956543, 'learning_rate': 0.0003161573201169729, 'epoch': 2.4772727272727275}\n{'loss': 0.1659, 'grad_norm': 0.8798856139183044, 'learning_rate': 0.0003152050390322832, 'epoch': 2.5}\n{'loss': 0.1801, 'grad_norm': 1.7944612503051758, 'learning_rate': 0.0003142527579475936, 'epoch': 2.5227272727272725}\n{'loss': 0.17, 'grad_norm': 2.2570810317993164, 'learning_rate': 0.00031330047686290385, 'epoch': 2.5454545454545454}\n{'loss': 0.1887, 'grad_norm': 1.9309760332107544, 'learning_rate': 0.00031234819577821417, 'epoch': 2.5681818181818183}\n{'loss': 0.2116, 'grad_norm': 1.7522540092468262, 'learning_rate': 0.00031139591469352455, 'epoch': 2.590909090909091}\n{'loss': 0.1721, 'grad_norm': 0.6546406149864197, 'learning_rate': 0.0003104436336088348, 'epoch': 2.6136363636363638}\n{'loss': 0.2056, 'grad_norm': 0.9844499230384827, 'learning_rate': 0.00030949135252414514, 'epoch': 2.6363636363636362}\n{'loss': 0.168, 'grad_norm': 1.0412553548812866, 'learning_rate': 0.0003085390714394555, 'epoch': 2.659090909090909}\n{'loss': 0.139, 'grad_norm': 1.25169837474823, 'learning_rate': 0.0003075867903547658, 'epoch': 2.6818181818181817}\n{'loss': 0.1448, 'grad_norm': 1.353783130645752, 'learning_rate': 0.0003066345092700761, 'epoch': 2.7045454545454546}\n{'loss': 0.2077, 'grad_norm': 1.1377615928649902, 'learning_rate': 0.00030568222818538643, 'epoch': 2.7272727272727275}\n{'loss': 0.1901, 'grad_norm': 1.1551589965820312, 'learning_rate': 0.00030472994710069675, 'epoch': 2.75}\n{'loss': 0.1784, 'grad_norm': 0.8692265152931213, 'learning_rate': 0.0003037776660160071, 'epoch': 2.7727272727272725}\n{'loss': 0.1746, 'grad_norm': 1.7844098806381226, 'learning_rate': 0.0003028253849313174, 'epoch': 2.7954545454545454}\n{'loss': 0.1872, 'grad_norm': 2.086782217025757, 'learning_rate': 0.0003018731038466277, 'epoch': 2.8181818181818183}\n{'loss': 0.2083, 'grad_norm': 1.3663240671157837, 'learning_rate': 0.00030092082276193805, 'epoch': 2.840909090909091}\n{'loss': 0.145, 'grad_norm': 1.2878988981246948, 'learning_rate': 0.00029996854167724837, 'epoch': 2.8636363636363638}\n{'loss': 0.1788, 'grad_norm': 1.0510615110397339, 'learning_rate': 0.0002990162605925587, 'epoch': 2.8863636363636362}\n{'loss': 0.1704, 'grad_norm': 0.992620587348938, 'learning_rate': 0.000298063979507869, 'epoch': 2.909090909090909}\n{'loss': 0.1923, 'grad_norm': 1.0784764289855957, 'learning_rate': 0.00029711169842317934, 'epoch': 2.9318181818181817}\n{'loss': 0.1827, 'grad_norm': 1.0757224559783936, 'learning_rate': 0.00029615941733848966, 'epoch': 2.9545454545454546}\n{'loss': 0.2027, 'grad_norm': 1.5127369165420532, 'learning_rate': 0.0002952071362538, 'epoch': 2.9772727272727275}\n{'loss': 0.1184, 'grad_norm': 2.352874994277954, 'learning_rate': 0.0002942548551691103, 'epoch': 3.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8352    0.9124    0.8721      3311\n     PERCENT     0.8587    0.9186    0.8876        86\n        TYPE     0.9496    0.9586    0.9541     11299\n      VOLUME     0.7209    0.7381    0.7294        42\n\n   micro avg     0.9211    0.9473    0.9340     14738\n   macro avg     0.8411    0.8819    0.8608     14738\nweighted avg     0.9227    0.9473    0.9346     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.2553575336933136, 'eval_f1_macro': 0.8608070899596147, 'eval_precision': 0.9210977701543739, 'eval_recall': 0.9473469941647442, 'eval_f1': 0.9340379983944341, 'eval_accuracy': 0.9262881809737172, 'eval_runtime': 1.4337, 'eval_samples_per_second': 3843.191, 'eval_steps_per_second': 7.672, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.1002, 'grad_norm': 1.0748826265335083, 'learning_rate': 0.00029330257408442063, 'epoch': 3.022727272727273}\n{'loss': 0.1405, 'grad_norm': 1.0490604639053345, 'learning_rate': 0.00029235029299973095, 'epoch': 3.0454545454545454}\n{'loss': 0.1198, 'grad_norm': 0.7158685922622681, 'learning_rate': 0.0002913980119150413, 'epoch': 3.0681818181818183}\n{'loss': 0.1408, 'grad_norm': 1.9183417558670044, 'learning_rate': 0.0002904457308303516, 'epoch': 3.090909090909091}\n{'loss': 0.1544, 'grad_norm': 2.244694232940674, 'learning_rate': 0.0002894934497456619, 'epoch': 3.1136363636363638}\n{'loss': 0.1206, 'grad_norm': 1.425843358039856, 'learning_rate': 0.00028854116866097225, 'epoch': 3.1363636363636362}\n{'loss': 0.1579, 'grad_norm': 1.4814718961715698, 'learning_rate': 0.00028758888757628257, 'epoch': 3.159090909090909}\n{'loss': 0.0864, 'grad_norm': 0.7956074476242065, 'learning_rate': 0.0002866366064915929, 'epoch': 3.1818181818181817}\n{'loss': 0.1128, 'grad_norm': 1.8649814128875732, 'learning_rate': 0.0002856843254069032, 'epoch': 3.2045454545454546}\n{'loss': 0.1296, 'grad_norm': 1.5598043203353882, 'learning_rate': 0.00028473204432221354, 'epoch': 3.227272727272727}\n{'loss': 0.1388, 'grad_norm': 1.6737065315246582, 'learning_rate': 0.00028377976323752386, 'epoch': 3.25}\n{'loss': 0.1233, 'grad_norm': 1.029483675956726, 'learning_rate': 0.0002828274821528342, 'epoch': 3.2727272727272725}\n{'loss': 0.1065, 'grad_norm': 0.7523295879364014, 'learning_rate': 0.0002818752010681445, 'epoch': 3.2954545454545454}\n{'loss': 0.1433, 'grad_norm': 1.4950048923492432, 'learning_rate': 0.00028092291998345483, 'epoch': 3.3181818181818183}\n{'loss': 0.0869, 'grad_norm': 0.7904081344604492, 'learning_rate': 0.00027997063889876516, 'epoch': 3.340909090909091}\n{'loss': 0.0895, 'grad_norm': 1.1948097944259644, 'learning_rate': 0.0002790183578140755, 'epoch': 3.3636363636363638}\n{'loss': 0.0932, 'grad_norm': 0.6558269262313843, 'learning_rate': 0.0002780660767293858, 'epoch': 3.3863636363636362}\n{'loss': 0.1066, 'grad_norm': 0.7845582365989685, 'learning_rate': 0.0002771137956446961, 'epoch': 3.409090909090909}\n{'loss': 0.099, 'grad_norm': 1.0241338014602661, 'learning_rate': 0.00027616151456000645, 'epoch': 3.4318181818181817}\n{'loss': 0.1251, 'grad_norm': 1.2329208850860596, 'learning_rate': 0.00027520923347531677, 'epoch': 3.4545454545454546}\n{'loss': 0.1119, 'grad_norm': 1.2965928316116333, 'learning_rate': 0.0002742569523906271, 'epoch': 3.4772727272727275}\n{'loss': 0.1237, 'grad_norm': 0.9338180422782898, 'learning_rate': 0.0002733046713059374, 'epoch': 3.5}\n{'loss': 0.1147, 'grad_norm': 1.3761382102966309, 'learning_rate': 0.00027235239022124774, 'epoch': 3.5227272727272725}\n{'loss': 0.0862, 'grad_norm': 0.7163923382759094, 'learning_rate': 0.00027140010913655806, 'epoch': 3.5454545454545454}\n{'loss': 0.1491, 'grad_norm': 1.1482317447662354, 'learning_rate': 0.0002704478280518684, 'epoch': 3.5681818181818183}\n{'loss': 0.1355, 'grad_norm': 1.5331212282180786, 'learning_rate': 0.0002694955469671787, 'epoch': 3.590909090909091}\n{'loss': 0.124, 'grad_norm': 1.1826707124710083, 'learning_rate': 0.00026854326588248903, 'epoch': 3.6136363636363638}\n{'loss': 0.1285, 'grad_norm': 0.7708932161331177, 'learning_rate': 0.00026759098479779936, 'epoch': 3.6363636363636362}\n{'loss': 0.1169, 'grad_norm': 2.0489938259124756, 'learning_rate': 0.0002666387037131097, 'epoch': 3.659090909090909}\n{'loss': 0.1335, 'grad_norm': 1.4020951986312866, 'learning_rate': 0.00026568642262842, 'epoch': 3.6818181818181817}\n{'loss': 0.1235, 'grad_norm': 2.139042854309082, 'learning_rate': 0.0002647341415437303, 'epoch': 3.7045454545454546}\n{'loss': 0.1245, 'grad_norm': 0.6566277742385864, 'learning_rate': 0.00026378186045904065, 'epoch': 3.7272727272727275}\n{'loss': 0.1341, 'grad_norm': 0.7097314596176147, 'learning_rate': 0.00026282957937435097, 'epoch': 3.75}\n{'loss': 0.1098, 'grad_norm': 0.7672587633132935, 'learning_rate': 0.0002618772982896613, 'epoch': 3.7727272727272725}\n{'loss': 0.1316, 'grad_norm': 0.8620023131370544, 'learning_rate': 0.0002609250172049716, 'epoch': 3.7954545454545454}\n{'loss': 0.1178, 'grad_norm': 1.1058387756347656, 'learning_rate': 0.00025997273612028194, 'epoch': 3.8181818181818183}\n{'loss': 0.167, 'grad_norm': 1.014306664466858, 'learning_rate': 0.00025902045503559226, 'epoch': 3.840909090909091}\n{'loss': 0.1488, 'grad_norm': 1.0501199960708618, 'learning_rate': 0.0002580681739509026, 'epoch': 3.8636363636363638}\n{'loss': 0.0874, 'grad_norm': 1.0215970277786255, 'learning_rate': 0.00025711589286621286, 'epoch': 3.8863636363636362}\n{'loss': 0.1262, 'grad_norm': 1.4047781229019165, 'learning_rate': 0.00025616361178152323, 'epoch': 3.909090909090909}\n{'loss': 0.1492, 'grad_norm': 1.452738642692566, 'learning_rate': 0.00025521133069683356, 'epoch': 3.9318181818181817}\n{'loss': 0.1323, 'grad_norm': 0.851482093334198, 'learning_rate': 0.0002542590496121438, 'epoch': 3.9545454545454546}\n{'loss': 0.1295, 'grad_norm': 0.6850618720054626, 'learning_rate': 0.0002533067685274542, 'epoch': 3.9772727272727275}\n{'loss': 0.0122, 'grad_norm': 0.571880042552948, 'learning_rate': 0.0002523544874427645, 'epoch': 4.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.9084    0.8810    0.8945      3311\n     PERCENT     0.8621    0.8721    0.8671        86\n        TYPE     0.9387    0.9738    0.9560     11299\n      VOLUME     0.7447    0.8333    0.7865        42\n\n   micro avg     0.9312    0.9520    0.9415     14738\n   macro avg     0.8635    0.8901    0.8760     14738\nweighted avg     0.9309    0.9520    0.9411     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.2546084225177765, 'eval_f1_macro': 0.8760077774850679, 'eval_precision': 0.9312358953936015, 'eval_recall': 0.9519609173564935, 'eval_f1': 0.9414843645148302, 'eval_accuracy': 0.9325719906016064, 'eval_runtime': 1.4311, 'eval_samples_per_second': 3850.222, 'eval_steps_per_second': 7.686, 'epoch': 4.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.0904, 'grad_norm': 1.300053596496582, 'learning_rate': 0.0002514022063580748, 'epoch': 4.0227272727272725}\n{'loss': 0.0677, 'grad_norm': 1.5368375778198242, 'learning_rate': 0.0002504499252733852, 'epoch': 4.045454545454546}\n{'loss': 0.112, 'grad_norm': 1.3456707000732422, 'learning_rate': 0.0002494976441886955, 'epoch': 4.068181818181818}\n{'loss': 0.0902, 'grad_norm': 1.3665732145309448, 'learning_rate': 0.00024854536310400577, 'epoch': 4.090909090909091}\n{'loss': 0.0728, 'grad_norm': 0.6142318844795227, 'learning_rate': 0.00024759308201931614, 'epoch': 4.113636363636363}\n{'loss': 0.0851, 'grad_norm': 1.812317967414856, 'learning_rate': 0.00024664080093462647, 'epoch': 4.136363636363637}\n{'loss': 0.1004, 'grad_norm': 2.5114355087280273, 'learning_rate': 0.00024568851984993673, 'epoch': 4.159090909090909}\n{'loss': 0.0773, 'grad_norm': 1.4548524618148804, 'learning_rate': 0.0002447362387652471, 'epoch': 4.181818181818182}\n{'loss': 0.0654, 'grad_norm': 0.9947723150253296, 'learning_rate': 0.00024378395768055744, 'epoch': 4.204545454545454}\n{'loss': 0.0925, 'grad_norm': 0.7769396901130676, 'learning_rate': 0.00024283167659586773, 'epoch': 4.2272727272727275}\n{'loss': 0.1004, 'grad_norm': 0.7474978566169739, 'learning_rate': 0.00024187939551117805, 'epoch': 4.25}\n{'loss': 0.104, 'grad_norm': 0.9429231882095337, 'learning_rate': 0.00024092711442648835, 'epoch': 4.2727272727272725}\n{'loss': 0.0828, 'grad_norm': 1.4230998754501343, 'learning_rate': 0.0002399748333417987, 'epoch': 4.295454545454546}\n{'loss': 0.1111, 'grad_norm': 1.6029061079025269, 'learning_rate': 0.00023902255225710902, 'epoch': 4.318181818181818}\n{'loss': 0.1, 'grad_norm': 0.952422559261322, 'learning_rate': 0.00023807027117241932, 'epoch': 4.340909090909091}\n{'loss': 0.0744, 'grad_norm': 0.9811713099479675, 'learning_rate': 0.00023711799008772967, 'epoch': 4.363636363636363}\n{'loss': 0.0952, 'grad_norm': 1.5176396369934082, 'learning_rate': 0.00023616570900304002, 'epoch': 4.386363636363637}\n{'loss': 0.1129, 'grad_norm': 1.5954244136810303, 'learning_rate': 0.0002352134279183503, 'epoch': 4.409090909090909}\n{'loss': 0.0701, 'grad_norm': 0.7290021777153015, 'learning_rate': 0.00023426114683366064, 'epoch': 4.431818181818182}\n{'loss': 0.0652, 'grad_norm': 0.7726999521255493, 'learning_rate': 0.000233308865748971, 'epoch': 4.454545454545454}\n{'loss': 0.0913, 'grad_norm': 0.6646669507026672, 'learning_rate': 0.00023235658466428129, 'epoch': 4.4772727272727275}\n{'loss': 0.1137, 'grad_norm': 1.0310022830963135, 'learning_rate': 0.0002314043035795916, 'epoch': 4.5}\n{'loss': 0.0806, 'grad_norm': 1.0102311372756958, 'learning_rate': 0.00023045202249490196, 'epoch': 4.5227272727272725}\n{'loss': 0.0761, 'grad_norm': 0.5585001111030579, 'learning_rate': 0.00022949974141021226, 'epoch': 4.545454545454545}\n{'loss': 0.0969, 'grad_norm': 0.7718586921691895, 'learning_rate': 0.00022854746032552258, 'epoch': 4.568181818181818}\n{'loss': 0.085, 'grad_norm': 0.7184104919433594, 'learning_rate': 0.00022759517924083287, 'epoch': 4.590909090909091}\n{'loss': 0.1105, 'grad_norm': 0.8042815327644348, 'learning_rate': 0.00022664289815614323, 'epoch': 4.613636363636363}\n{'loss': 0.0843, 'grad_norm': 0.9635081887245178, 'learning_rate': 0.00022569061707145355, 'epoch': 4.636363636363637}\n{'loss': 0.0972, 'grad_norm': 1.009185552597046, 'learning_rate': 0.00022473833598676384, 'epoch': 4.659090909090909}\n{'loss': 0.0783, 'grad_norm': 1.7792048454284668, 'learning_rate': 0.0002237860549020742, 'epoch': 4.681818181818182}\n{'loss': 0.0869, 'grad_norm': 0.6970709562301636, 'learning_rate': 0.00022283377381738452, 'epoch': 4.704545454545455}\n{'loss': 0.0728, 'grad_norm': 0.6592397093772888, 'learning_rate': 0.00022188149273269481, 'epoch': 4.7272727272727275}\n{'loss': 0.0918, 'grad_norm': 1.10769784450531, 'learning_rate': 0.00022092921164800516, 'epoch': 4.75}\n{'loss': 0.0957, 'grad_norm': 1.9438018798828125, 'learning_rate': 0.0002199769305633155, 'epoch': 4.7727272727272725}\n{'loss': 0.0816, 'grad_norm': 1.6283518075942993, 'learning_rate': 0.00021902464947862578, 'epoch': 4.795454545454545}\n{'loss': 0.0797, 'grad_norm': 0.9353163242340088, 'learning_rate': 0.00021807236839393613, 'epoch': 4.818181818181818}\n{'loss': 0.0623, 'grad_norm': 0.6252528429031372, 'learning_rate': 0.00021712008730924646, 'epoch': 4.840909090909091}\n{'loss': 0.088, 'grad_norm': 1.1258279085159302, 'learning_rate': 0.00021616780622455675, 'epoch': 4.863636363636363}\n{'loss': 0.1, 'grad_norm': 1.241145372390747, 'learning_rate': 0.0002152155251398671, 'epoch': 4.886363636363637}\n{'loss': 0.0831, 'grad_norm': 2.1164472103118896, 'learning_rate': 0.00021426324405517743, 'epoch': 4.909090909090909}\n{'loss': 0.0915, 'grad_norm': 1.6567350625991821, 'learning_rate': 0.00021331096297048772, 'epoch': 4.931818181818182}\n{'loss': 0.0572, 'grad_norm': 0.99610435962677, 'learning_rate': 0.00021235868188579807, 'epoch': 4.954545454545455}\n{'loss': 0.0676, 'grad_norm': 0.7027435898780823, 'learning_rate': 0.00021140640080110837, 'epoch': 4.9772727272727275}\n{'loss': 0.1601, 'grad_norm': 12.088275909423828, 'learning_rate': 0.0002104541197164187, 'epoch': 5.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.9159    0.8650    0.8897      3311\n     PERCENT     0.8621    0.8721    0.8671        86\n        TYPE     0.9458    0.9683    0.9569     11299\n      VOLUME     0.7255    0.8810    0.7957        42\n\n   micro avg     0.9382    0.9443    0.9413     14738\n   macro avg     0.8623    0.8966    0.8773     14738\nweighted avg     0.9380    0.9443    0.9408     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.27835988998413086, 'eval_f1_macro': 0.8773482693829229, 'eval_precision': 0.9382458032764781, 'eval_recall': 0.9442936626407925, 'eval_f1': 0.9412600182611343, 'eval_accuracy': 0.9302770340418557, 'eval_runtime': 1.4733, 'eval_samples_per_second': 3739.806, 'eval_steps_per_second': 7.466, 'epoch': 5.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.0496, 'grad_norm': 1.0139833688735962, 'learning_rate': 0.00020950183863172904, 'epoch': 5.0227272727272725}\n{'loss': 0.0916, 'grad_norm': 1.9086459875106812, 'learning_rate': 0.00020854955754703934, 'epoch': 5.045454545454546}\n{'loss': 0.0615, 'grad_norm': 0.9436097145080566, 'learning_rate': 0.00020759727646234966, 'epoch': 5.068181818181818}\n{'loss': 0.082, 'grad_norm': 0.9298124313354492, 'learning_rate': 0.00020664499537766, 'epoch': 5.090909090909091}\n{'loss': 0.0663, 'grad_norm': 1.1448270082473755, 'learning_rate': 0.0002056927142929703, 'epoch': 5.113636363636363}\n{'loss': 0.0677, 'grad_norm': 0.7470268607139587, 'learning_rate': 0.00020474043320828063, 'epoch': 5.136363636363637}\n{'loss': 0.0667, 'grad_norm': 0.6585965156555176, 'learning_rate': 0.00020378815212359098, 'epoch': 5.159090909090909}\n{'loss': 0.0583, 'grad_norm': 1.0819507837295532, 'learning_rate': 0.00020283587103890128, 'epoch': 5.181818181818182}\n{'loss': 0.0699, 'grad_norm': 0.769669771194458, 'learning_rate': 0.0002018835899542116, 'epoch': 5.204545454545454}\n{'loss': 0.071, 'grad_norm': 0.6953791975975037, 'learning_rate': 0.00020093130886952195, 'epoch': 5.2272727272727275}\n{'loss': 0.0528, 'grad_norm': 0.7035549283027649, 'learning_rate': 0.00019997902778483225, 'epoch': 5.25}\n{'loss': 0.0526, 'grad_norm': 0.9485743045806885, 'learning_rate': 0.00019902674670014257, 'epoch': 5.2727272727272725}\n{'loss': 0.0652, 'grad_norm': 0.6355888247489929, 'learning_rate': 0.00019807446561545292, 'epoch': 5.295454545454546}\n{'loss': 0.0539, 'grad_norm': 0.7281107306480408, 'learning_rate': 0.00019712218453076322, 'epoch': 5.318181818181818}\n{'loss': 0.0588, 'grad_norm': 1.2002921104431152, 'learning_rate': 0.00019616990344607354, 'epoch': 5.340909090909091}\n{'loss': 0.0527, 'grad_norm': 0.6484082937240601, 'learning_rate': 0.00019521762236138383, 'epoch': 5.363636363636363}\n{'loss': 0.0667, 'grad_norm': 1.2065883874893188, 'learning_rate': 0.00019426534127669419, 'epoch': 5.386363636363637}\n{'loss': 0.0444, 'grad_norm': 0.9410840272903442, 'learning_rate': 0.0001933130601920045, 'epoch': 5.409090909090909}\n{'loss': 0.05, 'grad_norm': 1.663072943687439, 'learning_rate': 0.0001923607791073148, 'epoch': 5.431818181818182}\n{'loss': 0.0593, 'grad_norm': 1.2561490535736084, 'learning_rate': 0.00019140849802262515, 'epoch': 5.454545454545454}\n{'loss': 0.096, 'grad_norm': 0.7273252010345459, 'learning_rate': 0.00019045621693793548, 'epoch': 5.4772727272727275}\n{'loss': 0.0637, 'grad_norm': 0.8036755919456482, 'learning_rate': 0.00018950393585324577, 'epoch': 5.5}\n{'loss': 0.0675, 'grad_norm': 0.6219359040260315, 'learning_rate': 0.00018855165476855612, 'epoch': 5.5227272727272725}\n{'loss': 0.0703, 'grad_norm': 1.0745693445205688, 'learning_rate': 0.00018759937368386645, 'epoch': 5.545454545454545}\n{'loss': 0.0494, 'grad_norm': 0.5983836054801941, 'learning_rate': 0.00018664709259917677, 'epoch': 5.568181818181818}\n{'loss': 0.0677, 'grad_norm': 1.0812172889709473, 'learning_rate': 0.0001856948115144871, 'epoch': 5.590909090909091}\n{'loss': 0.0919, 'grad_norm': 1.038124918937683, 'learning_rate': 0.00018474253042979742, 'epoch': 5.613636363636363}\n{'loss': 0.0698, 'grad_norm': 1.180834412574768, 'learning_rate': 0.00018379024934510774, 'epoch': 5.636363636363637}\n{'loss': 0.0692, 'grad_norm': 0.7762024402618408, 'learning_rate': 0.00018283796826041806, 'epoch': 5.659090909090909}\n{'loss': 0.0524, 'grad_norm': 0.7621697783470154, 'learning_rate': 0.00018188568717572839, 'epoch': 5.681818181818182}\n{'loss': 0.0392, 'grad_norm': 0.9420737028121948, 'learning_rate': 0.0001809334060910387, 'epoch': 5.704545454545455}\n{'loss': 0.0671, 'grad_norm': 1.1118710041046143, 'learning_rate': 0.00017998112500634903, 'epoch': 5.7272727272727275}\n{'loss': 0.0506, 'grad_norm': 0.623485267162323, 'learning_rate': 0.00017902884392165936, 'epoch': 5.75}\n{'loss': 0.0547, 'grad_norm': 0.9686184525489807, 'learning_rate': 0.00017807656283696968, 'epoch': 5.7727272727272725}\n{'loss': 0.0619, 'grad_norm': 0.781832754611969, 'learning_rate': 0.00017712428175228, 'epoch': 5.795454545454545}\n{'loss': 0.0793, 'grad_norm': 0.8722221255302429, 'learning_rate': 0.00017617200066759033, 'epoch': 5.818181818181818}\n{'loss': 0.0818, 'grad_norm': 0.8527354598045349, 'learning_rate': 0.00017521971958290065, 'epoch': 5.840909090909091}\n{'loss': 0.0476, 'grad_norm': 0.6511086821556091, 'learning_rate': 0.00017426743849821094, 'epoch': 5.863636363636363}\n{'loss': 0.031, 'grad_norm': 0.5078900456428528, 'learning_rate': 0.0001733151574135213, 'epoch': 5.886363636363637}\n{'loss': 0.0553, 'grad_norm': 0.6585411429405212, 'learning_rate': 0.00017236287632883162, 'epoch': 5.909090909090909}\n{'loss': 0.0761, 'grad_norm': 1.0277420282363892, 'learning_rate': 0.00017141059524414191, 'epoch': 5.931818181818182}\n{'loss': 0.0522, 'grad_norm': 0.8131682872772217, 'learning_rate': 0.00017045831415945226, 'epoch': 5.954545454545455}\n{'loss': 0.0836, 'grad_norm': 1.275797963142395, 'learning_rate': 0.0001695060330747626, 'epoch': 5.9772727272727275}\n{'loss': 0.1037, 'grad_norm': 5.488955497741699, 'learning_rate': 0.00016855375199007288, 'epoch': 6.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8978    0.8916    0.8947      3311\n     PERCENT     0.8690    0.8488    0.8588        86\n        TYPE     0.9477    0.9677    0.9576     11299\n      VOLUME     0.7407    0.9524    0.8333        42\n\n   micro avg     0.9355    0.9499    0.9426     14738\n   macro avg     0.8638    0.9151    0.8861     14738\nweighted avg     0.9354    0.9499    0.9425     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.2885929346084595, 'eval_f1_macro': 0.8861016831438515, 'eval_precision': 0.9355118952151831, 'eval_recall': 0.949857511195549, 'eval_f1': 0.9426301259174467, 'eval_accuracy': 0.9323534233102017, 'eval_runtime': 1.4491, 'eval_samples_per_second': 3802.236, 'eval_steps_per_second': 7.591, 'epoch': 6.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.0545, 'grad_norm': 0.8219282627105713, 'learning_rate': 0.0001676014709053832, 'epoch': 6.0227272727272725}\n{'loss': 0.0306, 'grad_norm': 0.6495397090911865, 'learning_rate': 0.00016664918982069356, 'epoch': 6.045454545454546}\n{'loss': 0.0448, 'grad_norm': 0.7439308762550354, 'learning_rate': 0.00016569690873600385, 'epoch': 6.068181818181818}\n{'loss': 0.0534, 'grad_norm': 0.8071447014808655, 'learning_rate': 0.00016474462765131418, 'epoch': 6.090909090909091}\n{'loss': 0.046, 'grad_norm': 1.0155857801437378, 'learning_rate': 0.00016379234656662453, 'epoch': 6.113636363636363}\n{'loss': 0.04, 'grad_norm': 0.8204148411750793, 'learning_rate': 0.00016284006548193485, 'epoch': 6.136363636363637}\n{'loss': 0.0483, 'grad_norm': 1.3084107637405396, 'learning_rate': 0.00016188778439724515, 'epoch': 6.159090909090909}\n{'loss': 0.0408, 'grad_norm': 0.8715615272521973, 'learning_rate': 0.00016093550331255547, 'epoch': 6.181818181818182}\n{'loss': 0.0447, 'grad_norm': 0.9822795391082764, 'learning_rate': 0.00015998322222786582, 'epoch': 6.204545454545454}\n{'loss': 0.0456, 'grad_norm': 0.9126309156417847, 'learning_rate': 0.00015903094114317611, 'epoch': 6.2272727272727275}\n{'loss': 0.0496, 'grad_norm': 0.6353853940963745, 'learning_rate': 0.00015807866005848644, 'epoch': 6.25}\n{'loss': 0.0384, 'grad_norm': 0.5169248580932617, 'learning_rate': 0.0001571263789737968, 'epoch': 6.2727272727272725}\n{'loss': 0.0416, 'grad_norm': 0.6749980449676514, 'learning_rate': 0.00015617409788910708, 'epoch': 6.295454545454546}\n{'loss': 0.041, 'grad_norm': 1.0803271532058716, 'learning_rate': 0.0001552218168044174, 'epoch': 6.318181818181818}\n{'loss': 0.0562, 'grad_norm': 0.9223275184631348, 'learning_rate': 0.00015426953571972776, 'epoch': 6.340909090909091}\n{'loss': 0.0364, 'grad_norm': 0.74437415599823, 'learning_rate': 0.00015331725463503805, 'epoch': 6.363636363636363}\n{'loss': 0.0569, 'grad_norm': 0.9990183711051941, 'learning_rate': 0.00015236497355034838, 'epoch': 6.386363636363637}\n{'loss': 0.0793, 'grad_norm': 1.6215373277664185, 'learning_rate': 0.0001514126924656587, 'epoch': 6.409090909090909}\n{'loss': 0.0606, 'grad_norm': 0.6131123900413513, 'learning_rate': 0.00015046041138096902, 'epoch': 6.431818181818182}\n{'loss': 0.0411, 'grad_norm': 1.450209140777588, 'learning_rate': 0.00014950813029627935, 'epoch': 6.454545454545454}\n{'loss': 0.0613, 'grad_norm': 1.0578999519348145, 'learning_rate': 0.00014855584921158967, 'epoch': 6.4772727272727275}\n{'loss': 0.0351, 'grad_norm': 0.5774297118186951, 'learning_rate': 0.0001476035681269, 'epoch': 6.5}\n{'loss': 0.0365, 'grad_norm': 0.6459997296333313, 'learning_rate': 0.00014665128704221032, 'epoch': 6.5227272727272725}\n{'loss': 0.0472, 'grad_norm': 0.969372570514679, 'learning_rate': 0.00014569900595752064, 'epoch': 6.545454545454545}\n{'loss': 0.0751, 'grad_norm': 0.7822242379188538, 'learning_rate': 0.00014474672487283096, 'epoch': 6.568181818181818}\n{'loss': 0.057, 'grad_norm': 1.1547768115997314, 'learning_rate': 0.00014379444378814129, 'epoch': 6.590909090909091}\n{'loss': 0.045, 'grad_norm': 1.001288890838623, 'learning_rate': 0.0001428421627034516, 'epoch': 6.613636363636363}\n{'loss': 0.0478, 'grad_norm': 0.9066094756126404, 'learning_rate': 0.00014188988161876193, 'epoch': 6.636363636363637}\n{'loss': 0.0541, 'grad_norm': 0.9011508226394653, 'learning_rate': 0.00014093760053407225, 'epoch': 6.659090909090909}\n{'loss': 0.0398, 'grad_norm': 0.7449431419372559, 'learning_rate': 0.00013998531944938258, 'epoch': 6.681818181818182}\n{'loss': 0.0616, 'grad_norm': 1.0563735961914062, 'learning_rate': 0.0001390330383646929, 'epoch': 6.704545454545455}\n{'loss': 0.0542, 'grad_norm': 1.0267325639724731, 'learning_rate': 0.00013808075728000322, 'epoch': 6.7272727272727275}\n{'loss': 0.0558, 'grad_norm': 1.0449141263961792, 'learning_rate': 0.00013712847619531355, 'epoch': 6.75}\n{'loss': 0.0495, 'grad_norm': 0.8752303719520569, 'learning_rate': 0.00013617619511062387, 'epoch': 6.7727272727272725}\n{'loss': 0.0563, 'grad_norm': 0.6493651270866394, 'learning_rate': 0.0001352239140259342, 'epoch': 6.795454545454545}\n{'loss': 0.0416, 'grad_norm': 0.993462085723877, 'learning_rate': 0.00013427163294124452, 'epoch': 6.818181818181818}\n{'loss': 0.057, 'grad_norm': 0.8826106786727905, 'learning_rate': 0.00013331935185655484, 'epoch': 6.840909090909091}\n{'loss': 0.0461, 'grad_norm': 0.6972562670707703, 'learning_rate': 0.00013236707077186516, 'epoch': 6.863636363636363}\n{'loss': 0.046, 'grad_norm': 1.0477557182312012, 'learning_rate': 0.00013141478968717549, 'epoch': 6.886363636363637}\n{'loss': 0.0569, 'grad_norm': 0.7291962504386902, 'learning_rate': 0.0001304625086024858, 'epoch': 6.909090909090909}\n{'loss': 0.0689, 'grad_norm': 1.7685093879699707, 'learning_rate': 0.00012951022751779613, 'epoch': 6.931818181818182}\n{'loss': 0.0382, 'grad_norm': 0.806264340877533, 'learning_rate': 0.00012855794643310643, 'epoch': 6.954545454545455}\n{'loss': 0.0701, 'grad_norm': 1.1591415405273438, 'learning_rate': 0.00012760566534841678, 'epoch': 6.9772727272727275}\n{'loss': 0.1224, 'grad_norm': 3.33140230178833, 'learning_rate': 0.0001266533842637271, 'epoch': 7.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8927    0.9024    0.8976      3311\n     PERCENT     0.8333    0.9302    0.8791        86\n        TYPE     0.9493    0.9650    0.9571     11299\n      VOLUME     0.9048    0.9048    0.9048        42\n\n   micro avg     0.9358    0.9505    0.9431     14738\n   macro avg     0.8950    0.9256    0.9096     14738\nweighted avg     0.9358    0.9505    0.9431     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.2986876368522644, 'eval_f1_macro': 0.9096311903050441, 'eval_precision': 0.9358049432197729, 'eval_recall': 0.9505360293119827, 'eval_f1': 0.9431129662043893, 'eval_accuracy': 0.932954483361565, 'eval_runtime': 1.4318, 'eval_samples_per_second': 3848.407, 'eval_steps_per_second': 7.683, 'epoch': 7.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.0412, 'grad_norm': 0.8688991069793701, 'learning_rate': 0.0001257011031790374, 'epoch': 7.0227272727272725}\n{'loss': 0.0544, 'grad_norm': 0.9856285452842712, 'learning_rate': 0.00012474882209434775, 'epoch': 7.045454545454546}\n{'loss': 0.038, 'grad_norm': 0.4886079728603363, 'learning_rate': 0.00012379654100965807, 'epoch': 7.068181818181818}\n{'loss': 0.0461, 'grad_norm': 0.8902899622917175, 'learning_rate': 0.00012284425992496837, 'epoch': 7.090909090909091}\n{'loss': 0.0438, 'grad_norm': 0.7732462286949158, 'learning_rate': 0.00012189197884027872, 'epoch': 7.113636363636363}\n{'loss': 0.0436, 'grad_norm': 0.6078423857688904, 'learning_rate': 0.00012093969775558903, 'epoch': 7.136363636363637}\n{'loss': 0.0391, 'grad_norm': 0.8090946078300476, 'learning_rate': 0.00011998741667089935, 'epoch': 7.159090909090909}\n{'loss': 0.0209, 'grad_norm': 0.4499879777431488, 'learning_rate': 0.00011903513558620966, 'epoch': 7.181818181818182}\n{'loss': 0.0393, 'grad_norm': 0.947723388671875, 'learning_rate': 0.00011808285450152001, 'epoch': 7.204545454545454}\n{'loss': 0.0264, 'grad_norm': 0.8883640766143799, 'learning_rate': 0.00011713057341683032, 'epoch': 7.2272727272727275}\n{'loss': 0.0367, 'grad_norm': 0.8157065510749817, 'learning_rate': 0.00011617829233214064, 'epoch': 7.25}\n{'loss': 0.0214, 'grad_norm': 0.3874320089817047, 'learning_rate': 0.00011522601124745098, 'epoch': 7.2727272727272725}\n{'loss': 0.0687, 'grad_norm': 0.8971245288848877, 'learning_rate': 0.00011427373016276129, 'epoch': 7.295454545454546}\n{'loss': 0.0486, 'grad_norm': 0.6880170106887817, 'learning_rate': 0.00011332144907807161, 'epoch': 7.318181818181818}\n{'loss': 0.0506, 'grad_norm': 0.687621533870697, 'learning_rate': 0.00011236916799338192, 'epoch': 7.340909090909091}\n{'loss': 0.0415, 'grad_norm': 0.49251508712768555, 'learning_rate': 0.00011141688690869226, 'epoch': 7.363636363636363}\n{'loss': 0.05, 'grad_norm': 0.5893071293830872, 'learning_rate': 0.00011046460582400258, 'epoch': 7.386363636363637}\n{'loss': 0.0296, 'grad_norm': 0.7263716459274292, 'learning_rate': 0.00010951232473931289, 'epoch': 7.409090909090909}\n{'loss': 0.0347, 'grad_norm': 0.6927521824836731, 'learning_rate': 0.00010856004365462323, 'epoch': 7.431818181818182}\n{'loss': 0.0352, 'grad_norm': 0.572602391242981, 'learning_rate': 0.00010760776256993355, 'epoch': 7.454545454545454}\n{'loss': 0.0272, 'grad_norm': 0.5168672800064087, 'learning_rate': 0.00010665548148524386, 'epoch': 7.4772727272727275}\n{'loss': 0.0451, 'grad_norm': 0.5978520512580872, 'learning_rate': 0.00010570320040055418, 'epoch': 7.5}\n{'loss': 0.0381, 'grad_norm': 0.9647589325904846, 'learning_rate': 0.00010475091931586452, 'epoch': 7.5227272727272725}\n{'loss': 0.044, 'grad_norm': 0.8474941253662109, 'learning_rate': 0.00010379863823117483, 'epoch': 7.545454545454545}\n{'loss': 0.0223, 'grad_norm': 0.6868211030960083, 'learning_rate': 0.00010284635714648515, 'epoch': 7.568181818181818}\n{'loss': 0.0343, 'grad_norm': 1.0085656642913818, 'learning_rate': 0.00010189407606179549, 'epoch': 7.590909090909091}\n{'loss': 0.0599, 'grad_norm': 1.1626918315887451, 'learning_rate': 0.0001009417949771058, 'epoch': 7.613636363636363}\n{'loss': 0.0429, 'grad_norm': 0.7882731556892395, 'learning_rate': 9.998951389241612e-05, 'epoch': 7.636363636363637}\n{'loss': 0.043, 'grad_norm': 0.6799256205558777, 'learning_rate': 9.903723280772646e-05, 'epoch': 7.659090909090909}\n{'loss': 0.0307, 'grad_norm': 0.8878659605979919, 'learning_rate': 9.808495172303677e-05, 'epoch': 7.681818181818182}\n{'loss': 0.0459, 'grad_norm': 0.5486525893211365, 'learning_rate': 9.713267063834709e-05, 'epoch': 7.704545454545455}\n{'loss': 0.0482, 'grad_norm': 0.9913833737373352, 'learning_rate': 9.61803895536574e-05, 'epoch': 7.7272727272727275}\n{'loss': 0.0551, 'grad_norm': 1.6454143524169922, 'learning_rate': 9.522810846896774e-05, 'epoch': 7.75}\n{'loss': 0.0315, 'grad_norm': 0.651225745677948, 'learning_rate': 9.427582738427806e-05, 'epoch': 7.7727272727272725}\n{'loss': 0.0334, 'grad_norm': 0.6862701773643494, 'learning_rate': 9.332354629958839e-05, 'epoch': 7.795454545454545}\n{'loss': 0.0339, 'grad_norm': 0.5621911883354187, 'learning_rate': 9.237126521489871e-05, 'epoch': 7.818181818181818}\n{'loss': 0.0376, 'grad_norm': 0.6780627369880676, 'learning_rate': 9.141898413020903e-05, 'epoch': 7.840909090909091}\n{'loss': 0.0437, 'grad_norm': 0.7046545743942261, 'learning_rate': 9.046670304551935e-05, 'epoch': 7.863636363636363}\n{'loss': 0.0533, 'grad_norm': 0.7233238816261292, 'learning_rate': 8.951442196082968e-05, 'epoch': 7.886363636363637}\n{'loss': 0.0367, 'grad_norm': 0.6342945694923401, 'learning_rate': 8.856214087614e-05, 'epoch': 7.909090909090909}\n{'loss': 0.0534, 'grad_norm': 0.7096673846244812, 'learning_rate': 8.760985979145032e-05, 'epoch': 7.931818181818182}\n{'loss': 0.0433, 'grad_norm': 0.8611350655555725, 'learning_rate': 8.665757870676065e-05, 'epoch': 7.954545454545455}\n{'loss': 0.0225, 'grad_norm': 0.7965535521507263, 'learning_rate': 8.570529762207096e-05, 'epoch': 7.9772727272727275}\n{'loss': 0.0243, 'grad_norm': 1.9148300886154175, 'learning_rate': 8.47530165373813e-05, 'epoch': 8.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.9010    0.8904    0.8956      3311\n     PERCENT     0.8681    0.9186    0.8927        86\n        TYPE     0.9478    0.9670    0.9573     11299\n      VOLUME     0.8222    0.8810    0.8506        42\n\n   micro avg     0.9367    0.9492    0.9429     14738\n   macro avg     0.8848    0.9142    0.8990     14738\nweighted avg     0.9364    0.9492    0.9428     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.3224537670612335, 'eval_f1_macro': 0.8990394470620499, 'eval_precision': 0.9366630958757365, 'eval_recall': 0.9492468448907586, 'eval_f1': 0.9429129878007683, 'eval_accuracy': 0.9312059450303262, 'eval_runtime': 1.8178, 'eval_samples_per_second': 3031.109, 'eval_steps_per_second': 6.051, 'epoch': 8.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.0399, 'grad_norm': 0.6551015377044678, 'learning_rate': 8.38007354526916e-05, 'epoch': 8.022727272727273}\n{'loss': 0.0266, 'grad_norm': 0.8615599274635315, 'learning_rate': 8.284845436800193e-05, 'epoch': 8.045454545454545}\n{'loss': 0.0257, 'grad_norm': 0.9123463034629822, 'learning_rate': 8.189617328331226e-05, 'epoch': 8.068181818181818}\n{'loss': 0.0304, 'grad_norm': 0.7688705921173096, 'learning_rate': 8.094389219862257e-05, 'epoch': 8.090909090909092}\n{'loss': 0.0349, 'grad_norm': 0.9610728025436401, 'learning_rate': 7.999161111393291e-05, 'epoch': 8.113636363636363}\n{'loss': 0.0331, 'grad_norm': 0.8985051512718201, 'learning_rate': 7.903933002924322e-05, 'epoch': 8.136363636363637}\n{'loss': 0.0361, 'grad_norm': 0.787453830242157, 'learning_rate': 7.808704894455354e-05, 'epoch': 8.159090909090908}\n{'loss': 0.0386, 'grad_norm': 0.8544257879257202, 'learning_rate': 7.713476785986388e-05, 'epoch': 8.181818181818182}\n{'loss': 0.0281, 'grad_norm': 0.5123920440673828, 'learning_rate': 7.618248677517419e-05, 'epoch': 8.204545454545455}\n{'loss': 0.0349, 'grad_norm': 0.5685794949531555, 'learning_rate': 7.523020569048451e-05, 'epoch': 8.227272727272727}\n{'loss': 0.0466, 'grad_norm': 0.8824388980865479, 'learning_rate': 7.427792460579483e-05, 'epoch': 8.25}\n{'loss': 0.0231, 'grad_norm': 0.5523720383644104, 'learning_rate': 7.332564352110516e-05, 'epoch': 8.272727272727273}\n{'loss': 0.0296, 'grad_norm': 0.9459792375564575, 'learning_rate': 7.237336243641548e-05, 'epoch': 8.295454545454545}\n{'loss': 0.0467, 'grad_norm': 0.9202712774276733, 'learning_rate': 7.14210813517258e-05, 'epoch': 8.318181818181818}\n{'loss': 0.0391, 'grad_norm': 1.4454939365386963, 'learning_rate': 7.046880026703613e-05, 'epoch': 8.340909090909092}\n{'loss': 0.0321, 'grad_norm': 0.8647661805152893, 'learning_rate': 6.951651918234645e-05, 'epoch': 8.363636363636363}\n{'loss': 0.0442, 'grad_norm': 0.9583169221878052, 'learning_rate': 6.856423809765677e-05, 'epoch': 8.386363636363637}\n{'loss': 0.0423, 'grad_norm': 0.8693668842315674, 'learning_rate': 6.76119570129671e-05, 'epoch': 8.409090909090908}\n{'loss': 0.0355, 'grad_norm': 0.5433083772659302, 'learning_rate': 6.665967592827742e-05, 'epoch': 8.431818181818182}\n{'loss': 0.0298, 'grad_norm': 0.49162232875823975, 'learning_rate': 6.570739484358774e-05, 'epoch': 8.454545454545455}\n{'loss': 0.0261, 'grad_norm': 0.4988921582698822, 'learning_rate': 6.475511375889807e-05, 'epoch': 8.477272727272727}\n{'loss': 0.023, 'grad_norm': 0.4599452018737793, 'learning_rate': 6.380283267420839e-05, 'epoch': 8.5}\n{'loss': 0.0284, 'grad_norm': 0.6605324149131775, 'learning_rate': 6.28505515895187e-05, 'epoch': 8.522727272727273}\n{'loss': 0.0424, 'grad_norm': 0.8006429672241211, 'learning_rate': 6.189827050482904e-05, 'epoch': 8.545454545454545}\n{'loss': 0.0214, 'grad_norm': 0.5175203084945679, 'learning_rate': 6.094598942013936e-05, 'epoch': 8.568181818181818}\n{'loss': 0.0326, 'grad_norm': 1.1122419834136963, 'learning_rate': 5.9993708335449675e-05, 'epoch': 8.590909090909092}\n{'loss': 0.0482, 'grad_norm': 1.175468921661377, 'learning_rate': 5.9041427250760005e-05, 'epoch': 8.613636363636363}\n{'loss': 0.0242, 'grad_norm': 0.5369446873664856, 'learning_rate': 5.808914616607032e-05, 'epoch': 8.636363636363637}\n{'loss': 0.0419, 'grad_norm': 0.7995728254318237, 'learning_rate': 5.7136865081380645e-05, 'epoch': 8.659090909090908}\n{'loss': 0.0275, 'grad_norm': 0.49645864963531494, 'learning_rate': 5.618458399669096e-05, 'epoch': 8.681818181818182}\n{'loss': 0.0377, 'grad_norm': 0.6646742820739746, 'learning_rate': 5.523230291200129e-05, 'epoch': 8.704545454545455}\n{'loss': 0.0395, 'grad_norm': 0.8527050018310547, 'learning_rate': 5.4280021827311614e-05, 'epoch': 8.727272727272727}\n{'loss': 0.0244, 'grad_norm': 1.0212397575378418, 'learning_rate': 5.332774074262193e-05, 'epoch': 8.75}\n{'loss': 0.0198, 'grad_norm': 0.4521167576313019, 'learning_rate': 5.237545965793226e-05, 'epoch': 8.772727272727273}\n{'loss': 0.0329, 'grad_norm': 0.7956954836845398, 'learning_rate': 5.142317857324258e-05, 'epoch': 8.795454545454545}\n{'loss': 0.0277, 'grad_norm': 0.6282309889793396, 'learning_rate': 5.04708974885529e-05, 'epoch': 8.818181818181818}\n{'loss': 0.0262, 'grad_norm': 0.37758341431617737, 'learning_rate': 4.951861640386323e-05, 'epoch': 8.840909090909092}\n{'loss': 0.0167, 'grad_norm': 0.9897569417953491, 'learning_rate': 4.8566335319173546e-05, 'epoch': 8.863636363636363}\n{'loss': 0.0361, 'grad_norm': 0.9931914210319519, 'learning_rate': 4.761405423448387e-05, 'epoch': 8.886363636363637}\n{'loss': 0.0596, 'grad_norm': 0.6296873092651367, 'learning_rate': 4.666177314979419e-05, 'epoch': 8.909090909090908}\n{'loss': 0.035, 'grad_norm': 0.4313793480396271, 'learning_rate': 4.5709492065104516e-05, 'epoch': 8.931818181818182}\n{'loss': 0.0229, 'grad_norm': 0.8291845321655273, 'learning_rate': 4.475721098041484e-05, 'epoch': 8.954545454545455}\n{'loss': 0.043, 'grad_norm': 0.677036464214325, 'learning_rate': 4.380492989572516e-05, 'epoch': 8.977272727272727}\n{'loss': 0.1508, 'grad_norm': 5.0600905418396, 'learning_rate': 4.285264881103548e-05, 'epoch': 9.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8957    0.8952    0.8955      3311\n     PERCENT     0.8667    0.9070    0.8864        86\n        TYPE     0.9484    0.9654    0.9568     11299\n      VOLUME     0.7872    0.8810    0.8315        42\n\n   micro avg     0.9357    0.9490    0.9423     14738\n   macro avg     0.8745    0.9121    0.8925     14738\nweighted avg     0.9356    0.9490    0.9423     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.3290221393108368, 'eval_f1_macro': 0.8925231822028203, 'eval_precision': 0.9357104629381857, 'eval_recall': 0.9490432894558285, 'eval_f1': 0.942329717712053, 'eval_accuracy': 0.9315884377902847, 'eval_runtime': 1.4305, 'eval_samples_per_second': 3851.921, 'eval_steps_per_second': 7.69, 'epoch': 9.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.0382, 'grad_norm': 0.5987717509269714, 'learning_rate': 4.19003677263458e-05, 'epoch': 9.022727272727273}\n{'loss': 0.0183, 'grad_norm': 0.5425883531570435, 'learning_rate': 4.094808664165613e-05, 'epoch': 9.045454545454545}\n{'loss': 0.0504, 'grad_norm': 0.7046436071395874, 'learning_rate': 3.9995805556966455e-05, 'epoch': 9.068181818181818}\n{'loss': 0.0263, 'grad_norm': 0.580414891242981, 'learning_rate': 3.904352447227677e-05, 'epoch': 9.090909090909092}\n{'loss': 0.0367, 'grad_norm': 0.8154889941215515, 'learning_rate': 3.8091243387587094e-05, 'epoch': 9.113636363636363}\n{'loss': 0.0259, 'grad_norm': 0.5929858088493347, 'learning_rate': 3.713896230289742e-05, 'epoch': 9.136363636363637}\n{'loss': 0.0253, 'grad_norm': 0.8667581081390381, 'learning_rate': 3.618668121820774e-05, 'epoch': 9.159090909090908}\n{'loss': 0.0216, 'grad_norm': 0.538216769695282, 'learning_rate': 3.5234400133518064e-05, 'epoch': 9.181818181818182}\n{'loss': 0.0223, 'grad_norm': 0.524869978427887, 'learning_rate': 3.428211904882839e-05, 'epoch': 9.204545454545455}\n{'loss': 0.0303, 'grad_norm': 0.756628692150116, 'learning_rate': 3.332983796413871e-05, 'epoch': 9.227272727272727}\n{'loss': 0.0369, 'grad_norm': 0.7371713519096375, 'learning_rate': 3.237755687944903e-05, 'epoch': 9.25}\n{'loss': 0.0267, 'grad_norm': 0.41968587040901184, 'learning_rate': 3.142527579475935e-05, 'epoch': 9.272727272727273}\n{'loss': 0.0297, 'grad_norm': 0.4289966821670532, 'learning_rate': 3.047299471006968e-05, 'epoch': 9.295454545454545}\n{'loss': 0.0391, 'grad_norm': 0.6118127703666687, 'learning_rate': 2.9520713625380003e-05, 'epoch': 9.318181818181818}\n{'loss': 0.0279, 'grad_norm': 0.4843194782733917, 'learning_rate': 2.8568432540690322e-05, 'epoch': 9.340909090909092}\n{'loss': 0.0258, 'grad_norm': 0.5191148519515991, 'learning_rate': 2.7616151456000645e-05, 'epoch': 9.363636363636363}\n{'loss': 0.0231, 'grad_norm': 0.5279912352561951, 'learning_rate': 2.6663870371310965e-05, 'epoch': 9.386363636363637}\n{'loss': 0.0268, 'grad_norm': 0.46871161460876465, 'learning_rate': 2.571158928662129e-05, 'epoch': 9.409090909090908}\n{'loss': 0.0329, 'grad_norm': 0.7010924816131592, 'learning_rate': 2.4759308201931615e-05, 'epoch': 9.431818181818182}\n{'loss': 0.0273, 'grad_norm': 0.452732115983963, 'learning_rate': 2.3807027117241935e-05, 'epoch': 9.454545454545455}\n{'loss': 0.0203, 'grad_norm': 0.5818914175033569, 'learning_rate': 2.2854746032552258e-05, 'epoch': 9.477272727272727}\n{'loss': 0.0398, 'grad_norm': 1.1266615390777588, 'learning_rate': 2.190246494786258e-05, 'epoch': 9.5}\n{'loss': 0.046, 'grad_norm': 0.6983525156974792, 'learning_rate': 2.09501838631729e-05, 'epoch': 9.522727272727273}\n{'loss': 0.0179, 'grad_norm': 0.5302532315254211, 'learning_rate': 1.9997902778483227e-05, 'epoch': 9.545454545454545}\n{'loss': 0.045, 'grad_norm': 0.6863429546356201, 'learning_rate': 1.9045621693793547e-05, 'epoch': 9.568181818181818}\n{'loss': 0.022, 'grad_norm': 0.7111542820930481, 'learning_rate': 1.809334060910387e-05, 'epoch': 9.590909090909092}\n{'loss': 0.0387, 'grad_norm': 0.595068633556366, 'learning_rate': 1.7141059524414193e-05, 'epoch': 9.613636363636363}\n{'loss': 0.0261, 'grad_norm': 0.7715058326721191, 'learning_rate': 1.6188778439724517e-05, 'epoch': 9.636363636363637}\n{'loss': 0.0242, 'grad_norm': 0.40973320603370667, 'learning_rate': 1.523649735503484e-05, 'epoch': 9.659090909090908}\n{'loss': 0.0202, 'grad_norm': 0.3125992715358734, 'learning_rate': 1.4284216270345161e-05, 'epoch': 9.681818181818182}\n{'loss': 0.0354, 'grad_norm': 0.64798903465271, 'learning_rate': 1.3331935185655483e-05, 'epoch': 9.704545454545455}\n{'loss': 0.0346, 'grad_norm': 0.3586283028125763, 'learning_rate': 1.2379654100965807e-05, 'epoch': 9.727272727272727}\n{'loss': 0.0261, 'grad_norm': 0.7608506679534912, 'learning_rate': 1.1427373016276129e-05, 'epoch': 9.75}\n{'loss': 0.0211, 'grad_norm': 0.39652326703071594, 'learning_rate': 1.047509193158645e-05, 'epoch': 9.772727272727273}\n{'loss': 0.0228, 'grad_norm': 0.510324239730835, 'learning_rate': 9.522810846896774e-06, 'epoch': 9.795454545454545}\n{'loss': 0.0222, 'grad_norm': 0.6781718730926514, 'learning_rate': 8.570529762207097e-06, 'epoch': 9.818181818181818}\n{'loss': 0.042, 'grad_norm': 0.7665966153144836, 'learning_rate': 7.61824867751742e-06, 'epoch': 9.840909090909092}\n{'loss': 0.0243, 'grad_norm': 0.6129181385040283, 'learning_rate': 6.665967592827741e-06, 'epoch': 9.863636363636363}\n{'loss': 0.0286, 'grad_norm': 0.5134802460670471, 'learning_rate': 5.7136865081380645e-06, 'epoch': 9.886363636363637}\n{'loss': 0.0241, 'grad_norm': 0.5091633200645447, 'learning_rate': 4.761405423448387e-06, 'epoch': 9.909090909090908}\n{'loss': 0.0306, 'grad_norm': 0.517906129360199, 'learning_rate': 3.80912433875871e-06, 'epoch': 9.931818181818182}\n{'loss': 0.0174, 'grad_norm': 0.6016659140586853, 'learning_rate': 2.8568432540690322e-06, 'epoch': 9.954545454545455}\n{'loss': 0.0181, 'grad_norm': 0.37358734011650085, 'learning_rate': 1.904562169379355e-06, 'epoch': 9.977272727272727}\n{'loss': 0.0861, 'grad_norm': 8.537496566772461, 'learning_rate': 9.522810846896775e-07, 'epoch': 10.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8908    0.8994    0.8951      3311\n     PERCENT     0.8889    0.9302    0.9091        86\n        TYPE     0.9492    0.9627    0.9559     11299\n      VOLUME     0.8444    0.9048    0.8736        42\n\n   micro avg     0.9355    0.9481    0.9417     14738\n   macro avg     0.8933    0.9243    0.9084     14738\nweighted avg     0.9354    0.9481    0.9417     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.3312247395515442, 'eval_f1_macro': 0.908409616026687, 'eval_precision': 0.9354622748878624, 'eval_recall': 0.9480933640928213, 'eval_f1': 0.9417354675652906, 'eval_accuracy': 0.9307688104475166, 'eval_runtime': 1.4332, 'eval_samples_per_second': 3844.581, 'eval_steps_per_second': 7.675, 'epoch': 10.0}\n{'train_runtime': 61.2473, 'train_samples_per_second': 3598.852, 'train_steps_per_second': 7.184, 'train_loss': 0.20203649904155596, 'epoch': 10.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nThe speedups for torchdynamo mostly come with GPU Ampere or higher and which is not detected here.\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8927    0.9024    0.8976      3311\n     PERCENT     0.8333    0.9302    0.8791        86\n        TYPE     0.9493    0.9650    0.9571     11299\n      VOLUME     0.9048    0.9048    0.9048        42\n\n   micro avg     0.9358    0.9505    0.9431     14738\n   macro avg     0.8950    0.9256    0.9096     14738\nweighted avg     0.9358    0.9505    0.9431     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.2986876368522644, 'eval_f1_macro': 0.9096311903050441, 'eval_precision': 0.9358049432197729, 'eval_recall': 0.9505360293119827, 'eval_f1': 0.9431129662043893, 'eval_accuracy': 0.932954483361565, 'eval_runtime': 1.5105, 'eval_samples_per_second': 3647.792, 'eval_steps_per_second': 7.282, 'epoch': 10.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/3364797558.py:66: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 2.2736, 'grad_norm': 6.993783473968506, 'learning_rate': 0.0, 'epoch': 0.022727272727272728}\n{'loss': 2.2792, 'grad_norm': 7.328367710113525, 'learning_rate': 8.570529762207097e-06, 'epoch': 0.045454545454545456}\n{'loss': 2.2534, 'grad_norm': 6.964651107788086, 'learning_rate': 1.7141059524414193e-05, 'epoch': 0.06818181818181818}\n{'loss': 2.2336, 'grad_norm': 7.423412322998047, 'learning_rate': 2.571158928662129e-05, 'epoch': 0.09090909090909091}\n{'loss': 2.165, 'grad_norm': 6.961561679840088, 'learning_rate': 3.428211904882839e-05, 'epoch': 0.11363636363636363}\n{'loss': 2.0917, 'grad_norm': 6.538268566131592, 'learning_rate': 4.285264881103548e-05, 'epoch': 0.13636363636363635}\n{'loss': 2.001, 'grad_norm': 6.567159652709961, 'learning_rate': 5.142317857324258e-05, 'epoch': 0.1590909090909091}\n{'loss': 1.9111, 'grad_norm': 6.125669002532959, 'learning_rate': 5.9993708335449675e-05, 'epoch': 0.18181818181818182}\n{'loss': 1.7832, 'grad_norm': 5.86303186416626, 'learning_rate': 6.856423809765677e-05, 'epoch': 0.20454545454545456}\n{'loss': 1.6519, 'grad_norm': 5.668679237365723, 'learning_rate': 7.713476785986388e-05, 'epoch': 0.22727272727272727}\n{'loss': 1.5246, 'grad_norm': 5.122692108154297, 'learning_rate': 8.570529762207096e-05, 'epoch': 0.25}\n{'loss': 1.411, 'grad_norm': 4.180367946624756, 'learning_rate': 9.427582738427806e-05, 'epoch': 0.2727272727272727}\n{'loss': 1.3736, 'grad_norm': 3.0147337913513184, 'learning_rate': 0.00010284635714648515, 'epoch': 0.29545454545454547}\n{'loss': 1.2956, 'grad_norm': 2.219463586807251, 'learning_rate': 0.00011141688690869226, 'epoch': 0.3181818181818182}\n{'loss': 1.2372, 'grad_norm': 2.0814006328582764, 'learning_rate': 0.00011998741667089935, 'epoch': 0.3409090909090909}\n{'loss': 1.1596, 'grad_norm': 1.8096530437469482, 'learning_rate': 0.00012855794643310643, 'epoch': 0.36363636363636365}\n{'loss': 1.1164, 'grad_norm': 1.7349677085876465, 'learning_rate': 0.00013712847619531355, 'epoch': 0.38636363636363635}\n{'loss': 1.0387, 'grad_norm': 1.5494524240493774, 'learning_rate': 0.00014569900595752064, 'epoch': 0.4090909090909091}\n{'loss': 1.0041, 'grad_norm': 1.344436526298523, 'learning_rate': 0.00015426953571972776, 'epoch': 0.4318181818181818}\n{'loss': 1.0029, 'grad_norm': 1.2525027990341187, 'learning_rate': 0.00016284006548193485, 'epoch': 0.45454545454545453}\n{'loss': 0.9533, 'grad_norm': 1.41774582862854, 'learning_rate': 0.00017141059524414191, 'epoch': 0.4772727272727273}\n{'loss': 0.9239, 'grad_norm': 2.1458616256713867, 'learning_rate': 0.00017998112500634903, 'epoch': 0.5}\n{'loss': 0.8768, 'grad_norm': 2.2139196395874023, 'learning_rate': 0.00018855165476855612, 'epoch': 0.5227272727272727}\n{'loss': 0.8396, 'grad_norm': 1.8192332983016968, 'learning_rate': 0.00019712218453076322, 'epoch': 0.5454545454545454}\n{'loss': 0.8309, 'grad_norm': 1.2011301517486572, 'learning_rate': 0.0002056927142929703, 'epoch': 0.5681818181818182}\n{'loss': 0.7538, 'grad_norm': 1.3801580667495728, 'learning_rate': 0.00021426324405517743, 'epoch': 0.5909090909090909}\n{'loss': 0.8479, 'grad_norm': 1.5038182735443115, 'learning_rate': 0.00022283377381738452, 'epoch': 0.6136363636363636}\n{'loss': 0.7656, 'grad_norm': 1.3677310943603516, 'learning_rate': 0.0002314043035795916, 'epoch': 0.6363636363636364}\n{'loss': 0.7727, 'grad_norm': 1.2193913459777832, 'learning_rate': 0.0002399748333417987, 'epoch': 0.6590909090909091}\n{'loss': 0.721, 'grad_norm': 0.9275084733963013, 'learning_rate': 0.00024854536310400577, 'epoch': 0.6818181818181818}\n{'loss': 0.6612, 'grad_norm': 1.1239428520202637, 'learning_rate': 0.00025711589286621286, 'epoch': 0.7045454545454546}\n{'loss': 0.6383, 'grad_norm': 0.8186392784118652, 'learning_rate': 0.00026568642262842, 'epoch': 0.7272727272727273}\n{'loss': 0.5836, 'grad_norm': 1.2669261693954468, 'learning_rate': 0.0002742569523906271, 'epoch': 0.75}\n{'loss': 0.6071, 'grad_norm': 0.9415273070335388, 'learning_rate': 0.0002828274821528342, 'epoch': 0.7727272727272727}\n{'loss': 0.5781, 'grad_norm': 1.0343170166015625, 'learning_rate': 0.0002913980119150413, 'epoch': 0.7954545454545454}\n{'loss': 0.5541, 'grad_norm': 1.8554315567016602, 'learning_rate': 0.00029996854167724837, 'epoch': 0.8181818181818182}\n{'loss': 0.5566, 'grad_norm': 1.3675707578659058, 'learning_rate': 0.0003085390714394555, 'epoch': 0.8409090909090909}\n{'loss': 0.5007, 'grad_norm': 1.080521821975708, 'learning_rate': 0.0003171096012016626, 'epoch': 0.8636363636363636}\n{'loss': 0.4704, 'grad_norm': 0.7154470086097717, 'learning_rate': 0.0003256801309638697, 'epoch': 0.8863636363636364}\n{'loss': 0.4576, 'grad_norm': 1.4734058380126953, 'learning_rate': 0.00033425066072607674, 'epoch': 0.9090909090909091}\n{'loss': 0.4831, 'grad_norm': 1.2377365827560425, 'learning_rate': 0.00034282119048828383, 'epoch': 0.9318181818181818}\n{'loss': 0.48, 'grad_norm': 0.8894120454788208, 'learning_rate': 0.0003513917202504909, 'epoch': 0.9545454545454546}\n{'loss': 0.486, 'grad_norm': 1.095178246498108, 'learning_rate': 0.00035996225001269806, 'epoch': 0.9772727272727273}\n{'loss': 0.3749, 'grad_norm': 2.7406184673309326, 'learning_rate': 0.00036853277977490516, 'epoch': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.7302    0.7752    0.7520      3456\n     PERCENT     0.5932    0.9091    0.7179        77\n        TYPE     0.8936    0.9325    0.9126     11282\n      VOLUME     0.0000    0.0000    0.0000        41\n\n   micro avg     0.8528    0.8932    0.8725     14856\n   macro avg     0.5542    0.6542    0.5956     14856\nweighted avg     0.8515    0.8932    0.8717     14856\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.42509031295776367, 'eval_f1_macro': 0.595637255392623, 'eval_precision': 0.8527634961439589, 'eval_recall': 0.8931744749596123, 'eval_f1': 0.8725013150973172, 'eval_accuracy': 0.873422541340296, 'eval_runtime': 1.4991, 'eval_samples_per_second': 3675.532, 'eval_steps_per_second': 7.338, 'epoch': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.4134, 'grad_norm': 1.6500309705734253, 'learning_rate': 0.00037710330953711225, 'epoch': 1.0227272727272727}\n{'loss': 0.4563, 'grad_norm': 1.7121303081512451, 'learning_rate': 0.00037615102845242257, 'epoch': 1.0454545454545454}\n{'loss': 0.4091, 'grad_norm': 0.9137527942657471, 'learning_rate': 0.0003751987473677329, 'epoch': 1.0681818181818181}\n{'loss': 0.3729, 'grad_norm': 1.2838211059570312, 'learning_rate': 0.0003742464662830432, 'epoch': 1.0909090909090908}\n{'loss': 0.3755, 'grad_norm': 2.1344385147094727, 'learning_rate': 0.00037329418519835354, 'epoch': 1.1136363636363635}\n{'loss': 0.3976, 'grad_norm': 1.743593454360962, 'learning_rate': 0.00037234190411366386, 'epoch': 1.1363636363636362}\n{'loss': 0.38, 'grad_norm': 1.9604566097259521, 'learning_rate': 0.0003713896230289742, 'epoch': 1.1590909090909092}\n{'loss': 0.2989, 'grad_norm': 1.2698218822479248, 'learning_rate': 0.0003704373419442845, 'epoch': 1.1818181818181819}\n{'loss': 0.3744, 'grad_norm': 1.6562930345535278, 'learning_rate': 0.00036948506085959483, 'epoch': 1.2045454545454546}\n{'loss': 0.3008, 'grad_norm': 1.5842969417572021, 'learning_rate': 0.00036853277977490516, 'epoch': 1.2272727272727273}\n{'loss': 0.3634, 'grad_norm': 0.9689779877662659, 'learning_rate': 0.0003675804986902155, 'epoch': 1.25}\n{'loss': 0.2996, 'grad_norm': 1.254606008529663, 'learning_rate': 0.0003666282176055258, 'epoch': 1.2727272727272727}\n{'loss': 0.3516, 'grad_norm': 1.1513183116912842, 'learning_rate': 0.0003656759365208361, 'epoch': 1.2954545454545454}\n{'loss': 0.3989, 'grad_norm': 0.9462827444076538, 'learning_rate': 0.00036472365543614645, 'epoch': 1.3181818181818181}\n{'loss': 0.3395, 'grad_norm': 0.9484292268753052, 'learning_rate': 0.00036377137435145677, 'epoch': 1.3409090909090908}\n{'loss': 0.3187, 'grad_norm': 0.9544074535369873, 'learning_rate': 0.0003628190932667671, 'epoch': 1.3636363636363638}\n{'loss': 0.3104, 'grad_norm': 0.7189529538154602, 'learning_rate': 0.0003618668121820774, 'epoch': 1.3863636363636362}\n{'loss': 0.3748, 'grad_norm': 1.211778163909912, 'learning_rate': 0.00036091453109738774, 'epoch': 1.4090909090909092}\n{'loss': 0.333, 'grad_norm': 0.9592229723930359, 'learning_rate': 0.00035996225001269806, 'epoch': 1.4318181818181819}\n{'loss': 0.2617, 'grad_norm': 1.3814988136291504, 'learning_rate': 0.0003590099689280084, 'epoch': 1.4545454545454546}\n{'loss': 0.28, 'grad_norm': 1.4592832326889038, 'learning_rate': 0.0003580576878433187, 'epoch': 1.4772727272727273}\n{'loss': 0.3129, 'grad_norm': 1.2595975399017334, 'learning_rate': 0.00035710540675862903, 'epoch': 1.5}\n{'loss': 0.3158, 'grad_norm': 1.3015516996383667, 'learning_rate': 0.00035615312567393936, 'epoch': 1.5227272727272727}\n{'loss': 0.2568, 'grad_norm': 0.6776670217514038, 'learning_rate': 0.0003552008445892497, 'epoch': 1.5454545454545454}\n{'loss': 0.3355, 'grad_norm': 0.7196291089057922, 'learning_rate': 0.00035424856350456, 'epoch': 1.5681818181818183}\n{'loss': 0.3581, 'grad_norm': 0.8230328559875488, 'learning_rate': 0.0003532962824198703, 'epoch': 1.5909090909090908}\n{'loss': 0.3105, 'grad_norm': 2.679572105407715, 'learning_rate': 0.00035234400133518065, 'epoch': 1.6136363636363638}\n{'loss': 0.3313, 'grad_norm': 2.2805466651916504, 'learning_rate': 0.0003513917202504909, 'epoch': 1.6363636363636362}\n{'loss': 0.2505, 'grad_norm': 1.1444272994995117, 'learning_rate': 0.0003504394391658013, 'epoch': 1.6590909090909092}\n{'loss': 0.28, 'grad_norm': 1.6503734588623047, 'learning_rate': 0.0003494871580811116, 'epoch': 1.6818181818181817}\n{'loss': 0.3577, 'grad_norm': 1.4811232089996338, 'learning_rate': 0.0003485348769964219, 'epoch': 1.7045454545454546}\n{'loss': 0.3306, 'grad_norm': 1.2188434600830078, 'learning_rate': 0.00034758259591173227, 'epoch': 1.7272727272727273}\n{'loss': 0.3356, 'grad_norm': 1.5118321180343628, 'learning_rate': 0.0003466303148270426, 'epoch': 1.75}\n{'loss': 0.2767, 'grad_norm': 1.503859519958496, 'learning_rate': 0.00034567803374235286, 'epoch': 1.7727272727272727}\n{'loss': 0.2476, 'grad_norm': 0.9205898642539978, 'learning_rate': 0.00034472575265766324, 'epoch': 1.7954545454545454}\n{'loss': 0.2685, 'grad_norm': 1.1726856231689453, 'learning_rate': 0.00034377347157297356, 'epoch': 1.8181818181818183}\n{'loss': 0.3089, 'grad_norm': 1.7319231033325195, 'learning_rate': 0.00034282119048828383, 'epoch': 1.8409090909090908}\n{'loss': 0.2699, 'grad_norm': 0.8176212906837463, 'learning_rate': 0.0003418689094035942, 'epoch': 1.8636363636363638}\n{'loss': 0.2829, 'grad_norm': 0.9364542961120605, 'learning_rate': 0.00034091662831890453, 'epoch': 1.8863636363636362}\n{'loss': 0.2717, 'grad_norm': 0.6946940422058105, 'learning_rate': 0.0003399643472342148, 'epoch': 1.9090909090909092}\n{'loss': 0.2816, 'grad_norm': 1.0911413431167603, 'learning_rate': 0.0003390120661495252, 'epoch': 1.9318181818181817}\n{'loss': 0.2228, 'grad_norm': 1.0178933143615723, 'learning_rate': 0.0003380597850648355, 'epoch': 1.9545454545454546}\n{'loss': 0.2749, 'grad_norm': 0.7455035448074341, 'learning_rate': 0.00033710750398014577, 'epoch': 1.9772727272727273}\n{'loss': 0.2311, 'grad_norm': 3.2014424800872803, 'learning_rate': 0.00033615522289545614, 'epoch': 2.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8824    0.8380    0.8596      3456\n     PERCENT     0.8846    0.8961    0.8903        77\n        TYPE     0.9305    0.9689    0.9493     11282\n      VOLUME     0.6087    0.6829    0.6437        41\n\n   micro avg     0.9189    0.9373    0.9280     14856\n   macro avg     0.8266    0.8465    0.8357     14856\nweighted avg     0.9182    0.9373    0.9273     14856\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.27126064896583557, 'eval_f1_macro': 0.835731940463609, 'eval_precision': 0.9188939483930575, 'eval_recall': 0.9372644049542272, 'eval_f1': 0.927988270185611, 'eval_accuracy': 0.9229221061792864, 'eval_runtime': 1.4399, 'eval_samples_per_second': 3826.621, 'eval_steps_per_second': 7.639, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.1834, 'grad_norm': 0.6242874264717102, 'learning_rate': 0.0003352029418107664, 'epoch': 2.022727272727273}\n{'loss': 0.2022, 'grad_norm': 0.6867008805274963, 'learning_rate': 0.00033425066072607674, 'epoch': 2.0454545454545454}\n{'loss': 0.2192, 'grad_norm': 1.252720832824707, 'learning_rate': 0.0003332983796413871, 'epoch': 2.0681818181818183}\n{'loss': 0.1876, 'grad_norm': 1.0161229372024536, 'learning_rate': 0.0003323460985566974, 'epoch': 2.090909090909091}\n{'loss': 0.1797, 'grad_norm': 1.496038556098938, 'learning_rate': 0.0003313938174720077, 'epoch': 2.1136363636363638}\n{'loss': 0.2001, 'grad_norm': 1.2587722539901733, 'learning_rate': 0.0003304415363873181, 'epoch': 2.1363636363636362}\n{'loss': 0.1517, 'grad_norm': 0.653069019317627, 'learning_rate': 0.00032948925530262835, 'epoch': 2.159090909090909}\n{'loss': 0.1533, 'grad_norm': 0.6986969113349915, 'learning_rate': 0.00032853697421793873, 'epoch': 2.1818181818181817}\n{'loss': 0.204, 'grad_norm': 1.5338505506515503, 'learning_rate': 0.00032758469313324905, 'epoch': 2.2045454545454546}\n{'loss': 0.1857, 'grad_norm': 1.015352487564087, 'learning_rate': 0.0003266324120485593, 'epoch': 2.227272727272727}\n{'loss': 0.2394, 'grad_norm': 1.391601800918579, 'learning_rate': 0.0003256801309638697, 'epoch': 2.25}\n{'loss': 0.1571, 'grad_norm': 0.9064469933509827, 'learning_rate': 0.00032472784987918, 'epoch': 2.2727272727272725}\n{'loss': 0.1429, 'grad_norm': 1.0314679145812988, 'learning_rate': 0.0003237755687944903, 'epoch': 2.2954545454545454}\n{'loss': 0.1961, 'grad_norm': 1.8818564414978027, 'learning_rate': 0.00032282328770980067, 'epoch': 2.3181818181818183}\n{'loss': 0.2, 'grad_norm': 1.0403492450714111, 'learning_rate': 0.00032187100662511094, 'epoch': 2.340909090909091}\n{'loss': 0.2041, 'grad_norm': 1.4770445823669434, 'learning_rate': 0.00032091872554042126, 'epoch': 2.3636363636363638}\n{'loss': 0.1707, 'grad_norm': 1.081010341644287, 'learning_rate': 0.00031996644445573164, 'epoch': 2.3863636363636362}\n{'loss': 0.1284, 'grad_norm': 0.5481173992156982, 'learning_rate': 0.0003190141633710419, 'epoch': 2.409090909090909}\n{'loss': 0.1877, 'grad_norm': 1.8830649852752686, 'learning_rate': 0.00031806188228635223, 'epoch': 2.4318181818181817}\n{'loss': 0.182, 'grad_norm': 1.6920197010040283, 'learning_rate': 0.0003171096012016626, 'epoch': 2.4545454545454546}\n{'loss': 0.2232, 'grad_norm': 1.138667345046997, 'learning_rate': 0.0003161573201169729, 'epoch': 2.4772727272727275}\n{'loss': 0.1534, 'grad_norm': 1.006725788116455, 'learning_rate': 0.0003152050390322832, 'epoch': 2.5}\n{'loss': 0.1393, 'grad_norm': 1.3163803815841675, 'learning_rate': 0.0003142527579475936, 'epoch': 2.5227272727272725}\n{'loss': 0.1644, 'grad_norm': 0.8163320422172546, 'learning_rate': 0.00031330047686290385, 'epoch': 2.5454545454545454}\n{'loss': 0.1465, 'grad_norm': 1.0675108432769775, 'learning_rate': 0.00031234819577821417, 'epoch': 2.5681818181818183}\n{'loss': 0.1899, 'grad_norm': 1.468599796295166, 'learning_rate': 0.00031139591469352455, 'epoch': 2.590909090909091}\n{'loss': 0.1393, 'grad_norm': 0.745818018913269, 'learning_rate': 0.0003104436336088348, 'epoch': 2.6136363636363638}\n{'loss': 0.2256, 'grad_norm': 1.3513832092285156, 'learning_rate': 0.00030949135252414514, 'epoch': 2.6363636363636362}\n{'loss': 0.2067, 'grad_norm': 1.2305271625518799, 'learning_rate': 0.0003085390714394555, 'epoch': 2.659090909090909}\n{'loss': 0.1848, 'grad_norm': 1.0086610317230225, 'learning_rate': 0.0003075867903547658, 'epoch': 2.6818181818181817}\n{'loss': 0.1849, 'grad_norm': 1.3989416360855103, 'learning_rate': 0.0003066345092700761, 'epoch': 2.7045454545454546}\n{'loss': 0.1398, 'grad_norm': 1.712003469467163, 'learning_rate': 0.00030568222818538643, 'epoch': 2.7272727272727275}\n{'loss': 0.1653, 'grad_norm': 1.082023024559021, 'learning_rate': 0.00030472994710069675, 'epoch': 2.75}\n{'loss': 0.2415, 'grad_norm': 1.1140358448028564, 'learning_rate': 0.0003037776660160071, 'epoch': 2.7727272727272725}\n{'loss': 0.1709, 'grad_norm': 1.3316015005111694, 'learning_rate': 0.0003028253849313174, 'epoch': 2.7954545454545454}\n{'loss': 0.1977, 'grad_norm': 1.427793025970459, 'learning_rate': 0.0003018731038466277, 'epoch': 2.8181818181818183}\n{'loss': 0.2084, 'grad_norm': 1.3820524215698242, 'learning_rate': 0.00030092082276193805, 'epoch': 2.840909090909091}\n{'loss': 0.2021, 'grad_norm': 1.4577702283859253, 'learning_rate': 0.00029996854167724837, 'epoch': 2.8636363636363638}\n{'loss': 0.1532, 'grad_norm': 1.055618405342102, 'learning_rate': 0.0002990162605925587, 'epoch': 2.8863636363636362}\n{'loss': 0.1682, 'grad_norm': 1.3275686502456665, 'learning_rate': 0.000298063979507869, 'epoch': 2.909090909090909}\n{'loss': 0.203, 'grad_norm': 1.5297818183898926, 'learning_rate': 0.00029711169842317934, 'epoch': 2.9318181818181817}\n{'loss': 0.207, 'grad_norm': 1.3472744226455688, 'learning_rate': 0.00029615941733848966, 'epoch': 2.9545454545454546}\n{'loss': 0.1835, 'grad_norm': 0.9940794110298157, 'learning_rate': 0.0002952071362538, 'epoch': 2.9772727272727275}\n{'loss': 0.1411, 'grad_norm': 4.1144185066223145, 'learning_rate': 0.0002942548551691103, 'epoch': 3.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8896    0.8698    0.8796      3456\n     PERCENT     0.8690    0.9481    0.9068        77\n        TYPE     0.9377    0.9697    0.9534     11282\n      VOLUME     0.8500    0.8293    0.8395        41\n\n   micro avg     0.9264    0.9459    0.9361     14856\n   macro avg     0.8866    0.9042    0.8948     14856\nweighted avg     0.9259    0.9459    0.9357     14856\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.24760869145393372, 'eval_f1_macro': 0.894836816100095, 'eval_precision': 0.9263678312458801, 'eval_recall': 0.9459477652127086, 'eval_f1': 0.9360554186371811, 'eval_accuracy': 0.9298302872062664, 'eval_runtime': 1.4362, 'eval_samples_per_second': 3836.393, 'eval_steps_per_second': 7.659, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.1219, 'grad_norm': 0.8991632461547852, 'learning_rate': 0.00029330257408442063, 'epoch': 3.022727272727273}\n{'loss': 0.1087, 'grad_norm': 0.7673982381820679, 'learning_rate': 0.00029235029299973095, 'epoch': 3.0454545454545454}\n{'loss': 0.1181, 'grad_norm': 0.8512858152389526, 'learning_rate': 0.0002913980119150413, 'epoch': 3.0681818181818183}\n{'loss': 0.1736, 'grad_norm': 1.1844474077224731, 'learning_rate': 0.0002904457308303516, 'epoch': 3.090909090909091}\n{'loss': 0.1096, 'grad_norm': 1.2907007932662964, 'learning_rate': 0.0002894934497456619, 'epoch': 3.1136363636363638}\n{'loss': 0.1237, 'grad_norm': 0.977424681186676, 'learning_rate': 0.00028854116866097225, 'epoch': 3.1363636363636362}\n{'loss': 0.1001, 'grad_norm': 0.7598902583122253, 'learning_rate': 0.00028758888757628257, 'epoch': 3.159090909090909}\n{'loss': 0.1325, 'grad_norm': 0.6460751295089722, 'learning_rate': 0.0002866366064915929, 'epoch': 3.1818181818181817}\n{'loss': 0.0936, 'grad_norm': 0.9640301465988159, 'learning_rate': 0.0002856843254069032, 'epoch': 3.2045454545454546}\n{'loss': 0.0885, 'grad_norm': 1.1500239372253418, 'learning_rate': 0.00028473204432221354, 'epoch': 3.227272727272727}\n{'loss': 0.1144, 'grad_norm': 1.30519437789917, 'learning_rate': 0.00028377976323752386, 'epoch': 3.25}\n{'loss': 0.1031, 'grad_norm': 1.0365376472473145, 'learning_rate': 0.0002828274821528342, 'epoch': 3.2727272727272725}\n{'loss': 0.1337, 'grad_norm': 1.0412263870239258, 'learning_rate': 0.0002818752010681445, 'epoch': 3.2954545454545454}\n{'loss': 0.1209, 'grad_norm': 1.3059457540512085, 'learning_rate': 0.00028092291998345483, 'epoch': 3.3181818181818183}\n{'loss': 0.0959, 'grad_norm': 0.8700608015060425, 'learning_rate': 0.00027997063889876516, 'epoch': 3.340909090909091}\n{'loss': 0.1262, 'grad_norm': 1.5571422576904297, 'learning_rate': 0.0002790183578140755, 'epoch': 3.3636363636363638}\n{'loss': 0.0928, 'grad_norm': 1.0111697912216187, 'learning_rate': 0.0002780660767293858, 'epoch': 3.3863636363636362}\n{'loss': 0.116, 'grad_norm': 1.3959099054336548, 'learning_rate': 0.0002771137956446961, 'epoch': 3.409090909090909}\n{'loss': 0.1097, 'grad_norm': 0.752796471118927, 'learning_rate': 0.00027616151456000645, 'epoch': 3.4318181818181817}\n{'loss': 0.1214, 'grad_norm': 0.9300021529197693, 'learning_rate': 0.00027520923347531677, 'epoch': 3.4545454545454546}\n{'loss': 0.0953, 'grad_norm': 1.184446930885315, 'learning_rate': 0.0002742569523906271, 'epoch': 3.4772727272727275}\n{'loss': 0.1057, 'grad_norm': 1.5058411359786987, 'learning_rate': 0.0002733046713059374, 'epoch': 3.5}\n{'loss': 0.1012, 'grad_norm': 0.9551328420639038, 'learning_rate': 0.00027235239022124774, 'epoch': 3.5227272727272725}\n{'loss': 0.125, 'grad_norm': 1.1621979475021362, 'learning_rate': 0.00027140010913655806, 'epoch': 3.5454545454545454}\n{'loss': 0.1383, 'grad_norm': 1.1350523233413696, 'learning_rate': 0.0002704478280518684, 'epoch': 3.5681818181818183}\n{'loss': 0.1302, 'grad_norm': 1.7989670038223267, 'learning_rate': 0.0002694955469671787, 'epoch': 3.590909090909091}\n{'loss': 0.131, 'grad_norm': 1.138218641281128, 'learning_rate': 0.00026854326588248903, 'epoch': 3.6136363636363638}\n{'loss': 0.0899, 'grad_norm': 0.9701962471008301, 'learning_rate': 0.00026759098479779936, 'epoch': 3.6363636363636362}\n{'loss': 0.1027, 'grad_norm': 0.7645391225814819, 'learning_rate': 0.0002666387037131097, 'epoch': 3.659090909090909}\n{'loss': 0.1147, 'grad_norm': 0.7240961790084839, 'learning_rate': 0.00026568642262842, 'epoch': 3.6818181818181817}\n{'loss': 0.0785, 'grad_norm': 1.2335282564163208, 'learning_rate': 0.0002647341415437303, 'epoch': 3.7045454545454546}\n{'loss': 0.1306, 'grad_norm': 0.7564274668693542, 'learning_rate': 0.00026378186045904065, 'epoch': 3.7272727272727275}\n{'loss': 0.0927, 'grad_norm': 1.1877726316452026, 'learning_rate': 0.00026282957937435097, 'epoch': 3.75}\n{'loss': 0.1142, 'grad_norm': 1.257987141609192, 'learning_rate': 0.0002618772982896613, 'epoch': 3.7727272727272725}\n{'loss': 0.1425, 'grad_norm': 0.7120847702026367, 'learning_rate': 0.0002609250172049716, 'epoch': 3.7954545454545454}\n{'loss': 0.1573, 'grad_norm': 0.9442885518074036, 'learning_rate': 0.00025997273612028194, 'epoch': 3.8181818181818183}\n{'loss': 0.123, 'grad_norm': 0.9934121370315552, 'learning_rate': 0.00025902045503559226, 'epoch': 3.840909090909091}\n{'loss': 0.0826, 'grad_norm': 0.7616294026374817, 'learning_rate': 0.0002580681739509026, 'epoch': 3.8636363636363638}\n{'loss': 0.1249, 'grad_norm': 0.9986718893051147, 'learning_rate': 0.00025711589286621286, 'epoch': 3.8863636363636362}\n{'loss': 0.1337, 'grad_norm': 0.8119160532951355, 'learning_rate': 0.00025616361178152323, 'epoch': 3.909090909090909}\n{'loss': 0.188, 'grad_norm': 1.700907588005066, 'learning_rate': 0.00025521133069683356, 'epoch': 3.9318181818181817}\n{'loss': 0.172, 'grad_norm': 1.0119256973266602, 'learning_rate': 0.0002542590496121438, 'epoch': 3.9545454545454546}\n{'loss': 0.1041, 'grad_norm': 0.7144106030464172, 'learning_rate': 0.0002533067685274542, 'epoch': 3.9772727272727275}\n{'loss': 0.143, 'grad_norm': 6.142531394958496, 'learning_rate': 0.0002523544874427645, 'epoch': 4.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.9103    0.8753    0.8925      3456\n     PERCENT     0.8690    0.9481    0.9068        77\n        TYPE     0.9476    0.9650    0.9562     11282\n      VOLUME     0.8636    0.9268    0.8941        41\n\n   micro avg     0.9386    0.9439    0.9413     14856\n   macro avg     0.8977    0.9288    0.9124     14856\nweighted avg     0.9383    0.9439    0.9410     14856\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.25417715311050415, 'eval_f1_macro': 0.9124070500490661, 'eval_precision': 0.9386211512717537, 'eval_recall': 0.9439283791060851, 'eval_f1': 0.9412672841992215, 'eval_accuracy': 0.9317885117493473, 'eval_runtime': 1.4336, 'eval_samples_per_second': 3843.511, 'eval_steps_per_second': 7.673, 'epoch': 4.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.0884, 'grad_norm': 0.7606625556945801, 'learning_rate': 0.0002514022063580748, 'epoch': 4.0227272727272725}\n{'loss': 0.0901, 'grad_norm': 0.8776394128799438, 'learning_rate': 0.0002504499252733852, 'epoch': 4.045454545454546}\n{'loss': 0.0963, 'grad_norm': 0.9188657402992249, 'learning_rate': 0.0002494976441886955, 'epoch': 4.068181818181818}\n{'loss': 0.1069, 'grad_norm': 1.2639811038970947, 'learning_rate': 0.00024854536310400577, 'epoch': 4.090909090909091}\n{'loss': 0.0876, 'grad_norm': 0.9398912787437439, 'learning_rate': 0.00024759308201931614, 'epoch': 4.113636363636363}\n{'loss': 0.0717, 'grad_norm': 0.568629801273346, 'learning_rate': 0.00024664080093462647, 'epoch': 4.136363636363637}\n{'loss': 0.0911, 'grad_norm': 0.9493163824081421, 'learning_rate': 0.00024568851984993673, 'epoch': 4.159090909090909}\n{'loss': 0.0829, 'grad_norm': 0.9961614608764648, 'learning_rate': 0.0002447362387652471, 'epoch': 4.181818181818182}\n{'loss': 0.1069, 'grad_norm': 0.92360919713974, 'learning_rate': 0.00024378395768055744, 'epoch': 4.204545454545454}\n{'loss': 0.1106, 'grad_norm': 0.9372161626815796, 'learning_rate': 0.00024283167659586773, 'epoch': 4.2272727272727275}\n{'loss': 0.0815, 'grad_norm': 1.2216899394989014, 'learning_rate': 0.00024187939551117805, 'epoch': 4.25}\n{'loss': 0.0747, 'grad_norm': 0.5045238733291626, 'learning_rate': 0.00024092711442648835, 'epoch': 4.2727272727272725}\n{'loss': 0.0727, 'grad_norm': 0.8062122464179993, 'learning_rate': 0.0002399748333417987, 'epoch': 4.295454545454546}\n{'loss': 0.0727, 'grad_norm': 0.7149714827537537, 'learning_rate': 0.00023902255225710902, 'epoch': 4.318181818181818}\n{'loss': 0.0608, 'grad_norm': 0.6213245987892151, 'learning_rate': 0.00023807027117241932, 'epoch': 4.340909090909091}\n{'loss': 0.0767, 'grad_norm': 0.7526984810829163, 'learning_rate': 0.00023711799008772967, 'epoch': 4.363636363636363}\n{'loss': 0.0743, 'grad_norm': 0.7592761516571045, 'learning_rate': 0.00023616570900304002, 'epoch': 4.386363636363637}\n{'loss': 0.0804, 'grad_norm': 0.9838989973068237, 'learning_rate': 0.0002352134279183503, 'epoch': 4.409090909090909}\n{'loss': 0.074, 'grad_norm': 0.6699413657188416, 'learning_rate': 0.00023426114683366064, 'epoch': 4.431818181818182}\n{'loss': 0.0815, 'grad_norm': 0.8439221978187561, 'learning_rate': 0.000233308865748971, 'epoch': 4.454545454545454}\n{'loss': 0.0647, 'grad_norm': 0.8852100968360901, 'learning_rate': 0.00023235658466428129, 'epoch': 4.4772727272727275}\n{'loss': 0.0934, 'grad_norm': 1.2473961114883423, 'learning_rate': 0.0002314043035795916, 'epoch': 4.5}\n{'loss': 0.0854, 'grad_norm': 0.5773962140083313, 'learning_rate': 0.00023045202249490196, 'epoch': 4.5227272727272725}\n{'loss': 0.0606, 'grad_norm': 0.7784162759780884, 'learning_rate': 0.00022949974141021226, 'epoch': 4.545454545454545}\n{'loss': 0.0765, 'grad_norm': 0.7123926281929016, 'learning_rate': 0.00022854746032552258, 'epoch': 4.568181818181818}\n{'loss': 0.0776, 'grad_norm': 1.0481841564178467, 'learning_rate': 0.00022759517924083287, 'epoch': 4.590909090909091}\n{'loss': 0.0862, 'grad_norm': 0.8330007791519165, 'learning_rate': 0.00022664289815614323, 'epoch': 4.613636363636363}\n{'loss': 0.0902, 'grad_norm': 1.1683063507080078, 'learning_rate': 0.00022569061707145355, 'epoch': 4.636363636363637}\n{'loss': 0.053, 'grad_norm': 0.7699335813522339, 'learning_rate': 0.00022473833598676384, 'epoch': 4.659090909090909}\n{'loss': 0.0941, 'grad_norm': 0.826330840587616, 'learning_rate': 0.0002237860549020742, 'epoch': 4.681818181818182}\n{'loss': 0.1021, 'grad_norm': 0.8167120218276978, 'learning_rate': 0.00022283377381738452, 'epoch': 4.704545454545455}\n{'loss': 0.0717, 'grad_norm': 0.6692941188812256, 'learning_rate': 0.00022188149273269481, 'epoch': 4.7272727272727275}\n{'loss': 0.0837, 'grad_norm': 1.741718053817749, 'learning_rate': 0.00022092921164800516, 'epoch': 4.75}\n{'loss': 0.07, 'grad_norm': 1.675672173500061, 'learning_rate': 0.0002199769305633155, 'epoch': 4.7727272727272725}\n{'loss': 0.1167, 'grad_norm': 1.334204912185669, 'learning_rate': 0.00021902464947862578, 'epoch': 4.795454545454545}\n{'loss': 0.062, 'grad_norm': 0.5194040536880493, 'learning_rate': 0.00021807236839393613, 'epoch': 4.818181818181818}\n{'loss': 0.0813, 'grad_norm': 0.5835597515106201, 'learning_rate': 0.00021712008730924646, 'epoch': 4.840909090909091}\n{'loss': 0.0811, 'grad_norm': 0.7076893448829651, 'learning_rate': 0.00021616780622455675, 'epoch': 4.863636363636363}\n{'loss': 0.0983, 'grad_norm': 1.323103666305542, 'learning_rate': 0.0002152155251398671, 'epoch': 4.886363636363637}\n{'loss': 0.1067, 'grad_norm': 1.2042111158370972, 'learning_rate': 0.00021426324405517743, 'epoch': 4.909090909090909}\n{'loss': 0.0933, 'grad_norm': 0.9059304594993591, 'learning_rate': 0.00021331096297048772, 'epoch': 4.931818181818182}\n{'loss': 0.0861, 'grad_norm': 0.689186692237854, 'learning_rate': 0.00021235868188579807, 'epoch': 4.954545454545455}\n{'loss': 0.071, 'grad_norm': 0.8701688051223755, 'learning_rate': 0.00021140640080110837, 'epoch': 4.9772727272727275}\n{'loss': 0.11, 'grad_norm': 4.8619608879089355, 'learning_rate': 0.0002104541197164187, 'epoch': 5.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8933    0.8843    0.8888      3456\n     PERCENT     0.8721    0.9740    0.9202        77\n        TYPE     0.9473    0.9646    0.9559     11282\n      VOLUME     0.9500    0.9268    0.9383        41\n\n   micro avg     0.9346    0.9459    0.9402     14856\n   macro avg     0.9157    0.9374    0.9258     14856\nweighted avg     0.9343    0.9459    0.9400     14856\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.27815312147140503, 'eval_f1_macro': 0.9257853882935572, 'eval_precision': 0.9345570630486831, 'eval_recall': 0.9458804523424879, 'eval_f1': 0.9401846647932557, 'eval_accuracy': 0.9318429068755439, 'eval_runtime': 1.4412, 'eval_samples_per_second': 3823.296, 'eval_steps_per_second': 7.633, 'epoch': 5.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.0461, 'grad_norm': 0.8307792544364929, 'learning_rate': 0.00020950183863172904, 'epoch': 5.0227272727272725}\n{'loss': 0.0791, 'grad_norm': 0.546360433101654, 'learning_rate': 0.00020854955754703934, 'epoch': 5.045454545454546}\n{'loss': 0.0597, 'grad_norm': 0.8951840400695801, 'learning_rate': 0.00020759727646234966, 'epoch': 5.068181818181818}\n{'loss': 0.0601, 'grad_norm': 0.4894338548183441, 'learning_rate': 0.00020664499537766, 'epoch': 5.090909090909091}\n{'loss': 0.0622, 'grad_norm': 0.8556700944900513, 'learning_rate': 0.0002056927142929703, 'epoch': 5.113636363636363}\n{'loss': 0.0443, 'grad_norm': 0.5751153230667114, 'learning_rate': 0.00020474043320828063, 'epoch': 5.136363636363637}\n{'loss': 0.0588, 'grad_norm': 1.0620508193969727, 'learning_rate': 0.00020378815212359098, 'epoch': 5.159090909090909}\n{'loss': 0.0548, 'grad_norm': 0.5479099750518799, 'learning_rate': 0.00020283587103890128, 'epoch': 5.181818181818182}\n{'loss': 0.0412, 'grad_norm': 0.7565774917602539, 'learning_rate': 0.0002018835899542116, 'epoch': 5.204545454545454}\n{'loss': 0.0818, 'grad_norm': 0.8105201125144958, 'learning_rate': 0.00020093130886952195, 'epoch': 5.2272727272727275}\n{'loss': 0.0363, 'grad_norm': 0.8116296529769897, 'learning_rate': 0.00019997902778483225, 'epoch': 5.25}\n{'loss': 0.0679, 'grad_norm': 1.2819621562957764, 'learning_rate': 0.00019902674670014257, 'epoch': 5.2727272727272725}\n{'loss': 0.0706, 'grad_norm': 0.67086261510849, 'learning_rate': 0.00019807446561545292, 'epoch': 5.295454545454546}\n{'loss': 0.052, 'grad_norm': 0.9811743497848511, 'learning_rate': 0.00019712218453076322, 'epoch': 5.318181818181818}\n{'loss': 0.0474, 'grad_norm': 0.791588544845581, 'learning_rate': 0.00019616990344607354, 'epoch': 5.340909090909091}\n{'loss': 0.0874, 'grad_norm': 0.9277262687683105, 'learning_rate': 0.00019521762236138383, 'epoch': 5.363636363636363}\n{'loss': 0.0643, 'grad_norm': 0.9472246170043945, 'learning_rate': 0.00019426534127669419, 'epoch': 5.386363636363637}\n{'loss': 0.0607, 'grad_norm': 1.1183948516845703, 'learning_rate': 0.0001933130601920045, 'epoch': 5.409090909090909}\n{'loss': 0.0557, 'grad_norm': 1.0506809949874878, 'learning_rate': 0.0001923607791073148, 'epoch': 5.431818181818182}\n{'loss': 0.0739, 'grad_norm': 0.7731499075889587, 'learning_rate': 0.00019140849802262515, 'epoch': 5.454545454545454}\n{'loss': 0.0502, 'grad_norm': 0.5222116112709045, 'learning_rate': 0.00019045621693793548, 'epoch': 5.4772727272727275}\n{'loss': 0.0493, 'grad_norm': 0.7995660901069641, 'learning_rate': 0.00018950393585324577, 'epoch': 5.5}\n{'loss': 0.0457, 'grad_norm': 1.219245433807373, 'learning_rate': 0.00018855165476855612, 'epoch': 5.5227272727272725}\n{'loss': 0.0816, 'grad_norm': 0.9140706658363342, 'learning_rate': 0.00018759937368386645, 'epoch': 5.545454545454545}\n{'loss': 0.065, 'grad_norm': 1.339098572731018, 'learning_rate': 0.00018664709259917677, 'epoch': 5.568181818181818}\n{'loss': 0.0395, 'grad_norm': 0.812472939491272, 'learning_rate': 0.0001856948115144871, 'epoch': 5.590909090909091}\n{'loss': 0.0802, 'grad_norm': 1.055596113204956, 'learning_rate': 0.00018474253042979742, 'epoch': 5.613636363636363}\n{'loss': 0.0609, 'grad_norm': 1.1879613399505615, 'learning_rate': 0.00018379024934510774, 'epoch': 5.636363636363637}\n{'loss': 0.0858, 'grad_norm': 1.4688256978988647, 'learning_rate': 0.00018283796826041806, 'epoch': 5.659090909090909}\n{'loss': 0.0487, 'grad_norm': 0.8157605528831482, 'learning_rate': 0.00018188568717572839, 'epoch': 5.681818181818182}\n{'loss': 0.0484, 'grad_norm': 1.2960121631622314, 'learning_rate': 0.0001809334060910387, 'epoch': 5.704545454545455}\n{'loss': 0.0762, 'grad_norm': 1.5997556447982788, 'learning_rate': 0.00017998112500634903, 'epoch': 5.7272727272727275}\n{'loss': 0.082, 'grad_norm': 1.6366602182388306, 'learning_rate': 0.00017902884392165936, 'epoch': 5.75}\n{'loss': 0.0461, 'grad_norm': 0.6008328795433044, 'learning_rate': 0.00017807656283696968, 'epoch': 5.7727272727272725}\n{'loss': 0.0904, 'grad_norm': 0.80521559715271, 'learning_rate': 0.00017712428175228, 'epoch': 5.795454545454545}\n{'loss': 0.0798, 'grad_norm': 1.0072391033172607, 'learning_rate': 0.00017617200066759033, 'epoch': 5.818181818181818}\n{'loss': 0.0411, 'grad_norm': 0.7000992298126221, 'learning_rate': 0.00017521971958290065, 'epoch': 5.840909090909091}\n{'loss': 0.0616, 'grad_norm': 0.9834904670715332, 'learning_rate': 0.00017426743849821094, 'epoch': 5.863636363636363}\n{'loss': 0.0568, 'grad_norm': 1.11323881149292, 'learning_rate': 0.0001733151574135213, 'epoch': 5.886363636363637}\n{'loss': 0.0785, 'grad_norm': 0.8145751357078552, 'learning_rate': 0.00017236287632883162, 'epoch': 5.909090909090909}\n{'loss': 0.0421, 'grad_norm': 0.8723819851875305, 'learning_rate': 0.00017141059524414191, 'epoch': 5.931818181818182}\n{'loss': 0.0679, 'grad_norm': 0.6698406338691711, 'learning_rate': 0.00017045831415945226, 'epoch': 5.954545454545455}\n{'loss': 0.0777, 'grad_norm': 1.0863226652145386, 'learning_rate': 0.0001695060330747626, 'epoch': 5.9772727272727275}\n{'loss': 0.1192, 'grad_norm': 6.873371124267578, 'learning_rate': 0.00016855375199007288, 'epoch': 6.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.9201    0.8727    0.8958      3456\n     PERCENT     0.8721    0.9740    0.9202        77\n        TYPE     0.9449    0.9690    0.9568     11282\n      VOLUME     0.9500    0.9268    0.9383        41\n\n   micro avg     0.9391    0.9465    0.9428     14856\n   macro avg     0.9218    0.9356    0.9278     14856\nweighted avg     0.9388    0.9465    0.9424     14856\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.29827314615249634, 'eval_f1_macro': 0.9277692603458911, 'eval_precision': 0.939090362652775, 'eval_recall': 0.946486268174475, 'eval_f1': 0.9427738107211104, 'eval_accuracy': 0.9329308093994778, 'eval_runtime': 1.6264, 'eval_samples_per_second': 3387.803, 'eval_steps_per_second': 6.763, 'epoch': 6.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.0307, 'grad_norm': 0.6320152878761292, 'learning_rate': 0.0001676014709053832, 'epoch': 6.0227272727272725}\n{'loss': 0.0528, 'grad_norm': 0.9920222759246826, 'learning_rate': 0.00016664918982069356, 'epoch': 6.045454545454546}\n{'loss': 0.0372, 'grad_norm': 0.5209749341011047, 'learning_rate': 0.00016569690873600385, 'epoch': 6.068181818181818}\n{'loss': 0.0424, 'grad_norm': 1.0163482427597046, 'learning_rate': 0.00016474462765131418, 'epoch': 6.090909090909091}\n{'loss': 0.0529, 'grad_norm': 1.0168768167495728, 'learning_rate': 0.00016379234656662453, 'epoch': 6.113636363636363}\n{'loss': 0.0349, 'grad_norm': 0.5101386904716492, 'learning_rate': 0.00016284006548193485, 'epoch': 6.136363636363637}\n{'loss': 0.0527, 'grad_norm': 0.7801684737205505, 'learning_rate': 0.00016188778439724515, 'epoch': 6.159090909090909}\n{'loss': 0.0531, 'grad_norm': 0.6788747310638428, 'learning_rate': 0.00016093550331255547, 'epoch': 6.181818181818182}\n{'loss': 0.0647, 'grad_norm': 0.9580628275871277, 'learning_rate': 0.00015998322222786582, 'epoch': 6.204545454545454}\n{'loss': 0.0496, 'grad_norm': 0.6330195069313049, 'learning_rate': 0.00015903094114317611, 'epoch': 6.2272727272727275}\n{'loss': 0.0425, 'grad_norm': 0.8566004633903503, 'learning_rate': 0.00015807866005848644, 'epoch': 6.25}\n{'loss': 0.0429, 'grad_norm': 1.0569632053375244, 'learning_rate': 0.0001571263789737968, 'epoch': 6.2727272727272725}\n{'loss': 0.0353, 'grad_norm': 0.6022163033485413, 'learning_rate': 0.00015617409788910708, 'epoch': 6.295454545454546}\n{'loss': 0.0455, 'grad_norm': 0.542657196521759, 'learning_rate': 0.0001552218168044174, 'epoch': 6.318181818181818}\n{'loss': 0.0578, 'grad_norm': 0.5713463425636292, 'learning_rate': 0.00015426953571972776, 'epoch': 6.340909090909091}\n{'loss': 0.072, 'grad_norm': 1.1231309175491333, 'learning_rate': 0.00015331725463503805, 'epoch': 6.363636363636363}\n{'loss': 0.0288, 'grad_norm': 0.47789132595062256, 'learning_rate': 0.00015236497355034838, 'epoch': 6.386363636363637}\n{'loss': 0.0535, 'grad_norm': 1.0315519571304321, 'learning_rate': 0.0001514126924656587, 'epoch': 6.409090909090909}\n{'loss': 0.0544, 'grad_norm': 0.8026590943336487, 'learning_rate': 0.00015046041138096902, 'epoch': 6.431818181818182}\n{'loss': 0.0245, 'grad_norm': 0.5534695982933044, 'learning_rate': 0.00014950813029627935, 'epoch': 6.454545454545454}\n{'loss': 0.0272, 'grad_norm': 0.8238474726676941, 'learning_rate': 0.00014855584921158967, 'epoch': 6.4772727272727275}\n{'loss': 0.0573, 'grad_norm': 1.1158058643341064, 'learning_rate': 0.0001476035681269, 'epoch': 6.5}\n{'loss': 0.0436, 'grad_norm': 1.16342031955719, 'learning_rate': 0.00014665128704221032, 'epoch': 6.5227272727272725}\n{'loss': 0.0666, 'grad_norm': 1.014016032218933, 'learning_rate': 0.00014569900595752064, 'epoch': 6.545454545454545}\n{'loss': 0.0529, 'grad_norm': 0.7371175289154053, 'learning_rate': 0.00014474672487283096, 'epoch': 6.568181818181818}\n{'loss': 0.055, 'grad_norm': 0.9236902594566345, 'learning_rate': 0.00014379444378814129, 'epoch': 6.590909090909091}\n{'loss': 0.0427, 'grad_norm': 0.8912021517753601, 'learning_rate': 0.0001428421627034516, 'epoch': 6.613636363636363}\n{'loss': 0.0371, 'grad_norm': 0.6204680800437927, 'learning_rate': 0.00014188988161876193, 'epoch': 6.636363636363637}\n{'loss': 0.0475, 'grad_norm': 0.8196551203727722, 'learning_rate': 0.00014093760053407225, 'epoch': 6.659090909090909}\n{'loss': 0.0551, 'grad_norm': 0.6475508809089661, 'learning_rate': 0.00013998531944938258, 'epoch': 6.681818181818182}\n{'loss': 0.0404, 'grad_norm': 0.6744859218597412, 'learning_rate': 0.0001390330383646929, 'epoch': 6.704545454545455}\n{'loss': 0.0669, 'grad_norm': 1.0347120761871338, 'learning_rate': 0.00013808075728000322, 'epoch': 6.7272727272727275}\n{'loss': 0.0374, 'grad_norm': 0.6879774332046509, 'learning_rate': 0.00013712847619531355, 'epoch': 6.75}\n{'loss': 0.0478, 'grad_norm': 0.8455259799957275, 'learning_rate': 0.00013617619511062387, 'epoch': 6.7727272727272725}\n{'loss': 0.0645, 'grad_norm': 1.1035358905792236, 'learning_rate': 0.0001352239140259342, 'epoch': 6.795454545454545}\n{'loss': 0.0601, 'grad_norm': 0.8340948224067688, 'learning_rate': 0.00013427163294124452, 'epoch': 6.818181818181818}\n{'loss': 0.0535, 'grad_norm': 0.9305357933044434, 'learning_rate': 0.00013331935185655484, 'epoch': 6.840909090909091}\n{'loss': 0.0643, 'grad_norm': 0.9937087297439575, 'learning_rate': 0.00013236707077186516, 'epoch': 6.863636363636363}\n{'loss': 0.0371, 'grad_norm': 0.5358138680458069, 'learning_rate': 0.00013141478968717549, 'epoch': 6.886363636363637}\n{'loss': 0.0515, 'grad_norm': 0.8038597702980042, 'learning_rate': 0.0001304625086024858, 'epoch': 6.909090909090909}\n{'loss': 0.0709, 'grad_norm': 1.3226597309112549, 'learning_rate': 0.00012951022751779613, 'epoch': 6.931818181818182}\n{'loss': 0.0397, 'grad_norm': 0.9973755478858948, 'learning_rate': 0.00012855794643310643, 'epoch': 6.954545454545455}\n{'loss': 0.0478, 'grad_norm': 0.6359300017356873, 'learning_rate': 0.00012760566534841678, 'epoch': 6.9772727272727275}\n{'loss': 0.0068, 'grad_norm': 0.27827736735343933, 'learning_rate': 0.0001266533842637271, 'epoch': 7.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.9128    0.8779    0.8950      3456\n     PERCENT     0.8721    0.9740    0.9202        77\n        TYPE     0.9485    0.9618    0.9551     11282\n      VOLUME     0.8810    0.9024    0.8916        41\n\n   micro avg     0.9399    0.9422    0.9410     14856\n   macro avg     0.9036    0.9290    0.9155     14856\nweighted avg     0.9396    0.9422    0.9408     14856\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.3093871772289276, 'eval_f1_macro': 0.9154766249986321, 'eval_precision': 0.9399006177813591, 'eval_recall': 0.9421782444803446, 'eval_f1': 0.9410380529783514, 'eval_accuracy': 0.9308637946040035, 'eval_runtime': 1.4483, 'eval_samples_per_second': 3804.446, 'eval_steps_per_second': 7.595, 'epoch': 7.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.0437, 'grad_norm': 0.9828845262527466, 'learning_rate': 0.0001257011031790374, 'epoch': 7.0227272727272725}\n{'loss': 0.0338, 'grad_norm': 1.2921974658966064, 'learning_rate': 0.00012474882209434775, 'epoch': 7.045454545454546}\n{'loss': 0.0383, 'grad_norm': 0.7223597764968872, 'learning_rate': 0.00012379654100965807, 'epoch': 7.068181818181818}\n{'loss': 0.0507, 'grad_norm': 0.8098681569099426, 'learning_rate': 0.00012284425992496837, 'epoch': 7.090909090909091}\n{'loss': 0.0304, 'grad_norm': 0.7600889205932617, 'learning_rate': 0.00012189197884027872, 'epoch': 7.113636363636363}\n{'loss': 0.0273, 'grad_norm': 0.762365996837616, 'learning_rate': 0.00012093969775558903, 'epoch': 7.136363636363637}\n{'loss': 0.0409, 'grad_norm': 0.6649178266525269, 'learning_rate': 0.00011998741667089935, 'epoch': 7.159090909090909}\n{'loss': 0.0388, 'grad_norm': 0.8690189123153687, 'learning_rate': 0.00011903513558620966, 'epoch': 7.181818181818182}\n{'loss': 0.0472, 'grad_norm': 0.7793386578559875, 'learning_rate': 0.00011808285450152001, 'epoch': 7.204545454545454}\n{'loss': 0.0412, 'grad_norm': 0.9514992237091064, 'learning_rate': 0.00011713057341683032, 'epoch': 7.2272727272727275}\n{'loss': 0.0569, 'grad_norm': 2.126715660095215, 'learning_rate': 0.00011617829233214064, 'epoch': 7.25}\n{'loss': 0.0311, 'grad_norm': 0.6957566142082214, 'learning_rate': 0.00011522601124745098, 'epoch': 7.2727272727272725}\n{'loss': 0.0592, 'grad_norm': 0.7329285144805908, 'learning_rate': 0.00011427373016276129, 'epoch': 7.295454545454546}\n{'loss': 0.0273, 'grad_norm': 0.38298362493515015, 'learning_rate': 0.00011332144907807161, 'epoch': 7.318181818181818}\n{'loss': 0.0398, 'grad_norm': 0.7968241572380066, 'learning_rate': 0.00011236916799338192, 'epoch': 7.340909090909091}\n{'loss': 0.0418, 'grad_norm': 0.49488988518714905, 'learning_rate': 0.00011141688690869226, 'epoch': 7.363636363636363}\n{'loss': 0.0294, 'grad_norm': 0.30510464310646057, 'learning_rate': 0.00011046460582400258, 'epoch': 7.386363636363637}\n{'loss': 0.0433, 'grad_norm': 0.6627395153045654, 'learning_rate': 0.00010951232473931289, 'epoch': 7.409090909090909}\n{'loss': 0.0454, 'grad_norm': 0.7476467490196228, 'learning_rate': 0.00010856004365462323, 'epoch': 7.431818181818182}\n{'loss': 0.0343, 'grad_norm': 0.897969663143158, 'learning_rate': 0.00010760776256993355, 'epoch': 7.454545454545454}\n{'loss': 0.0319, 'grad_norm': 0.7511216998100281, 'learning_rate': 0.00010665548148524386, 'epoch': 7.4772727272727275}\n{'loss': 0.0433, 'grad_norm': 0.7500384449958801, 'learning_rate': 0.00010570320040055418, 'epoch': 7.5}\n{'loss': 0.0277, 'grad_norm': 0.5537475347518921, 'learning_rate': 0.00010475091931586452, 'epoch': 7.5227272727272725}\n{'loss': 0.0463, 'grad_norm': 0.6821805238723755, 'learning_rate': 0.00010379863823117483, 'epoch': 7.545454545454545}\n{'loss': 0.0339, 'grad_norm': 0.4749763309955597, 'learning_rate': 0.00010284635714648515, 'epoch': 7.568181818181818}\n{'loss': 0.0459, 'grad_norm': 1.2182838916778564, 'learning_rate': 0.00010189407606179549, 'epoch': 7.590909090909091}\n{'loss': 0.049, 'grad_norm': 0.6527551412582397, 'learning_rate': 0.0001009417949771058, 'epoch': 7.613636363636363}\n{'loss': 0.0394, 'grad_norm': 0.8051902651786804, 'learning_rate': 9.998951389241612e-05, 'epoch': 7.636363636363637}\n{'loss': 0.0495, 'grad_norm': 0.6537036299705505, 'learning_rate': 9.903723280772646e-05, 'epoch': 7.659090909090909}\n{'loss': 0.0322, 'grad_norm': 0.6676492094993591, 'learning_rate': 9.808495172303677e-05, 'epoch': 7.681818181818182}\n{'loss': 0.0217, 'grad_norm': 0.483774334192276, 'learning_rate': 9.713267063834709e-05, 'epoch': 7.704545454545455}\n{'loss': 0.0338, 'grad_norm': 0.6194101572036743, 'learning_rate': 9.61803895536574e-05, 'epoch': 7.7272727272727275}\n{'loss': 0.0391, 'grad_norm': 0.6352093815803528, 'learning_rate': 9.522810846896774e-05, 'epoch': 7.75}\n{'loss': 0.0336, 'grad_norm': 0.8125383853912354, 'learning_rate': 9.427582738427806e-05, 'epoch': 7.7727272727272725}\n{'loss': 0.0316, 'grad_norm': 1.0422509908676147, 'learning_rate': 9.332354629958839e-05, 'epoch': 7.795454545454545}\n{'loss': 0.0239, 'grad_norm': 0.35655477643013, 'learning_rate': 9.237126521489871e-05, 'epoch': 7.818181818181818}\n{'loss': 0.0386, 'grad_norm': 0.8469451665878296, 'learning_rate': 9.141898413020903e-05, 'epoch': 7.840909090909091}\n{'loss': 0.0329, 'grad_norm': 0.678962767124176, 'learning_rate': 9.046670304551935e-05, 'epoch': 7.863636363636363}\n{'loss': 0.0431, 'grad_norm': 0.49699756503105164, 'learning_rate': 8.951442196082968e-05, 'epoch': 7.886363636363637}\n{'loss': 0.0329, 'grad_norm': 1.0174736976623535, 'learning_rate': 8.856214087614e-05, 'epoch': 7.909090909090909}\n{'loss': 0.0484, 'grad_norm': 0.9556695222854614, 'learning_rate': 8.760985979145032e-05, 'epoch': 7.931818181818182}\n{'loss': 0.0475, 'grad_norm': 0.7554863095283508, 'learning_rate': 8.665757870676065e-05, 'epoch': 7.954545454545455}\n{'loss': 0.0323, 'grad_norm': 0.6677287220954895, 'learning_rate': 8.570529762207096e-05, 'epoch': 7.9772727272727275}\n{'loss': 0.0302, 'grad_norm': 2.5908424854278564, 'learning_rate': 8.47530165373813e-05, 'epoch': 8.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.9103    0.8872    0.8986      3456\n     PERCENT     0.8721    0.9740    0.9202        77\n        TYPE     0.9488    0.9658    0.9572     11282\n      VOLUME     0.9250    0.9024    0.9136        41\n\n   micro avg     0.9396    0.9474    0.9435     14856\n   macro avg     0.9141    0.9324    0.9224     14856\nweighted avg     0.9394    0.9474    0.9433     14856\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.31992077827453613, 'eval_f1_macro': 0.9224089371384582, 'eval_precision': 0.9396448123915075, 'eval_recall': 0.9473613354873451, 'eval_f1': 0.9434872963732653, 'eval_accuracy': 0.9333659704090513, 'eval_runtime': 1.8532, 'eval_samples_per_second': 2973.236, 'eval_steps_per_second': 5.936, 'epoch': 8.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.0204, 'grad_norm': 0.5493097305297852, 'learning_rate': 8.38007354526916e-05, 'epoch': 8.022727272727273}\n{'loss': 0.0485, 'grad_norm': 0.6564518213272095, 'learning_rate': 8.284845436800193e-05, 'epoch': 8.045454545454545}\n{'loss': 0.0401, 'grad_norm': 0.7400782108306885, 'learning_rate': 8.189617328331226e-05, 'epoch': 8.068181818181818}\n{'loss': 0.0281, 'grad_norm': 0.9285740256309509, 'learning_rate': 8.094389219862257e-05, 'epoch': 8.090909090909092}\n{'loss': 0.0222, 'grad_norm': 0.6561432480812073, 'learning_rate': 7.999161111393291e-05, 'epoch': 8.113636363636363}\n{'loss': 0.0386, 'grad_norm': 0.6553654670715332, 'learning_rate': 7.903933002924322e-05, 'epoch': 8.136363636363637}\n{'loss': 0.036, 'grad_norm': 0.5144176483154297, 'learning_rate': 7.808704894455354e-05, 'epoch': 8.159090909090908}\n{'loss': 0.0404, 'grad_norm': 0.7408321499824524, 'learning_rate': 7.713476785986388e-05, 'epoch': 8.181818181818182}\n{'loss': 0.0397, 'grad_norm': 0.7100220322608948, 'learning_rate': 7.618248677517419e-05, 'epoch': 8.204545454545455}\n{'loss': 0.0385, 'grad_norm': 0.7763263583183289, 'learning_rate': 7.523020569048451e-05, 'epoch': 8.227272727272727}\n{'loss': 0.043, 'grad_norm': 0.5124692320823669, 'learning_rate': 7.427792460579483e-05, 'epoch': 8.25}\n{'loss': 0.024, 'grad_norm': 0.7097371816635132, 'learning_rate': 7.332564352110516e-05, 'epoch': 8.272727272727273}\n{'loss': 0.0272, 'grad_norm': 0.5185973644256592, 'learning_rate': 7.237336243641548e-05, 'epoch': 8.295454545454545}\n{'loss': 0.0269, 'grad_norm': 0.7601723074913025, 'learning_rate': 7.14210813517258e-05, 'epoch': 8.318181818181818}\n{'loss': 0.0347, 'grad_norm': 1.0071467161178589, 'learning_rate': 7.046880026703613e-05, 'epoch': 8.340909090909092}\n{'loss': 0.0345, 'grad_norm': 0.6093938946723938, 'learning_rate': 6.951651918234645e-05, 'epoch': 8.363636363636363}\n{'loss': 0.0269, 'grad_norm': 0.6404441595077515, 'learning_rate': 6.856423809765677e-05, 'epoch': 8.386363636363637}\n{'loss': 0.0287, 'grad_norm': 0.5459426641464233, 'learning_rate': 6.76119570129671e-05, 'epoch': 8.409090909090908}\n{'loss': 0.0207, 'grad_norm': 0.48083755373954773, 'learning_rate': 6.665967592827742e-05, 'epoch': 8.431818181818182}\n{'loss': 0.0338, 'grad_norm': 0.6960550546646118, 'learning_rate': 6.570739484358774e-05, 'epoch': 8.454545454545455}\n{'loss': 0.0383, 'grad_norm': 0.7023628950119019, 'learning_rate': 6.475511375889807e-05, 'epoch': 8.477272727272727}\n{'loss': 0.0316, 'grad_norm': 0.5774529576301575, 'learning_rate': 6.380283267420839e-05, 'epoch': 8.5}\n{'loss': 0.0287, 'grad_norm': 0.627565860748291, 'learning_rate': 6.28505515895187e-05, 'epoch': 8.522727272727273}\n{'loss': 0.031, 'grad_norm': 0.6404892206192017, 'learning_rate': 6.189827050482904e-05, 'epoch': 8.545454545454545}\n{'loss': 0.0267, 'grad_norm': 0.718417763710022, 'learning_rate': 6.094598942013936e-05, 'epoch': 8.568181818181818}\n{'loss': 0.0365, 'grad_norm': 0.6215259432792664, 'learning_rate': 5.9993708335449675e-05, 'epoch': 8.590909090909092}\n{'loss': 0.0302, 'grad_norm': 0.5768715739250183, 'learning_rate': 5.9041427250760005e-05, 'epoch': 8.613636363636363}\n{'loss': 0.0257, 'grad_norm': 0.7080629467964172, 'learning_rate': 5.808914616607032e-05, 'epoch': 8.636363636363637}\n{'loss': 0.0206, 'grad_norm': 0.44504886865615845, 'learning_rate': 5.7136865081380645e-05, 'epoch': 8.659090909090908}\n{'loss': 0.0343, 'grad_norm': 0.8947566151618958, 'learning_rate': 5.618458399669096e-05, 'epoch': 8.681818181818182}\n{'loss': 0.0285, 'grad_norm': 0.5588499903678894, 'learning_rate': 5.523230291200129e-05, 'epoch': 8.704545454545455}\n{'loss': 0.0394, 'grad_norm': 0.6009902954101562, 'learning_rate': 5.4280021827311614e-05, 'epoch': 8.727272727272727}\n{'loss': 0.0367, 'grad_norm': 0.757280707359314, 'learning_rate': 5.332774074262193e-05, 'epoch': 8.75}\n{'loss': 0.0353, 'grad_norm': 0.746609628200531, 'learning_rate': 5.237545965793226e-05, 'epoch': 8.772727272727273}\n{'loss': 0.0376, 'grad_norm': 0.6421764492988586, 'learning_rate': 5.142317857324258e-05, 'epoch': 8.795454545454545}\n{'loss': 0.0476, 'grad_norm': 0.5854908227920532, 'learning_rate': 5.04708974885529e-05, 'epoch': 8.818181818181818}\n{'loss': 0.037, 'grad_norm': 0.616590142250061, 'learning_rate': 4.951861640386323e-05, 'epoch': 8.840909090909092}\n{'loss': 0.0339, 'grad_norm': 0.7170025706291199, 'learning_rate': 4.8566335319173546e-05, 'epoch': 8.863636363636363}\n{'loss': 0.0288, 'grad_norm': 0.7315890789031982, 'learning_rate': 4.761405423448387e-05, 'epoch': 8.886363636363637}\n{'loss': 0.0411, 'grad_norm': 0.7591223120689392, 'learning_rate': 4.666177314979419e-05, 'epoch': 8.909090909090908}\n{'loss': 0.046, 'grad_norm': 0.8829277753829956, 'learning_rate': 4.5709492065104516e-05, 'epoch': 8.931818181818182}\n{'loss': 0.0446, 'grad_norm': 1.5179190635681152, 'learning_rate': 4.475721098041484e-05, 'epoch': 8.954545454545455}\n{'loss': 0.0158, 'grad_norm': 0.3770735263824463, 'learning_rate': 4.380492989572516e-05, 'epoch': 8.977272727272727}\n{'loss': 0.0175, 'grad_norm': 2.2150943279266357, 'learning_rate': 4.285264881103548e-05, 'epoch': 9.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.9125    0.8811    0.8965      3456\n     PERCENT     0.8721    0.9740    0.9202        77\n        TYPE     0.9470    0.9661    0.9564     11282\n      VOLUME     0.9500    0.9268    0.9383        41\n\n   micro avg     0.9389    0.9462    0.9425     14856\n   macro avg     0.9204    0.9370    0.9279     14856\nweighted avg     0.9386    0.9462    0.9423     14856\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.33192747831344604, 'eval_f1_macro': 0.9278645719744001, 'eval_precision': 0.9388859203847182, 'eval_recall': 0.9462170166935918, 'eval_f1': 0.9425372133565778, 'eval_accuracy': 0.9322236727589208, 'eval_runtime': 1.6194, 'eval_samples_per_second': 3402.587, 'eval_steps_per_second': 6.793, 'epoch': 9.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.0341, 'grad_norm': 0.5541712045669556, 'learning_rate': 4.19003677263458e-05, 'epoch': 9.022727272727273}\n{'loss': 0.0199, 'grad_norm': 0.46613043546676636, 'learning_rate': 4.094808664165613e-05, 'epoch': 9.045454545454545}\n{'loss': 0.0205, 'grad_norm': 0.4313705861568451, 'learning_rate': 3.9995805556966455e-05, 'epoch': 9.068181818181818}\n{'loss': 0.0278, 'grad_norm': 0.6343562006950378, 'learning_rate': 3.904352447227677e-05, 'epoch': 9.090909090909092}\n{'loss': 0.0293, 'grad_norm': 0.5141933560371399, 'learning_rate': 3.8091243387587094e-05, 'epoch': 9.113636363636363}\n{'loss': 0.029, 'grad_norm': 0.8270730376243591, 'learning_rate': 3.713896230289742e-05, 'epoch': 9.136363636363637}\n{'loss': 0.0289, 'grad_norm': 0.6209843754768372, 'learning_rate': 3.618668121820774e-05, 'epoch': 9.159090909090908}\n{'loss': 0.0319, 'grad_norm': 0.5139965415000916, 'learning_rate': 3.5234400133518064e-05, 'epoch': 9.181818181818182}\n{'loss': 0.0342, 'grad_norm': 0.6669213771820068, 'learning_rate': 3.428211904882839e-05, 'epoch': 9.204545454545455}\n{'loss': 0.0395, 'grad_norm': 0.5408391952514648, 'learning_rate': 3.332983796413871e-05, 'epoch': 9.227272727272727}\n{'loss': 0.0206, 'grad_norm': 0.4549006521701813, 'learning_rate': 3.237755687944903e-05, 'epoch': 9.25}\n{'loss': 0.0213, 'grad_norm': 0.4111410975456238, 'learning_rate': 3.142527579475935e-05, 'epoch': 9.272727272727273}\n{'loss': 0.0322, 'grad_norm': 0.47084829211235046, 'learning_rate': 3.047299471006968e-05, 'epoch': 9.295454545454545}\n{'loss': 0.0468, 'grad_norm': 0.7939935922622681, 'learning_rate': 2.9520713625380003e-05, 'epoch': 9.318181818181818}\n{'loss': 0.0475, 'grad_norm': 0.9713749289512634, 'learning_rate': 2.8568432540690322e-05, 'epoch': 9.340909090909092}\n{'loss': 0.0224, 'grad_norm': 0.544333279132843, 'learning_rate': 2.7616151456000645e-05, 'epoch': 9.363636363636363}\n{'loss': 0.0286, 'grad_norm': 0.6183943748474121, 'learning_rate': 2.6663870371310965e-05, 'epoch': 9.386363636363637}\n{'loss': 0.021, 'grad_norm': 0.6342751979827881, 'learning_rate': 2.571158928662129e-05, 'epoch': 9.409090909090908}\n{'loss': 0.0373, 'grad_norm': 0.5112915635108948, 'learning_rate': 2.4759308201931615e-05, 'epoch': 9.431818181818182}\n{'loss': 0.0267, 'grad_norm': 0.7437041997909546, 'learning_rate': 2.3807027117241935e-05, 'epoch': 9.454545454545455}\n{'loss': 0.0261, 'grad_norm': 0.6353192329406738, 'learning_rate': 2.2854746032552258e-05, 'epoch': 9.477272727272727}\n{'loss': 0.031, 'grad_norm': 0.41386935114860535, 'learning_rate': 2.190246494786258e-05, 'epoch': 9.5}\n{'loss': 0.0203, 'grad_norm': 0.6775792837142944, 'learning_rate': 2.09501838631729e-05, 'epoch': 9.522727272727273}\n{'loss': 0.0296, 'grad_norm': 0.651141881942749, 'learning_rate': 1.9997902778483227e-05, 'epoch': 9.545454545454545}\n{'loss': 0.0223, 'grad_norm': 0.49855417013168335, 'learning_rate': 1.9045621693793547e-05, 'epoch': 9.568181818181818}\n{'loss': 0.0334, 'grad_norm': 1.1063579320907593, 'learning_rate': 1.809334060910387e-05, 'epoch': 9.590909090909092}\n{'loss': 0.0175, 'grad_norm': 0.4414147138595581, 'learning_rate': 1.7141059524414193e-05, 'epoch': 9.613636363636363}\n{'loss': 0.0318, 'grad_norm': 0.4292861521244049, 'learning_rate': 1.6188778439724517e-05, 'epoch': 9.636363636363637}\n{'loss': 0.03, 'grad_norm': 0.583226203918457, 'learning_rate': 1.523649735503484e-05, 'epoch': 9.659090909090908}\n{'loss': 0.0173, 'grad_norm': 0.2663096785545349, 'learning_rate': 1.4284216270345161e-05, 'epoch': 9.681818181818182}\n{'loss': 0.0192, 'grad_norm': 0.7914794087409973, 'learning_rate': 1.3331935185655483e-05, 'epoch': 9.704545454545455}\n{'loss': 0.0274, 'grad_norm': 0.48356467485427856, 'learning_rate': 1.2379654100965807e-05, 'epoch': 9.727272727272727}\n{'loss': 0.0217, 'grad_norm': 0.4930391311645508, 'learning_rate': 1.1427373016276129e-05, 'epoch': 9.75}\n{'loss': 0.0329, 'grad_norm': 0.6559065580368042, 'learning_rate': 1.047509193158645e-05, 'epoch': 9.772727272727273}\n{'loss': 0.0282, 'grad_norm': 0.7130504846572876, 'learning_rate': 9.522810846896774e-06, 'epoch': 9.795454545454545}\n{'loss': 0.0235, 'grad_norm': 0.4387940466403961, 'learning_rate': 8.570529762207097e-06, 'epoch': 9.818181818181818}\n{'loss': 0.0299, 'grad_norm': 0.5004055500030518, 'learning_rate': 7.61824867751742e-06, 'epoch': 9.840909090909092}\n{'loss': 0.0195, 'grad_norm': 0.5135296583175659, 'learning_rate': 6.665967592827741e-06, 'epoch': 9.863636363636363}\n{'loss': 0.0145, 'grad_norm': 0.5126386880874634, 'learning_rate': 5.7136865081380645e-06, 'epoch': 9.886363636363637}\n{'loss': 0.0515, 'grad_norm': 0.6741926670074463, 'learning_rate': 4.761405423448387e-06, 'epoch': 9.909090909090908}\n{'loss': 0.025, 'grad_norm': 0.594059944152832, 'learning_rate': 3.80912433875871e-06, 'epoch': 9.931818181818182}\n{'loss': 0.0273, 'grad_norm': 0.7949895262718201, 'learning_rate': 2.8568432540690322e-06, 'epoch': 9.954545454545455}\n{'loss': 0.0363, 'grad_norm': 0.36565089225769043, 'learning_rate': 1.904562169379355e-06, 'epoch': 9.977272727272727}\n{'loss': 0.1037, 'grad_norm': 4.240450859069824, 'learning_rate': 9.522810846896775e-07, 'epoch': 10.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.9093    0.8874    0.8982      3456\n     PERCENT     0.8721    0.9740    0.9202        77\n        TYPE     0.9494    0.9661    0.9576     11282\n      VOLUME     0.9500    0.9268    0.9383        41\n\n   micro avg     0.9399    0.9477    0.9438     14856\n   macro avg     0.9202    0.9386    0.9286     14856\nweighted avg     0.9397    0.9477    0.9436     14856\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.3338875472545624, 'eval_f1_macro': 0.9285984652888206, 'eval_precision': 0.9399158822351292, 'eval_recall': 0.9476978998384491, 'eval_f1': 0.9437908496732026, 'eval_accuracy': 0.9336923411662315, 'eval_runtime': 1.5236, 'eval_samples_per_second': 3616.407, 'eval_steps_per_second': 7.22, 'epoch': 10.0}\n{'train_runtime': 62.1638, 'train_samples_per_second': 3545.795, 'train_steps_per_second': 7.078, 'train_loss': 0.20209376966284417, 'epoch': 10.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nThe speedups for torchdynamo mostly come with GPU Ampere or higher and which is not detected here.\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.9093    0.8874    0.8982      3456\n     PERCENT     0.8721    0.9740    0.9202        77\n        TYPE     0.9494    0.9661    0.9576     11282\n      VOLUME     0.9500    0.9268    0.9383        41\n\n   micro avg     0.9399    0.9477    0.9438     14856\n   macro avg     0.9202    0.9386    0.9286     14856\nweighted avg     0.9397    0.9477    0.9436     14856\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.3338875472545624, 'eval_f1_macro': 0.9285984652888206, 'eval_precision': 0.9399158822351292, 'eval_recall': 0.9476978998384491, 'eval_f1': 0.9437908496732026, 'eval_accuracy': 0.9336923411662315, 'eval_runtime': 1.5636, 'eval_samples_per_second': 3523.957, 'eval_steps_per_second': 7.035, 'epoch': 10.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/3364797558.py:66: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 2.1831, 'grad_norm': 7.3428192138671875, 'learning_rate': 0.0, 'epoch': 0.022727272727272728}\n{'loss': 2.1806, 'grad_norm': 7.105165004730225, 'learning_rate': 8.570529762207097e-06, 'epoch': 0.045454545454545456}\n{'loss': 2.1687, 'grad_norm': 6.906354904174805, 'learning_rate': 1.7141059524414193e-05, 'epoch': 0.06818181818181818}\n{'loss': 2.1162, 'grad_norm': 7.125436305999756, 'learning_rate': 2.571158928662129e-05, 'epoch': 0.09090909090909091}\n{'loss': 2.0643, 'grad_norm': 6.975474834442139, 'learning_rate': 3.428211904882839e-05, 'epoch': 0.11363636363636363}\n{'loss': 1.9887, 'grad_norm': 6.662441253662109, 'learning_rate': 4.285264881103548e-05, 'epoch': 0.13636363636363635}\n{'loss': 1.9136, 'grad_norm': 6.108415126800537, 'learning_rate': 5.142317857324258e-05, 'epoch': 0.1590909090909091}\n{'loss': 1.8068, 'grad_norm': 5.936633110046387, 'learning_rate': 5.9993708335449675e-05, 'epoch': 0.18181818181818182}\n{'loss': 1.6795, 'grad_norm': 5.599733352661133, 'learning_rate': 6.856423809765677e-05, 'epoch': 0.20454545454545456}\n{'loss': 1.5948, 'grad_norm': 4.7127251625061035, 'learning_rate': 7.713476785986388e-05, 'epoch': 0.22727272727272727}\n{'loss': 1.4597, 'grad_norm': 4.285719871520996, 'learning_rate': 8.570529762207096e-05, 'epoch': 0.25}\n{'loss': 1.4082, 'grad_norm': 3.2069435119628906, 'learning_rate': 9.427582738427806e-05, 'epoch': 0.2727272727272727}\n{'loss': 1.2897, 'grad_norm': 2.6043806076049805, 'learning_rate': 0.00010284635714648515, 'epoch': 0.29545454545454547}\n{'loss': 1.2461, 'grad_norm': 1.991142749786377, 'learning_rate': 0.00011141688690869226, 'epoch': 0.3181818181818182}\n{'loss': 1.2756, 'grad_norm': 1.9406907558441162, 'learning_rate': 0.00011998741667089935, 'epoch': 0.3409090909090909}\n{'loss': 1.1, 'grad_norm': 1.8800071477890015, 'learning_rate': 0.00012855794643310643, 'epoch': 0.36363636363636365}\n{'loss': 1.1021, 'grad_norm': 2.0394175052642822, 'learning_rate': 0.00013712847619531355, 'epoch': 0.38636363636363635}\n{'loss': 1.0754, 'grad_norm': 1.8965606689453125, 'learning_rate': 0.00014569900595752064, 'epoch': 0.4090909090909091}\n{'loss': 1.0477, 'grad_norm': 1.205289602279663, 'learning_rate': 0.00015426953571972776, 'epoch': 0.4318181818181818}\n{'loss': 0.9843, 'grad_norm': 1.3908421993255615, 'learning_rate': 0.00016284006548193485, 'epoch': 0.45454545454545453}\n{'loss': 0.9821, 'grad_norm': 1.7282274961471558, 'learning_rate': 0.00017141059524414191, 'epoch': 0.4772727272727273}\n{'loss': 0.9687, 'grad_norm': 1.9510293006896973, 'learning_rate': 0.00017998112500634903, 'epoch': 0.5}\n{'loss': 0.9207, 'grad_norm': 1.8090044260025024, 'learning_rate': 0.00018855165476855612, 'epoch': 0.5227272727272727}\n{'loss': 0.8662, 'grad_norm': 1.2547290325164795, 'learning_rate': 0.00019712218453076322, 'epoch': 0.5454545454545454}\n{'loss': 0.8393, 'grad_norm': 1.2098678350448608, 'learning_rate': 0.0002056927142929703, 'epoch': 0.5681818181818182}\n{'loss': 0.8023, 'grad_norm': 1.4759864807128906, 'learning_rate': 0.00021426324405517743, 'epoch': 0.5909090909090909}\n{'loss': 0.7711, 'grad_norm': 1.3587498664855957, 'learning_rate': 0.00022283377381738452, 'epoch': 0.6136363636363636}\n{'loss': 0.719, 'grad_norm': 0.8101633191108704, 'learning_rate': 0.0002314043035795916, 'epoch': 0.6363636363636364}\n{'loss': 0.7384, 'grad_norm': 1.1926214694976807, 'learning_rate': 0.0002399748333417987, 'epoch': 0.6590909090909091}\n{'loss': 0.6619, 'grad_norm': 1.2073196172714233, 'learning_rate': 0.00024854536310400577, 'epoch': 0.6818181818181818}\n{'loss': 0.6334, 'grad_norm': 1.365310788154602, 'learning_rate': 0.00025711589286621286, 'epoch': 0.7045454545454546}\n{'loss': 0.6114, 'grad_norm': 1.1282809972763062, 'learning_rate': 0.00026568642262842, 'epoch': 0.7272727272727273}\n{'loss': 0.5792, 'grad_norm': 1.166630506515503, 'learning_rate': 0.0002742569523906271, 'epoch': 0.75}\n{'loss': 0.5583, 'grad_norm': 0.758485734462738, 'learning_rate': 0.0002828274821528342, 'epoch': 0.7727272727272727}\n{'loss': 0.5858, 'grad_norm': 0.9826481938362122, 'learning_rate': 0.0002913980119150413, 'epoch': 0.7954545454545454}\n{'loss': 0.5249, 'grad_norm': 0.8823051452636719, 'learning_rate': 0.00029996854167724837, 'epoch': 0.8181818181818182}\n{'loss': 0.5947, 'grad_norm': 1.426782250404358, 'learning_rate': 0.0003085390714394555, 'epoch': 0.8409090909090909}\n{'loss': 0.5005, 'grad_norm': 1.5654778480529785, 'learning_rate': 0.0003171096012016626, 'epoch': 0.8636363636363636}\n{'loss': 0.5639, 'grad_norm': 1.062095284461975, 'learning_rate': 0.0003256801309638697, 'epoch': 0.8863636363636364}\n{'loss': 0.5098, 'grad_norm': 1.5494234561920166, 'learning_rate': 0.00033425066072607674, 'epoch': 0.9090909090909091}\n{'loss': 0.4391, 'grad_norm': 0.9634894132614136, 'learning_rate': 0.00034282119048828383, 'epoch': 0.9318181818181818}\n{'loss': 0.4454, 'grad_norm': 1.581215739250183, 'learning_rate': 0.0003513917202504909, 'epoch': 0.9545454545454546}\n{'loss': 0.4894, 'grad_norm': 0.9020959734916687, 'learning_rate': 0.00035996225001269806, 'epoch': 0.9772727272727273}\n{'loss': 0.4408, 'grad_norm': 2.6581215858459473, 'learning_rate': 0.00036853277977490516, 'epoch': 1.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8185    0.7169    0.7643      3221\n     PERCENT     0.4154    0.9310    0.5745        87\n        TYPE     0.9008    0.9435    0.9216     11501\n      VOLUME     0.1429    0.0169    0.0303        59\n\n   micro avg     0.8788    0.8906    0.8847     14868\n   macro avg     0.5694    0.6521    0.5727     14868\nweighted avg     0.8771    0.8906    0.8820     14868\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.38941603899002075, 'eval_f1_macro': 0.5726834091196347, 'eval_precision': 0.8787577145132391, 'eval_recall': 0.890637610976594, 'eval_f1': 0.8846577813408157, 'eval_accuracy': 0.8809007053716766, 'eval_runtime': 1.4692, 'eval_samples_per_second': 3750.229, 'eval_steps_per_second': 7.487, 'epoch': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.4117, 'grad_norm': 1.0162553787231445, 'learning_rate': 0.00037710330953711225, 'epoch': 1.0227272727272727}\n{'loss': 0.4455, 'grad_norm': 1.6373751163482666, 'learning_rate': 0.00037615102845242257, 'epoch': 1.0454545454545454}\n{'loss': 0.3591, 'grad_norm': 0.7885792255401611, 'learning_rate': 0.0003751987473677329, 'epoch': 1.0681818181818181}\n{'loss': 0.424, 'grad_norm': 1.8679605722427368, 'learning_rate': 0.0003742464662830432, 'epoch': 1.0909090909090908}\n{'loss': 0.4244, 'grad_norm': 1.3862230777740479, 'learning_rate': 0.00037329418519835354, 'epoch': 1.1136363636363635}\n{'loss': 0.3745, 'grad_norm': 0.9536611437797546, 'learning_rate': 0.00037234190411366386, 'epoch': 1.1363636363636362}\n{'loss': 0.3717, 'grad_norm': 1.3922677040100098, 'learning_rate': 0.0003713896230289742, 'epoch': 1.1590909090909092}\n{'loss': 0.3895, 'grad_norm': 2.070923328399658, 'learning_rate': 0.0003704373419442845, 'epoch': 1.1818181818181819}\n{'loss': 0.3559, 'grad_norm': 0.9905338287353516, 'learning_rate': 0.00036948506085959483, 'epoch': 1.2045454545454546}\n{'loss': 0.345, 'grad_norm': 1.566542625427246, 'learning_rate': 0.00036853277977490516, 'epoch': 1.2272727272727273}\n{'loss': 0.3832, 'grad_norm': 2.0663540363311768, 'learning_rate': 0.0003675804986902155, 'epoch': 1.25}\n{'loss': 0.3756, 'grad_norm': 1.009778380393982, 'learning_rate': 0.0003666282176055258, 'epoch': 1.2727272727272727}\n{'loss': 0.3202, 'grad_norm': 0.9799037575721741, 'learning_rate': 0.0003656759365208361, 'epoch': 1.2954545454545454}\n{'loss': 0.384, 'grad_norm': 1.525354266166687, 'learning_rate': 0.00036472365543614645, 'epoch': 1.3181818181818181}\n{'loss': 0.3094, 'grad_norm': 1.5866258144378662, 'learning_rate': 0.00036377137435145677, 'epoch': 1.3409090909090908}\n{'loss': 0.3533, 'grad_norm': 0.890016496181488, 'learning_rate': 0.0003628190932667671, 'epoch': 1.3636363636363638}\n{'loss': 0.2877, 'grad_norm': 1.3546432256698608, 'learning_rate': 0.0003618668121820774, 'epoch': 1.3863636363636362}\n{'loss': 0.304, 'grad_norm': 0.9580742716789246, 'learning_rate': 0.00036091453109738774, 'epoch': 1.4090909090909092}\n{'loss': 0.2962, 'grad_norm': 1.1648683547973633, 'learning_rate': 0.00035996225001269806, 'epoch': 1.4318181818181819}\n{'loss': 0.3041, 'grad_norm': 1.3517301082611084, 'learning_rate': 0.0003590099689280084, 'epoch': 1.4545454545454546}\n{'loss': 0.2813, 'grad_norm': 1.083979606628418, 'learning_rate': 0.0003580576878433187, 'epoch': 1.4772727272727273}\n{'loss': 0.287, 'grad_norm': 1.1566855907440186, 'learning_rate': 0.00035710540675862903, 'epoch': 1.5}\n{'loss': 0.3142, 'grad_norm': 1.6444880962371826, 'learning_rate': 0.00035615312567393936, 'epoch': 1.5227272727272727}\n{'loss': 0.343, 'grad_norm': 1.5935018062591553, 'learning_rate': 0.0003552008445892497, 'epoch': 1.5454545454545454}\n{'loss': 0.3232, 'grad_norm': 2.657285213470459, 'learning_rate': 0.00035424856350456, 'epoch': 1.5681818181818183}\n{'loss': 0.334, 'grad_norm': 1.0248689651489258, 'learning_rate': 0.0003532962824198703, 'epoch': 1.5909090909090908}\n{'loss': 0.3257, 'grad_norm': 1.5026696920394897, 'learning_rate': 0.00035234400133518065, 'epoch': 1.6136363636363638}\n{'loss': 0.3173, 'grad_norm': 2.011260747909546, 'learning_rate': 0.0003513917202504909, 'epoch': 1.6363636363636362}\n{'loss': 0.3466, 'grad_norm': 3.0070512294769287, 'learning_rate': 0.0003504394391658013, 'epoch': 1.6590909090909092}\n{'loss': 0.3196, 'grad_norm': 1.9003674983978271, 'learning_rate': 0.0003494871580811116, 'epoch': 1.6818181818181817}\n{'loss': 0.3085, 'grad_norm': 1.2773964405059814, 'learning_rate': 0.0003485348769964219, 'epoch': 1.7045454545454546}\n{'loss': 0.2593, 'grad_norm': 1.111290693283081, 'learning_rate': 0.00034758259591173227, 'epoch': 1.7272727272727273}\n{'loss': 0.3114, 'grad_norm': 1.6195167303085327, 'learning_rate': 0.0003466303148270426, 'epoch': 1.75}\n{'loss': 0.2977, 'grad_norm': 0.835004985332489, 'learning_rate': 0.00034567803374235286, 'epoch': 1.7727272727272727}\n{'loss': 0.2385, 'grad_norm': 1.1833853721618652, 'learning_rate': 0.00034472575265766324, 'epoch': 1.7954545454545454}\n{'loss': 0.2908, 'grad_norm': 0.7823330163955688, 'learning_rate': 0.00034377347157297356, 'epoch': 1.8181818181818183}\n{'loss': 0.2831, 'grad_norm': 0.7870082259178162, 'learning_rate': 0.00034282119048828383, 'epoch': 1.8409090909090908}\n{'loss': 0.2822, 'grad_norm': 0.9697102308273315, 'learning_rate': 0.0003418689094035942, 'epoch': 1.8636363636363638}\n{'loss': 0.269, 'grad_norm': 0.8578859567642212, 'learning_rate': 0.00034091662831890453, 'epoch': 1.8863636363636362}\n{'loss': 0.2802, 'grad_norm': 0.6300197839736938, 'learning_rate': 0.0003399643472342148, 'epoch': 1.9090909090909092}\n{'loss': 0.2505, 'grad_norm': 0.7421262264251709, 'learning_rate': 0.0003390120661495252, 'epoch': 1.9318181818181817}\n{'loss': 0.3153, 'grad_norm': 0.9843944907188416, 'learning_rate': 0.0003380597850648355, 'epoch': 1.9545454545454546}\n{'loss': 0.2922, 'grad_norm': 0.7733315825462341, 'learning_rate': 0.00033710750398014577, 'epoch': 1.9772727272727273}\n{'loss': 0.1717, 'grad_norm': 2.3112380504608154, 'learning_rate': 0.00033615522289545614, 'epoch': 2.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8269    0.8926    0.8585      3221\n     PERCENT     0.8511    0.9195    0.8840        87\n        TYPE     0.9471    0.9565    0.9518     11501\n      VOLUME     0.4605    0.5932    0.5185        59\n\n   micro avg     0.9167    0.9410    0.9287     14868\n   macro avg     0.7714    0.8405    0.8032     14868\nweighted avg     0.9185    0.9410    0.9294     14868\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.24820885062217712, 'eval_f1_macro': 0.8031821825935949, 'eval_precision': 0.9166612068400708, 'eval_recall': 0.941014258810869, 'eval_f1': 0.9286781056055227, 'eval_accuracy': 0.9228974498100923, 'eval_runtime': 1.5252, 'eval_samples_per_second': 3612.578, 'eval_steps_per_second': 7.212, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.263, 'grad_norm': 1.3533657789230347, 'learning_rate': 0.0003352029418107664, 'epoch': 2.022727272727273}\n{'loss': 0.223, 'grad_norm': 1.9327350854873657, 'learning_rate': 0.00033425066072607674, 'epoch': 2.0454545454545454}\n{'loss': 0.2166, 'grad_norm': 0.7533036470413208, 'learning_rate': 0.0003332983796413871, 'epoch': 2.0681818181818183}\n{'loss': 0.2149, 'grad_norm': 1.0384577512741089, 'learning_rate': 0.0003323460985566974, 'epoch': 2.090909090909091}\n{'loss': 0.2171, 'grad_norm': 1.2488607168197632, 'learning_rate': 0.0003313938174720077, 'epoch': 2.1136363636363638}\n{'loss': 0.1929, 'grad_norm': 1.1855837106704712, 'learning_rate': 0.0003304415363873181, 'epoch': 2.1363636363636362}\n{'loss': 0.1787, 'grad_norm': 0.8306446671485901, 'learning_rate': 0.00032948925530262835, 'epoch': 2.159090909090909}\n{'loss': 0.1885, 'grad_norm': 1.594208002090454, 'learning_rate': 0.00032853697421793873, 'epoch': 2.1818181818181817}\n{'loss': 0.2648, 'grad_norm': 2.3627212047576904, 'learning_rate': 0.00032758469313324905, 'epoch': 2.2045454545454546}\n{'loss': 0.1479, 'grad_norm': 2.7903966903686523, 'learning_rate': 0.0003266324120485593, 'epoch': 2.227272727272727}\n{'loss': 0.1982, 'grad_norm': 1.2945408821105957, 'learning_rate': 0.0003256801309638697, 'epoch': 2.25}\n{'loss': 0.1761, 'grad_norm': 0.8260450959205627, 'learning_rate': 0.00032472784987918, 'epoch': 2.2727272727272725}\n{'loss': 0.1522, 'grad_norm': 0.9257491827011108, 'learning_rate': 0.0003237755687944903, 'epoch': 2.2954545454545454}\n{'loss': 0.1925, 'grad_norm': 2.4247424602508545, 'learning_rate': 0.00032282328770980067, 'epoch': 2.3181818181818183}\n{'loss': 0.199, 'grad_norm': 2.187717914581299, 'learning_rate': 0.00032187100662511094, 'epoch': 2.340909090909091}\n{'loss': 0.2217, 'grad_norm': 2.2993521690368652, 'learning_rate': 0.00032091872554042126, 'epoch': 2.3636363636363638}\n{'loss': 0.2277, 'grad_norm': 2.2981951236724854, 'learning_rate': 0.00031996644445573164, 'epoch': 2.3863636363636362}\n{'loss': 0.1984, 'grad_norm': 1.3188568353652954, 'learning_rate': 0.0003190141633710419, 'epoch': 2.409090909090909}\n{'loss': 0.1798, 'grad_norm': 2.1230804920196533, 'learning_rate': 0.00031806188228635223, 'epoch': 2.4318181818181817}\n{'loss': 0.2082, 'grad_norm': 2.26088809967041, 'learning_rate': 0.0003171096012016626, 'epoch': 2.4545454545454546}\n{'loss': 0.1803, 'grad_norm': 2.4065611362457275, 'learning_rate': 0.0003161573201169729, 'epoch': 2.4772727272727275}\n{'loss': 0.1955, 'grad_norm': 1.337710976600647, 'learning_rate': 0.0003152050390322832, 'epoch': 2.5}\n{'loss': 0.2264, 'grad_norm': 1.322386622428894, 'learning_rate': 0.0003142527579475936, 'epoch': 2.5227272727272725}\n{'loss': 0.1686, 'grad_norm': 1.2466328144073486, 'learning_rate': 0.00031330047686290385, 'epoch': 2.5454545454545454}\n{'loss': 0.1637, 'grad_norm': 1.1581919193267822, 'learning_rate': 0.00031234819577821417, 'epoch': 2.5681818181818183}\n{'loss': 0.1797, 'grad_norm': 0.8025692701339722, 'learning_rate': 0.00031139591469352455, 'epoch': 2.590909090909091}\n{'loss': 0.1841, 'grad_norm': 0.6349999308586121, 'learning_rate': 0.0003104436336088348, 'epoch': 2.6136363636363638}\n{'loss': 0.2122, 'grad_norm': 1.4152414798736572, 'learning_rate': 0.00030949135252414514, 'epoch': 2.6363636363636362}\n{'loss': 0.1886, 'grad_norm': 1.3489688634872437, 'learning_rate': 0.0003085390714394555, 'epoch': 2.659090909090909}\n{'loss': 0.2554, 'grad_norm': 1.0361367464065552, 'learning_rate': 0.0003075867903547658, 'epoch': 2.6818181818181817}\n{'loss': 0.1677, 'grad_norm': 1.0724724531173706, 'learning_rate': 0.0003066345092700761, 'epoch': 2.7045454545454546}\n{'loss': 0.193, 'grad_norm': 1.1900955438613892, 'learning_rate': 0.00030568222818538643, 'epoch': 2.7272727272727275}\n{'loss': 0.227, 'grad_norm': 1.75005304813385, 'learning_rate': 0.00030472994710069675, 'epoch': 2.75}\n{'loss': 0.1732, 'grad_norm': 1.6952908039093018, 'learning_rate': 0.0003037776660160071, 'epoch': 2.7727272727272725}\n{'loss': 0.2252, 'grad_norm': 1.116762638092041, 'learning_rate': 0.0003028253849313174, 'epoch': 2.7954545454545454}\n{'loss': 0.1901, 'grad_norm': 0.9049407839775085, 'learning_rate': 0.0003018731038466277, 'epoch': 2.8181818181818183}\n{'loss': 0.2092, 'grad_norm': 1.060198426246643, 'learning_rate': 0.00030092082276193805, 'epoch': 2.840909090909091}\n{'loss': 0.1626, 'grad_norm': 0.9237739443778992, 'learning_rate': 0.00029996854167724837, 'epoch': 2.8636363636363638}\n{'loss': 0.1681, 'grad_norm': 1.7038646936416626, 'learning_rate': 0.0002990162605925587, 'epoch': 2.8863636363636362}\n{'loss': 0.1666, 'grad_norm': 1.0741336345672607, 'learning_rate': 0.000298063979507869, 'epoch': 2.909090909090909}\n{'loss': 0.1722, 'grad_norm': 1.3364300727844238, 'learning_rate': 0.00029711169842317934, 'epoch': 2.9318181818181817}\n{'loss': 0.1825, 'grad_norm': 0.8891886472702026, 'learning_rate': 0.00029615941733848966, 'epoch': 2.9545454545454546}\n{'loss': 0.1643, 'grad_norm': 0.8900250792503357, 'learning_rate': 0.0002952071362538, 'epoch': 2.9772727272727275}\n{'loss': 0.0704, 'grad_norm': 1.9867662191390991, 'learning_rate': 0.0002942548551691103, 'epoch': 3.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8943    0.8876    0.8909      3221\n     PERCENT     0.8247    0.9195    0.8696        87\n        TYPE     0.9489    0.9684    0.9586     11501\n      VOLUME     0.6719    0.7288    0.6992        59\n\n   micro avg     0.9353    0.9497    0.9425     14868\n   macro avg     0.8349    0.8761    0.8546     14868\nweighted avg     0.9352    0.9497    0.9424     14868\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.21559292078018188, 'eval_f1_macro': 0.8545612508333781, 'eval_precision': 0.9353471118177, 'eval_recall': 0.9496906107075599, 'eval_f1': 0.9424642904819116, 'eval_accuracy': 0.9371676614215952, 'eval_runtime': 1.527, 'eval_samples_per_second': 3608.421, 'eval_steps_per_second': 7.204, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.1576, 'grad_norm': 0.8634305596351624, 'learning_rate': 0.00029330257408442063, 'epoch': 3.022727272727273}\n{'loss': 0.1328, 'grad_norm': 0.5213193893432617, 'learning_rate': 0.00029235029299973095, 'epoch': 3.0454545454545454}\n{'loss': 0.1546, 'grad_norm': 1.1564223766326904, 'learning_rate': 0.0002913980119150413, 'epoch': 3.0681818181818183}\n{'loss': 0.1338, 'grad_norm': 0.9506633877754211, 'learning_rate': 0.0002904457308303516, 'epoch': 3.090909090909091}\n{'loss': 0.1293, 'grad_norm': 1.0308866500854492, 'learning_rate': 0.0002894934497456619, 'epoch': 3.1136363636363638}\n{'loss': 0.1052, 'grad_norm': 0.9335260391235352, 'learning_rate': 0.00028854116866097225, 'epoch': 3.1363636363636362}\n{'loss': 0.1442, 'grad_norm': 1.2986973524093628, 'learning_rate': 0.00028758888757628257, 'epoch': 3.159090909090909}\n{'loss': 0.1541, 'grad_norm': 1.2010196447372437, 'learning_rate': 0.0002866366064915929, 'epoch': 3.1818181818181817}\n{'loss': 0.1282, 'grad_norm': 1.1360853910446167, 'learning_rate': 0.0002856843254069032, 'epoch': 3.2045454545454546}\n{'loss': 0.1371, 'grad_norm': 1.2519302368164062, 'learning_rate': 0.00028473204432221354, 'epoch': 3.227272727272727}\n{'loss': 0.0995, 'grad_norm': 0.7587196826934814, 'learning_rate': 0.00028377976323752386, 'epoch': 3.25}\n{'loss': 0.115, 'grad_norm': 1.1803525686264038, 'learning_rate': 0.0002828274821528342, 'epoch': 3.2727272727272725}\n{'loss': 0.1346, 'grad_norm': 1.778655767440796, 'learning_rate': 0.0002818752010681445, 'epoch': 3.2954545454545454}\n{'loss': 0.1254, 'grad_norm': 1.4657930135726929, 'learning_rate': 0.00028092291998345483, 'epoch': 3.3181818181818183}\n{'loss': 0.1246, 'grad_norm': 0.9856293201446533, 'learning_rate': 0.00027997063889876516, 'epoch': 3.340909090909091}\n{'loss': 0.1254, 'grad_norm': 1.293399691581726, 'learning_rate': 0.0002790183578140755, 'epoch': 3.3636363636363638}\n{'loss': 0.1256, 'grad_norm': 1.2125717401504517, 'learning_rate': 0.0002780660767293858, 'epoch': 3.3863636363636362}\n{'loss': 0.0949, 'grad_norm': 0.7932489514350891, 'learning_rate': 0.0002771137956446961, 'epoch': 3.409090909090909}\n{'loss': 0.122, 'grad_norm': 1.6481120586395264, 'learning_rate': 0.00027616151456000645, 'epoch': 3.4318181818181817}\n{'loss': 0.1387, 'grad_norm': 1.2849152088165283, 'learning_rate': 0.00027520923347531677, 'epoch': 3.4545454545454546}\n{'loss': 0.1196, 'grad_norm': 0.7737487554550171, 'learning_rate': 0.0002742569523906271, 'epoch': 3.4772727272727275}\n{'loss': 0.1205, 'grad_norm': 1.420262098312378, 'learning_rate': 0.0002733046713059374, 'epoch': 3.5}\n{'loss': 0.1298, 'grad_norm': 0.880341112613678, 'learning_rate': 0.00027235239022124774, 'epoch': 3.5227272727272725}\n{'loss': 0.1665, 'grad_norm': 1.3059759140014648, 'learning_rate': 0.00027140010913655806, 'epoch': 3.5454545454545454}\n{'loss': 0.1154, 'grad_norm': 1.5864746570587158, 'learning_rate': 0.0002704478280518684, 'epoch': 3.5681818181818183}\n{'loss': 0.1298, 'grad_norm': 1.2866590023040771, 'learning_rate': 0.0002694955469671787, 'epoch': 3.590909090909091}\n{'loss': 0.1701, 'grad_norm': 1.4653849601745605, 'learning_rate': 0.00026854326588248903, 'epoch': 3.6136363636363638}\n{'loss': 0.1162, 'grad_norm': 1.19914710521698, 'learning_rate': 0.00026759098479779936, 'epoch': 3.6363636363636362}\n{'loss': 0.1402, 'grad_norm': 1.6725691556930542, 'learning_rate': 0.0002666387037131097, 'epoch': 3.659090909090909}\n{'loss': 0.1188, 'grad_norm': 1.258190631866455, 'learning_rate': 0.00026568642262842, 'epoch': 3.6818181818181817}\n{'loss': 0.1109, 'grad_norm': 1.1619935035705566, 'learning_rate': 0.0002647341415437303, 'epoch': 3.7045454545454546}\n{'loss': 0.1344, 'grad_norm': 0.7727434039115906, 'learning_rate': 0.00026378186045904065, 'epoch': 3.7272727272727275}\n{'loss': 0.1184, 'grad_norm': 0.8103422522544861, 'learning_rate': 0.00026282957937435097, 'epoch': 3.75}\n{'loss': 0.0903, 'grad_norm': 0.5765495896339417, 'learning_rate': 0.0002618772982896613, 'epoch': 3.7727272727272725}\n{'loss': 0.1129, 'grad_norm': 0.8062249422073364, 'learning_rate': 0.0002609250172049716, 'epoch': 3.7954545454545454}\n{'loss': 0.1547, 'grad_norm': 0.9804006814956665, 'learning_rate': 0.00025997273612028194, 'epoch': 3.8181818181818183}\n{'loss': 0.1289, 'grad_norm': 1.689968228340149, 'learning_rate': 0.00025902045503559226, 'epoch': 3.840909090909091}\n{'loss': 0.1227, 'grad_norm': 1.088544487953186, 'learning_rate': 0.0002580681739509026, 'epoch': 3.8636363636363638}\n{'loss': 0.1338, 'grad_norm': 1.0518450736999512, 'learning_rate': 0.00025711589286621286, 'epoch': 3.8863636363636362}\n{'loss': 0.1135, 'grad_norm': 0.9703267216682434, 'learning_rate': 0.00025616361178152323, 'epoch': 3.909090909090909}\n{'loss': 0.1587, 'grad_norm': 0.9062376618385315, 'learning_rate': 0.00025521133069683356, 'epoch': 3.9318181818181817}\n{'loss': 0.1375, 'grad_norm': 1.4309635162353516, 'learning_rate': 0.0002542590496121438, 'epoch': 3.9545454545454546}\n{'loss': 0.1298, 'grad_norm': 0.9570082426071167, 'learning_rate': 0.0002533067685274542, 'epoch': 3.9772727272727275}\n{'loss': 0.1112, 'grad_norm': 2.3031198978424072, 'learning_rate': 0.0002523544874427645, 'epoch': 4.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8884    0.8969    0.8926      3221\n     PERCENT     0.8421    0.9195    0.8791        87\n        TYPE     0.9484    0.9676    0.9579     11501\n      VOLUME     0.8421    0.8136    0.8276        59\n\n   micro avg     0.9345    0.9514    0.9428     14868\n   macro avg     0.8803    0.8994    0.8893     14868\nweighted avg     0.9344    0.9514    0.9428     14868\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.22249047458171844, 'eval_f1_macro': 0.8893111327004617, 'eval_precision': 0.9344652176785361, 'eval_recall': 0.9513720742534302, 'eval_f1': 0.9428428595234128, 'eval_accuracy': 0.9372219207813348, 'eval_runtime': 1.5394, 'eval_samples_per_second': 3579.237, 'eval_steps_per_second': 7.145, 'epoch': 4.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.0861, 'grad_norm': 1.1884981393814087, 'learning_rate': 0.0002514022063580748, 'epoch': 4.0227272727272725}\n{'loss': 0.1013, 'grad_norm': 0.813225269317627, 'learning_rate': 0.0002504499252733852, 'epoch': 4.045454545454546}\n{'loss': 0.07, 'grad_norm': 0.9191758036613464, 'learning_rate': 0.0002494976441886955, 'epoch': 4.068181818181818}\n{'loss': 0.1085, 'grad_norm': 1.178754448890686, 'learning_rate': 0.00024854536310400577, 'epoch': 4.090909090909091}\n{'loss': 0.0886, 'grad_norm': 0.680018424987793, 'learning_rate': 0.00024759308201931614, 'epoch': 4.113636363636363}\n{'loss': 0.0886, 'grad_norm': 0.9008263349533081, 'learning_rate': 0.00024664080093462647, 'epoch': 4.136363636363637}\n{'loss': 0.0699, 'grad_norm': 0.7326313853263855, 'learning_rate': 0.00024568851984993673, 'epoch': 4.159090909090909}\n{'loss': 0.0762, 'grad_norm': 0.9365483522415161, 'learning_rate': 0.0002447362387652471, 'epoch': 4.181818181818182}\n{'loss': 0.1071, 'grad_norm': 0.8060885071754456, 'learning_rate': 0.00024378395768055744, 'epoch': 4.204545454545454}\n{'loss': 0.0878, 'grad_norm': 1.0431122779846191, 'learning_rate': 0.00024283167659586773, 'epoch': 4.2272727272727275}\n{'loss': 0.0736, 'grad_norm': 1.0724704265594482, 'learning_rate': 0.00024187939551117805, 'epoch': 4.25}\n{'loss': 0.0845, 'grad_norm': 0.8405771851539612, 'learning_rate': 0.00024092711442648835, 'epoch': 4.2727272727272725}\n{'loss': 0.0929, 'grad_norm': 1.105589509010315, 'learning_rate': 0.0002399748333417987, 'epoch': 4.295454545454546}\n{'loss': 0.0767, 'grad_norm': 0.870037317276001, 'learning_rate': 0.00023902255225710902, 'epoch': 4.318181818181818}\n{'loss': 0.0825, 'grad_norm': 0.7760505080223083, 'learning_rate': 0.00023807027117241932, 'epoch': 4.340909090909091}\n{'loss': 0.0768, 'grad_norm': 1.2776933908462524, 'learning_rate': 0.00023711799008772967, 'epoch': 4.363636363636363}\n{'loss': 0.0985, 'grad_norm': 0.9733566641807556, 'learning_rate': 0.00023616570900304002, 'epoch': 4.386363636363637}\n{'loss': 0.0772, 'grad_norm': 0.7252458333969116, 'learning_rate': 0.0002352134279183503, 'epoch': 4.409090909090909}\n{'loss': 0.0992, 'grad_norm': 0.7854671478271484, 'learning_rate': 0.00023426114683366064, 'epoch': 4.431818181818182}\n{'loss': 0.1027, 'grad_norm': 0.9595621824264526, 'learning_rate': 0.000233308865748971, 'epoch': 4.454545454545454}\n{'loss': 0.094, 'grad_norm': 1.1268540620803833, 'learning_rate': 0.00023235658466428129, 'epoch': 4.4772727272727275}\n{'loss': 0.0625, 'grad_norm': 0.8629418611526489, 'learning_rate': 0.0002314043035795916, 'epoch': 4.5}\n{'loss': 0.1026, 'grad_norm': 0.9741349816322327, 'learning_rate': 0.00023045202249490196, 'epoch': 4.5227272727272725}\n{'loss': 0.1216, 'grad_norm': 1.0245461463928223, 'learning_rate': 0.00022949974141021226, 'epoch': 4.545454545454545}\n{'loss': 0.0885, 'grad_norm': 0.616731584072113, 'learning_rate': 0.00022854746032552258, 'epoch': 4.568181818181818}\n{'loss': 0.1003, 'grad_norm': 1.3618792295455933, 'learning_rate': 0.00022759517924083287, 'epoch': 4.590909090909091}\n{'loss': 0.0714, 'grad_norm': 0.8095633387565613, 'learning_rate': 0.00022664289815614323, 'epoch': 4.613636363636363}\n{'loss': 0.0807, 'grad_norm': 0.6809694766998291, 'learning_rate': 0.00022569061707145355, 'epoch': 4.636363636363637}\n{'loss': 0.0769, 'grad_norm': 1.0333342552185059, 'learning_rate': 0.00022473833598676384, 'epoch': 4.659090909090909}\n{'loss': 0.1006, 'grad_norm': 1.1127877235412598, 'learning_rate': 0.0002237860549020742, 'epoch': 4.681818181818182}\n{'loss': 0.1064, 'grad_norm': 0.7335881590843201, 'learning_rate': 0.00022283377381738452, 'epoch': 4.704545454545455}\n{'loss': 0.0833, 'grad_norm': 0.9946065545082092, 'learning_rate': 0.00022188149273269481, 'epoch': 4.7272727272727275}\n{'loss': 0.0874, 'grad_norm': 0.5614136457443237, 'learning_rate': 0.00022092921164800516, 'epoch': 4.75}\n{'loss': 0.0908, 'grad_norm': 1.124846339225769, 'learning_rate': 0.0002199769305633155, 'epoch': 4.7727272727272725}\n{'loss': 0.0963, 'grad_norm': 1.252595067024231, 'learning_rate': 0.00021902464947862578, 'epoch': 4.795454545454545}\n{'loss': 0.0868, 'grad_norm': 1.11319899559021, 'learning_rate': 0.00021807236839393613, 'epoch': 4.818181818181818}\n{'loss': 0.084, 'grad_norm': 1.0576077699661255, 'learning_rate': 0.00021712008730924646, 'epoch': 4.840909090909091}\n{'loss': 0.0761, 'grad_norm': 1.098176121711731, 'learning_rate': 0.00021616780622455675, 'epoch': 4.863636363636363}\n{'loss': 0.0954, 'grad_norm': 0.8644190430641174, 'learning_rate': 0.0002152155251398671, 'epoch': 4.886363636363637}\n{'loss': 0.0849, 'grad_norm': 0.9205381274223328, 'learning_rate': 0.00021426324405517743, 'epoch': 4.909090909090909}\n{'loss': 0.1152, 'grad_norm': 1.4296009540557861, 'learning_rate': 0.00021331096297048772, 'epoch': 4.931818181818182}\n{'loss': 0.0922, 'grad_norm': 0.7328872680664062, 'learning_rate': 0.00021235868188579807, 'epoch': 4.954545454545455}\n{'loss': 0.1227, 'grad_norm': 1.527274250984192, 'learning_rate': 0.00021140640080110837, 'epoch': 4.9772727272727275}\n{'loss': 0.0689, 'grad_norm': 2.145164728164673, 'learning_rate': 0.0002104541197164187, 'epoch': 5.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8981    0.8944    0.8963      3221\n     PERCENT     0.8791    0.9195    0.8989        87\n        TYPE     0.9513    0.9649    0.9580     11501\n      VOLUME     0.8730    0.9322    0.9016        59\n\n   micro avg     0.9392    0.9492    0.9442     14868\n   macro avg     0.9004    0.9278    0.9137     14868\nweighted avg     0.9390    0.9492    0.9441     14868\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.24066056311130524, 'eval_f1_macro': 0.9137022669547529, 'eval_precision': 0.9391761495973914, 'eval_recall': 0.9492198009147161, 'eval_f1': 0.9441712660980096, 'eval_accuracy': 0.937438958220293, 'eval_runtime': 1.5306, 'eval_samples_per_second': 3599.852, 'eval_steps_per_second': 7.187, 'epoch': 5.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.0459, 'grad_norm': 0.5650039911270142, 'learning_rate': 0.00020950183863172904, 'epoch': 5.0227272727272725}\n{'loss': 0.0709, 'grad_norm': 0.807579517364502, 'learning_rate': 0.00020854955754703934, 'epoch': 5.045454545454546}\n{'loss': 0.0731, 'grad_norm': 1.142306923866272, 'learning_rate': 0.00020759727646234966, 'epoch': 5.068181818181818}\n{'loss': 0.0941, 'grad_norm': 0.7585324645042419, 'learning_rate': 0.00020664499537766, 'epoch': 5.090909090909091}\n{'loss': 0.0665, 'grad_norm': 0.9361142516136169, 'learning_rate': 0.0002056927142929703, 'epoch': 5.113636363636363}\n{'loss': 0.0408, 'grad_norm': 0.40550222992897034, 'learning_rate': 0.00020474043320828063, 'epoch': 5.136363636363637}\n{'loss': 0.0444, 'grad_norm': 0.7858253121376038, 'learning_rate': 0.00020378815212359098, 'epoch': 5.159090909090909}\n{'loss': 0.0464, 'grad_norm': 0.8468865156173706, 'learning_rate': 0.00020283587103890128, 'epoch': 5.181818181818182}\n{'loss': 0.0583, 'grad_norm': 0.5412742495536804, 'learning_rate': 0.0002018835899542116, 'epoch': 5.204545454545454}\n{'loss': 0.0701, 'grad_norm': 1.9151238203048706, 'learning_rate': 0.00020093130886952195, 'epoch': 5.2272727272727275}\n{'loss': 0.073, 'grad_norm': 0.9133213758468628, 'learning_rate': 0.00019997902778483225, 'epoch': 5.25}\n{'loss': 0.0486, 'grad_norm': 0.5657020807266235, 'learning_rate': 0.00019902674670014257, 'epoch': 5.2727272727272725}\n{'loss': 0.053, 'grad_norm': 1.1813178062438965, 'learning_rate': 0.00019807446561545292, 'epoch': 5.295454545454546}\n{'loss': 0.0435, 'grad_norm': 1.0786906480789185, 'learning_rate': 0.00019712218453076322, 'epoch': 5.318181818181818}\n{'loss': 0.0531, 'grad_norm': 0.7237239480018616, 'learning_rate': 0.00019616990344607354, 'epoch': 5.340909090909091}\n{'loss': 0.0585, 'grad_norm': 0.6143941283226013, 'learning_rate': 0.00019521762236138383, 'epoch': 5.363636363636363}\n{'loss': 0.0701, 'grad_norm': 1.262888789176941, 'learning_rate': 0.00019426534127669419, 'epoch': 5.386363636363637}\n{'loss': 0.0465, 'grad_norm': 0.9386967420578003, 'learning_rate': 0.0001933130601920045, 'epoch': 5.409090909090909}\n{'loss': 0.0672, 'grad_norm': 0.9796546697616577, 'learning_rate': 0.0001923607791073148, 'epoch': 5.431818181818182}\n{'loss': 0.0642, 'grad_norm': 1.033598780632019, 'learning_rate': 0.00019140849802262515, 'epoch': 5.454545454545454}\n{'loss': 0.0704, 'grad_norm': 0.943454921245575, 'learning_rate': 0.00019045621693793548, 'epoch': 5.4772727272727275}\n{'loss': 0.0824, 'grad_norm': 0.6405516862869263, 'learning_rate': 0.00018950393585324577, 'epoch': 5.5}\n{'loss': 0.0552, 'grad_norm': 0.9402536153793335, 'learning_rate': 0.00018855165476855612, 'epoch': 5.5227272727272725}\n{'loss': 0.0442, 'grad_norm': 0.9450170993804932, 'learning_rate': 0.00018759937368386645, 'epoch': 5.545454545454545}\n{'loss': 0.088, 'grad_norm': 1.1123096942901611, 'learning_rate': 0.00018664709259917677, 'epoch': 5.568181818181818}\n{'loss': 0.0719, 'grad_norm': 0.6742140054702759, 'learning_rate': 0.0001856948115144871, 'epoch': 5.590909090909091}\n{'loss': 0.0539, 'grad_norm': 0.7884738445281982, 'learning_rate': 0.00018474253042979742, 'epoch': 5.613636363636363}\n{'loss': 0.0657, 'grad_norm': 0.9990381002426147, 'learning_rate': 0.00018379024934510774, 'epoch': 5.636363636363637}\n{'loss': 0.0618, 'grad_norm': 1.2366899251937866, 'learning_rate': 0.00018283796826041806, 'epoch': 5.659090909090909}\n{'loss': 0.0554, 'grad_norm': 1.5408287048339844, 'learning_rate': 0.00018188568717572839, 'epoch': 5.681818181818182}\n{'loss': 0.0638, 'grad_norm': 0.7159706950187683, 'learning_rate': 0.0001809334060910387, 'epoch': 5.704545454545455}\n{'loss': 0.0904, 'grad_norm': 0.8541784286499023, 'learning_rate': 0.00017998112500634903, 'epoch': 5.7272727272727275}\n{'loss': 0.0698, 'grad_norm': 1.1595351696014404, 'learning_rate': 0.00017902884392165936, 'epoch': 5.75}\n{'loss': 0.0583, 'grad_norm': 1.5847084522247314, 'learning_rate': 0.00017807656283696968, 'epoch': 5.7727272727272725}\n{'loss': 0.0752, 'grad_norm': 1.1047338247299194, 'learning_rate': 0.00017712428175228, 'epoch': 5.795454545454545}\n{'loss': 0.057, 'grad_norm': 1.1675490140914917, 'learning_rate': 0.00017617200066759033, 'epoch': 5.818181818181818}\n{'loss': 0.0731, 'grad_norm': 0.8522348999977112, 'learning_rate': 0.00017521971958290065, 'epoch': 5.840909090909091}\n{'loss': 0.0691, 'grad_norm': 1.1494128704071045, 'learning_rate': 0.00017426743849821094, 'epoch': 5.863636363636363}\n{'loss': 0.0928, 'grad_norm': 1.0856592655181885, 'learning_rate': 0.0001733151574135213, 'epoch': 5.886363636363637}\n{'loss': 0.0541, 'grad_norm': 0.7959376573562622, 'learning_rate': 0.00017236287632883162, 'epoch': 5.909090909090909}\n{'loss': 0.0656, 'grad_norm': 0.7273625135421753, 'learning_rate': 0.00017141059524414191, 'epoch': 5.931818181818182}\n{'loss': 0.0532, 'grad_norm': 0.5730637907981873, 'learning_rate': 0.00017045831415945226, 'epoch': 5.954545454545455}\n{'loss': 0.0655, 'grad_norm': 0.9884257316589355, 'learning_rate': 0.0001695060330747626, 'epoch': 5.9772727272727275}\n{'loss': 0.0981, 'grad_norm': 2.8356997966766357, 'learning_rate': 0.00016855375199007288, 'epoch': 6.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8778    0.9075    0.8924      3221\n     PERCENT     0.8696    0.9195    0.8939        87\n        TYPE     0.9555    0.9557    0.9556     11501\n      VOLUME     0.8571    0.9153    0.8852        59\n\n   micro avg     0.9373    0.9449    0.9411     14868\n   macro avg     0.8900    0.9245    0.9068     14868\nweighted avg     0.9378    0.9449    0.9413     14868\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.2696644365787506, 'eval_f1_macro': 0.9067754590987857, 'eval_precision': 0.9372873440523051, 'eval_recall': 0.9449152542372882, 'eval_f1': 0.9410858425159928, 'eval_accuracy': 0.933532284319045, 'eval_runtime': 1.5772, 'eval_samples_per_second': 3493.455, 'eval_steps_per_second': 6.974, 'epoch': 6.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.046, 'grad_norm': 0.5773150324821472, 'learning_rate': 0.0001676014709053832, 'epoch': 6.0227272727272725}\n{'loss': 0.0424, 'grad_norm': 0.8105073571205139, 'learning_rate': 0.00016664918982069356, 'epoch': 6.045454545454546}\n{'loss': 0.0633, 'grad_norm': 1.1521340608596802, 'learning_rate': 0.00016569690873600385, 'epoch': 6.068181818181818}\n{'loss': 0.0473, 'grad_norm': 1.1981953382492065, 'learning_rate': 0.00016474462765131418, 'epoch': 6.090909090909091}\n{'loss': 0.0594, 'grad_norm': 1.5368362665176392, 'learning_rate': 0.00016379234656662453, 'epoch': 6.113636363636363}\n{'loss': 0.034, 'grad_norm': 0.6235653162002563, 'learning_rate': 0.00016284006548193485, 'epoch': 6.136363636363637}\n{'loss': 0.0507, 'grad_norm': 0.6927785277366638, 'learning_rate': 0.00016188778439724515, 'epoch': 6.159090909090909}\n{'loss': 0.0471, 'grad_norm': 0.6633279919624329, 'learning_rate': 0.00016093550331255547, 'epoch': 6.181818181818182}\n{'loss': 0.0444, 'grad_norm': 0.4769558012485504, 'learning_rate': 0.00015998322222786582, 'epoch': 6.204545454545454}\n{'loss': 0.0529, 'grad_norm': 0.9012635946273804, 'learning_rate': 0.00015903094114317611, 'epoch': 6.2272727272727275}\n{'loss': 0.0736, 'grad_norm': 1.7485623359680176, 'learning_rate': 0.00015807866005848644, 'epoch': 6.25}\n{'loss': 0.0484, 'grad_norm': 0.9686011672019958, 'learning_rate': 0.0001571263789737968, 'epoch': 6.2727272727272725}\n{'loss': 0.0457, 'grad_norm': 0.9749917387962341, 'learning_rate': 0.00015617409788910708, 'epoch': 6.295454545454546}\n{'loss': 0.0414, 'grad_norm': 0.8787487745285034, 'learning_rate': 0.0001552218168044174, 'epoch': 6.318181818181818}\n{'loss': 0.0563, 'grad_norm': 0.6804289221763611, 'learning_rate': 0.00015426953571972776, 'epoch': 6.340909090909091}\n{'loss': 0.057, 'grad_norm': 0.9215162396430969, 'learning_rate': 0.00015331725463503805, 'epoch': 6.363636363636363}\n{'loss': 0.0616, 'grad_norm': 1.3905867338180542, 'learning_rate': 0.00015236497355034838, 'epoch': 6.386363636363637}\n{'loss': 0.0567, 'grad_norm': 1.403455376625061, 'learning_rate': 0.0001514126924656587, 'epoch': 6.409090909090909}\n{'loss': 0.0514, 'grad_norm': 0.8142610192298889, 'learning_rate': 0.00015046041138096902, 'epoch': 6.431818181818182}\n{'loss': 0.0461, 'grad_norm': 0.6583994626998901, 'learning_rate': 0.00014950813029627935, 'epoch': 6.454545454545454}\n{'loss': 0.0309, 'grad_norm': 0.4831893742084503, 'learning_rate': 0.00014855584921158967, 'epoch': 6.4772727272727275}\n{'loss': 0.0548, 'grad_norm': 0.5145581364631653, 'learning_rate': 0.0001476035681269, 'epoch': 6.5}\n{'loss': 0.0591, 'grad_norm': 0.9786102175712585, 'learning_rate': 0.00014665128704221032, 'epoch': 6.5227272727272725}\n{'loss': 0.0569, 'grad_norm': 0.8149836659431458, 'learning_rate': 0.00014569900595752064, 'epoch': 6.545454545454545}\n{'loss': 0.0364, 'grad_norm': 0.5686028003692627, 'learning_rate': 0.00014474672487283096, 'epoch': 6.568181818181818}\n{'loss': 0.0413, 'grad_norm': 0.8541408777236938, 'learning_rate': 0.00014379444378814129, 'epoch': 6.590909090909091}\n{'loss': 0.081, 'grad_norm': 1.6429357528686523, 'learning_rate': 0.0001428421627034516, 'epoch': 6.613636363636363}\n{'loss': 0.0397, 'grad_norm': 0.6738986372947693, 'learning_rate': 0.00014188988161876193, 'epoch': 6.636363636363637}\n{'loss': 0.0273, 'grad_norm': 0.5105409622192383, 'learning_rate': 0.00014093760053407225, 'epoch': 6.659090909090909}\n{'loss': 0.0404, 'grad_norm': 0.6045301556587219, 'learning_rate': 0.00013998531944938258, 'epoch': 6.681818181818182}\n{'loss': 0.0445, 'grad_norm': 0.5496943593025208, 'learning_rate': 0.0001390330383646929, 'epoch': 6.704545454545455}\n{'loss': 0.0552, 'grad_norm': 2.043724536895752, 'learning_rate': 0.00013808075728000322, 'epoch': 6.7272727272727275}\n{'loss': 0.0415, 'grad_norm': 0.6570039391517639, 'learning_rate': 0.00013712847619531355, 'epoch': 6.75}\n{'loss': 0.056, 'grad_norm': 0.8125004172325134, 'learning_rate': 0.00013617619511062387, 'epoch': 6.7727272727272725}\n{'loss': 0.047, 'grad_norm': 0.7201725244522095, 'learning_rate': 0.0001352239140259342, 'epoch': 6.795454545454545}\n{'loss': 0.0489, 'grad_norm': 0.9723631739616394, 'learning_rate': 0.00013427163294124452, 'epoch': 6.818181818181818}\n{'loss': 0.0283, 'grad_norm': 0.6193912625312805, 'learning_rate': 0.00013331935185655484, 'epoch': 6.840909090909091}\n{'loss': 0.0499, 'grad_norm': 0.7333372235298157, 'learning_rate': 0.00013236707077186516, 'epoch': 6.863636363636363}\n{'loss': 0.0703, 'grad_norm': 1.085921287536621, 'learning_rate': 0.00013141478968717549, 'epoch': 6.886363636363637}\n{'loss': 0.0535, 'grad_norm': 0.9199158549308777, 'learning_rate': 0.0001304625086024858, 'epoch': 6.909090909090909}\n{'loss': 0.0761, 'grad_norm': 1.8572019338607788, 'learning_rate': 0.00012951022751779613, 'epoch': 6.931818181818182}\n{'loss': 0.0435, 'grad_norm': 0.6983147859573364, 'learning_rate': 0.00012855794643310643, 'epoch': 6.954545454545455}\n{'loss': 0.0625, 'grad_norm': 0.5442447662353516, 'learning_rate': 0.00012760566534841678, 'epoch': 6.9772727272727275}\n{'loss': 0.0152, 'grad_norm': 1.3065840005874634, 'learning_rate': 0.0001266533842637271, 'epoch': 7.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8772    0.9028    0.8898      3221\n     PERCENT     0.8791    0.9195    0.8989        87\n        TYPE     0.9554    0.9576    0.9565     11501\n      VOLUME     0.8594    0.9322    0.8943        59\n\n   micro avg     0.9373    0.9454    0.9413     14868\n   macro avg     0.8928    0.9280    0.9099     14868\nweighted avg     0.9376    0.9454    0.9415     14868\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.27617332339286804, 'eval_f1_macro': 0.9098784957238311, 'eval_precision': 0.9372541174901647, 'eval_recall': 0.9453860640301318, 'eval_f1': 0.9413025280428596, 'eval_accuracy': 0.9344546934346175, 'eval_runtime': 1.5923, 'eval_samples_per_second': 3460.384, 'eval_steps_per_second': 6.908, 'epoch': 7.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.0364, 'grad_norm': 0.9200683236122131, 'learning_rate': 0.0001257011031790374, 'epoch': 7.0227272727272725}\n{'loss': 0.0454, 'grad_norm': 0.6892034411430359, 'learning_rate': 0.00012474882209434775, 'epoch': 7.045454545454546}\n{'loss': 0.0282, 'grad_norm': 0.752844512462616, 'learning_rate': 0.00012379654100965807, 'epoch': 7.068181818181818}\n{'loss': 0.0309, 'grad_norm': 0.6050562858581543, 'learning_rate': 0.00012284425992496837, 'epoch': 7.090909090909091}\n{'loss': 0.0411, 'grad_norm': 0.6291449666023254, 'learning_rate': 0.00012189197884027872, 'epoch': 7.113636363636363}\n{'loss': 0.0716, 'grad_norm': 0.8818997740745544, 'learning_rate': 0.00012093969775558903, 'epoch': 7.136363636363637}\n{'loss': 0.0417, 'grad_norm': 0.7666783928871155, 'learning_rate': 0.00011998741667089935, 'epoch': 7.159090909090909}\n{'loss': 0.0441, 'grad_norm': 1.1548117399215698, 'learning_rate': 0.00011903513558620966, 'epoch': 7.181818181818182}\n{'loss': 0.034, 'grad_norm': 0.7924163937568665, 'learning_rate': 0.00011808285450152001, 'epoch': 7.204545454545454}\n{'loss': 0.0378, 'grad_norm': 0.4841240346431732, 'learning_rate': 0.00011713057341683032, 'epoch': 7.2272727272727275}\n{'loss': 0.0503, 'grad_norm': 0.8140901327133179, 'learning_rate': 0.00011617829233214064, 'epoch': 7.25}\n{'loss': 0.0293, 'grad_norm': 0.4774080216884613, 'learning_rate': 0.00011522601124745098, 'epoch': 7.2727272727272725}\n{'loss': 0.0319, 'grad_norm': 0.8697926998138428, 'learning_rate': 0.00011427373016276129, 'epoch': 7.295454545454546}\n{'loss': 0.0404, 'grad_norm': 0.7129719257354736, 'learning_rate': 0.00011332144907807161, 'epoch': 7.318181818181818}\n{'loss': 0.0386, 'grad_norm': 0.656201958656311, 'learning_rate': 0.00011236916799338192, 'epoch': 7.340909090909091}\n{'loss': 0.0494, 'grad_norm': 0.7280343174934387, 'learning_rate': 0.00011141688690869226, 'epoch': 7.363636363636363}\n{'loss': 0.0308, 'grad_norm': 0.5116012692451477, 'learning_rate': 0.00011046460582400258, 'epoch': 7.386363636363637}\n{'loss': 0.0289, 'grad_norm': 1.0542490482330322, 'learning_rate': 0.00010951232473931289, 'epoch': 7.409090909090909}\n{'loss': 0.0584, 'grad_norm': 1.0333259105682373, 'learning_rate': 0.00010856004365462323, 'epoch': 7.431818181818182}\n{'loss': 0.0405, 'grad_norm': 0.5379130244255066, 'learning_rate': 0.00010760776256993355, 'epoch': 7.454545454545454}\n{'loss': 0.0471, 'grad_norm': 1.6538957357406616, 'learning_rate': 0.00010665548148524386, 'epoch': 7.4772727272727275}\n{'loss': 0.0535, 'grad_norm': 0.8466701507568359, 'learning_rate': 0.00010570320040055418, 'epoch': 7.5}\n{'loss': 0.0245, 'grad_norm': 0.6905555129051208, 'learning_rate': 0.00010475091931586452, 'epoch': 7.5227272727272725}\n{'loss': 0.036, 'grad_norm': 1.6932257413864136, 'learning_rate': 0.00010379863823117483, 'epoch': 7.545454545454545}\n{'loss': 0.049, 'grad_norm': 0.6624545454978943, 'learning_rate': 0.00010284635714648515, 'epoch': 7.568181818181818}\n{'loss': 0.0316, 'grad_norm': 0.5009294748306274, 'learning_rate': 0.00010189407606179549, 'epoch': 7.590909090909091}\n{'loss': 0.0303, 'grad_norm': 0.5053093433380127, 'learning_rate': 0.0001009417949771058, 'epoch': 7.613636363636363}\n{'loss': 0.065, 'grad_norm': 0.7860348224639893, 'learning_rate': 9.998951389241612e-05, 'epoch': 7.636363636363637}\n{'loss': 0.0433, 'grad_norm': 0.5144691467285156, 'learning_rate': 9.903723280772646e-05, 'epoch': 7.659090909090909}\n{'loss': 0.0276, 'grad_norm': 0.5700079798698425, 'learning_rate': 9.808495172303677e-05, 'epoch': 7.681818181818182}\n{'loss': 0.0368, 'grad_norm': 0.6465777158737183, 'learning_rate': 9.713267063834709e-05, 'epoch': 7.704545454545455}\n{'loss': 0.0316, 'grad_norm': 0.7297047972679138, 'learning_rate': 9.61803895536574e-05, 'epoch': 7.7272727272727275}\n{'loss': 0.0311, 'grad_norm': 0.5644083023071289, 'learning_rate': 9.522810846896774e-05, 'epoch': 7.75}\n{'loss': 0.0416, 'grad_norm': 0.7965272068977356, 'learning_rate': 9.427582738427806e-05, 'epoch': 7.7727272727272725}\n{'loss': 0.0371, 'grad_norm': 0.4513283371925354, 'learning_rate': 9.332354629958839e-05, 'epoch': 7.795454545454545}\n{'loss': 0.0413, 'grad_norm': 0.4725233018398285, 'learning_rate': 9.237126521489871e-05, 'epoch': 7.818181818181818}\n{'loss': 0.033, 'grad_norm': 0.7261372804641724, 'learning_rate': 9.141898413020903e-05, 'epoch': 7.840909090909091}\n{'loss': 0.0316, 'grad_norm': 0.6754110455513, 'learning_rate': 9.046670304551935e-05, 'epoch': 7.863636363636363}\n{'loss': 0.0422, 'grad_norm': 0.6965950727462769, 'learning_rate': 8.951442196082968e-05, 'epoch': 7.886363636363637}\n{'loss': 0.0447, 'grad_norm': 0.7498248219490051, 'learning_rate': 8.856214087614e-05, 'epoch': 7.909090909090909}\n{'loss': 0.0303, 'grad_norm': 0.6379469633102417, 'learning_rate': 8.760985979145032e-05, 'epoch': 7.931818181818182}\n{'loss': 0.0307, 'grad_norm': 0.6662924289703369, 'learning_rate': 8.665757870676065e-05, 'epoch': 7.954545454545455}\n{'loss': 0.0287, 'grad_norm': 0.37657079100608826, 'learning_rate': 8.570529762207096e-05, 'epoch': 7.9772727272727275}\n{'loss': 0.0244, 'grad_norm': 1.084733247756958, 'learning_rate': 8.47530165373813e-05, 'epoch': 8.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8953    0.9000    0.8977      3221\n     PERCENT     0.8791    0.9195    0.8989        87\n        TYPE     0.9536    0.9640    0.9588     11501\n      VOLUME     0.8871    0.9322    0.9091        59\n\n   micro avg     0.9403    0.9498    0.9450     14868\n   macro avg     0.9038    0.9289    0.9161     14868\nweighted avg     0.9402    0.9498    0.9450     14868\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.28191158175468445, 'eval_f1_macro': 0.9160951968804669, 'eval_precision': 0.9402716739912106, 'eval_recall': 0.9497578692493946, 'eval_f1': 0.9449909656695443, 'eval_accuracy': 0.9376559956592512, 'eval_runtime': 1.946, 'eval_samples_per_second': 2831.449, 'eval_steps_per_second': 5.653, 'epoch': 8.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.0365, 'grad_norm': 0.5113925337791443, 'learning_rate': 8.38007354526916e-05, 'epoch': 8.022727272727273}\n{'loss': 0.0313, 'grad_norm': 0.6987825036048889, 'learning_rate': 8.284845436800193e-05, 'epoch': 8.045454545454545}\n{'loss': 0.032, 'grad_norm': 0.5505416393280029, 'learning_rate': 8.189617328331226e-05, 'epoch': 8.068181818181818}\n{'loss': 0.021, 'grad_norm': 0.5086889863014221, 'learning_rate': 8.094389219862257e-05, 'epoch': 8.090909090909092}\n{'loss': 0.0437, 'grad_norm': 1.0151582956314087, 'learning_rate': 7.999161111393291e-05, 'epoch': 8.113636363636363}\n{'loss': 0.034, 'grad_norm': 0.5297490358352661, 'learning_rate': 7.903933002924322e-05, 'epoch': 8.136363636363637}\n{'loss': 0.0314, 'grad_norm': 0.5011924505233765, 'learning_rate': 7.808704894455354e-05, 'epoch': 8.159090909090908}\n{'loss': 0.0331, 'grad_norm': 0.6695435643196106, 'learning_rate': 7.713476785986388e-05, 'epoch': 8.181818181818182}\n{'loss': 0.0346, 'grad_norm': 0.5475926995277405, 'learning_rate': 7.618248677517419e-05, 'epoch': 8.204545454545455}\n{'loss': 0.0261, 'grad_norm': 0.5579484105110168, 'learning_rate': 7.523020569048451e-05, 'epoch': 8.227272727272727}\n{'loss': 0.0281, 'grad_norm': 0.46735355257987976, 'learning_rate': 7.427792460579483e-05, 'epoch': 8.25}\n{'loss': 0.0438, 'grad_norm': 1.0816665887832642, 'learning_rate': 7.332564352110516e-05, 'epoch': 8.272727272727273}\n{'loss': 0.0397, 'grad_norm': 0.6481165289878845, 'learning_rate': 7.237336243641548e-05, 'epoch': 8.295454545454545}\n{'loss': 0.0303, 'grad_norm': 0.7146983742713928, 'learning_rate': 7.14210813517258e-05, 'epoch': 8.318181818181818}\n{'loss': 0.0343, 'grad_norm': 1.0471066236495972, 'learning_rate': 7.046880026703613e-05, 'epoch': 8.340909090909092}\n{'loss': 0.0282, 'grad_norm': 0.7315685749053955, 'learning_rate': 6.951651918234645e-05, 'epoch': 8.363636363636363}\n{'loss': 0.0315, 'grad_norm': 0.7538424730300903, 'learning_rate': 6.856423809765677e-05, 'epoch': 8.386363636363637}\n{'loss': 0.0394, 'grad_norm': 0.5858554840087891, 'learning_rate': 6.76119570129671e-05, 'epoch': 8.409090909090908}\n{'loss': 0.0343, 'grad_norm': 0.6394667625427246, 'learning_rate': 6.665967592827742e-05, 'epoch': 8.431818181818182}\n{'loss': 0.0265, 'grad_norm': 0.5863359570503235, 'learning_rate': 6.570739484358774e-05, 'epoch': 8.454545454545455}\n{'loss': 0.0461, 'grad_norm': 0.9471100568771362, 'learning_rate': 6.475511375889807e-05, 'epoch': 8.477272727272727}\n{'loss': 0.0213, 'grad_norm': 0.5585301518440247, 'learning_rate': 6.380283267420839e-05, 'epoch': 8.5}\n{'loss': 0.0201, 'grad_norm': 0.5133561491966248, 'learning_rate': 6.28505515895187e-05, 'epoch': 8.522727272727273}\n{'loss': 0.036, 'grad_norm': 0.7701215744018555, 'learning_rate': 6.189827050482904e-05, 'epoch': 8.545454545454545}\n{'loss': 0.0459, 'grad_norm': 1.240255355834961, 'learning_rate': 6.094598942013936e-05, 'epoch': 8.568181818181818}\n{'loss': 0.0429, 'grad_norm': 0.7001743912696838, 'learning_rate': 5.9993708335449675e-05, 'epoch': 8.590909090909092}\n{'loss': 0.0506, 'grad_norm': 0.6791318655014038, 'learning_rate': 5.9041427250760005e-05, 'epoch': 8.613636363636363}\n{'loss': 0.0221, 'grad_norm': 0.40443938970565796, 'learning_rate': 5.808914616607032e-05, 'epoch': 8.636363636363637}\n{'loss': 0.0252, 'grad_norm': 0.7780418992042542, 'learning_rate': 5.7136865081380645e-05, 'epoch': 8.659090909090908}\n{'loss': 0.0386, 'grad_norm': 0.8945763111114502, 'learning_rate': 5.618458399669096e-05, 'epoch': 8.681818181818182}\n{'loss': 0.0325, 'grad_norm': 0.36720290780067444, 'learning_rate': 5.523230291200129e-05, 'epoch': 8.704545454545455}\n{'loss': 0.0234, 'grad_norm': 0.7756730914115906, 'learning_rate': 5.4280021827311614e-05, 'epoch': 8.727272727272727}\n{'loss': 0.0474, 'grad_norm': 0.8825711011886597, 'learning_rate': 5.332774074262193e-05, 'epoch': 8.75}\n{'loss': 0.0283, 'grad_norm': 0.7392993569374084, 'learning_rate': 5.237545965793226e-05, 'epoch': 8.772727272727273}\n{'loss': 0.0265, 'grad_norm': 0.46732670068740845, 'learning_rate': 5.142317857324258e-05, 'epoch': 8.795454545454545}\n{'loss': 0.0329, 'grad_norm': 0.841338038444519, 'learning_rate': 5.04708974885529e-05, 'epoch': 8.818181818181818}\n{'loss': 0.033, 'grad_norm': 0.8113183379173279, 'learning_rate': 4.951861640386323e-05, 'epoch': 8.840909090909092}\n{'loss': 0.0341, 'grad_norm': 0.7068490386009216, 'learning_rate': 4.8566335319173546e-05, 'epoch': 8.863636363636363}\n{'loss': 0.0245, 'grad_norm': 0.7266334295272827, 'learning_rate': 4.761405423448387e-05, 'epoch': 8.886363636363637}\n{'loss': 0.0488, 'grad_norm': 0.7360364198684692, 'learning_rate': 4.666177314979419e-05, 'epoch': 8.909090909090908}\n{'loss': 0.0446, 'grad_norm': 0.8218030333518982, 'learning_rate': 4.5709492065104516e-05, 'epoch': 8.931818181818182}\n{'loss': 0.0323, 'grad_norm': 0.925401508808136, 'learning_rate': 4.475721098041484e-05, 'epoch': 8.954545454545455}\n{'loss': 0.0324, 'grad_norm': 0.5588446855545044, 'learning_rate': 4.380492989572516e-05, 'epoch': 8.977272727272727}\n{'loss': 0.1295, 'grad_norm': 3.850637435913086, 'learning_rate': 4.285264881103548e-05, 'epoch': 9.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8870    0.9019    0.8944      3221\n     PERCENT     0.8602    0.9195    0.8889        87\n        TYPE     0.9553    0.9618    0.9585     11501\n      VOLUME     0.8710    0.9153    0.8926        59\n\n   micro avg     0.9394    0.9484    0.9439     14868\n   macro avg     0.8934    0.9246    0.9086     14868\nweighted avg     0.9396    0.9484    0.9440     14868\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.29721346497535706, 'eval_f1_macro': 0.908596187289168, 'eval_precision': 0.9394403730846103, 'eval_recall': 0.9484126984126984, 'eval_f1': 0.9439052145391257, 'eval_accuracy': 0.9363537710255019, 'eval_runtime': 1.5079, 'eval_samples_per_second': 3653.969, 'eval_steps_per_second': 7.295, 'epoch': 9.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.0384, 'grad_norm': 0.6859121918678284, 'learning_rate': 4.19003677263458e-05, 'epoch': 9.022727272727273}\n{'loss': 0.0223, 'grad_norm': 0.5703559517860413, 'learning_rate': 4.094808664165613e-05, 'epoch': 9.045454545454545}\n{'loss': 0.0221, 'grad_norm': 0.6168438196182251, 'learning_rate': 3.9995805556966455e-05, 'epoch': 9.068181818181818}\n{'loss': 0.0296, 'grad_norm': 0.5467605590820312, 'learning_rate': 3.904352447227677e-05, 'epoch': 9.090909090909092}\n{'loss': 0.0423, 'grad_norm': 0.6582891941070557, 'learning_rate': 3.8091243387587094e-05, 'epoch': 9.113636363636363}\n{'loss': 0.025, 'grad_norm': 0.5490008592605591, 'learning_rate': 3.713896230289742e-05, 'epoch': 9.136363636363637}\n{'loss': 0.0202, 'grad_norm': 0.4684399664402008, 'learning_rate': 3.618668121820774e-05, 'epoch': 9.159090909090908}\n{'loss': 0.0204, 'grad_norm': 0.5550007820129395, 'learning_rate': 3.5234400133518064e-05, 'epoch': 9.181818181818182}\n{'loss': 0.0311, 'grad_norm': 0.6721740961074829, 'learning_rate': 3.428211904882839e-05, 'epoch': 9.204545454545455}\n{'loss': 0.0228, 'grad_norm': 0.3416570723056793, 'learning_rate': 3.332983796413871e-05, 'epoch': 9.227272727272727}\n{'loss': 0.0363, 'grad_norm': 0.6221135854721069, 'learning_rate': 3.237755687944903e-05, 'epoch': 9.25}\n{'loss': 0.0375, 'grad_norm': 0.6384022831916809, 'learning_rate': 3.142527579475935e-05, 'epoch': 9.272727272727273}\n{'loss': 0.0263, 'grad_norm': 0.6524648666381836, 'learning_rate': 3.047299471006968e-05, 'epoch': 9.295454545454545}\n{'loss': 0.0268, 'grad_norm': 0.4396744668483734, 'learning_rate': 2.9520713625380003e-05, 'epoch': 9.318181818181818}\n{'loss': 0.0196, 'grad_norm': 0.5102365016937256, 'learning_rate': 2.8568432540690322e-05, 'epoch': 9.340909090909092}\n{'loss': 0.0405, 'grad_norm': 0.6581394672393799, 'learning_rate': 2.7616151456000645e-05, 'epoch': 9.363636363636363}\n{'loss': 0.0375, 'grad_norm': 0.6397756934165955, 'learning_rate': 2.6663870371310965e-05, 'epoch': 9.386363636363637}\n{'loss': 0.0307, 'grad_norm': 0.5529307126998901, 'learning_rate': 2.571158928662129e-05, 'epoch': 9.409090909090908}\n{'loss': 0.0229, 'grad_norm': 0.45601293444633484, 'learning_rate': 2.4759308201931615e-05, 'epoch': 9.431818181818182}\n{'loss': 0.0352, 'grad_norm': 0.8765130043029785, 'learning_rate': 2.3807027117241935e-05, 'epoch': 9.454545454545455}\n{'loss': 0.0172, 'grad_norm': 0.517990231513977, 'learning_rate': 2.2854746032552258e-05, 'epoch': 9.477272727272727}\n{'loss': 0.0246, 'grad_norm': 0.40296903252601624, 'learning_rate': 2.190246494786258e-05, 'epoch': 9.5}\n{'loss': 0.0486, 'grad_norm': 0.5771934390068054, 'learning_rate': 2.09501838631729e-05, 'epoch': 9.522727272727273}\n{'loss': 0.0316, 'grad_norm': 0.6799627542495728, 'learning_rate': 1.9997902778483227e-05, 'epoch': 9.545454545454545}\n{'loss': 0.0304, 'grad_norm': 0.6224042177200317, 'learning_rate': 1.9045621693793547e-05, 'epoch': 9.568181818181818}\n{'loss': 0.0289, 'grad_norm': 0.5970988869667053, 'learning_rate': 1.809334060910387e-05, 'epoch': 9.590909090909092}\n{'loss': 0.0159, 'grad_norm': 0.33343082666397095, 'learning_rate': 1.7141059524414193e-05, 'epoch': 9.613636363636363}\n{'loss': 0.0309, 'grad_norm': 0.6590813398361206, 'learning_rate': 1.6188778439724517e-05, 'epoch': 9.636363636363637}\n{'loss': 0.0161, 'grad_norm': 0.4844546318054199, 'learning_rate': 1.523649735503484e-05, 'epoch': 9.659090909090908}\n{'loss': 0.0245, 'grad_norm': 0.8230695128440857, 'learning_rate': 1.4284216270345161e-05, 'epoch': 9.681818181818182}\n{'loss': 0.0273, 'grad_norm': 0.6934105157852173, 'learning_rate': 1.3331935185655483e-05, 'epoch': 9.704545454545455}\n{'loss': 0.0301, 'grad_norm': 0.5149052143096924, 'learning_rate': 1.2379654100965807e-05, 'epoch': 9.727272727272727}\n{'loss': 0.0171, 'grad_norm': 0.31289786100387573, 'learning_rate': 1.1427373016276129e-05, 'epoch': 9.75}\n{'loss': 0.0413, 'grad_norm': 0.6778160929679871, 'learning_rate': 1.047509193158645e-05, 'epoch': 9.772727272727273}\n{'loss': 0.0344, 'grad_norm': 0.616905689239502, 'learning_rate': 9.522810846896774e-06, 'epoch': 9.795454545454545}\n{'loss': 0.0176, 'grad_norm': 0.4135047197341919, 'learning_rate': 8.570529762207097e-06, 'epoch': 9.818181818181818}\n{'loss': 0.0254, 'grad_norm': 0.4472958445549011, 'learning_rate': 7.61824867751742e-06, 'epoch': 9.840909090909092}\n{'loss': 0.0262, 'grad_norm': 0.7071242332458496, 'learning_rate': 6.665967592827741e-06, 'epoch': 9.863636363636363}\n{'loss': 0.0241, 'grad_norm': 0.6627174019813538, 'learning_rate': 5.7136865081380645e-06, 'epoch': 9.886363636363637}\n{'loss': 0.0469, 'grad_norm': 0.628035306930542, 'learning_rate': 4.761405423448387e-06, 'epoch': 9.909090909090908}\n{'loss': 0.0357, 'grad_norm': 0.70820152759552, 'learning_rate': 3.80912433875871e-06, 'epoch': 9.931818181818182}\n{'loss': 0.0473, 'grad_norm': 0.9793189167976379, 'learning_rate': 2.8568432540690322e-06, 'epoch': 9.954545454545455}\n{'loss': 0.0364, 'grad_norm': 0.5819095969200134, 'learning_rate': 1.904562169379355e-06, 'epoch': 9.977272727272727}\n{'loss': 0.0071, 'grad_norm': 0.6701061129570007, 'learning_rate': 9.522810846896775e-07, 'epoch': 10.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8903    0.9022    0.8962      3221\n     PERCENT     0.8602    0.9195    0.8889        87\n        TYPE     0.9554    0.9636    0.9595     11501\n      VOLUME     0.8710    0.9153    0.8926        59\n\n   micro avg     0.9403    0.9498    0.9451     14868\n   macro avg     0.8942    0.9251    0.9093     14868\nweighted avg     0.9404    0.9498    0.9451     14868\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.2980576455593109, 'eval_f1_macro': 0.9092883606817862, 'eval_precision': 0.9403382607537621, 'eval_recall': 0.9498251277912295, 'eval_f1': 0.9450578866358831, 'eval_accuracy': 0.9374932175800326, 'eval_runtime': 1.5449, 'eval_samples_per_second': 3566.681, 'eval_steps_per_second': 7.12, 'epoch': 10.0}\n{'train_runtime': 63.6697, 'train_samples_per_second': 3461.929, 'train_steps_per_second': 6.911, 'train_loss': 0.20306908379266547, 'epoch': 10.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n[I 2025-09-26 17:59:08,959] Trial 0 finished with value: 0.9174994186455976 and parameters: {'learning_rate': 0.00037710330953711225, 'weight_decay': 0.040411977783360725, 'num_train_epochs': 10}. Best is trial 0 with value: 0.9174994186455976.\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8953    0.9000    0.8977      3221\n     PERCENT     0.8791    0.9195    0.8989        87\n        TYPE     0.9536    0.9640    0.9588     11501\n      VOLUME     0.8871    0.9322    0.9091        59\n\n   micro avg     0.9403    0.9498    0.9450     14868\n   macro avg     0.9038    0.9289    0.9161     14868\nweighted avg     0.9402    0.9498    0.9450     14868\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.28191158175468445, 'eval_f1_macro': 0.9160951968804669, 'eval_precision': 0.9402716739912106, 'eval_recall': 0.9497578692493946, 'eval_f1': 0.9449909656695443, 'eval_accuracy': 0.9376559956592512, 'eval_runtime': 1.5756, 'eval_samples_per_second': 3497.088, 'eval_steps_per_second': 6.981, 'epoch': 10.0}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/27552 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6542901afc2149e6ad187b92ed2d8a73"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nThe speedups for torchdynamo mostly come with GPU Ampere or higher and which is not detected here.\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/tmp/ipykernel_36/3364797558.py:66: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 2.2869, 'grad_norm': 6.933525562286377, 'learning_rate': 0.0, 'epoch': 0.022727272727272728}\n{'loss': 2.2967, 'grad_norm': 7.072678089141846, 'learning_rate': 1.353172260519668e-05, 'epoch': 0.045454545454545456}\n{'loss': 2.2553, 'grad_norm': 6.921706199645996, 'learning_rate': 2.706344521039336e-05, 'epoch': 0.06818181818181818}\n{'loss': 2.194, 'grad_norm': 7.130190372467041, 'learning_rate': 4.059516781559004e-05, 'epoch': 0.09090909090909091}\n{'loss': 2.1039, 'grad_norm': 6.579970359802246, 'learning_rate': 5.412689042078672e-05, 'epoch': 0.11363636363636363}\n{'loss': 1.9927, 'grad_norm': 6.494531631469727, 'learning_rate': 6.76586130259834e-05, 'epoch': 0.13636363636363635}\n{'loss': 1.8798, 'grad_norm': 5.930492401123047, 'learning_rate': 8.119033563118008e-05, 'epoch': 0.1590909090909091}\n{'loss': 1.7052, 'grad_norm': 5.762612342834473, 'learning_rate': 9.472205823637677e-05, 'epoch': 0.18181818181818182}\n{'loss': 1.5844, 'grad_norm': 4.848269939422607, 'learning_rate': 0.00010825378084157344, 'epoch': 0.20454545454545456}\n{'loss': 1.422, 'grad_norm': 4.214940547943115, 'learning_rate': 0.00012178550344677013, 'epoch': 0.22727272727272727}\n{'loss': 1.2647, 'grad_norm': 3.1881182193756104, 'learning_rate': 0.0001353172260519668, 'epoch': 0.25}\n{'loss': 1.1817, 'grad_norm': 2.0752742290496826, 'learning_rate': 0.0001488489486571635, 'epoch': 0.2727272727272727}\n{'loss': 1.1611, 'grad_norm': 1.9041978120803833, 'learning_rate': 0.00016238067126236015, 'epoch': 0.29545454545454547}\n{'loss': 1.1501, 'grad_norm': 2.436711311340332, 'learning_rate': 0.00017591239386755683, 'epoch': 0.3181818181818182}\n{'loss': 1.1187, 'grad_norm': 2.330120325088501, 'learning_rate': 0.00018944411647275354, 'epoch': 0.3409090909090909}\n{'loss': 1.0796, 'grad_norm': 1.9153705835342407, 'learning_rate': 0.00020297583907795023, 'epoch': 0.36363636363636365}\n{'loss': 0.9995, 'grad_norm': 1.3153815269470215, 'learning_rate': 0.00021650756168314689, 'epoch': 0.38636363636363635}\n{'loss': 0.9728, 'grad_norm': 2.0619122982025146, 'learning_rate': 0.00023003928428834357, 'epoch': 0.4090909090909091}\n{'loss': 0.9536, 'grad_norm': 2.2353382110595703, 'learning_rate': 0.00024357100689354025, 'epoch': 0.4318181818181818}\n{'loss': 0.884, 'grad_norm': 1.9665499925613403, 'learning_rate': 0.00024202941824231532, 'epoch': 0.45454545454545453}\n{'loss': 0.8842, 'grad_norm': 1.1096950769424438, 'learning_rate': 0.00024048782959109038, 'epoch': 0.4772727272727273}\n{'loss': 0.8026, 'grad_norm': 1.1347135305404663, 'learning_rate': 0.00023894624093986544, 'epoch': 0.5}\n{'loss': 0.8172, 'grad_norm': 1.6128630638122559, 'learning_rate': 0.0002374046522886405, 'epoch': 0.5227272727272727}\n{'loss': 0.7737, 'grad_norm': 1.0831953287124634, 'learning_rate': 0.00023586306363741557, 'epoch': 0.5454545454545454}\n{'loss': 0.7542, 'grad_norm': 0.8286730647087097, 'learning_rate': 0.00023432147498619063, 'epoch': 0.5681818181818182}\n{'loss': 0.7162, 'grad_norm': 0.951158344745636, 'learning_rate': 0.00023277988633496567, 'epoch': 0.5909090909090909}\n{'loss': 0.6375, 'grad_norm': 1.809584140777588, 'learning_rate': 0.00023123829768374073, 'epoch': 0.6136363636363636}\n{'loss': 0.6383, 'grad_norm': 1.1931816339492798, 'learning_rate': 0.00022969670903251582, 'epoch': 0.6363636363636364}\n{'loss': 0.6543, 'grad_norm': 0.8399784564971924, 'learning_rate': 0.00022815512038129086, 'epoch': 0.6590909090909091}\n{'loss': 0.6081, 'grad_norm': 1.0778510570526123, 'learning_rate': 0.00022661353173006592, 'epoch': 0.6818181818181818}\n{'loss': 0.5873, 'grad_norm': 0.9235129952430725, 'learning_rate': 0.00022507194307884101, 'epoch': 0.7045454545454546}\n{'loss': 0.595, 'grad_norm': 0.81534743309021, 'learning_rate': 0.00022353035442761605, 'epoch': 0.7272727272727273}\n{'loss': 0.5775, 'grad_norm': 1.0632565021514893, 'learning_rate': 0.0002219887657763911, 'epoch': 0.75}\n{'loss': 0.6002, 'grad_norm': 0.7207691073417664, 'learning_rate': 0.00022044717712516618, 'epoch': 0.7727272727272727}\n{'loss': 0.5475, 'grad_norm': 1.978426218032837, 'learning_rate': 0.00021890558847394124, 'epoch': 0.7954545454545454}\n{'loss': 0.5268, 'grad_norm': 1.6287150382995605, 'learning_rate': 0.0002173639998227163, 'epoch': 0.8181818181818182}\n{'loss': 0.5181, 'grad_norm': 1.166808843612671, 'learning_rate': 0.00021582241117149134, 'epoch': 0.8409090909090909}\n{'loss': 0.5001, 'grad_norm': 0.8828245997428894, 'learning_rate': 0.00021428082252026643, 'epoch': 0.8636363636363636}\n{'loss': 0.5084, 'grad_norm': 1.3072963953018188, 'learning_rate': 0.0002127392338690415, 'epoch': 0.8863636363636364}\n{'loss': 0.4926, 'grad_norm': 1.4583680629730225, 'learning_rate': 0.00021119764521781653, 'epoch': 0.9090909090909091}\n{'loss': 0.5325, 'grad_norm': 1.2208149433135986, 'learning_rate': 0.00020965605656659162, 'epoch': 0.9318181818181818}\n{'loss': 0.4679, 'grad_norm': 1.0409191846847534, 'learning_rate': 0.00020811446791536668, 'epoch': 0.9545454545454546}\n{'loss': 0.4647, 'grad_norm': 1.8692970275878906, 'learning_rate': 0.00020657287926414172, 'epoch': 0.9772727272727273}\n{'loss': 0.4478, 'grad_norm': 3.4509968757629395, 'learning_rate': 0.0002050312906129168, 'epoch': 1.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.7603    0.7187    0.7389      3142\n     PERCENT     0.4724    0.9091    0.6218        66\n        TYPE     0.8945    0.9368    0.9152     11415\n      VOLUME     0.0000    0.0000    0.0000        70\n\n   micro avg     0.8643    0.8856    0.8748     14693\n   macro avg     0.5318    0.6411    0.5690     14693\nweighted avg     0.8597    0.8856    0.8718     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.42764511704444885, 'eval_f1_macro': 0.568956604660718, 'eval_precision': 0.8642975755562936, 'eval_recall': 0.8855917783978765, 'eval_f1': 0.8748151136210839, 'eval_accuracy': 0.8683752312547611, 'eval_runtime': 1.4605, 'eval_samples_per_second': 3773.487, 'eval_steps_per_second': 7.532, 'epoch': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.386, 'grad_norm': 1.0307737588882446, 'learning_rate': 0.00020348970196169185, 'epoch': 1.0227272727272727}\n{'loss': 0.4176, 'grad_norm': 0.9355615973472595, 'learning_rate': 0.0002019481133104669, 'epoch': 1.0454545454545454}\n{'loss': 0.4546, 'grad_norm': 1.467802882194519, 'learning_rate': 0.000200406524659242, 'epoch': 1.0681818181818181}\n{'loss': 0.3916, 'grad_norm': 1.4462194442749023, 'learning_rate': 0.00019886493600801704, 'epoch': 1.0909090909090908}\n{'loss': 0.3513, 'grad_norm': 0.9727166295051575, 'learning_rate': 0.0001973233473567921, 'epoch': 1.1136363636363635}\n{'loss': 0.4059, 'grad_norm': 1.1570947170257568, 'learning_rate': 0.00019578175870556716, 'epoch': 1.1363636363636362}\n{'loss': 0.399, 'grad_norm': 0.7727317810058594, 'learning_rate': 0.00019424017005434223, 'epoch': 1.1590909090909092}\n{'loss': 0.3963, 'grad_norm': 2.139085531234741, 'learning_rate': 0.0001926985814031173, 'epoch': 1.1818181818181819}\n{'loss': 0.383, 'grad_norm': 1.2519192695617676, 'learning_rate': 0.00019115699275189235, 'epoch': 1.2045454545454546}\n{'loss': 0.3958, 'grad_norm': 1.2237790822982788, 'learning_rate': 0.00018961540410066742, 'epoch': 1.2272727272727273}\n{'loss': 0.3734, 'grad_norm': 1.1608073711395264, 'learning_rate': 0.00018807381544944248, 'epoch': 1.25}\n{'loss': 0.3741, 'grad_norm': 1.012603521347046, 'learning_rate': 0.00018653222679821752, 'epoch': 1.2727272727272727}\n{'loss': 0.3629, 'grad_norm': 0.9616353511810303, 'learning_rate': 0.0001849906381469926, 'epoch': 1.2954545454545454}\n{'loss': 0.3335, 'grad_norm': 1.1480401754379272, 'learning_rate': 0.00018344904949576767, 'epoch': 1.3181818181818181}\n{'loss': 0.3493, 'grad_norm': 1.1580454111099243, 'learning_rate': 0.0001819074608445427, 'epoch': 1.3409090909090908}\n{'loss': 0.37, 'grad_norm': 0.8245440125465393, 'learning_rate': 0.00018036587219331777, 'epoch': 1.3636363636363638}\n{'loss': 0.3725, 'grad_norm': 0.8354818820953369, 'learning_rate': 0.00017882428354209286, 'epoch': 1.3863636363636362}\n{'loss': 0.3182, 'grad_norm': 1.2342959642410278, 'learning_rate': 0.0001772826948908679, 'epoch': 1.4090909090909092}\n{'loss': 0.3518, 'grad_norm': 1.3287930488586426, 'learning_rate': 0.00017574110623964296, 'epoch': 1.4318181818181819}\n{'loss': 0.3652, 'grad_norm': 1.080747127532959, 'learning_rate': 0.00017419951758841802, 'epoch': 1.4545454545454546}\n{'loss': 0.3034, 'grad_norm': 1.5466086864471436, 'learning_rate': 0.0001726579289371931, 'epoch': 1.4772727272727273}\n{'loss': 0.3313, 'grad_norm': 1.0129519701004028, 'learning_rate': 0.00017111634028596815, 'epoch': 1.5}\n{'loss': 0.3771, 'grad_norm': 0.9758182764053345, 'learning_rate': 0.00016957475163474321, 'epoch': 1.5227272727272727}\n{'loss': 0.3113, 'grad_norm': 0.8762342929840088, 'learning_rate': 0.00016803316298351828, 'epoch': 1.5454545454545454}\n{'loss': 0.2866, 'grad_norm': 1.355621337890625, 'learning_rate': 0.00016649157433229334, 'epoch': 1.5681818181818183}\n{'loss': 0.2924, 'grad_norm': 1.4643267393112183, 'learning_rate': 0.00016494998568106838, 'epoch': 1.5909090909090908}\n{'loss': 0.311, 'grad_norm': 0.8082621097564697, 'learning_rate': 0.00016340839702984347, 'epoch': 1.6136363636363638}\n{'loss': 0.3482, 'grad_norm': 1.261899709701538, 'learning_rate': 0.00016186680837861853, 'epoch': 1.6363636363636362}\n{'loss': 0.2797, 'grad_norm': 1.9740010499954224, 'learning_rate': 0.00016032521972739357, 'epoch': 1.6590909090909092}\n{'loss': 0.3388, 'grad_norm': 0.7587141394615173, 'learning_rate': 0.00015878363107616866, 'epoch': 1.6818181818181817}\n{'loss': 0.2725, 'grad_norm': 1.1921329498291016, 'learning_rate': 0.0001572420424249437, 'epoch': 1.7045454545454546}\n{'loss': 0.2854, 'grad_norm': 0.786821186542511, 'learning_rate': 0.00015570045377371876, 'epoch': 1.7272727272727273}\n{'loss': 0.3796, 'grad_norm': 2.0817062854766846, 'learning_rate': 0.00015415886512249385, 'epoch': 1.75}\n{'loss': 0.3487, 'grad_norm': 1.6907061338424683, 'learning_rate': 0.00015261727647126889, 'epoch': 1.7727272727272727}\n{'loss': 0.3208, 'grad_norm': 1.9987086057662964, 'learning_rate': 0.00015107568782004395, 'epoch': 1.7954545454545454}\n{'loss': 0.3035, 'grad_norm': 1.2200983762741089, 'learning_rate': 0.00014953409916881904, 'epoch': 1.8181818181818183}\n{'loss': 0.2903, 'grad_norm': 0.7083464860916138, 'learning_rate': 0.00014799251051759408, 'epoch': 1.8409090909090908}\n{'loss': 0.3426, 'grad_norm': 2.1143500804901123, 'learning_rate': 0.00014645092186636914, 'epoch': 1.8636363636363638}\n{'loss': 0.2975, 'grad_norm': 2.428180694580078, 'learning_rate': 0.0001449093332151442, 'epoch': 1.8863636363636362}\n{'loss': 0.3619, 'grad_norm': 1.7704628705978394, 'learning_rate': 0.00014336774456391927, 'epoch': 1.9090909090909092}\n{'loss': 0.2643, 'grad_norm': 2.1807751655578613, 'learning_rate': 0.00014182615591269433, 'epoch': 1.9318181818181817}\n{'loss': 0.3409, 'grad_norm': 0.856483519077301, 'learning_rate': 0.00014028456726146937, 'epoch': 1.9545454545454546}\n{'loss': 0.2808, 'grad_norm': 1.0920729637145996, 'learning_rate': 0.00013874297861024446, 'epoch': 1.9772727272727273}\n{'loss': 0.4235, 'grad_norm': 3.6049270629882812, 'learning_rate': 0.00013720138995901952, 'epoch': 2.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8754    0.7712    0.8200      3142\n     PERCENT     0.5941    0.9091    0.7186        66\n        TYPE     0.9149    0.9665    0.9400     11415\n      VOLUME     0.6604    0.5000    0.5691        70\n\n   micro avg     0.9045    0.9223    0.9133     14693\n   macro avg     0.7612    0.7867    0.7619     14693\nweighted avg     0.9038    0.9223    0.9116     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.3158419728279114, 'eval_f1_macro': 0.7619133671258291, 'eval_precision': 0.9045457579600827, 'eval_recall': 0.9222759137004015, 'eval_f1': 0.9133247961178135, 'eval_accuracy': 0.9058657089998912, 'eval_runtime': 1.524, 'eval_samples_per_second': 3616.211, 'eval_steps_per_second': 7.218, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.2728, 'grad_norm': 2.2507073879241943, 'learning_rate': 0.00013565980130779456, 'epoch': 2.022727272727273}\n{'loss': 0.2909, 'grad_norm': 1.7264857292175293, 'learning_rate': 0.00013411821265656965, 'epoch': 2.0454545454545454}\n{'loss': 0.3033, 'grad_norm': 1.6056143045425415, 'learning_rate': 0.0001325766240053447, 'epoch': 2.0681818181818183}\n{'loss': 0.219, 'grad_norm': 0.7857262492179871, 'learning_rate': 0.00013103503535411975, 'epoch': 2.090909090909091}\n{'loss': 0.2224, 'grad_norm': 0.8449456095695496, 'learning_rate': 0.0001294934467028948, 'epoch': 2.1136363636363638}\n{'loss': 0.1983, 'grad_norm': 1.0978666543960571, 'learning_rate': 0.00012795185805166987, 'epoch': 2.1363636363636362}\n{'loss': 0.2314, 'grad_norm': 1.6068285703659058, 'learning_rate': 0.00012641026940044494, 'epoch': 2.159090909090909}\n{'loss': 0.2426, 'grad_norm': 1.2751097679138184, 'learning_rate': 0.00012486868074922, 'epoch': 2.1818181818181817}\n{'loss': 0.2649, 'grad_norm': 0.9235432147979736, 'learning_rate': 0.00012332709209799506, 'epoch': 2.2045454545454546}\n{'loss': 0.2515, 'grad_norm': 1.1581162214279175, 'learning_rate': 0.00012178550344677013, 'epoch': 2.227272727272727}\n{'loss': 0.2428, 'grad_norm': 0.9707279205322266, 'learning_rate': 0.00012024391479554519, 'epoch': 2.25}\n{'loss': 0.2029, 'grad_norm': 0.7579477429389954, 'learning_rate': 0.00011870232614432025, 'epoch': 2.2727272727272725}\n{'loss': 0.2459, 'grad_norm': 1.3899134397506714, 'learning_rate': 0.00011716073749309532, 'epoch': 2.2954545454545454}\n{'loss': 0.2407, 'grad_norm': 0.8017491698265076, 'learning_rate': 0.00011561914884187037, 'epoch': 2.3181818181818183}\n{'loss': 0.2229, 'grad_norm': 0.9094311594963074, 'learning_rate': 0.00011407756019064543, 'epoch': 2.340909090909091}\n{'loss': 0.2403, 'grad_norm': 1.1668481826782227, 'learning_rate': 0.00011253597153942051, 'epoch': 2.3636363636363638}\n{'loss': 0.2239, 'grad_norm': 0.9554410576820374, 'learning_rate': 0.00011099438288819556, 'epoch': 2.3863636363636362}\n{'loss': 0.2922, 'grad_norm': 0.883265495300293, 'learning_rate': 0.00010945279423697062, 'epoch': 2.409090909090909}\n{'loss': 0.2189, 'grad_norm': 1.0258241891860962, 'learning_rate': 0.00010791120558574567, 'epoch': 2.4318181818181817}\n{'loss': 0.249, 'grad_norm': 1.2432280778884888, 'learning_rate': 0.00010636961693452075, 'epoch': 2.4545454545454546}\n{'loss': 0.2289, 'grad_norm': 0.6606753468513489, 'learning_rate': 0.00010482802828329581, 'epoch': 2.4772727272727275}\n{'loss': 0.2195, 'grad_norm': 0.7215567827224731, 'learning_rate': 0.00010328643963207086, 'epoch': 2.5}\n{'loss': 0.2697, 'grad_norm': 1.1563329696655273, 'learning_rate': 0.00010174485098084592, 'epoch': 2.5227272727272725}\n{'loss': 0.2503, 'grad_norm': 1.081735610961914, 'learning_rate': 0.000100203262329621, 'epoch': 2.5454545454545454}\n{'loss': 0.2632, 'grad_norm': 0.8717104196548462, 'learning_rate': 9.866167367839605e-05, 'epoch': 2.5681818181818183}\n{'loss': 0.3141, 'grad_norm': 1.34133780002594, 'learning_rate': 9.712008502717111e-05, 'epoch': 2.590909090909091}\n{'loss': 0.198, 'grad_norm': 1.3938956260681152, 'learning_rate': 9.557849637594618e-05, 'epoch': 2.6136363636363638}\n{'loss': 0.2287, 'grad_norm': 1.2705713510513306, 'learning_rate': 9.403690772472124e-05, 'epoch': 2.6363636363636362}\n{'loss': 0.2191, 'grad_norm': 1.4355199337005615, 'learning_rate': 9.24953190734963e-05, 'epoch': 2.659090909090909}\n{'loss': 0.1931, 'grad_norm': 1.0480077266693115, 'learning_rate': 9.095373042227135e-05, 'epoch': 2.6818181818181817}\n{'loss': 0.2429, 'grad_norm': 0.9311524629592896, 'learning_rate': 8.941214177104643e-05, 'epoch': 2.7045454545454546}\n{'loss': 0.2127, 'grad_norm': 0.7385703325271606, 'learning_rate': 8.787055311982148e-05, 'epoch': 2.7272727272727275}\n{'loss': 0.2216, 'grad_norm': 1.0005635023117065, 'learning_rate': 8.632896446859654e-05, 'epoch': 2.75}\n{'loss': 0.1959, 'grad_norm': 0.6308778524398804, 'learning_rate': 8.478737581737161e-05, 'epoch': 2.7727272727272725}\n{'loss': 0.2693, 'grad_norm': 1.171735405921936, 'learning_rate': 8.324578716614667e-05, 'epoch': 2.7954545454545454}\n{'loss': 0.2474, 'grad_norm': 0.8051801323890686, 'learning_rate': 8.170419851492173e-05, 'epoch': 2.8181818181818183}\n{'loss': 0.2381, 'grad_norm': 0.8775888681411743, 'learning_rate': 8.016260986369678e-05, 'epoch': 2.840909090909091}\n{'loss': 0.2118, 'grad_norm': 1.308610439300537, 'learning_rate': 7.862102121247185e-05, 'epoch': 2.8636363636363638}\n{'loss': 0.2082, 'grad_norm': 1.6347838640213013, 'learning_rate': 7.707943256124692e-05, 'epoch': 2.8863636363636362}\n{'loss': 0.2418, 'grad_norm': 1.345854640007019, 'learning_rate': 7.553784391002197e-05, 'epoch': 2.909090909090909}\n{'loss': 0.2228, 'grad_norm': 0.7866722345352173, 'learning_rate': 7.399625525879704e-05, 'epoch': 2.9318181818181817}\n{'loss': 0.2623, 'grad_norm': 1.4740113019943237, 'learning_rate': 7.24546666075721e-05, 'epoch': 2.9545454545454546}\n{'loss': 0.2723, 'grad_norm': 0.9585940837860107, 'learning_rate': 7.091307795634716e-05, 'epoch': 2.9772727272727275}\n{'loss': 0.1113, 'grad_norm': 3.2987987995147705, 'learning_rate': 6.937148930512223e-05, 'epoch': 3.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8708    0.8386    0.8544      3142\n     PERCENT     0.7349    0.9242    0.8188        66\n        TYPE     0.9326    0.9700    0.9510     11415\n      VOLUME     0.7742    0.6857    0.7273        70\n\n   micro avg     0.9184    0.9404    0.9293     14693\n   macro avg     0.8281    0.8547    0.8379     14693\nweighted avg     0.9178    0.9404    0.9287     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.2674920856952667, 'eval_f1_macro': 0.8378590999208037, 'eval_precision': 0.9184392448816804, 'eval_recall': 0.9403797726808685, 'eval_f1': 0.9292800215220096, 'eval_accuracy': 0.920992491021874, 'eval_runtime': 1.5679, 'eval_samples_per_second': 3514.994, 'eval_steps_per_second': 7.016, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.201, 'grad_norm': 1.275692343711853, 'learning_rate': 6.782990065389728e-05, 'epoch': 3.022727272727273}\n{'loss': 0.2009, 'grad_norm': 1.047837257385254, 'learning_rate': 6.628831200267235e-05, 'epoch': 3.0454545454545454}\n{'loss': 0.1825, 'grad_norm': 0.9639742970466614, 'learning_rate': 6.47467233514474e-05, 'epoch': 3.0681818181818183}\n{'loss': 0.1652, 'grad_norm': 0.811236560344696, 'learning_rate': 6.320513470022247e-05, 'epoch': 3.090909090909091}\n{'loss': 0.2207, 'grad_norm': 1.1655373573303223, 'learning_rate': 6.166354604899753e-05, 'epoch': 3.1136363636363638}\n{'loss': 0.178, 'grad_norm': 0.8443369269371033, 'learning_rate': 6.0121957397772595e-05, 'epoch': 3.1363636363636362}\n{'loss': 0.1835, 'grad_norm': 1.2339190244674683, 'learning_rate': 5.858036874654766e-05, 'epoch': 3.159090909090909}\n{'loss': 0.1863, 'grad_norm': 0.7432040572166443, 'learning_rate': 5.7038780095322715e-05, 'epoch': 3.1818181818181817}\n{'loss': 0.2438, 'grad_norm': 0.9595929384231567, 'learning_rate': 5.549719144409778e-05, 'epoch': 3.2045454545454546}\n{'loss': 0.1665, 'grad_norm': 1.3108112812042236, 'learning_rate': 5.3955602792872835e-05, 'epoch': 3.227272727272727}\n{'loss': 0.2162, 'grad_norm': 0.8011360764503479, 'learning_rate': 5.2414014141647905e-05, 'epoch': 3.25}\n{'loss': 0.1761, 'grad_norm': 0.9177542328834534, 'learning_rate': 5.087242549042296e-05, 'epoch': 3.2727272727272725}\n{'loss': 0.1993, 'grad_norm': 0.8151801228523254, 'learning_rate': 4.9330836839198025e-05, 'epoch': 3.2954545454545454}\n{'loss': 0.2031, 'grad_norm': 0.8324171304702759, 'learning_rate': 4.778924818797309e-05, 'epoch': 3.3181818181818183}\n{'loss': 0.1978, 'grad_norm': 1.099184513092041, 'learning_rate': 4.624765953674815e-05, 'epoch': 3.340909090909091}\n{'loss': 0.1849, 'grad_norm': 1.088967204093933, 'learning_rate': 4.4706070885523215e-05, 'epoch': 3.3636363636363638}\n{'loss': 0.165, 'grad_norm': 0.880301296710968, 'learning_rate': 4.316448223429827e-05, 'epoch': 3.3863636363636362}\n{'loss': 0.1689, 'grad_norm': 0.6471871733665466, 'learning_rate': 4.1622893583073335e-05, 'epoch': 3.409090909090909}\n{'loss': 0.1697, 'grad_norm': 0.9920056462287903, 'learning_rate': 4.008130493184839e-05, 'epoch': 3.4318181818181817}\n{'loss': 0.2615, 'grad_norm': 1.4706090688705444, 'learning_rate': 3.853971628062346e-05, 'epoch': 3.4545454545454546}\n{'loss': 0.1778, 'grad_norm': 0.9679720997810364, 'learning_rate': 3.699812762939852e-05, 'epoch': 3.4772727272727275}\n{'loss': 0.215, 'grad_norm': 1.0487784147262573, 'learning_rate': 3.545653897817358e-05, 'epoch': 3.5}\n{'loss': 0.2176, 'grad_norm': 1.0803073644638062, 'learning_rate': 3.391495032694864e-05, 'epoch': 3.5227272727272725}\n{'loss': 0.1629, 'grad_norm': 1.3599666357040405, 'learning_rate': 3.23733616757237e-05, 'epoch': 3.5454545454545454}\n{'loss': 0.1675, 'grad_norm': 0.872104823589325, 'learning_rate': 3.0831773024498766e-05, 'epoch': 3.5681818181818183}\n{'loss': 0.214, 'grad_norm': 1.0164536237716675, 'learning_rate': 2.929018437327383e-05, 'epoch': 3.590909090909091}\n{'loss': 0.2001, 'grad_norm': 0.7218509912490845, 'learning_rate': 2.774859572204889e-05, 'epoch': 3.6136363636363638}\n{'loss': 0.1708, 'grad_norm': 0.8822301030158997, 'learning_rate': 2.6207007070823953e-05, 'epoch': 3.6363636363636362}\n{'loss': 0.1552, 'grad_norm': 1.035835862159729, 'learning_rate': 2.4665418419599013e-05, 'epoch': 3.659090909090909}\n{'loss': 0.1607, 'grad_norm': 0.8453102111816406, 'learning_rate': 2.3123829768374076e-05, 'epoch': 3.6818181818181817}\n{'loss': 0.1588, 'grad_norm': 0.8730362057685852, 'learning_rate': 2.1582241117149136e-05, 'epoch': 3.7045454545454546}\n{'loss': 0.1778, 'grad_norm': 1.099928855895996, 'learning_rate': 2.0040652465924196e-05, 'epoch': 3.7272727272727275}\n{'loss': 0.1497, 'grad_norm': 1.1073558330535889, 'learning_rate': 1.849906381469926e-05, 'epoch': 3.75}\n{'loss': 0.2043, 'grad_norm': 0.7140297889709473, 'learning_rate': 1.695747516347432e-05, 'epoch': 3.7727272727272725}\n{'loss': 0.1839, 'grad_norm': 1.2905521392822266, 'learning_rate': 1.5415886512249383e-05, 'epoch': 3.7954545454545454}\n{'loss': 0.1872, 'grad_norm': 0.9349186420440674, 'learning_rate': 1.3874297861024445e-05, 'epoch': 3.8181818181818183}\n{'loss': 0.1737, 'grad_norm': 1.4941911697387695, 'learning_rate': 1.2332709209799506e-05, 'epoch': 3.840909090909091}\n{'loss': 0.2658, 'grad_norm': 1.6215957403182983, 'learning_rate': 1.0791120558574568e-05, 'epoch': 3.8636363636363638}\n{'loss': 0.2094, 'grad_norm': 1.8182111978530884, 'learning_rate': 9.24953190734963e-06, 'epoch': 3.8863636363636362}\n{'loss': 0.1686, 'grad_norm': 1.0299452543258667, 'learning_rate': 7.707943256124691e-06, 'epoch': 3.909090909090909}\n{'loss': 0.1675, 'grad_norm': 0.8775315880775452, 'learning_rate': 6.166354604899753e-06, 'epoch': 3.9318181818181817}\n{'loss': 0.2118, 'grad_norm': 1.6327321529388428, 'learning_rate': 4.624765953674815e-06, 'epoch': 3.9545454545454546}\n{'loss': 0.2079, 'grad_norm': 0.8521811962127686, 'learning_rate': 3.0831773024498766e-06, 'epoch': 3.9772727272727275}\n{'loss': 0.2494, 'grad_norm': 4.25023078918457, 'learning_rate': 1.5415886512249383e-06, 'epoch': 4.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8652    0.8660    0.8656      3142\n     PERCENT     0.7381    0.9394    0.8267        66\n        TYPE     0.9418    0.9688    0.9551     11415\n      VOLUME     0.7937    0.7143    0.7519        70\n\n   micro avg     0.9240    0.9455    0.9346     14693\n   macro avg     0.8347    0.8721    0.8498     14693\nweighted avg     0.9238    0.9455    0.9344     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.26095059514045715, 'eval_f1_macro': 0.8498185992283845, 'eval_precision': 0.9240388452840229, 'eval_recall': 0.9454842441979173, 'eval_f1': 0.9346385440845024, 'eval_accuracy': 0.9257808248993361, 'eval_runtime': 1.5685, 'eval_samples_per_second': 3513.596, 'eval_steps_per_second': 7.013, 'epoch': 4.0}\n{'train_runtime': 25.0247, 'train_samples_per_second': 3523.082, 'train_steps_per_second': 7.033, 'train_loss': 0.45018214656209404, 'epoch': 4.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nThe speedups for torchdynamo mostly come with GPU Ampere or higher and which is not detected here.\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8652    0.8660    0.8656      3142\n     PERCENT     0.7381    0.9394    0.8267        66\n        TYPE     0.9418    0.9688    0.9551     11415\n      VOLUME     0.7937    0.7143    0.7519        70\n\n   micro avg     0.9240    0.9455    0.9346     14693\n   macro avg     0.8347    0.8721    0.8498     14693\nweighted avg     0.9238    0.9455    0.9344     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.26095059514045715, 'eval_f1_macro': 0.8498185992283845, 'eval_precision': 0.9240388452840229, 'eval_recall': 0.9454842441979173, 'eval_f1': 0.9346385440845024, 'eval_accuracy': 0.9257808248993361, 'eval_runtime': 1.5535, 'eval_samples_per_second': 3547.469, 'eval_steps_per_second': 7.081, 'epoch': 4.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/3364797558.py:66: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 2.1128, 'grad_norm': 6.952020645141602, 'learning_rate': 0.0, 'epoch': 0.022727272727272728}\n{'loss': 2.1098, 'grad_norm': 7.102254390716553, 'learning_rate': 1.353172260519668e-05, 'epoch': 0.045454545454545456}\n{'loss': 2.0821, 'grad_norm': 7.056573390960693, 'learning_rate': 2.706344521039336e-05, 'epoch': 0.06818181818181818}\n{'loss': 2.0242, 'grad_norm': 6.6975250244140625, 'learning_rate': 4.059516781559004e-05, 'epoch': 0.09090909090909091}\n{'loss': 1.9575, 'grad_norm': 6.4439496994018555, 'learning_rate': 5.412689042078672e-05, 'epoch': 0.11363636363636363}\n{'loss': 1.8199, 'grad_norm': 6.191910266876221, 'learning_rate': 6.76586130259834e-05, 'epoch': 0.13636363636363635}\n{'loss': 1.7126, 'grad_norm': 5.647718906402588, 'learning_rate': 8.119033563118008e-05, 'epoch': 0.1590909090909091}\n{'loss': 1.6212, 'grad_norm': 4.680385112762451, 'learning_rate': 9.472205823637677e-05, 'epoch': 0.18181818181818182}\n{'loss': 1.4355, 'grad_norm': 4.382861614227295, 'learning_rate': 0.00010825378084157344, 'epoch': 0.20454545454545456}\n{'loss': 1.3618, 'grad_norm': 3.0457823276519775, 'learning_rate': 0.00012178550344677013, 'epoch': 0.22727272727272727}\n{'loss': 1.2125, 'grad_norm': 2.3325679302215576, 'learning_rate': 0.0001353172260519668, 'epoch': 0.25}\n{'loss': 1.2409, 'grad_norm': 1.9371203184127808, 'learning_rate': 0.0001488489486571635, 'epoch': 0.2727272727272727}\n{'loss': 1.1368, 'grad_norm': 1.9253132343292236, 'learning_rate': 0.00016238067126236015, 'epoch': 0.29545454545454547}\n{'loss': 1.0397, 'grad_norm': 1.6919786930084229, 'learning_rate': 0.00017591239386755683, 'epoch': 0.3181818181818182}\n{'loss': 1.0964, 'grad_norm': 1.863709568977356, 'learning_rate': 0.00018944411647275354, 'epoch': 0.3409090909090909}\n{'loss': 1.0253, 'grad_norm': 0.9914093017578125, 'learning_rate': 0.00020297583907795023, 'epoch': 0.36363636363636365}\n{'loss': 0.9916, 'grad_norm': 1.5932508707046509, 'learning_rate': 0.00021650756168314689, 'epoch': 0.38636363636363635}\n{'loss': 0.9686, 'grad_norm': 2.109127998352051, 'learning_rate': 0.00023003928428834357, 'epoch': 0.4090909090909091}\n{'loss': 0.9329, 'grad_norm': 1.8357274532318115, 'learning_rate': 0.00024357100689354025, 'epoch': 0.4318181818181818}\n{'loss': 0.9013, 'grad_norm': 1.2147777080535889, 'learning_rate': 0.00024202941824231532, 'epoch': 0.45454545454545453}\n{'loss': 0.8416, 'grad_norm': 1.0804569721221924, 'learning_rate': 0.00024048782959109038, 'epoch': 0.4772727272727273}\n{'loss': 0.7396, 'grad_norm': 1.119565725326538, 'learning_rate': 0.00023894624093986544, 'epoch': 0.5}\n{'loss': 0.7307, 'grad_norm': 0.940867006778717, 'learning_rate': 0.0002374046522886405, 'epoch': 0.5227272727272727}\n{'loss': 0.7553, 'grad_norm': 1.0598030090332031, 'learning_rate': 0.00023586306363741557, 'epoch': 0.5454545454545454}\n{'loss': 0.685, 'grad_norm': 1.0750199556350708, 'learning_rate': 0.00023432147498619063, 'epoch': 0.5681818181818182}\n{'loss': 0.5922, 'grad_norm': 1.0585752725601196, 'learning_rate': 0.00023277988633496567, 'epoch': 0.5909090909090909}\n{'loss': 0.6655, 'grad_norm': 0.9900097846984863, 'learning_rate': 0.00023123829768374073, 'epoch': 0.6136363636363636}\n{'loss': 0.5791, 'grad_norm': 0.9197538495063782, 'learning_rate': 0.00022969670903251582, 'epoch': 0.6363636363636364}\n{'loss': 0.673, 'grad_norm': 1.5032416582107544, 'learning_rate': 0.00022815512038129086, 'epoch': 0.6590909090909091}\n{'loss': 0.5953, 'grad_norm': 0.715537965297699, 'learning_rate': 0.00022661353173006592, 'epoch': 0.6818181818181818}\n{'loss': 0.5644, 'grad_norm': 1.1714810132980347, 'learning_rate': 0.00022507194307884101, 'epoch': 0.7045454545454546}\n{'loss': 0.5906, 'grad_norm': 0.8607551455497742, 'learning_rate': 0.00022353035442761605, 'epoch': 0.7272727272727273}\n{'loss': 0.5395, 'grad_norm': 1.6164090633392334, 'learning_rate': 0.0002219887657763911, 'epoch': 0.75}\n{'loss': 0.5343, 'grad_norm': 0.9203586578369141, 'learning_rate': 0.00022044717712516618, 'epoch': 0.7727272727272727}\n{'loss': 0.5022, 'grad_norm': 1.2168855667114258, 'learning_rate': 0.00021890558847394124, 'epoch': 0.7954545454545454}\n{'loss': 0.5068, 'grad_norm': 1.3734911680221558, 'learning_rate': 0.0002173639998227163, 'epoch': 0.8181818181818182}\n{'loss': 0.4798, 'grad_norm': 1.2585241794586182, 'learning_rate': 0.00021582241117149134, 'epoch': 0.8409090909090909}\n{'loss': 0.5058, 'grad_norm': 0.6520668864250183, 'learning_rate': 0.00021428082252026643, 'epoch': 0.8636363636363636}\n{'loss': 0.449, 'grad_norm': 0.9324743151664734, 'learning_rate': 0.0002127392338690415, 'epoch': 0.8863636363636364}\n{'loss': 0.4644, 'grad_norm': 0.860601007938385, 'learning_rate': 0.00021119764521781653, 'epoch': 0.9090909090909091}\n{'loss': 0.5482, 'grad_norm': 1.1967343091964722, 'learning_rate': 0.00020965605656659162, 'epoch': 0.9318181818181818}\n{'loss': 0.4791, 'grad_norm': 0.7649173736572266, 'learning_rate': 0.00020811446791536668, 'epoch': 0.9545454545454546}\n{'loss': 0.4707, 'grad_norm': 0.9345853924751282, 'learning_rate': 0.00020657287926414172, 'epoch': 0.9772727272727273}\n{'loss': 0.3032, 'grad_norm': 2.873323440551758, 'learning_rate': 0.0002050312906129168, 'epoch': 1.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.7731    0.6716    0.7188      3404\n     PERCENT     0.4383    1.0000    0.6094        71\n        TYPE     0.8766    0.9393    0.9068     11194\n      VOLUME     0.3333    0.0179    0.0339        56\n\n   micro avg     0.8515    0.8742    0.8627     14725\n   macro avg     0.6053    0.6572    0.5672     14725\nweighted avg     0.8485    0.8742    0.8586     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.4360354244709015, 'eval_f1_macro': 0.5672359119416556, 'eval_precision': 0.8515480285789891, 'eval_recall': 0.8741595925297114, 'eval_f1': 0.8627056734023658, 'eval_accuracy': 0.8617003782272653, 'eval_runtime': 1.5231, 'eval_samples_per_second': 3618.308, 'eval_steps_per_second': 7.222, 'epoch': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.4118, 'grad_norm': 0.8192799091339111, 'learning_rate': 0.00020348970196169185, 'epoch': 1.0227272727272727}\n{'loss': 0.3676, 'grad_norm': 0.7390214800834656, 'learning_rate': 0.0002019481133104669, 'epoch': 1.0454545454545454}\n{'loss': 0.378, 'grad_norm': 0.808535099029541, 'learning_rate': 0.000200406524659242, 'epoch': 1.0681818181818181}\n{'loss': 0.3557, 'grad_norm': 0.7861117124557495, 'learning_rate': 0.00019886493600801704, 'epoch': 1.0909090909090908}\n{'loss': 0.4157, 'grad_norm': 1.1438212394714355, 'learning_rate': 0.0001973233473567921, 'epoch': 1.1136363636363635}\n{'loss': 0.3119, 'grad_norm': 1.24083411693573, 'learning_rate': 0.00019578175870556716, 'epoch': 1.1363636363636362}\n{'loss': 0.3755, 'grad_norm': 1.2655820846557617, 'learning_rate': 0.00019424017005434223, 'epoch': 1.1590909090909092}\n{'loss': 0.4118, 'grad_norm': 0.912490725517273, 'learning_rate': 0.0001926985814031173, 'epoch': 1.1818181818181819}\n{'loss': 0.3698, 'grad_norm': 0.7838431000709534, 'learning_rate': 0.00019115699275189235, 'epoch': 1.2045454545454546}\n{'loss': 0.408, 'grad_norm': 1.4925576448440552, 'learning_rate': 0.00018961540410066742, 'epoch': 1.2272727272727273}\n{'loss': 0.3829, 'grad_norm': 0.8238720893859863, 'learning_rate': 0.00018807381544944248, 'epoch': 1.25}\n{'loss': 0.3545, 'grad_norm': 2.09798526763916, 'learning_rate': 0.00018653222679821752, 'epoch': 1.2727272727272727}\n{'loss': 0.3027, 'grad_norm': 1.5380420684814453, 'learning_rate': 0.0001849906381469926, 'epoch': 1.2954545454545454}\n{'loss': 0.3304, 'grad_norm': 0.9889216423034668, 'learning_rate': 0.00018344904949576767, 'epoch': 1.3181818181818181}\n{'loss': 0.3814, 'grad_norm': 0.9419398903846741, 'learning_rate': 0.0001819074608445427, 'epoch': 1.3409090909090908}\n{'loss': 0.393, 'grad_norm': 1.5121517181396484, 'learning_rate': 0.00018036587219331777, 'epoch': 1.3636363636363638}\n{'loss': 0.3252, 'grad_norm': 1.663527011871338, 'learning_rate': 0.00017882428354209286, 'epoch': 1.3863636363636362}\n{'loss': 0.3135, 'grad_norm': 1.0334559679031372, 'learning_rate': 0.0001772826948908679, 'epoch': 1.4090909090909092}\n{'loss': 0.3775, 'grad_norm': 0.8845500946044922, 'learning_rate': 0.00017574110623964296, 'epoch': 1.4318181818181819}\n{'loss': 0.3537, 'grad_norm': 1.0142552852630615, 'learning_rate': 0.00017419951758841802, 'epoch': 1.4545454545454546}\n{'loss': 0.3532, 'grad_norm': 2.3876209259033203, 'learning_rate': 0.0001726579289371931, 'epoch': 1.4772727272727273}\n{'loss': 0.3671, 'grad_norm': 2.4468438625335693, 'learning_rate': 0.00017111634028596815, 'epoch': 1.5}\n{'loss': 0.3785, 'grad_norm': 1.0195064544677734, 'learning_rate': 0.00016957475163474321, 'epoch': 1.5227272727272727}\n{'loss': 0.2738, 'grad_norm': 1.2748332023620605, 'learning_rate': 0.00016803316298351828, 'epoch': 1.5454545454545454}\n{'loss': 0.346, 'grad_norm': 2.080904245376587, 'learning_rate': 0.00016649157433229334, 'epoch': 1.5681818181818183}\n{'loss': 0.3254, 'grad_norm': 1.2630596160888672, 'learning_rate': 0.00016494998568106838, 'epoch': 1.5909090909090908}\n{'loss': 0.3149, 'grad_norm': 0.7123517394065857, 'learning_rate': 0.00016340839702984347, 'epoch': 1.6136363636363638}\n{'loss': 0.3343, 'grad_norm': 0.7606217265129089, 'learning_rate': 0.00016186680837861853, 'epoch': 1.6363636363636362}\n{'loss': 0.3382, 'grad_norm': 0.9627371430397034, 'learning_rate': 0.00016032521972739357, 'epoch': 1.6590909090909092}\n{'loss': 0.3253, 'grad_norm': 0.7981459498405457, 'learning_rate': 0.00015878363107616866, 'epoch': 1.6818181818181817}\n{'loss': 0.3035, 'grad_norm': 0.8445058465003967, 'learning_rate': 0.0001572420424249437, 'epoch': 1.7045454545454546}\n{'loss': 0.3418, 'grad_norm': 0.9318151473999023, 'learning_rate': 0.00015570045377371876, 'epoch': 1.7272727272727273}\n{'loss': 0.2847, 'grad_norm': 0.8076028823852539, 'learning_rate': 0.00015415886512249385, 'epoch': 1.75}\n{'loss': 0.2579, 'grad_norm': 0.9325210452079773, 'learning_rate': 0.00015261727647126889, 'epoch': 1.7727272727272727}\n{'loss': 0.3543, 'grad_norm': 1.022711157798767, 'learning_rate': 0.00015107568782004395, 'epoch': 1.7954545454545454}\n{'loss': 0.3185, 'grad_norm': 0.6218895316123962, 'learning_rate': 0.00014953409916881904, 'epoch': 1.8181818181818183}\n{'loss': 0.3575, 'grad_norm': 0.7326783537864685, 'learning_rate': 0.00014799251051759408, 'epoch': 1.8409090909090908}\n{'loss': 0.3087, 'grad_norm': 1.041130781173706, 'learning_rate': 0.00014645092186636914, 'epoch': 1.8636363636363638}\n{'loss': 0.3161, 'grad_norm': 0.8430068492889404, 'learning_rate': 0.0001449093332151442, 'epoch': 1.8863636363636362}\n{'loss': 0.3238, 'grad_norm': 1.4612807035446167, 'learning_rate': 0.00014336774456391927, 'epoch': 1.9090909090909092}\n{'loss': 0.2648, 'grad_norm': 0.5919577479362488, 'learning_rate': 0.00014182615591269433, 'epoch': 1.9318181818181817}\n{'loss': 0.3019, 'grad_norm': 0.8361890316009521, 'learning_rate': 0.00014028456726146937, 'epoch': 1.9545454545454546}\n{'loss': 0.2812, 'grad_norm': 1.4367027282714844, 'learning_rate': 0.00013874297861024446, 'epoch': 1.9772727272727273}\n{'loss': 0.1542, 'grad_norm': 2.552725315093994, 'learning_rate': 0.00013720138995901952, 'epoch': 2.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8245    0.8214    0.8230      3404\n     PERCENT     0.8734    0.9718    0.9200        71\n        TYPE     0.9165    0.9660    0.9406     11194\n      VOLUME     0.6182    0.6071    0.6126        56\n\n   micro avg     0.8949    0.9312    0.9127     14725\n   macro avg     0.8082    0.8416    0.8240     14725\nweighted avg     0.8939    0.9312    0.9120     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.3108997344970703, 'eval_f1_macro': 0.8240396751567755, 'eval_precision': 0.8948639300398095, 'eval_recall': 0.9312054329371816, 'eval_f1': 0.9126730564430245, 'eval_accuracy': 0.9072520966946226, 'eval_runtime': 1.5623, 'eval_samples_per_second': 3527.426, 'eval_steps_per_second': 7.041, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.2291, 'grad_norm': 0.8712170720100403, 'learning_rate': 0.00013565980130779456, 'epoch': 2.022727272727273}\n{'loss': 0.2814, 'grad_norm': 0.6724693775177002, 'learning_rate': 0.00013411821265656965, 'epoch': 2.0454545454545454}\n{'loss': 0.2247, 'grad_norm': 1.110094666481018, 'learning_rate': 0.0001325766240053447, 'epoch': 2.0681818181818183}\n{'loss': 0.2711, 'grad_norm': 0.8871815800666809, 'learning_rate': 0.00013103503535411975, 'epoch': 2.090909090909091}\n{'loss': 0.2617, 'grad_norm': 1.0256028175354004, 'learning_rate': 0.0001294934467028948, 'epoch': 2.1136363636363638}\n{'loss': 0.2611, 'grad_norm': 0.7790181040763855, 'learning_rate': 0.00012795185805166987, 'epoch': 2.1363636363636362}\n{'loss': 0.2492, 'grad_norm': 1.2673789262771606, 'learning_rate': 0.00012641026940044494, 'epoch': 2.159090909090909}\n{'loss': 0.3078, 'grad_norm': 0.7174808979034424, 'learning_rate': 0.00012486868074922, 'epoch': 2.1818181818181817}\n{'loss': 0.2705, 'grad_norm': 1.5413503646850586, 'learning_rate': 0.00012332709209799506, 'epoch': 2.2045454545454546}\n{'loss': 0.2394, 'grad_norm': 1.670477032661438, 'learning_rate': 0.00012178550344677013, 'epoch': 2.227272727272727}\n{'loss': 0.2834, 'grad_norm': 1.4711155891418457, 'learning_rate': 0.00012024391479554519, 'epoch': 2.25}\n{'loss': 0.2539, 'grad_norm': 0.9499964714050293, 'learning_rate': 0.00011870232614432025, 'epoch': 2.2727272727272725}\n{'loss': 0.2125, 'grad_norm': 0.8890696167945862, 'learning_rate': 0.00011716073749309532, 'epoch': 2.2954545454545454}\n{'loss': 0.2121, 'grad_norm': 0.6726983785629272, 'learning_rate': 0.00011561914884187037, 'epoch': 2.3181818181818183}\n{'loss': 0.2161, 'grad_norm': 1.1699726581573486, 'learning_rate': 0.00011407756019064543, 'epoch': 2.340909090909091}\n{'loss': 0.2724, 'grad_norm': 1.5755186080932617, 'learning_rate': 0.00011253597153942051, 'epoch': 2.3636363636363638}\n{'loss': 0.2433, 'grad_norm': 0.7845408916473389, 'learning_rate': 0.00011099438288819556, 'epoch': 2.3863636363636362}\n{'loss': 0.2409, 'grad_norm': 0.7578244805335999, 'learning_rate': 0.00010945279423697062, 'epoch': 2.409090909090909}\n{'loss': 0.2118, 'grad_norm': 0.7146647572517395, 'learning_rate': 0.00010791120558574567, 'epoch': 2.4318181818181817}\n{'loss': 0.2343, 'grad_norm': 0.9876314401626587, 'learning_rate': 0.00010636961693452075, 'epoch': 2.4545454545454546}\n{'loss': 0.2205, 'grad_norm': 1.138563632965088, 'learning_rate': 0.00010482802828329581, 'epoch': 2.4772727272727275}\n{'loss': 0.2134, 'grad_norm': 0.835328221321106, 'learning_rate': 0.00010328643963207086, 'epoch': 2.5}\n{'loss': 0.2339, 'grad_norm': 0.7738116979598999, 'learning_rate': 0.00010174485098084592, 'epoch': 2.5227272727272725}\n{'loss': 0.2176, 'grad_norm': 0.8871990442276001, 'learning_rate': 0.000100203262329621, 'epoch': 2.5454545454545454}\n{'loss': 0.2697, 'grad_norm': 1.228013515472412, 'learning_rate': 9.866167367839605e-05, 'epoch': 2.5681818181818183}\n{'loss': 0.2101, 'grad_norm': 1.4061576128005981, 'learning_rate': 9.712008502717111e-05, 'epoch': 2.590909090909091}\n{'loss': 0.2668, 'grad_norm': 1.0615384578704834, 'learning_rate': 9.557849637594618e-05, 'epoch': 2.6136363636363638}\n{'loss': 0.1754, 'grad_norm': 1.026623249053955, 'learning_rate': 9.403690772472124e-05, 'epoch': 2.6363636363636362}\n{'loss': 0.2003, 'grad_norm': 0.7156203389167786, 'learning_rate': 9.24953190734963e-05, 'epoch': 2.659090909090909}\n{'loss': 0.2089, 'grad_norm': 1.1819326877593994, 'learning_rate': 9.095373042227135e-05, 'epoch': 2.6818181818181817}\n{'loss': 0.2534, 'grad_norm': 0.7255541086196899, 'learning_rate': 8.941214177104643e-05, 'epoch': 2.7045454545454546}\n{'loss': 0.1903, 'grad_norm': 1.208275318145752, 'learning_rate': 8.787055311982148e-05, 'epoch': 2.7272727272727275}\n{'loss': 0.2487, 'grad_norm': 0.9356756210327148, 'learning_rate': 8.632896446859654e-05, 'epoch': 2.75}\n{'loss': 0.2562, 'grad_norm': 0.6840094327926636, 'learning_rate': 8.478737581737161e-05, 'epoch': 2.7727272727272725}\n{'loss': 0.2097, 'grad_norm': 0.8351050615310669, 'learning_rate': 8.324578716614667e-05, 'epoch': 2.7954545454545454}\n{'loss': 0.2277, 'grad_norm': 1.2438148260116577, 'learning_rate': 8.170419851492173e-05, 'epoch': 2.8181818181818183}\n{'loss': 0.2347, 'grad_norm': 1.2317413091659546, 'learning_rate': 8.016260986369678e-05, 'epoch': 2.840909090909091}\n{'loss': 0.2602, 'grad_norm': 1.1098525524139404, 'learning_rate': 7.862102121247185e-05, 'epoch': 2.8636363636363638}\n{'loss': 0.2434, 'grad_norm': 0.7384714484214783, 'learning_rate': 7.707943256124692e-05, 'epoch': 2.8863636363636362}\n{'loss': 0.1848, 'grad_norm': 1.185400366783142, 'learning_rate': 7.553784391002197e-05, 'epoch': 2.909090909090909}\n{'loss': 0.2024, 'grad_norm': 2.026926279067993, 'learning_rate': 7.399625525879704e-05, 'epoch': 2.9318181818181817}\n{'loss': 0.2131, 'grad_norm': 0.7538588047027588, 'learning_rate': 7.24546666075721e-05, 'epoch': 2.9545454545454546}\n{'loss': 0.2089, 'grad_norm': 1.5857959985733032, 'learning_rate': 7.091307795634716e-05, 'epoch': 2.9772727272727275}\n{'loss': 0.121, 'grad_norm': 2.175118923187256, 'learning_rate': 6.937148930512223e-05, 'epoch': 3.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8769    0.8311    0.8534      3404\n     PERCENT     0.9701    0.9155    0.9420        71\n        TYPE     0.9246    0.9701    0.9468     11194\n      VOLUME     0.6607    0.6607    0.6607        56\n\n   micro avg     0.9137    0.9365    0.9249     14725\n   macro avg     0.8581    0.8443    0.8507     14725\nweighted avg     0.9128    0.9365    0.9241     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.2885679602622986, 'eval_f1_macro': 0.8507375212167824, 'eval_precision': 0.9136685880871928, 'eval_recall': 0.9365025466893039, 'eval_f1': 0.9249446642967335, 'eval_accuracy': 0.9173381571013539, 'eval_runtime': 1.4812, 'eval_samples_per_second': 3720.583, 'eval_steps_per_second': 7.426, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.1973, 'grad_norm': 0.9860126376152039, 'learning_rate': 6.782990065389728e-05, 'epoch': 3.022727272727273}\n{'loss': 0.2099, 'grad_norm': 1.1178200244903564, 'learning_rate': 6.628831200267235e-05, 'epoch': 3.0454545454545454}\n{'loss': 0.2131, 'grad_norm': 1.2287174463272095, 'learning_rate': 6.47467233514474e-05, 'epoch': 3.0681818181818183}\n{'loss': 0.1722, 'grad_norm': 1.119145393371582, 'learning_rate': 6.320513470022247e-05, 'epoch': 3.090909090909091}\n{'loss': 0.165, 'grad_norm': 0.8844251036643982, 'learning_rate': 6.166354604899753e-05, 'epoch': 3.1136363636363638}\n{'loss': 0.2298, 'grad_norm': 1.1267306804656982, 'learning_rate': 6.0121957397772595e-05, 'epoch': 3.1363636363636362}\n{'loss': 0.2263, 'grad_norm': 0.8673144578933716, 'learning_rate': 5.858036874654766e-05, 'epoch': 3.159090909090909}\n{'loss': 0.1903, 'grad_norm': 0.7704402208328247, 'learning_rate': 5.7038780095322715e-05, 'epoch': 3.1818181818181817}\n{'loss': 0.2065, 'grad_norm': 1.358937382698059, 'learning_rate': 5.549719144409778e-05, 'epoch': 3.2045454545454546}\n{'loss': 0.1993, 'grad_norm': 0.8379549384117126, 'learning_rate': 5.3955602792872835e-05, 'epoch': 3.227272727272727}\n{'loss': 0.1649, 'grad_norm': 2.175975799560547, 'learning_rate': 5.2414014141647905e-05, 'epoch': 3.25}\n{'loss': 0.1859, 'grad_norm': 1.5547070503234863, 'learning_rate': 5.087242549042296e-05, 'epoch': 3.2727272727272725}\n{'loss': 0.2006, 'grad_norm': 1.3810408115386963, 'learning_rate': 4.9330836839198025e-05, 'epoch': 3.2954545454545454}\n{'loss': 0.2151, 'grad_norm': 0.9860113263130188, 'learning_rate': 4.778924818797309e-05, 'epoch': 3.3181818181818183}\n{'loss': 0.1773, 'grad_norm': 1.285995364189148, 'learning_rate': 4.624765953674815e-05, 'epoch': 3.340909090909091}\n{'loss': 0.1855, 'grad_norm': 0.8127644062042236, 'learning_rate': 4.4706070885523215e-05, 'epoch': 3.3636363636363638}\n{'loss': 0.2212, 'grad_norm': 1.706342339515686, 'learning_rate': 4.316448223429827e-05, 'epoch': 3.3863636363636362}\n{'loss': 0.1861, 'grad_norm': 1.1722408533096313, 'learning_rate': 4.1622893583073335e-05, 'epoch': 3.409090909090909}\n{'loss': 0.1986, 'grad_norm': 1.0614643096923828, 'learning_rate': 4.008130493184839e-05, 'epoch': 3.4318181818181817}\n{'loss': 0.1718, 'grad_norm': 0.7331863045692444, 'learning_rate': 3.853971628062346e-05, 'epoch': 3.4545454545454546}\n{'loss': 0.2023, 'grad_norm': 1.46330726146698, 'learning_rate': 3.699812762939852e-05, 'epoch': 3.4772727272727275}\n{'loss': 0.1461, 'grad_norm': 0.8146343231201172, 'learning_rate': 3.545653897817358e-05, 'epoch': 3.5}\n{'loss': 0.2068, 'grad_norm': 0.8601053953170776, 'learning_rate': 3.391495032694864e-05, 'epoch': 3.5227272727272725}\n{'loss': 0.2127, 'grad_norm': 0.8608089685440063, 'learning_rate': 3.23733616757237e-05, 'epoch': 3.5454545454545454}\n{'loss': 0.2392, 'grad_norm': 1.246675968170166, 'learning_rate': 3.0831773024498766e-05, 'epoch': 3.5681818181818183}\n{'loss': 0.2151, 'grad_norm': 1.4750854969024658, 'learning_rate': 2.929018437327383e-05, 'epoch': 3.590909090909091}\n{'loss': 0.1873, 'grad_norm': 0.8596183657646179, 'learning_rate': 2.774859572204889e-05, 'epoch': 3.6136363636363638}\n{'loss': 0.1895, 'grad_norm': 0.7908536195755005, 'learning_rate': 2.6207007070823953e-05, 'epoch': 3.6363636363636362}\n{'loss': 0.1714, 'grad_norm': 0.6440073847770691, 'learning_rate': 2.4665418419599013e-05, 'epoch': 3.659090909090909}\n{'loss': 0.1993, 'grad_norm': 0.900364100933075, 'learning_rate': 2.3123829768374076e-05, 'epoch': 3.6818181818181817}\n{'loss': 0.1989, 'grad_norm': 1.0209167003631592, 'learning_rate': 2.1582241117149136e-05, 'epoch': 3.7045454545454546}\n{'loss': 0.1851, 'grad_norm': 0.7503256797790527, 'learning_rate': 2.0040652465924196e-05, 'epoch': 3.7272727272727275}\n{'loss': 0.1741, 'grad_norm': 0.926318347454071, 'learning_rate': 1.849906381469926e-05, 'epoch': 3.75}\n{'loss': 0.1812, 'grad_norm': 0.8522571325302124, 'learning_rate': 1.695747516347432e-05, 'epoch': 3.7727272727272725}\n{'loss': 0.1587, 'grad_norm': 0.6695502996444702, 'learning_rate': 1.5415886512249383e-05, 'epoch': 3.7954545454545454}\n{'loss': 0.1821, 'grad_norm': 0.7225543260574341, 'learning_rate': 1.3874297861024445e-05, 'epoch': 3.8181818181818183}\n{'loss': 0.1768, 'grad_norm': 2.2982065677642822, 'learning_rate': 1.2332709209799506e-05, 'epoch': 3.840909090909091}\n{'loss': 0.2141, 'grad_norm': 1.5557571649551392, 'learning_rate': 1.0791120558574568e-05, 'epoch': 3.8636363636363638}\n{'loss': 0.1845, 'grad_norm': 0.8735235929489136, 'learning_rate': 9.24953190734963e-06, 'epoch': 3.8863636363636362}\n{'loss': 0.1725, 'grad_norm': 0.6735549569129944, 'learning_rate': 7.707943256124691e-06, 'epoch': 3.909090909090909}\n{'loss': 0.1798, 'grad_norm': 0.8991141319274902, 'learning_rate': 6.166354604899753e-06, 'epoch': 3.9318181818181817}\n{'loss': 0.1887, 'grad_norm': 0.9573186039924622, 'learning_rate': 4.624765953674815e-06, 'epoch': 3.9545454545454546}\n{'loss': 0.1727, 'grad_norm': 0.8277694582939148, 'learning_rate': 3.0831773024498766e-06, 'epoch': 3.9772727272727275}\n{'loss': 0.2342, 'grad_norm': 2.4746644496917725, 'learning_rate': 1.5415886512249383e-06, 'epoch': 4.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8661    0.8587    0.8624      3404\n     PERCENT     0.9851    0.9296    0.9565        71\n        TYPE     0.9322    0.9694    0.9504     11194\n      VOLUME     0.6842    0.6964    0.6903        56\n\n   micro avg     0.9168    0.9425    0.9295     14725\n   macro avg     0.8669    0.8635    0.8649     14725\nweighted avg     0.9162    0.9425    0.9291     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.27838265895843506, 'eval_f1_macro': 0.8648952779891683, 'eval_precision': 0.9167712530550235, 'eval_recall': 0.942546689303905, 'eval_f1': 0.9294803107420305, 'eval_accuracy': 0.9217782163021433, 'eval_runtime': 1.5013, 'eval_samples_per_second': 3670.772, 'eval_steps_per_second': 7.327, 'epoch': 4.0}\n{'train_runtime': 25.0756, 'train_samples_per_second': 3515.926, 'train_steps_per_second': 7.019, 'train_loss': 0.4327441702427512, 'epoch': 4.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nThe speedups for torchdynamo mostly come with GPU Ampere or higher and which is not detected here.\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8661    0.8587    0.8624      3404\n     PERCENT     0.9851    0.9296    0.9565        71\n        TYPE     0.9322    0.9694    0.9504     11194\n      VOLUME     0.6842    0.6964    0.6903        56\n\n   micro avg     0.9168    0.9425    0.9295     14725\n   macro avg     0.8669    0.8635    0.8649     14725\nweighted avg     0.9162    0.9425    0.9291     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.27838265895843506, 'eval_f1_macro': 0.8648952779891683, 'eval_precision': 0.9167712530550235, 'eval_recall': 0.942546689303905, 'eval_f1': 0.9294803107420305, 'eval_accuracy': 0.9217782163021433, 'eval_runtime': 1.647, 'eval_samples_per_second': 3346.089, 'eval_steps_per_second': 6.679, 'epoch': 4.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/3364797558.py:66: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 2.1859, 'grad_norm': 6.638328552246094, 'learning_rate': 0.0, 'epoch': 0.022727272727272728}\n{'loss': 2.193, 'grad_norm': 6.709334850311279, 'learning_rate': 1.353172260519668e-05, 'epoch': 0.045454545454545456}\n{'loss': 2.1704, 'grad_norm': 6.6725993156433105, 'learning_rate': 2.706344521039336e-05, 'epoch': 0.06818181818181818}\n{'loss': 2.1072, 'grad_norm': 6.3523125648498535, 'learning_rate': 4.059516781559004e-05, 'epoch': 0.09090909090909091}\n{'loss': 2.036, 'grad_norm': 6.251642227172852, 'learning_rate': 5.412689042078672e-05, 'epoch': 0.11363636363636363}\n{'loss': 1.9228, 'grad_norm': 6.361886024475098, 'learning_rate': 6.76586130259834e-05, 'epoch': 0.13636363636363635}\n{'loss': 1.8409, 'grad_norm': 5.41917085647583, 'learning_rate': 8.119033563118008e-05, 'epoch': 0.1590909090909091}\n{'loss': 1.7127, 'grad_norm': 5.290202617645264, 'learning_rate': 9.472205823637677e-05, 'epoch': 0.18181818181818182}\n{'loss': 1.5804, 'grad_norm': 4.725815296173096, 'learning_rate': 0.00010825378084157344, 'epoch': 0.20454545454545456}\n{'loss': 1.4583, 'grad_norm': 3.9513280391693115, 'learning_rate': 0.00012178550344677013, 'epoch': 0.22727272727272727}\n{'loss': 1.3837, 'grad_norm': 2.9890289306640625, 'learning_rate': 0.0001353172260519668, 'epoch': 0.25}\n{'loss': 1.2911, 'grad_norm': 2.2628414630889893, 'learning_rate': 0.0001488489486571635, 'epoch': 0.2727272727272727}\n{'loss': 1.1863, 'grad_norm': 1.9543393850326538, 'learning_rate': 0.00016238067126236015, 'epoch': 0.29545454545454547}\n{'loss': 1.1459, 'grad_norm': 2.0808911323547363, 'learning_rate': 0.00017591239386755683, 'epoch': 0.3181818181818182}\n{'loss': 1.133, 'grad_norm': 2.2216312885284424, 'learning_rate': 0.00018944411647275354, 'epoch': 0.3409090909090909}\n{'loss': 0.9976, 'grad_norm': 1.1331521272659302, 'learning_rate': 0.00020297583907795023, 'epoch': 0.36363636363636365}\n{'loss': 0.9724, 'grad_norm': 1.403761386871338, 'learning_rate': 0.00021650756168314689, 'epoch': 0.38636363636363635}\n{'loss': 0.9377, 'grad_norm': 2.372283935546875, 'learning_rate': 0.00023003928428834357, 'epoch': 0.4090909090909091}\n{'loss': 0.9049, 'grad_norm': 2.3678414821624756, 'learning_rate': 0.00024357100689354025, 'epoch': 0.4318181818181818}\n{'loss': 0.8944, 'grad_norm': 1.5354784727096558, 'learning_rate': 0.00024202941824231532, 'epoch': 0.45454545454545453}\n{'loss': 0.8208, 'grad_norm': 0.9366535544395447, 'learning_rate': 0.00024048782959109038, 'epoch': 0.4772727272727273}\n{'loss': 0.8027, 'grad_norm': 1.2105460166931152, 'learning_rate': 0.00023894624093986544, 'epoch': 0.5}\n{'loss': 0.7372, 'grad_norm': 1.1120152473449707, 'learning_rate': 0.0002374046522886405, 'epoch': 0.5227272727272727}\n{'loss': 0.675, 'grad_norm': 0.6957074403762817, 'learning_rate': 0.00023586306363741557, 'epoch': 0.5454545454545454}\n{'loss': 0.6483, 'grad_norm': 1.1188337802886963, 'learning_rate': 0.00023432147498619063, 'epoch': 0.5681818181818182}\n{'loss': 0.6899, 'grad_norm': 0.9170272946357727, 'learning_rate': 0.00023277988633496567, 'epoch': 0.5909090909090909}\n{'loss': 0.6899, 'grad_norm': 0.7582008838653564, 'learning_rate': 0.00023123829768374073, 'epoch': 0.6136363636363636}\n{'loss': 0.7008, 'grad_norm': 1.4006540775299072, 'learning_rate': 0.00022969670903251582, 'epoch': 0.6363636363636364}\n{'loss': 0.5446, 'grad_norm': 0.7523611783981323, 'learning_rate': 0.00022815512038129086, 'epoch': 0.6590909090909091}\n{'loss': 0.5978, 'grad_norm': 1.4647153615951538, 'learning_rate': 0.00022661353173006592, 'epoch': 0.6818181818181818}\n{'loss': 0.6081, 'grad_norm': 1.0909779071807861, 'learning_rate': 0.00022507194307884101, 'epoch': 0.7045454545454546}\n{'loss': 0.6344, 'grad_norm': 0.8944599628448486, 'learning_rate': 0.00022353035442761605, 'epoch': 0.7272727272727273}\n{'loss': 0.5925, 'grad_norm': 2.447563648223877, 'learning_rate': 0.0002219887657763911, 'epoch': 0.75}\n{'loss': 0.554, 'grad_norm': 2.345468282699585, 'learning_rate': 0.00022044717712516618, 'epoch': 0.7727272727272727}\n{'loss': 0.6279, 'grad_norm': 2.043818235397339, 'learning_rate': 0.00021890558847394124, 'epoch': 0.7954545454545454}\n{'loss': 0.5685, 'grad_norm': 0.8282990455627441, 'learning_rate': 0.0002173639998227163, 'epoch': 0.8181818181818182}\n{'loss': 0.4636, 'grad_norm': 1.0121967792510986, 'learning_rate': 0.00021582241117149134, 'epoch': 0.8409090909090909}\n{'loss': 0.5162, 'grad_norm': 0.9017302989959717, 'learning_rate': 0.00021428082252026643, 'epoch': 0.8636363636363636}\n{'loss': 0.4982, 'grad_norm': 1.3632330894470215, 'learning_rate': 0.0002127392338690415, 'epoch': 0.8863636363636364}\n{'loss': 0.4563, 'grad_norm': 0.8494128584861755, 'learning_rate': 0.00021119764521781653, 'epoch': 0.9090909090909091}\n{'loss': 0.4282, 'grad_norm': 0.7602680921554565, 'learning_rate': 0.00020965605656659162, 'epoch': 0.9318181818181818}\n{'loss': 0.4523, 'grad_norm': 0.9755218625068665, 'learning_rate': 0.00020811446791536668, 'epoch': 0.9545454545454546}\n{'loss': 0.4739, 'grad_norm': 0.6579062938690186, 'learning_rate': 0.00020657287926414172, 'epoch': 0.9772727272727273}\n{'loss': 0.4796, 'grad_norm': 2.8730883598327637, 'learning_rate': 0.0002050312906129168, 'epoch': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.7668    0.7469    0.7567      3311\n     PERCENT     0.4847    0.9186    0.6345        86\n        TYPE     0.8917    0.9428    0.9165     11299\n      VOLUME     0.0000    0.0000    0.0000        42\n\n   micro avg     0.8611    0.8960    0.8782     14738\n   macro avg     0.5358    0.6521    0.5770     14738\nweighted avg     0.8587    0.8960    0.8764     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.4137490689754486, 'eval_f1_macro': 0.5769537201632084, 'eval_precision': 0.8611020541245517, 'eval_recall': 0.8959831727507125, 'eval_f1': 0.8781963887872843, 'eval_accuracy': 0.8744330910879187, 'eval_runtime': 1.4828, 'eval_samples_per_second': 3715.995, 'eval_steps_per_second': 7.419, 'epoch': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.3985, 'grad_norm': 1.094394326210022, 'learning_rate': 0.00020348970196169185, 'epoch': 1.0227272727272727}\n{'loss': 0.4213, 'grad_norm': 0.9626965522766113, 'learning_rate': 0.0002019481133104669, 'epoch': 1.0454545454545454}\n{'loss': 0.4266, 'grad_norm': 1.3158237934112549, 'learning_rate': 0.000200406524659242, 'epoch': 1.0681818181818181}\n{'loss': 0.3949, 'grad_norm': 0.8581278920173645, 'learning_rate': 0.00019886493600801704, 'epoch': 1.0909090909090908}\n{'loss': 0.3949, 'grad_norm': 1.508475661277771, 'learning_rate': 0.0001973233473567921, 'epoch': 1.1136363636363635}\n{'loss': 0.4296, 'grad_norm': 1.1065223217010498, 'learning_rate': 0.00019578175870556716, 'epoch': 1.1363636363636362}\n{'loss': 0.3304, 'grad_norm': 1.125667691230774, 'learning_rate': 0.00019424017005434223, 'epoch': 1.1590909090909092}\n{'loss': 0.3718, 'grad_norm': 0.7879140973091125, 'learning_rate': 0.0001926985814031173, 'epoch': 1.1818181818181819}\n{'loss': 0.365, 'grad_norm': 0.7609970569610596, 'learning_rate': 0.00019115699275189235, 'epoch': 1.2045454545454546}\n{'loss': 0.4056, 'grad_norm': 0.8877059817314148, 'learning_rate': 0.00018961540410066742, 'epoch': 1.2272727272727273}\n{'loss': 0.391, 'grad_norm': 1.0639389753341675, 'learning_rate': 0.00018807381544944248, 'epoch': 1.25}\n{'loss': 0.3416, 'grad_norm': 0.7398760318756104, 'learning_rate': 0.00018653222679821752, 'epoch': 1.2727272727272727}\n{'loss': 0.3432, 'grad_norm': 1.0476285219192505, 'learning_rate': 0.0001849906381469926, 'epoch': 1.2954545454545454}\n{'loss': 0.3935, 'grad_norm': 0.7719459533691406, 'learning_rate': 0.00018344904949576767, 'epoch': 1.3181818181818181}\n{'loss': 0.3122, 'grad_norm': 1.7477800846099854, 'learning_rate': 0.0001819074608445427, 'epoch': 1.3409090909090908}\n{'loss': 0.3753, 'grad_norm': 0.8934581875801086, 'learning_rate': 0.00018036587219331777, 'epoch': 1.3636363636363638}\n{'loss': 0.3483, 'grad_norm': 1.6530482769012451, 'learning_rate': 0.00017882428354209286, 'epoch': 1.3863636363636362}\n{'loss': 0.365, 'grad_norm': 0.922630786895752, 'learning_rate': 0.0001772826948908679, 'epoch': 1.4090909090909092}\n{'loss': 0.3209, 'grad_norm': 0.6741411685943604, 'learning_rate': 0.00017574110623964296, 'epoch': 1.4318181818181819}\n{'loss': 0.3213, 'grad_norm': 0.9035417437553406, 'learning_rate': 0.00017419951758841802, 'epoch': 1.4545454545454546}\n{'loss': 0.2906, 'grad_norm': 1.178689956665039, 'learning_rate': 0.0001726579289371931, 'epoch': 1.4772727272727273}\n{'loss': 0.3464, 'grad_norm': 1.0725833177566528, 'learning_rate': 0.00017111634028596815, 'epoch': 1.5}\n{'loss': 0.3234, 'grad_norm': 1.1587011814117432, 'learning_rate': 0.00016957475163474321, 'epoch': 1.5227272727272727}\n{'loss': 0.2963, 'grad_norm': 1.2516584396362305, 'learning_rate': 0.00016803316298351828, 'epoch': 1.5454545454545454}\n{'loss': 0.2968, 'grad_norm': 1.0494070053100586, 'learning_rate': 0.00016649157433229334, 'epoch': 1.5681818181818183}\n{'loss': 0.3057, 'grad_norm': 0.659223735332489, 'learning_rate': 0.00016494998568106838, 'epoch': 1.5909090909090908}\n{'loss': 0.3061, 'grad_norm': 1.6587716341018677, 'learning_rate': 0.00016340839702984347, 'epoch': 1.6136363636363638}\n{'loss': 0.3367, 'grad_norm': 1.4056037664413452, 'learning_rate': 0.00016186680837861853, 'epoch': 1.6363636363636362}\n{'loss': 0.3041, 'grad_norm': 0.7682608366012573, 'learning_rate': 0.00016032521972739357, 'epoch': 1.6590909090909092}\n{'loss': 0.3276, 'grad_norm': 0.9229142069816589, 'learning_rate': 0.00015878363107616866, 'epoch': 1.6818181818181817}\n{'loss': 0.3068, 'grad_norm': 1.2791773080825806, 'learning_rate': 0.0001572420424249437, 'epoch': 1.7045454545454546}\n{'loss': 0.3086, 'grad_norm': 0.7789873480796814, 'learning_rate': 0.00015570045377371876, 'epoch': 1.7272727272727273}\n{'loss': 0.3077, 'grad_norm': 1.1953201293945312, 'learning_rate': 0.00015415886512249385, 'epoch': 1.75}\n{'loss': 0.3152, 'grad_norm': 0.846812903881073, 'learning_rate': 0.00015261727647126889, 'epoch': 1.7727272727272727}\n{'loss': 0.2659, 'grad_norm': 0.930529773235321, 'learning_rate': 0.00015107568782004395, 'epoch': 1.7954545454545454}\n{'loss': 0.3028, 'grad_norm': 0.7893833518028259, 'learning_rate': 0.00014953409916881904, 'epoch': 1.8181818181818183}\n{'loss': 0.2564, 'grad_norm': 0.7787051200866699, 'learning_rate': 0.00014799251051759408, 'epoch': 1.8409090909090908}\n{'loss': 0.3152, 'grad_norm': 0.8683134913444519, 'learning_rate': 0.00014645092186636914, 'epoch': 1.8636363636363638}\n{'loss': 0.3205, 'grad_norm': 0.9320837259292603, 'learning_rate': 0.0001449093332151442, 'epoch': 1.8863636363636362}\n{'loss': 0.3233, 'grad_norm': 1.135690450668335, 'learning_rate': 0.00014336774456391927, 'epoch': 1.9090909090909092}\n{'loss': 0.3642, 'grad_norm': 0.905340313911438, 'learning_rate': 0.00014182615591269433, 'epoch': 1.9318181818181817}\n{'loss': 0.3157, 'grad_norm': 0.7817673087120056, 'learning_rate': 0.00014028456726146937, 'epoch': 1.9545454545454546}\n{'loss': 0.2623, 'grad_norm': 0.9140432476997375, 'learning_rate': 0.00013874297861024446, 'epoch': 1.9772727272727273}\n{'loss': 0.2597, 'grad_norm': 3.691474437713623, 'learning_rate': 0.00013720138995901952, 'epoch': 2.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8523    0.8505    0.8514      3311\n     PERCENT     0.7130    0.8953    0.7938        86\n        TYPE     0.9303    0.9624    0.9461     11299\n      VOLUME     0.5000    0.4048    0.4474        42\n\n   micro avg     0.9107    0.9353    0.9228     14738\n   macro avg     0.7489    0.7782    0.7597     14738\nweighted avg     0.9103    0.9353    0.9225     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.28742069005966187, 'eval_f1_macro': 0.7596600011122093, 'eval_precision': 0.9107367030062768, 'eval_recall': 0.9352693716922241, 'eval_f1': 0.9228400227630301, 'eval_accuracy': 0.9160155182776898, 'eval_runtime': 1.4792, 'eval_samples_per_second': 3724.908, 'eval_steps_per_second': 7.436, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.2288, 'grad_norm': 0.81313556432724, 'learning_rate': 0.00013565980130779456, 'epoch': 2.022727272727273}\n{'loss': 0.2312, 'grad_norm': 0.9615729451179504, 'learning_rate': 0.00013411821265656965, 'epoch': 2.0454545454545454}\n{'loss': 0.2669, 'grad_norm': 1.0015869140625, 'learning_rate': 0.0001325766240053447, 'epoch': 2.0681818181818183}\n{'loss': 0.2877, 'grad_norm': 1.2211486101150513, 'learning_rate': 0.00013103503535411975, 'epoch': 2.090909090909091}\n{'loss': 0.2434, 'grad_norm': 0.813378095626831, 'learning_rate': 0.0001294934467028948, 'epoch': 2.1136363636363638}\n{'loss': 0.1825, 'grad_norm': 0.6197338104248047, 'learning_rate': 0.00012795185805166987, 'epoch': 2.1363636363636362}\n{'loss': 0.2395, 'grad_norm': 0.7687793970108032, 'learning_rate': 0.00012641026940044494, 'epoch': 2.159090909090909}\n{'loss': 0.2559, 'grad_norm': 0.9889585375785828, 'learning_rate': 0.00012486868074922, 'epoch': 2.1818181818181817}\n{'loss': 0.2721, 'grad_norm': 1.2437198162078857, 'learning_rate': 0.00012332709209799506, 'epoch': 2.2045454545454546}\n{'loss': 0.2861, 'grad_norm': 1.021931529045105, 'learning_rate': 0.00012178550344677013, 'epoch': 2.227272727272727}\n{'loss': 0.2716, 'grad_norm': 0.8507551550865173, 'learning_rate': 0.00012024391479554519, 'epoch': 2.25}\n{'loss': 0.2081, 'grad_norm': 1.0232102870941162, 'learning_rate': 0.00011870232614432025, 'epoch': 2.2727272727272725}\n{'loss': 0.2828, 'grad_norm': 0.8620778918266296, 'learning_rate': 0.00011716073749309532, 'epoch': 2.2954545454545454}\n{'loss': 0.2708, 'grad_norm': 1.3238539695739746, 'learning_rate': 0.00011561914884187037, 'epoch': 2.3181818181818183}\n{'loss': 0.1779, 'grad_norm': 1.1639000177383423, 'learning_rate': 0.00011407756019064543, 'epoch': 2.340909090909091}\n{'loss': 0.2243, 'grad_norm': 0.8110899329185486, 'learning_rate': 0.00011253597153942051, 'epoch': 2.3636363636363638}\n{'loss': 0.2098, 'grad_norm': 0.7724423408508301, 'learning_rate': 0.00011099438288819556, 'epoch': 2.3863636363636362}\n{'loss': 0.2914, 'grad_norm': 1.261498212814331, 'learning_rate': 0.00010945279423697062, 'epoch': 2.409090909090909}\n{'loss': 0.2434, 'grad_norm': 1.1220917701721191, 'learning_rate': 0.00010791120558574567, 'epoch': 2.4318181818181817}\n{'loss': 0.187, 'grad_norm': 1.217252254486084, 'learning_rate': 0.00010636961693452075, 'epoch': 2.4545454545454546}\n{'loss': 0.2187, 'grad_norm': 1.0265305042266846, 'learning_rate': 0.00010482802828329581, 'epoch': 2.4772727272727275}\n{'loss': 0.1923, 'grad_norm': 1.5761061906814575, 'learning_rate': 0.00010328643963207086, 'epoch': 2.5}\n{'loss': 0.2137, 'grad_norm': 0.9994041323661804, 'learning_rate': 0.00010174485098084592, 'epoch': 2.5227272727272725}\n{'loss': 0.2121, 'grad_norm': 0.6809350252151489, 'learning_rate': 0.000100203262329621, 'epoch': 2.5454545454545454}\n{'loss': 0.2188, 'grad_norm': 1.3400955200195312, 'learning_rate': 9.866167367839605e-05, 'epoch': 2.5681818181818183}\n{'loss': 0.2362, 'grad_norm': 1.4238688945770264, 'learning_rate': 9.712008502717111e-05, 'epoch': 2.590909090909091}\n{'loss': 0.2317, 'grad_norm': 0.8730016946792603, 'learning_rate': 9.557849637594618e-05, 'epoch': 2.6136363636363638}\n{'loss': 0.2658, 'grad_norm': 0.9247503280639648, 'learning_rate': 9.403690772472124e-05, 'epoch': 2.6363636363636362}\n{'loss': 0.2103, 'grad_norm': 0.6304610371589661, 'learning_rate': 9.24953190734963e-05, 'epoch': 2.659090909090909}\n{'loss': 0.1683, 'grad_norm': 0.6696241497993469, 'learning_rate': 9.095373042227135e-05, 'epoch': 2.6818181818181817}\n{'loss': 0.1887, 'grad_norm': 0.6673873662948608, 'learning_rate': 8.941214177104643e-05, 'epoch': 2.7045454545454546}\n{'loss': 0.2831, 'grad_norm': 0.9388028979301453, 'learning_rate': 8.787055311982148e-05, 'epoch': 2.7272727272727275}\n{'loss': 0.2324, 'grad_norm': 1.1790871620178223, 'learning_rate': 8.632896446859654e-05, 'epoch': 2.75}\n{'loss': 0.2379, 'grad_norm': 1.6328773498535156, 'learning_rate': 8.478737581737161e-05, 'epoch': 2.7727272727272725}\n{'loss': 0.2092, 'grad_norm': 0.7764594554901123, 'learning_rate': 8.324578716614667e-05, 'epoch': 2.7954545454545454}\n{'loss': 0.2398, 'grad_norm': 1.0741995573043823, 'learning_rate': 8.170419851492173e-05, 'epoch': 2.8181818181818183}\n{'loss': 0.2609, 'grad_norm': 1.1954982280731201, 'learning_rate': 8.016260986369678e-05, 'epoch': 2.840909090909091}\n{'loss': 0.1932, 'grad_norm': 1.0925179719924927, 'learning_rate': 7.862102121247185e-05, 'epoch': 2.8636363636363638}\n{'loss': 0.2385, 'grad_norm': 1.5710748434066772, 'learning_rate': 7.707943256124692e-05, 'epoch': 2.8863636363636362}\n{'loss': 0.2105, 'grad_norm': 0.7802397012710571, 'learning_rate': 7.553784391002197e-05, 'epoch': 2.909090909090909}\n{'loss': 0.2257, 'grad_norm': 0.771641194820404, 'learning_rate': 7.399625525879704e-05, 'epoch': 2.9318181818181817}\n{'loss': 0.2301, 'grad_norm': 0.875029981136322, 'learning_rate': 7.24546666075721e-05, 'epoch': 2.9545454545454546}\n{'loss': 0.2383, 'grad_norm': 0.7965391874313354, 'learning_rate': 7.091307795634716e-05, 'epoch': 2.9772727272727275}\n{'loss': 0.1394, 'grad_norm': 2.098515272140503, 'learning_rate': 6.937148930512223e-05, 'epoch': 3.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8664    0.8698    0.8681      3311\n     PERCENT     0.8352    0.8837    0.8588        86\n        TYPE     0.9385    0.9662    0.9522     11299\n      VOLUME     0.6522    0.7143    0.6818        42\n\n   micro avg     0.9212    0.9433    0.9321     14738\n   macro avg     0.8231    0.8585    0.8402     14738\nweighted avg     0.9209    0.9433    0.9320     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.25876346230506897, 'eval_f1_macro': 0.8402149153301364, 'eval_precision': 0.9211555025508514, 'eval_recall': 0.9433437372777853, 'eval_f1': 0.9321175957896148, 'eval_accuracy': 0.9240478662368177, 'eval_runtime': 1.4735, 'eval_samples_per_second': 3739.336, 'eval_steps_per_second': 7.465, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.159, 'grad_norm': 0.778001070022583, 'learning_rate': 6.782990065389728e-05, 'epoch': 3.022727272727273}\n{'loss': 0.2098, 'grad_norm': 0.7927142381668091, 'learning_rate': 6.628831200267235e-05, 'epoch': 3.0454545454545454}\n{'loss': 0.184, 'grad_norm': 0.6575337648391724, 'learning_rate': 6.47467233514474e-05, 'epoch': 3.0681818181818183}\n{'loss': 0.1926, 'grad_norm': 1.0427777767181396, 'learning_rate': 6.320513470022247e-05, 'epoch': 3.090909090909091}\n{'loss': 0.2025, 'grad_norm': 1.1496652364730835, 'learning_rate': 6.166354604899753e-05, 'epoch': 3.1136363636363638}\n{'loss': 0.1583, 'grad_norm': 0.9500421285629272, 'learning_rate': 6.0121957397772595e-05, 'epoch': 3.1363636363636362}\n{'loss': 0.2423, 'grad_norm': 1.1624187231063843, 'learning_rate': 5.858036874654766e-05, 'epoch': 3.159090909090909}\n{'loss': 0.1528, 'grad_norm': 0.7121138572692871, 'learning_rate': 5.7038780095322715e-05, 'epoch': 3.1818181818181817}\n{'loss': 0.2013, 'grad_norm': 1.079443097114563, 'learning_rate': 5.549719144409778e-05, 'epoch': 3.2045454545454546}\n{'loss': 0.2007, 'grad_norm': 0.8297497630119324, 'learning_rate': 5.3955602792872835e-05, 'epoch': 3.227272727272727}\n{'loss': 0.2093, 'grad_norm': 0.7245709300041199, 'learning_rate': 5.2414014141647905e-05, 'epoch': 3.25}\n{'loss': 0.1985, 'grad_norm': 1.172494888305664, 'learning_rate': 5.087242549042296e-05, 'epoch': 3.2727272727272725}\n{'loss': 0.1726, 'grad_norm': 1.0193662643432617, 'learning_rate': 4.9330836839198025e-05, 'epoch': 3.2954545454545454}\n{'loss': 0.2282, 'grad_norm': 0.7982431650161743, 'learning_rate': 4.778924818797309e-05, 'epoch': 3.3181818181818183}\n{'loss': 0.1724, 'grad_norm': 0.763757050037384, 'learning_rate': 4.624765953674815e-05, 'epoch': 3.340909090909091}\n{'loss': 0.1527, 'grad_norm': 0.8670226335525513, 'learning_rate': 4.4706070885523215e-05, 'epoch': 3.3636363636363638}\n{'loss': 0.152, 'grad_norm': 0.8840233087539673, 'learning_rate': 4.316448223429827e-05, 'epoch': 3.3863636363636362}\n{'loss': 0.1947, 'grad_norm': 0.9594319462776184, 'learning_rate': 4.1622893583073335e-05, 'epoch': 3.409090909090909}\n{'loss': 0.1622, 'grad_norm': 0.8100289702415466, 'learning_rate': 4.008130493184839e-05, 'epoch': 3.4318181818181817}\n{'loss': 0.2004, 'grad_norm': 0.9840840697288513, 'learning_rate': 3.853971628062346e-05, 'epoch': 3.4545454545454546}\n{'loss': 0.1571, 'grad_norm': 0.6426869034767151, 'learning_rate': 3.699812762939852e-05, 'epoch': 3.4772727272727275}\n{'loss': 0.1783, 'grad_norm': 1.0659387111663818, 'learning_rate': 3.545653897817358e-05, 'epoch': 3.5}\n{'loss': 0.186, 'grad_norm': 0.8067634701728821, 'learning_rate': 3.391495032694864e-05, 'epoch': 3.5227272727272725}\n{'loss': 0.1728, 'grad_norm': 1.38700270652771, 'learning_rate': 3.23733616757237e-05, 'epoch': 3.5454545454545454}\n{'loss': 0.1968, 'grad_norm': 1.4595144987106323, 'learning_rate': 3.0831773024498766e-05, 'epoch': 3.5681818181818183}\n{'loss': 0.1983, 'grad_norm': 0.8195117712020874, 'learning_rate': 2.929018437327383e-05, 'epoch': 3.590909090909091}\n{'loss': 0.1982, 'grad_norm': 1.0332863330841064, 'learning_rate': 2.774859572204889e-05, 'epoch': 3.6136363636363638}\n{'loss': 0.1774, 'grad_norm': 0.8183155059814453, 'learning_rate': 2.6207007070823953e-05, 'epoch': 3.6363636363636362}\n{'loss': 0.1847, 'grad_norm': 0.9384541511535645, 'learning_rate': 2.4665418419599013e-05, 'epoch': 3.659090909090909}\n{'loss': 0.1857, 'grad_norm': 0.8099784851074219, 'learning_rate': 2.3123829768374076e-05, 'epoch': 3.6818181818181817}\n{'loss': 0.1682, 'grad_norm': 0.8913534879684448, 'learning_rate': 2.1582241117149136e-05, 'epoch': 3.7045454545454546}\n{'loss': 0.1912, 'grad_norm': 1.2195500135421753, 'learning_rate': 2.0040652465924196e-05, 'epoch': 3.7272727272727275}\n{'loss': 0.2196, 'grad_norm': 0.8858369588851929, 'learning_rate': 1.849906381469926e-05, 'epoch': 3.75}\n{'loss': 0.1751, 'grad_norm': 1.2271840572357178, 'learning_rate': 1.695747516347432e-05, 'epoch': 3.7727272727272725}\n{'loss': 0.2068, 'grad_norm': 1.034685730934143, 'learning_rate': 1.5415886512249383e-05, 'epoch': 3.7954545454545454}\n{'loss': 0.1823, 'grad_norm': 0.7763810753822327, 'learning_rate': 1.3874297861024445e-05, 'epoch': 3.8181818181818183}\n{'loss': 0.2469, 'grad_norm': 1.0396535396575928, 'learning_rate': 1.2332709209799506e-05, 'epoch': 3.840909090909091}\n{'loss': 0.2107, 'grad_norm': 1.0297811031341553, 'learning_rate': 1.0791120558574568e-05, 'epoch': 3.8636363636363638}\n{'loss': 0.1428, 'grad_norm': 0.7694553136825562, 'learning_rate': 9.24953190734963e-06, 'epoch': 3.8863636363636362}\n{'loss': 0.1999, 'grad_norm': 0.7323921918869019, 'learning_rate': 7.707943256124691e-06, 'epoch': 3.909090909090909}\n{'loss': 0.2, 'grad_norm': 1.0804495811462402, 'learning_rate': 6.166354604899753e-06, 'epoch': 3.9318181818181817}\n{'loss': 0.1838, 'grad_norm': 0.7695369720458984, 'learning_rate': 4.624765953674815e-06, 'epoch': 3.9545454545454546}\n{'loss': 0.1946, 'grad_norm': 0.7554831504821777, 'learning_rate': 3.0831773024498766e-06, 'epoch': 3.9772727272727275}\n{'loss': 0.0207, 'grad_norm': 0.5257002115249634, 'learning_rate': 1.5415886512249383e-06, 'epoch': 4.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8721    0.8756    0.8739      3311\n     PERCENT     0.8352    0.8837    0.8588        86\n        TYPE     0.9406    0.9663    0.9533     11299\n      VOLUME     0.6522    0.7143    0.6818        42\n\n   micro avg     0.9240    0.9447    0.9342     14738\n   macro avg     0.8250    0.8600    0.8419     14738\nweighted avg     0.9238    0.9447    0.9341     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.2565346956253052, 'eval_f1_macro': 0.841928345879144, 'eval_precision': 0.9240111494558003, 'eval_recall': 0.9447007735106527, 'eval_f1': 0.9342414279004226, 'eval_accuracy': 0.925687120922354, 'eval_runtime': 1.4811, 'eval_samples_per_second': 3720.27, 'eval_steps_per_second': 7.427, 'epoch': 4.0}\n{'train_runtime': 24.8953, 'train_samples_per_second': 3541.546, 'train_steps_per_second': 7.07, 'train_loss': 0.4398021403149786, 'epoch': 4.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nThe speedups for torchdynamo mostly come with GPU Ampere or higher and which is not detected here.\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8721    0.8756    0.8739      3311\n     PERCENT     0.8352    0.8837    0.8588        86\n        TYPE     0.9406    0.9663    0.9533     11299\n      VOLUME     0.6522    0.7143    0.6818        42\n\n   micro avg     0.9240    0.9447    0.9342     14738\n   macro avg     0.8250    0.8600    0.8419     14738\nweighted avg     0.9238    0.9447    0.9341     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.2565346956253052, 'eval_f1_macro': 0.841928345879144, 'eval_precision': 0.9240111494558003, 'eval_recall': 0.9447007735106527, 'eval_f1': 0.9342414279004226, 'eval_accuracy': 0.925687120922354, 'eval_runtime': 1.6117, 'eval_samples_per_second': 3418.726, 'eval_steps_per_second': 6.825, 'epoch': 4.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/3364797558.py:66: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 2.357, 'grad_norm': 7.17248010635376, 'learning_rate': 0.0, 'epoch': 0.022727272727272728}\n{'loss': 2.3537, 'grad_norm': 7.493842124938965, 'learning_rate': 1.353172260519668e-05, 'epoch': 0.045454545454545456}\n{'loss': 2.318, 'grad_norm': 7.129880905151367, 'learning_rate': 2.706344521039336e-05, 'epoch': 0.06818181818181818}\n{'loss': 2.2625, 'grad_norm': 7.506792068481445, 'learning_rate': 4.059516781559004e-05, 'epoch': 0.09090909090909091}\n{'loss': 2.17, 'grad_norm': 7.015715599060059, 'learning_rate': 5.412689042078672e-05, 'epoch': 0.11363636363636363}\n{'loss': 2.055, 'grad_norm': 6.5146307945251465, 'learning_rate': 6.76586130259834e-05, 'epoch': 0.13636363636363635}\n{'loss': 1.9111, 'grad_norm': 6.371921539306641, 'learning_rate': 8.119033563118008e-05, 'epoch': 0.1590909090909091}\n{'loss': 1.7661, 'grad_norm': 5.673150539398193, 'learning_rate': 9.472205823637677e-05, 'epoch': 0.18181818181818182}\n{'loss': 1.5962, 'grad_norm': 5.088191032409668, 'learning_rate': 0.00010825378084157344, 'epoch': 0.20454545454545456}\n{'loss': 1.4085, 'grad_norm': 4.274866104125977, 'learning_rate': 0.00012178550344677013, 'epoch': 0.22727272727272727}\n{'loss': 1.2758, 'grad_norm': 3.0758068561553955, 'learning_rate': 0.0001353172260519668, 'epoch': 0.25}\n{'loss': 1.2082, 'grad_norm': 2.1168477535247803, 'learning_rate': 0.0001488489486571635, 'epoch': 0.2727272727272727}\n{'loss': 1.2284, 'grad_norm': 2.1187188625335693, 'learning_rate': 0.00016238067126236015, 'epoch': 0.29545454545454547}\n{'loss': 1.1943, 'grad_norm': 2.1478371620178223, 'learning_rate': 0.00017591239386755683, 'epoch': 0.3181818181818182}\n{'loss': 1.1167, 'grad_norm': 2.012416124343872, 'learning_rate': 0.00018944411647275354, 'epoch': 0.3409090909090909}\n{'loss': 1.0578, 'grad_norm': 1.4034104347229004, 'learning_rate': 0.00020297583907795023, 'epoch': 0.36363636363636365}\n{'loss': 1.038, 'grad_norm': 1.770851492881775, 'learning_rate': 0.00021650756168314689, 'epoch': 0.38636363636363635}\n{'loss': 0.97, 'grad_norm': 2.2700040340423584, 'learning_rate': 0.00023003928428834357, 'epoch': 0.4090909090909091}\n{'loss': 0.9349, 'grad_norm': 2.0261049270629883, 'learning_rate': 0.00024357100689354025, 'epoch': 0.4318181818181818}\n{'loss': 0.9129, 'grad_norm': 1.4497705698013306, 'learning_rate': 0.00024202941824231532, 'epoch': 0.45454545454545453}\n{'loss': 0.861, 'grad_norm': 1.3826366662979126, 'learning_rate': 0.00024048782959109038, 'epoch': 0.4772727272727273}\n{'loss': 0.8027, 'grad_norm': 0.9219517707824707, 'learning_rate': 0.00023894624093986544, 'epoch': 0.5}\n{'loss': 0.7632, 'grad_norm': 0.8007129430770874, 'learning_rate': 0.0002374046522886405, 'epoch': 0.5227272727272727}\n{'loss': 0.7169, 'grad_norm': 1.0979605913162231, 'learning_rate': 0.00023586306363741557, 'epoch': 0.5454545454545454}\n{'loss': 0.7116, 'grad_norm': 1.1417876482009888, 'learning_rate': 0.00023432147498619063, 'epoch': 0.5681818181818182}\n{'loss': 0.6631, 'grad_norm': 1.0437536239624023, 'learning_rate': 0.00023277988633496567, 'epoch': 0.5909090909090909}\n{'loss': 0.7583, 'grad_norm': 1.1746227741241455, 'learning_rate': 0.00023123829768374073, 'epoch': 0.6136363636363636}\n{'loss': 0.6745, 'grad_norm': 0.8200123310089111, 'learning_rate': 0.00022969670903251582, 'epoch': 0.6363636363636364}\n{'loss': 0.7089, 'grad_norm': 1.0007654428482056, 'learning_rate': 0.00022815512038129086, 'epoch': 0.6590909090909091}\n{'loss': 0.6447, 'grad_norm': 0.7833718657493591, 'learning_rate': 0.00022661353173006592, 'epoch': 0.6818181818181818}\n{'loss': 0.5827, 'grad_norm': 0.7923204898834229, 'learning_rate': 0.00022507194307884101, 'epoch': 0.7045454545454546}\n{'loss': 0.5955, 'grad_norm': 1.3794782161712646, 'learning_rate': 0.00022353035442761605, 'epoch': 0.7272727272727273}\n{'loss': 0.5418, 'grad_norm': 1.562235713005066, 'learning_rate': 0.0002219887657763911, 'epoch': 0.75}\n{'loss': 0.5628, 'grad_norm': 0.8105593919754028, 'learning_rate': 0.00022044717712516618, 'epoch': 0.7727272727272727}\n{'loss': 0.5435, 'grad_norm': 1.4907283782958984, 'learning_rate': 0.00021890558847394124, 'epoch': 0.7954545454545454}\n{'loss': 0.5045, 'grad_norm': 0.8221482634544373, 'learning_rate': 0.0002173639998227163, 'epoch': 0.8181818181818182}\n{'loss': 0.518, 'grad_norm': 1.0740662813186646, 'learning_rate': 0.00021582241117149134, 'epoch': 0.8409090909090909}\n{'loss': 0.4751, 'grad_norm': 1.1187626123428345, 'learning_rate': 0.00021428082252026643, 'epoch': 0.8636363636363636}\n{'loss': 0.4468, 'grad_norm': 1.204162836074829, 'learning_rate': 0.0002127392338690415, 'epoch': 0.8863636363636364}\n{'loss': 0.4461, 'grad_norm': 1.2231500148773193, 'learning_rate': 0.00021119764521781653, 'epoch': 0.9090909090909091}\n{'loss': 0.4506, 'grad_norm': 1.1103650331497192, 'learning_rate': 0.00020965605656659162, 'epoch': 0.9318181818181818}\n{'loss': 0.4675, 'grad_norm': 0.9363895654678345, 'learning_rate': 0.00020811446791536668, 'epoch': 0.9545454545454546}\n{'loss': 0.4642, 'grad_norm': 1.1050843000411987, 'learning_rate': 0.00020657287926414172, 'epoch': 0.9772727272727273}\n{'loss': 0.4631, 'grad_norm': 4.18701696395874, 'learning_rate': 0.0002050312906129168, 'epoch': 1.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.7689    0.7335    0.7508      3456\n     PERCENT     0.5401    0.9610    0.6916        77\n        TYPE     0.8969    0.9286    0.9125     11282\n      VOLUME     0.0000    0.0000    0.0000        41\n\n   micro avg     0.8654    0.8809    0.8730     14856\n   macro avg     0.5515    0.6558    0.5887     14856\nweighted avg     0.8628    0.8809    0.8712     14856\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.4283609986305237, 'eval_f1_macro': 0.5887194121830591, 'eval_precision': 0.8653617246395979, 'eval_recall': 0.8808562197092084, 'eval_f1': 0.8730402295016344, 'eval_accuracy': 0.8717362924281984, 'eval_runtime': 1.482, 'eval_samples_per_second': 3718.008, 'eval_steps_per_second': 7.423, 'epoch': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.424, 'grad_norm': 2.046693801879883, 'learning_rate': 0.00020348970196169185, 'epoch': 1.0227272727272727}\n{'loss': 0.4472, 'grad_norm': 1.6575206518173218, 'learning_rate': 0.0002019481133104669, 'epoch': 1.0454545454545454}\n{'loss': 0.4217, 'grad_norm': 1.3763338327407837, 'learning_rate': 0.000200406524659242, 'epoch': 1.0681818181818181}\n{'loss': 0.4109, 'grad_norm': 2.6200671195983887, 'learning_rate': 0.00019886493600801704, 'epoch': 1.0909090909090908}\n{'loss': 0.368, 'grad_norm': 1.0134574174880981, 'learning_rate': 0.0001973233473567921, 'epoch': 1.1136363636363635}\n{'loss': 0.4096, 'grad_norm': 1.3331667184829712, 'learning_rate': 0.00019578175870556716, 'epoch': 1.1363636363636362}\n{'loss': 0.3878, 'grad_norm': 1.7301923036575317, 'learning_rate': 0.00019424017005434223, 'epoch': 1.1590909090909092}\n{'loss': 0.3294, 'grad_norm': 1.1224826574325562, 'learning_rate': 0.0001926985814031173, 'epoch': 1.1818181818181819}\n{'loss': 0.3721, 'grad_norm': 0.694612443447113, 'learning_rate': 0.00019115699275189235, 'epoch': 1.2045454545454546}\n{'loss': 0.3322, 'grad_norm': 1.1384693384170532, 'learning_rate': 0.00018961540410066742, 'epoch': 1.2272727272727273}\n{'loss': 0.3911, 'grad_norm': 1.5959886312484741, 'learning_rate': 0.00018807381544944248, 'epoch': 1.25}\n{'loss': 0.3157, 'grad_norm': 1.0853017568588257, 'learning_rate': 0.00018653222679821752, 'epoch': 1.2727272727272727}\n{'loss': 0.3757, 'grad_norm': 0.7792909741401672, 'learning_rate': 0.0001849906381469926, 'epoch': 1.2954545454545454}\n{'loss': 0.4019, 'grad_norm': 1.2482308149337769, 'learning_rate': 0.00018344904949576767, 'epoch': 1.3181818181818181}\n{'loss': 0.3652, 'grad_norm': 1.656762719154358, 'learning_rate': 0.0001819074608445427, 'epoch': 1.3409090909090908}\n{'loss': 0.3443, 'grad_norm': 0.87900710105896, 'learning_rate': 0.00018036587219331777, 'epoch': 1.3636363636363638}\n{'loss': 0.3315, 'grad_norm': 0.7293729782104492, 'learning_rate': 0.00017882428354209286, 'epoch': 1.3863636363636362}\n{'loss': 0.3903, 'grad_norm': 0.9019904732704163, 'learning_rate': 0.0001772826948908679, 'epoch': 1.4090909090909092}\n{'loss': 0.3534, 'grad_norm': 1.220394492149353, 'learning_rate': 0.00017574110623964296, 'epoch': 1.4318181818181819}\n{'loss': 0.2894, 'grad_norm': 1.551109790802002, 'learning_rate': 0.00017419951758841802, 'epoch': 1.4545454545454546}\n{'loss': 0.2898, 'grad_norm': 1.2370377779006958, 'learning_rate': 0.0001726579289371931, 'epoch': 1.4772727272727273}\n{'loss': 0.3479, 'grad_norm': 1.219374179840088, 'learning_rate': 0.00017111634028596815, 'epoch': 1.5}\n{'loss': 0.3443, 'grad_norm': 1.5913100242614746, 'learning_rate': 0.00016957475163474321, 'epoch': 1.5227272727272727}\n{'loss': 0.2954, 'grad_norm': 1.0907092094421387, 'learning_rate': 0.00016803316298351828, 'epoch': 1.5454545454545454}\n{'loss': 0.3628, 'grad_norm': 1.3270765542984009, 'learning_rate': 0.00016649157433229334, 'epoch': 1.5681818181818183}\n{'loss': 0.3788, 'grad_norm': 0.8411149978637695, 'learning_rate': 0.00016494998568106838, 'epoch': 1.5909090909090908}\n{'loss': 0.3423, 'grad_norm': 2.102539300918579, 'learning_rate': 0.00016340839702984347, 'epoch': 1.6136363636363638}\n{'loss': 0.3598, 'grad_norm': 2.748217821121216, 'learning_rate': 0.00016186680837861853, 'epoch': 1.6363636363636362}\n{'loss': 0.3025, 'grad_norm': 1.9075952768325806, 'learning_rate': 0.00016032521972739357, 'epoch': 1.6590909090909092}\n{'loss': 0.313, 'grad_norm': 1.0370457172393799, 'learning_rate': 0.00015878363107616866, 'epoch': 1.6818181818181817}\n{'loss': 0.3632, 'grad_norm': 1.1877782344818115, 'learning_rate': 0.0001572420424249437, 'epoch': 1.7045454545454546}\n{'loss': 0.3467, 'grad_norm': 0.9227777123451233, 'learning_rate': 0.00015570045377371876, 'epoch': 1.7272727272727273}\n{'loss': 0.3572, 'grad_norm': 0.7460796236991882, 'learning_rate': 0.00015415886512249385, 'epoch': 1.75}\n{'loss': 0.2865, 'grad_norm': 0.8672989010810852, 'learning_rate': 0.00015261727647126889, 'epoch': 1.7727272727272727}\n{'loss': 0.3003, 'grad_norm': 0.9957617521286011, 'learning_rate': 0.00015107568782004395, 'epoch': 1.7954545454545454}\n{'loss': 0.2921, 'grad_norm': 0.7636321187019348, 'learning_rate': 0.00014953409916881904, 'epoch': 1.8181818181818183}\n{'loss': 0.3476, 'grad_norm': 0.839706540107727, 'learning_rate': 0.00014799251051759408, 'epoch': 1.8409090909090908}\n{'loss': 0.3073, 'grad_norm': 1.8423165082931519, 'learning_rate': 0.00014645092186636914, 'epoch': 1.8636363636363638}\n{'loss': 0.3084, 'grad_norm': 1.0949798822402954, 'learning_rate': 0.0001449093332151442, 'epoch': 1.8863636363636362}\n{'loss': 0.3036, 'grad_norm': 0.9330010414123535, 'learning_rate': 0.00014336774456391927, 'epoch': 1.9090909090909092}\n{'loss': 0.3309, 'grad_norm': 0.9802820086479187, 'learning_rate': 0.00014182615591269433, 'epoch': 1.9318181818181817}\n{'loss': 0.2539, 'grad_norm': 0.903911292552948, 'learning_rate': 0.00014028456726146937, 'epoch': 1.9545454545454546}\n{'loss': 0.3207, 'grad_norm': 0.779375433921814, 'learning_rate': 0.00013874297861024446, 'epoch': 1.9772727272727273}\n{'loss': 0.3261, 'grad_norm': 3.713038921356201, 'learning_rate': 0.00013720138995901952, 'epoch': 2.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8574    0.8212    0.8389      3456\n     PERCENT     0.8353    0.9221    0.8765        77\n        TYPE     0.9254    0.9668    0.9456     11282\n      VOLUME     0.6889    0.7561    0.7209        41\n\n   micro avg     0.9094    0.9321    0.9206     14856\n   macro avg     0.8268    0.8665    0.8455     14856\nweighted avg     0.9085    0.9321    0.9198     14856\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.28957483172416687, 'eval_f1_macro': 0.8455032017786881, 'eval_precision': 0.9094312360436096, 'eval_recall': 0.9320813139472267, 'eval_f1': 0.9206169802539725, 'eval_accuracy': 0.9152523933855526, 'eval_runtime': 1.525, 'eval_samples_per_second': 3613.03, 'eval_steps_per_second': 7.213, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.2322, 'grad_norm': 0.7009688019752502, 'learning_rate': 0.00013565980130779456, 'epoch': 2.022727272727273}\n{'loss': 0.2655, 'grad_norm': 0.81163090467453, 'learning_rate': 0.00013411821265656965, 'epoch': 2.0454545454545454}\n{'loss': 0.2794, 'grad_norm': 0.8230133056640625, 'learning_rate': 0.0001325766240053447, 'epoch': 2.0681818181818183}\n{'loss': 0.2602, 'grad_norm': 0.6900928020477295, 'learning_rate': 0.00013103503535411975, 'epoch': 2.090909090909091}\n{'loss': 0.2189, 'grad_norm': 0.8308278322219849, 'learning_rate': 0.0001294934467028948, 'epoch': 2.1136363636363638}\n{'loss': 0.2488, 'grad_norm': 0.7224227786064148, 'learning_rate': 0.00012795185805166987, 'epoch': 2.1363636363636362}\n{'loss': 0.2076, 'grad_norm': 0.6853867769241333, 'learning_rate': 0.00012641026940044494, 'epoch': 2.159090909090909}\n{'loss': 0.1978, 'grad_norm': 0.698492705821991, 'learning_rate': 0.00012486868074922, 'epoch': 2.1818181818181817}\n{'loss': 0.2404, 'grad_norm': 0.8351691961288452, 'learning_rate': 0.00012332709209799506, 'epoch': 2.2045454545454546}\n{'loss': 0.2153, 'grad_norm': 0.5801098942756653, 'learning_rate': 0.00012178550344677013, 'epoch': 2.227272727272727}\n{'loss': 0.2846, 'grad_norm': 1.1211163997650146, 'learning_rate': 0.00012024391479554519, 'epoch': 2.25}\n{'loss': 0.2078, 'grad_norm': 0.7477526664733887, 'learning_rate': 0.00011870232614432025, 'epoch': 2.2727272727272725}\n{'loss': 0.1976, 'grad_norm': 0.7399776577949524, 'learning_rate': 0.00011716073749309532, 'epoch': 2.2954545454545454}\n{'loss': 0.2445, 'grad_norm': 1.014270305633545, 'learning_rate': 0.00011561914884187037, 'epoch': 2.3181818181818183}\n{'loss': 0.2426, 'grad_norm': 0.9901723265647888, 'learning_rate': 0.00011407756019064543, 'epoch': 2.340909090909091}\n{'loss': 0.2496, 'grad_norm': 1.4237734079360962, 'learning_rate': 0.00011253597153942051, 'epoch': 2.3636363636363638}\n{'loss': 0.2106, 'grad_norm': 0.666170060634613, 'learning_rate': 0.00011099438288819556, 'epoch': 2.3863636363636362}\n{'loss': 0.2079, 'grad_norm': 1.2008377313613892, 'learning_rate': 0.00010945279423697062, 'epoch': 2.409090909090909}\n{'loss': 0.219, 'grad_norm': 0.9328877329826355, 'learning_rate': 0.00010791120558574567, 'epoch': 2.4318181818181817}\n{'loss': 0.2183, 'grad_norm': 0.7988738417625427, 'learning_rate': 0.00010636961693452075, 'epoch': 2.4545454545454546}\n{'loss': 0.2752, 'grad_norm': 1.136138677597046, 'learning_rate': 0.00010482802828329581, 'epoch': 2.4772727272727275}\n{'loss': 0.2011, 'grad_norm': 0.8560296297073364, 'learning_rate': 0.00010328643963207086, 'epoch': 2.5}\n{'loss': 0.1893, 'grad_norm': 0.9936878085136414, 'learning_rate': 0.00010174485098084592, 'epoch': 2.5227272727272725}\n{'loss': 0.238, 'grad_norm': 1.1743547916412354, 'learning_rate': 0.000100203262329621, 'epoch': 2.5454545454545454}\n{'loss': 0.2048, 'grad_norm': 1.1276007890701294, 'learning_rate': 9.866167367839605e-05, 'epoch': 2.5681818181818183}\n{'loss': 0.2711, 'grad_norm': 1.0662606954574585, 'learning_rate': 9.712008502717111e-05, 'epoch': 2.590909090909091}\n{'loss': 0.2032, 'grad_norm': 1.1308470964431763, 'learning_rate': 9.557849637594618e-05, 'epoch': 2.6136363636363638}\n{'loss': 0.2667, 'grad_norm': 0.9322308897972107, 'learning_rate': 9.403690772472124e-05, 'epoch': 2.6363636363636362}\n{'loss': 0.2602, 'grad_norm': 0.9328582286834717, 'learning_rate': 9.24953190734963e-05, 'epoch': 2.659090909090909}\n{'loss': 0.2138, 'grad_norm': 0.8515421152114868, 'learning_rate': 9.095373042227135e-05, 'epoch': 2.6818181818181817}\n{'loss': 0.2192, 'grad_norm': 0.8651453256607056, 'learning_rate': 8.941214177104643e-05, 'epoch': 2.7045454545454546}\n{'loss': 0.1866, 'grad_norm': 0.8033224940299988, 'learning_rate': 8.787055311982148e-05, 'epoch': 2.7272727272727275}\n{'loss': 0.2084, 'grad_norm': 0.9725374579429626, 'learning_rate': 8.632896446859654e-05, 'epoch': 2.75}\n{'loss': 0.3062, 'grad_norm': 1.1645623445510864, 'learning_rate': 8.478737581737161e-05, 'epoch': 2.7727272727272725}\n{'loss': 0.2142, 'grad_norm': 1.1470146179199219, 'learning_rate': 8.324578716614667e-05, 'epoch': 2.7954545454545454}\n{'loss': 0.2427, 'grad_norm': 0.9411166906356812, 'learning_rate': 8.170419851492173e-05, 'epoch': 2.8181818181818183}\n{'loss': 0.2414, 'grad_norm': 1.4849400520324707, 'learning_rate': 8.016260986369678e-05, 'epoch': 2.840909090909091}\n{'loss': 0.2548, 'grad_norm': 1.4251916408538818, 'learning_rate': 7.862102121247185e-05, 'epoch': 2.8636363636363638}\n{'loss': 0.207, 'grad_norm': 1.0528228282928467, 'learning_rate': 7.707943256124692e-05, 'epoch': 2.8863636363636362}\n{'loss': 0.2164, 'grad_norm': 0.9055365324020386, 'learning_rate': 7.553784391002197e-05, 'epoch': 2.909090909090909}\n{'loss': 0.2338, 'grad_norm': 1.0023906230926514, 'learning_rate': 7.399625525879704e-05, 'epoch': 2.9318181818181817}\n{'loss': 0.2704, 'grad_norm': 0.8279665112495422, 'learning_rate': 7.24546666075721e-05, 'epoch': 2.9545454545454546}\n{'loss': 0.2445, 'grad_norm': 0.9069889187812805, 'learning_rate': 7.091307795634716e-05, 'epoch': 2.9772727272727275}\n{'loss': 0.1735, 'grad_norm': 2.73419451713562, 'learning_rate': 6.937148930512223e-05, 'epoch': 3.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8784    0.8510    0.8645      3456\n     PERCENT     0.8452    0.9221    0.8820        77\n        TYPE     0.9392    0.9665    0.9526     11282\n      VOLUME     0.7955    0.8537    0.8235        41\n\n   micro avg     0.9248    0.9391    0.9319     14856\n   macro avg     0.8646    0.8983    0.8807     14856\nweighted avg     0.9242    0.9391    0.9314     14856\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.2599274814128876, 'eval_f1_macro': 0.8806639195018047, 'eval_precision': 0.9247646824870741, 'eval_recall': 0.9390818524501885, 'eval_f1': 0.9318682786720994, 'eval_accuracy': 0.924390774586597, 'eval_runtime': 1.4919, 'eval_samples_per_second': 3693.252, 'eval_steps_per_second': 7.373, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.2198, 'grad_norm': 1.027688980102539, 'learning_rate': 6.782990065389728e-05, 'epoch': 3.022727272727273}\n{'loss': 0.1735, 'grad_norm': 1.0795005559921265, 'learning_rate': 6.628831200267235e-05, 'epoch': 3.0454545454545454}\n{'loss': 0.2015, 'grad_norm': 1.2314033508300781, 'learning_rate': 6.47467233514474e-05, 'epoch': 3.0681818181818183}\n{'loss': 0.237, 'grad_norm': 0.7915734052658081, 'learning_rate': 6.320513470022247e-05, 'epoch': 3.090909090909091}\n{'loss': 0.1674, 'grad_norm': 1.613892912864685, 'learning_rate': 6.166354604899753e-05, 'epoch': 3.1136363636363638}\n{'loss': 0.2125, 'grad_norm': 0.9466063380241394, 'learning_rate': 6.0121957397772595e-05, 'epoch': 3.1363636363636362}\n{'loss': 0.1576, 'grad_norm': 0.8639324903488159, 'learning_rate': 5.858036874654766e-05, 'epoch': 3.159090909090909}\n{'loss': 0.2199, 'grad_norm': 0.9445258378982544, 'learning_rate': 5.7038780095322715e-05, 'epoch': 3.1818181818181817}\n{'loss': 0.1515, 'grad_norm': 0.67479008436203, 'learning_rate': 5.549719144409778e-05, 'epoch': 3.2045454545454546}\n{'loss': 0.1592, 'grad_norm': 0.8695372343063354, 'learning_rate': 5.3955602792872835e-05, 'epoch': 3.227272727272727}\n{'loss': 0.2146, 'grad_norm': 1.3928886651992798, 'learning_rate': 5.2414014141647905e-05, 'epoch': 3.25}\n{'loss': 0.1712, 'grad_norm': 1.3218060731887817, 'learning_rate': 5.087242549042296e-05, 'epoch': 3.2727272727272725}\n{'loss': 0.226, 'grad_norm': 1.7777447700500488, 'learning_rate': 4.9330836839198025e-05, 'epoch': 3.2954545454545454}\n{'loss': 0.1985, 'grad_norm': 0.9043771028518677, 'learning_rate': 4.778924818797309e-05, 'epoch': 3.3181818181818183}\n{'loss': 0.1341, 'grad_norm': 0.9109300374984741, 'learning_rate': 4.624765953674815e-05, 'epoch': 3.340909090909091}\n{'loss': 0.2043, 'grad_norm': 1.024956226348877, 'learning_rate': 4.4706070885523215e-05, 'epoch': 3.3636363636363638}\n{'loss': 0.1682, 'grad_norm': 0.9143079519271851, 'learning_rate': 4.316448223429827e-05, 'epoch': 3.3863636363636362}\n{'loss': 0.2031, 'grad_norm': 1.083664894104004, 'learning_rate': 4.1622893583073335e-05, 'epoch': 3.409090909090909}\n{'loss': 0.1497, 'grad_norm': 0.9850839972496033, 'learning_rate': 4.008130493184839e-05, 'epoch': 3.4318181818181817}\n{'loss': 0.1783, 'grad_norm': 0.752906084060669, 'learning_rate': 3.853971628062346e-05, 'epoch': 3.4545454545454546}\n{'loss': 0.1702, 'grad_norm': 0.87212735414505, 'learning_rate': 3.699812762939852e-05, 'epoch': 3.4772727272727275}\n{'loss': 0.167, 'grad_norm': 0.6630405187606812, 'learning_rate': 3.545653897817358e-05, 'epoch': 3.5}\n{'loss': 0.1901, 'grad_norm': 0.9142149686813354, 'learning_rate': 3.391495032694864e-05, 'epoch': 3.5227272727272725}\n{'loss': 0.2155, 'grad_norm': 1.2758424282073975, 'learning_rate': 3.23733616757237e-05, 'epoch': 3.5454545454545454}\n{'loss': 0.2277, 'grad_norm': 1.715740442276001, 'learning_rate': 3.0831773024498766e-05, 'epoch': 3.5681818181818183}\n{'loss': 0.2008, 'grad_norm': 0.9385841488838196, 'learning_rate': 2.929018437327383e-05, 'epoch': 3.590909090909091}\n{'loss': 0.2085, 'grad_norm': 1.1773566007614136, 'learning_rate': 2.774859572204889e-05, 'epoch': 3.6136363636363638}\n{'loss': 0.1633, 'grad_norm': 1.0956233739852905, 'learning_rate': 2.6207007070823953e-05, 'epoch': 3.6363636363636362}\n{'loss': 0.1924, 'grad_norm': 1.2034721374511719, 'learning_rate': 2.4665418419599013e-05, 'epoch': 3.659090909090909}\n{'loss': 0.1905, 'grad_norm': 0.7125837206840515, 'learning_rate': 2.3123829768374076e-05, 'epoch': 3.6818181818181817}\n{'loss': 0.1497, 'grad_norm': 1.0037943124771118, 'learning_rate': 2.1582241117149136e-05, 'epoch': 3.7045454545454546}\n{'loss': 0.195, 'grad_norm': 1.3195809125900269, 'learning_rate': 2.0040652465924196e-05, 'epoch': 3.7272727272727275}\n{'loss': 0.1512, 'grad_norm': 0.8879609704017639, 'learning_rate': 1.849906381469926e-05, 'epoch': 3.75}\n{'loss': 0.1942, 'grad_norm': 0.7473487257957458, 'learning_rate': 1.695747516347432e-05, 'epoch': 3.7727272727272725}\n{'loss': 0.2127, 'grad_norm': 0.7550995945930481, 'learning_rate': 1.5415886512249383e-05, 'epoch': 3.7954545454545454}\n{'loss': 0.1888, 'grad_norm': 0.9158342480659485, 'learning_rate': 1.3874297861024445e-05, 'epoch': 3.8181818181818183}\n{'loss': 0.1895, 'grad_norm': 0.7137234210968018, 'learning_rate': 1.2332709209799506e-05, 'epoch': 3.840909090909091}\n{'loss': 0.1359, 'grad_norm': 0.768113911151886, 'learning_rate': 1.0791120558574568e-05, 'epoch': 3.8636363636363638}\n{'loss': 0.1816, 'grad_norm': 1.3922722339630127, 'learning_rate': 9.24953190734963e-06, 'epoch': 3.8863636363636362}\n{'loss': 0.2238, 'grad_norm': 1.0010571479797363, 'learning_rate': 7.707943256124691e-06, 'epoch': 3.909090909090909}\n{'loss': 0.2347, 'grad_norm': 0.7483922243118286, 'learning_rate': 6.166354604899753e-06, 'epoch': 3.9318181818181817}\n{'loss': 0.2188, 'grad_norm': 0.8012614846229553, 'learning_rate': 4.624765953674815e-06, 'epoch': 3.9545454545454546}\n{'loss': 0.1592, 'grad_norm': 0.6295344233512878, 'learning_rate': 3.0831773024498766e-06, 'epoch': 3.9772727272727275}\n{'loss': 0.3043, 'grad_norm': 5.4405975341796875, 'learning_rate': 1.5415886512249383e-06, 'epoch': 4.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8750    0.8646    0.8697      3456\n     PERCENT     0.8452    0.9221    0.8820        77\n        TYPE     0.9414    0.9656    0.9534     11282\n      VOLUME     0.7955    0.8537    0.8235        41\n\n   micro avg     0.9254    0.9416    0.9334     14856\n   macro avg     0.8643    0.9015    0.8822     14856\nweighted avg     0.9251    0.9416    0.9332     14856\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.25670912861824036, 'eval_f1_macro': 0.8821538678604883, 'eval_precision': 0.9254383063182269, 'eval_recall': 0.9415724286483576, 'eval_f1': 0.9334356544659836, 'eval_accuracy': 0.9263489991296779, 'eval_runtime': 1.5315, 'eval_samples_per_second': 3597.772, 'eval_steps_per_second': 7.182, 'epoch': 4.0}\n{'train_runtime': 25.1115, 'train_samples_per_second': 3511.063, 'train_steps_per_second': 7.009, 'train_loss': 0.4508492209186608, 'epoch': 4.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nThe speedups for torchdynamo mostly come with GPU Ampere or higher and which is not detected here.\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8750    0.8646    0.8697      3456\n     PERCENT     0.8452    0.9221    0.8820        77\n        TYPE     0.9414    0.9656    0.9534     11282\n      VOLUME     0.7955    0.8537    0.8235        41\n\n   micro avg     0.9254    0.9416    0.9334     14856\n   macro avg     0.8643    0.9015    0.8822     14856\nweighted avg     0.9251    0.9416    0.9332     14856\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.25670912861824036, 'eval_f1_macro': 0.8821538678604883, 'eval_precision': 0.9254383063182269, 'eval_recall': 0.9415724286483576, 'eval_f1': 0.9334356544659836, 'eval_accuracy': 0.9263489991296779, 'eval_runtime': 1.5662, 'eval_samples_per_second': 3518.062, 'eval_steps_per_second': 7.023, 'epoch': 4.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/3364797558.py:66: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 2.1267, 'grad_norm': 7.444742679595947, 'learning_rate': 0.0, 'epoch': 0.022727272727272728}\n{'loss': 2.1124, 'grad_norm': 7.162879467010498, 'learning_rate': 1.353172260519668e-05, 'epoch': 0.045454545454545456}\n{'loss': 2.0833, 'grad_norm': 6.903763771057129, 'learning_rate': 2.706344521039336e-05, 'epoch': 0.06818181818181818}\n{'loss': 2.0323, 'grad_norm': 7.159304141998291, 'learning_rate': 4.059516781559004e-05, 'epoch': 0.09090909090909091}\n{'loss': 1.9417, 'grad_norm': 6.886257648468018, 'learning_rate': 5.412689042078672e-05, 'epoch': 0.11363636363636363}\n{'loss': 1.8336, 'grad_norm': 6.460484504699707, 'learning_rate': 6.76586130259834e-05, 'epoch': 0.13636363636363635}\n{'loss': 1.7204, 'grad_norm': 5.682925224304199, 'learning_rate': 8.119033563118008e-05, 'epoch': 0.1590909090909091}\n{'loss': 1.5765, 'grad_norm': 5.200504302978516, 'learning_rate': 9.472205823637677e-05, 'epoch': 0.18181818181818182}\n{'loss': 1.4322, 'grad_norm': 4.479733467102051, 'learning_rate': 0.00010825378084157344, 'epoch': 0.20454545454545456}\n{'loss': 1.3644, 'grad_norm': 3.184450387954712, 'learning_rate': 0.00012178550344677013, 'epoch': 0.22727272727272727}\n{'loss': 1.2317, 'grad_norm': 2.3408684730529785, 'learning_rate': 0.0001353172260519668, 'epoch': 0.25}\n{'loss': 1.2381, 'grad_norm': 2.207463264465332, 'learning_rate': 0.0001488489486571635, 'epoch': 0.2727272727272727}\n{'loss': 1.1451, 'grad_norm': 2.1554529666900635, 'learning_rate': 0.00016238067126236015, 'epoch': 0.29545454545454547}\n{'loss': 1.1265, 'grad_norm': 2.4650919437408447, 'learning_rate': 0.00017591239386755683, 'epoch': 0.3181818181818182}\n{'loss': 1.1564, 'grad_norm': 2.1798813343048096, 'learning_rate': 0.00018944411647275354, 'epoch': 0.3409090909090909}\n{'loss': 0.953, 'grad_norm': 1.1152514219284058, 'learning_rate': 0.00020297583907795023, 'epoch': 0.36363636363636365}\n{'loss': 0.9601, 'grad_norm': 1.4556658267974854, 'learning_rate': 0.00021650756168314689, 'epoch': 0.38636363636363635}\n{'loss': 0.9414, 'grad_norm': 2.102165699005127, 'learning_rate': 0.00023003928428834357, 'epoch': 0.4090909090909091}\n{'loss': 0.9289, 'grad_norm': 1.8135719299316406, 'learning_rate': 0.00024357100689354025, 'epoch': 0.4318181818181818}\n{'loss': 0.8378, 'grad_norm': 1.172229290008545, 'learning_rate': 0.00024202941824231532, 'epoch': 0.45454545454545453}\n{'loss': 0.8167, 'grad_norm': 1.2464728355407715, 'learning_rate': 0.00024048782959109038, 'epoch': 0.4772727272727273}\n{'loss': 0.7899, 'grad_norm': 1.2941641807556152, 'learning_rate': 0.00023894624093986544, 'epoch': 0.5}\n{'loss': 0.7407, 'grad_norm': 0.9840328693389893, 'learning_rate': 0.0002374046522886405, 'epoch': 0.5227272727272727}\n{'loss': 0.7171, 'grad_norm': 0.8222997784614563, 'learning_rate': 0.00023586306363741557, 'epoch': 0.5454545454545454}\n{'loss': 0.6703, 'grad_norm': 1.4291679859161377, 'learning_rate': 0.00023432147498619063, 'epoch': 0.5681818181818182}\n{'loss': 0.6485, 'grad_norm': 1.1027718782424927, 'learning_rate': 0.00023277988633496567, 'epoch': 0.5909090909090909}\n{'loss': 0.6623, 'grad_norm': 0.8436496257781982, 'learning_rate': 0.00023123829768374073, 'epoch': 0.6136363636363636}\n{'loss': 0.6223, 'grad_norm': 0.7450189590454102, 'learning_rate': 0.00022969670903251582, 'epoch': 0.6363636363636364}\n{'loss': 0.6575, 'grad_norm': 1.3779337406158447, 'learning_rate': 0.00022815512038129086, 'epoch': 0.6590909090909091}\n{'loss': 0.5767, 'grad_norm': 0.7466426491737366, 'learning_rate': 0.00022661353173006592, 'epoch': 0.6818181818181818}\n{'loss': 0.5592, 'grad_norm': 1.0200283527374268, 'learning_rate': 0.00022507194307884101, 'epoch': 0.7045454545454546}\n{'loss': 0.5479, 'grad_norm': 1.2008966207504272, 'learning_rate': 0.00022353035442761605, 'epoch': 0.7272727272727273}\n{'loss': 0.5156, 'grad_norm': 0.8229328393936157, 'learning_rate': 0.0002219887657763911, 'epoch': 0.75}\n{'loss': 0.4997, 'grad_norm': 0.5971966981887817, 'learning_rate': 0.00022044717712516618, 'epoch': 0.7727272727272727}\n{'loss': 0.5266, 'grad_norm': 1.3598400354385376, 'learning_rate': 0.00021890558847394124, 'epoch': 0.7954545454545454}\n{'loss': 0.4605, 'grad_norm': 0.9417668581008911, 'learning_rate': 0.0002173639998227163, 'epoch': 0.8181818181818182}\n{'loss': 0.5447, 'grad_norm': 1.1919854879379272, 'learning_rate': 0.00021582241117149134, 'epoch': 0.8409090909090909}\n{'loss': 0.4817, 'grad_norm': 0.9884603023529053, 'learning_rate': 0.00021428082252026643, 'epoch': 0.8636363636363636}\n{'loss': 0.5303, 'grad_norm': 0.9239588379859924, 'learning_rate': 0.0002127392338690415, 'epoch': 0.8863636363636364}\n{'loss': 0.4882, 'grad_norm': 0.8936663269996643, 'learning_rate': 0.00021119764521781653, 'epoch': 0.9090909090909091}\n{'loss': 0.4332, 'grad_norm': 1.1755696535110474, 'learning_rate': 0.00020965605656659162, 'epoch': 0.9318181818181818}\n{'loss': 0.4196, 'grad_norm': 1.0084599256515503, 'learning_rate': 0.00020811446791536668, 'epoch': 0.9545454545454546}\n{'loss': 0.4811, 'grad_norm': 0.946797788143158, 'learning_rate': 0.00020657287926414172, 'epoch': 0.9772727272727273}\n{'loss': 0.4699, 'grad_norm': 3.039071559906006, 'learning_rate': 0.0002050312906129168, 'epoch': 1.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8099    0.7184    0.7614      3221\n     PERCENT     0.4939    0.9310    0.6454        87\n        TYPE     0.8931    0.9531    0.9221     11501\n      VOLUME     0.0714    0.0339    0.0460        59\n\n   micro avg     0.8718    0.8985    0.8850     14868\n   macro avg     0.5671    0.6591    0.5937     14868\nweighted avg     0.8695    0.8985    0.8822     14868\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.3883676528930664, 'eval_f1_macro': 0.5937437827647277, 'eval_precision': 0.871826665796515, 'eval_recall': 0.8985068603712671, 'eval_f1': 0.8849657182604087, 'eval_accuracy': 0.8830710797612589, 'eval_runtime': 1.5582, 'eval_samples_per_second': 3536.023, 'eval_steps_per_second': 7.059, 'epoch': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.4127, 'grad_norm': 0.8932725787162781, 'learning_rate': 0.00020348970196169185, 'epoch': 1.0227272727272727}\n{'loss': 0.4531, 'grad_norm': 1.2465972900390625, 'learning_rate': 0.0002019481133104669, 'epoch': 1.0454545454545454}\n{'loss': 0.3587, 'grad_norm': 0.7893691062927246, 'learning_rate': 0.000200406524659242, 'epoch': 1.0681818181818181}\n{'loss': 0.4286, 'grad_norm': 0.8809006214141846, 'learning_rate': 0.00019886493600801704, 'epoch': 1.0909090909090908}\n{'loss': 0.4124, 'grad_norm': 0.6996863484382629, 'learning_rate': 0.0001973233473567921, 'epoch': 1.1136363636363635}\n{'loss': 0.3793, 'grad_norm': 0.744926393032074, 'learning_rate': 0.00019578175870556716, 'epoch': 1.1363636363636362}\n{'loss': 0.363, 'grad_norm': 0.8707554340362549, 'learning_rate': 0.00019424017005434223, 'epoch': 1.1590909090909092}\n{'loss': 0.3695, 'grad_norm': 0.9488845467567444, 'learning_rate': 0.0001926985814031173, 'epoch': 1.1818181818181819}\n{'loss': 0.3686, 'grad_norm': 0.6661096811294556, 'learning_rate': 0.00019115699275189235, 'epoch': 1.2045454545454546}\n{'loss': 0.3413, 'grad_norm': 0.9878603219985962, 'learning_rate': 0.00018961540410066742, 'epoch': 1.2272727272727273}\n{'loss': 0.364, 'grad_norm': 1.1008391380310059, 'learning_rate': 0.00018807381544944248, 'epoch': 1.25}\n{'loss': 0.3768, 'grad_norm': 1.3686976432800293, 'learning_rate': 0.00018653222679821752, 'epoch': 1.2727272727272727}\n{'loss': 0.3404, 'grad_norm': 1.0589792728424072, 'learning_rate': 0.0001849906381469926, 'epoch': 1.2954545454545454}\n{'loss': 0.4041, 'grad_norm': 1.1389236450195312, 'learning_rate': 0.00018344904949576767, 'epoch': 1.3181818181818181}\n{'loss': 0.3317, 'grad_norm': 1.2187238931655884, 'learning_rate': 0.0001819074608445427, 'epoch': 1.3409090909090908}\n{'loss': 0.3795, 'grad_norm': 1.6338660717010498, 'learning_rate': 0.00018036587219331777, 'epoch': 1.3636363636363638}\n{'loss': 0.3278, 'grad_norm': 2.0268056392669678, 'learning_rate': 0.00017882428354209286, 'epoch': 1.3863636363636362}\n{'loss': 0.3221, 'grad_norm': 1.263197422027588, 'learning_rate': 0.0001772826948908679, 'epoch': 1.4090909090909092}\n{'loss': 0.3186, 'grad_norm': 0.9302381277084351, 'learning_rate': 0.00017574110623964296, 'epoch': 1.4318181818181819}\n{'loss': 0.3315, 'grad_norm': 1.4398223161697388, 'learning_rate': 0.00017419951758841802, 'epoch': 1.4545454545454546}\n{'loss': 0.2969, 'grad_norm': 1.190129041671753, 'learning_rate': 0.0001726579289371931, 'epoch': 1.4772727272727273}\n{'loss': 0.3212, 'grad_norm': 1.1466327905654907, 'learning_rate': 0.00017111634028596815, 'epoch': 1.5}\n{'loss': 0.354, 'grad_norm': 1.8973121643066406, 'learning_rate': 0.00016957475163474321, 'epoch': 1.5227272727272727}\n{'loss': 0.3612, 'grad_norm': 0.8918053507804871, 'learning_rate': 0.00016803316298351828, 'epoch': 1.5454545454545454}\n{'loss': 0.3149, 'grad_norm': 1.0963653326034546, 'learning_rate': 0.00016649157433229334, 'epoch': 1.5681818181818183}\n{'loss': 0.3598, 'grad_norm': 1.1811375617980957, 'learning_rate': 0.00016494998568106838, 'epoch': 1.5909090909090908}\n{'loss': 0.3454, 'grad_norm': 1.4815679788589478, 'learning_rate': 0.00016340839702984347, 'epoch': 1.6136363636363638}\n{'loss': 0.3324, 'grad_norm': 0.831257164478302, 'learning_rate': 0.00016186680837861853, 'epoch': 1.6363636363636362}\n{'loss': 0.3354, 'grad_norm': 0.7301406860351562, 'learning_rate': 0.00016032521972739357, 'epoch': 1.6590909090909092}\n{'loss': 0.313, 'grad_norm': 1.3471750020980835, 'learning_rate': 0.00015878363107616866, 'epoch': 1.6818181818181817}\n{'loss': 0.3246, 'grad_norm': 1.0618096590042114, 'learning_rate': 0.0001572420424249437, 'epoch': 1.7045454545454546}\n{'loss': 0.2837, 'grad_norm': 0.6581006050109863, 'learning_rate': 0.00015570045377371876, 'epoch': 1.7272727272727273}\n{'loss': 0.3243, 'grad_norm': 0.8521677851676941, 'learning_rate': 0.00015415886512249385, 'epoch': 1.75}\n{'loss': 0.3375, 'grad_norm': 1.8924497365951538, 'learning_rate': 0.00015261727647126889, 'epoch': 1.7727272727272727}\n{'loss': 0.2731, 'grad_norm': 1.929657220840454, 'learning_rate': 0.00015107568782004395, 'epoch': 1.7954545454545454}\n{'loss': 0.3102, 'grad_norm': 0.683742880821228, 'learning_rate': 0.00014953409916881904, 'epoch': 1.8181818181818183}\n{'loss': 0.3073, 'grad_norm': 0.8246033191680908, 'learning_rate': 0.00014799251051759408, 'epoch': 1.8409090909090908}\n{'loss': 0.2978, 'grad_norm': 0.9827561974525452, 'learning_rate': 0.00014645092186636914, 'epoch': 1.8636363636363638}\n{'loss': 0.3024, 'grad_norm': 1.1957672834396362, 'learning_rate': 0.0001449093332151442, 'epoch': 1.8863636363636362}\n{'loss': 0.3118, 'grad_norm': 0.8083361387252808, 'learning_rate': 0.00014336774456391927, 'epoch': 1.9090909090909092}\n{'loss': 0.2823, 'grad_norm': 0.8434830904006958, 'learning_rate': 0.00014182615591269433, 'epoch': 1.9318181818181817}\n{'loss': 0.3398, 'grad_norm': 1.2274787425994873, 'learning_rate': 0.00014028456726146937, 'epoch': 1.9545454545454546}\n{'loss': 0.3081, 'grad_norm': 0.9774182438850403, 'learning_rate': 0.00013874297861024446, 'epoch': 1.9772727272727273}\n{'loss': 0.201, 'grad_norm': 3.4421308040618896, 'learning_rate': 0.00013720138995901952, 'epoch': 2.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8234    0.8659    0.8441      3221\n     PERCENT     0.7692    0.9195    0.8377        87\n        TYPE     0.9383    0.9610    0.9495     11501\n      VOLUME     0.4918    0.5085    0.5000        59\n\n   micro avg     0.9100    0.9383    0.9239     14868\n   macro avg     0.7557    0.8137    0.7828     14868\nweighted avg     0.9106    0.9383    0.9242     14868\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.2698580324649811, 'eval_f1_macro': 0.7828273001147072, 'eval_precision': 0.909986302263388, 'eval_recall': 0.9383239171374764, 'eval_f1': 0.9239378787377066, 'eval_accuracy': 0.9198589256646772, 'eval_runtime': 1.4781, 'eval_samples_per_second': 3727.64, 'eval_steps_per_second': 7.442, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.3226, 'grad_norm': 0.9044327139854431, 'learning_rate': 0.00013565980130779456, 'epoch': 2.022727272727273}\n{'loss': 0.2695, 'grad_norm': 1.499923825263977, 'learning_rate': 0.00013411821265656965, 'epoch': 2.0454545454545454}\n{'loss': 0.2773, 'grad_norm': 0.7990208864212036, 'learning_rate': 0.0001325766240053447, 'epoch': 2.0681818181818183}\n{'loss': 0.258, 'grad_norm': 0.7580889463424683, 'learning_rate': 0.00013103503535411975, 'epoch': 2.090909090909091}\n{'loss': 0.2863, 'grad_norm': 1.500407099723816, 'learning_rate': 0.0001294934467028948, 'epoch': 2.1136363636363638}\n{'loss': 0.2403, 'grad_norm': 1.1841191053390503, 'learning_rate': 0.00012795185805166987, 'epoch': 2.1363636363636362}\n{'loss': 0.2269, 'grad_norm': 1.4444000720977783, 'learning_rate': 0.00012641026940044494, 'epoch': 2.159090909090909}\n{'loss': 0.2424, 'grad_norm': 0.7135036587715149, 'learning_rate': 0.00012486868074922, 'epoch': 2.1818181818181817}\n{'loss': 0.2898, 'grad_norm': 0.7983167171478271, 'learning_rate': 0.00012332709209799506, 'epoch': 2.2045454545454546}\n{'loss': 0.1913, 'grad_norm': 1.8818976879119873, 'learning_rate': 0.00012178550344677013, 'epoch': 2.227272727272727}\n{'loss': 0.2413, 'grad_norm': 1.4853123426437378, 'learning_rate': 0.00012024391479554519, 'epoch': 2.25}\n{'loss': 0.2169, 'grad_norm': 1.2385320663452148, 'learning_rate': 0.00011870232614432025, 'epoch': 2.2727272727272725}\n{'loss': 0.194, 'grad_norm': 0.745090663433075, 'learning_rate': 0.00011716073749309532, 'epoch': 2.2954545454545454}\n{'loss': 0.2246, 'grad_norm': 1.3639711141586304, 'learning_rate': 0.00011561914884187037, 'epoch': 2.3181818181818183}\n{'loss': 0.222, 'grad_norm': 1.5562037229537964, 'learning_rate': 0.00011407756019064543, 'epoch': 2.340909090909091}\n{'loss': 0.2752, 'grad_norm': 2.27558970451355, 'learning_rate': 0.00011253597153942051, 'epoch': 2.3636363636363638}\n{'loss': 0.3127, 'grad_norm': 3.0083258152008057, 'learning_rate': 0.00011099438288819556, 'epoch': 2.3863636363636362}\n{'loss': 0.2298, 'grad_norm': 1.4395617246627808, 'learning_rate': 0.00010945279423697062, 'epoch': 2.409090909090909}\n{'loss': 0.2095, 'grad_norm': 0.7243887186050415, 'learning_rate': 0.00010791120558574567, 'epoch': 2.4318181818181817}\n{'loss': 0.2516, 'grad_norm': 1.2182466983795166, 'learning_rate': 0.00010636961693452075, 'epoch': 2.4545454545454546}\n{'loss': 0.2248, 'grad_norm': 2.285083055496216, 'learning_rate': 0.00010482802828329581, 'epoch': 2.4772727272727275}\n{'loss': 0.242, 'grad_norm': 2.8161072731018066, 'learning_rate': 0.00010328643963207086, 'epoch': 2.5}\n{'loss': 0.2685, 'grad_norm': 2.1509828567504883, 'learning_rate': 0.00010174485098084592, 'epoch': 2.5227272727272725}\n{'loss': 0.2227, 'grad_norm': 1.4970169067382812, 'learning_rate': 0.000100203262329621, 'epoch': 2.5454545454545454}\n{'loss': 0.2129, 'grad_norm': 1.3953269720077515, 'learning_rate': 9.866167367839605e-05, 'epoch': 2.5681818181818183}\n{'loss': 0.2351, 'grad_norm': 0.751822829246521, 'learning_rate': 9.712008502717111e-05, 'epoch': 2.590909090909091}\n{'loss': 0.2282, 'grad_norm': 1.5161736011505127, 'learning_rate': 9.557849637594618e-05, 'epoch': 2.6136363636363638}\n{'loss': 0.2858, 'grad_norm': 2.322845935821533, 'learning_rate': 9.403690772472124e-05, 'epoch': 2.6363636363636362}\n{'loss': 0.2171, 'grad_norm': 1.4428187608718872, 'learning_rate': 9.24953190734963e-05, 'epoch': 2.659090909090909}\n{'loss': 0.3214, 'grad_norm': 2.8591017723083496, 'learning_rate': 9.095373042227135e-05, 'epoch': 2.6818181818181817}\n{'loss': 0.2083, 'grad_norm': 0.6473548412322998, 'learning_rate': 8.941214177104643e-05, 'epoch': 2.7045454545454546}\n{'loss': 0.254, 'grad_norm': 0.9863613247871399, 'learning_rate': 8.787055311982148e-05, 'epoch': 2.7272727272727275}\n{'loss': 0.2601, 'grad_norm': 0.9210057854652405, 'learning_rate': 8.632896446859654e-05, 'epoch': 2.75}\n{'loss': 0.2119, 'grad_norm': 0.7571763396263123, 'learning_rate': 8.478737581737161e-05, 'epoch': 2.7727272727272725}\n{'loss': 0.266, 'grad_norm': 1.2028673887252808, 'learning_rate': 8.324578716614667e-05, 'epoch': 2.7954545454545454}\n{'loss': 0.2453, 'grad_norm': 1.6187632083892822, 'learning_rate': 8.170419851492173e-05, 'epoch': 2.8181818181818183}\n{'loss': 0.2403, 'grad_norm': 0.7943611741065979, 'learning_rate': 8.016260986369678e-05, 'epoch': 2.840909090909091}\n{'loss': 0.2035, 'grad_norm': 0.8508268594741821, 'learning_rate': 7.862102121247185e-05, 'epoch': 2.8636363636363638}\n{'loss': 0.2274, 'grad_norm': 0.9460361003875732, 'learning_rate': 7.707943256124692e-05, 'epoch': 2.8863636363636362}\n{'loss': 0.2163, 'grad_norm': 0.9055337309837341, 'learning_rate': 7.553784391002197e-05, 'epoch': 2.909090909090909}\n{'loss': 0.2103, 'grad_norm': 1.9498425722122192, 'learning_rate': 7.399625525879704e-05, 'epoch': 2.9318181818181817}\n{'loss': 0.2272, 'grad_norm': 1.1227115392684937, 'learning_rate': 7.24546666075721e-05, 'epoch': 2.9545454545454546}\n{'loss': 0.2279, 'grad_norm': 0.9968805909156799, 'learning_rate': 7.091307795634716e-05, 'epoch': 2.9772727272727275}\n{'loss': 0.1021, 'grad_norm': 3.0827364921569824, 'learning_rate': 6.937148930512223e-05, 'epoch': 3.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8629    0.8736    0.8683      3221\n     PERCENT     0.7767    0.9195    0.8421        87\n        TYPE     0.9422    0.9661    0.9540     11501\n      VOLUME     0.6727    0.6271    0.6491        59\n\n   micro avg     0.9231    0.9444    0.9337     14868\n   macro avg     0.8137    0.8466    0.8284     14868\nweighted avg     0.9230    0.9444    0.9336     14868\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.23658989369869232, 'eval_f1_macro': 0.8283747828297161, 'eval_precision': 0.9231477220432581, 'eval_recall': 0.9444444444444444, 'eval_f1': 0.9336746567372586, 'eval_accuracy': 0.9287574606619642, 'eval_runtime': 1.525, 'eval_samples_per_second': 3613.092, 'eval_steps_per_second': 7.213, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.2306, 'grad_norm': 1.169543743133545, 'learning_rate': 6.782990065389728e-05, 'epoch': 3.022727272727273}\n{'loss': 0.2074, 'grad_norm': 0.6580648422241211, 'learning_rate': 6.628831200267235e-05, 'epoch': 3.0454545454545454}\n{'loss': 0.2235, 'grad_norm': 1.160107970237732, 'learning_rate': 6.47467233514474e-05, 'epoch': 3.0681818181818183}\n{'loss': 0.2248, 'grad_norm': 1.728158950805664, 'learning_rate': 6.320513470022247e-05, 'epoch': 3.090909090909091}\n{'loss': 0.2313, 'grad_norm': 1.2433483600616455, 'learning_rate': 6.166354604899753e-05, 'epoch': 3.1136363636363638}\n{'loss': 0.1922, 'grad_norm': 1.2974345684051514, 'learning_rate': 6.0121957397772595e-05, 'epoch': 3.1363636363636362}\n{'loss': 0.202, 'grad_norm': 1.1218175888061523, 'learning_rate': 5.858036874654766e-05, 'epoch': 3.159090909090909}\n{'loss': 0.2497, 'grad_norm': 1.1428492069244385, 'learning_rate': 5.7038780095322715e-05, 'epoch': 3.1818181818181817}\n{'loss': 0.2071, 'grad_norm': 1.024304747581482, 'learning_rate': 5.549719144409778e-05, 'epoch': 3.2045454545454546}\n{'loss': 0.2032, 'grad_norm': 0.8619809746742249, 'learning_rate': 5.3955602792872835e-05, 'epoch': 3.227272727272727}\n{'loss': 0.1643, 'grad_norm': 0.9097282886505127, 'learning_rate': 5.2414014141647905e-05, 'epoch': 3.25}\n{'loss': 0.1807, 'grad_norm': 0.7421820163726807, 'learning_rate': 5.087242549042296e-05, 'epoch': 3.2727272727272725}\n{'loss': 0.2062, 'grad_norm': 1.1966321468353271, 'learning_rate': 4.9330836839198025e-05, 'epoch': 3.2954545454545454}\n{'loss': 0.1679, 'grad_norm': 0.848317563533783, 'learning_rate': 4.778924818797309e-05, 'epoch': 3.3181818181818183}\n{'loss': 0.1814, 'grad_norm': 1.0480146408081055, 'learning_rate': 4.624765953674815e-05, 'epoch': 3.340909090909091}\n{'loss': 0.1884, 'grad_norm': 0.9491150975227356, 'learning_rate': 4.4706070885523215e-05, 'epoch': 3.3636363636363638}\n{'loss': 0.211, 'grad_norm': 0.9953162670135498, 'learning_rate': 4.316448223429827e-05, 'epoch': 3.3863636363636362}\n{'loss': 0.1745, 'grad_norm': 0.8493349552154541, 'learning_rate': 4.1622893583073335e-05, 'epoch': 3.409090909090909}\n{'loss': 0.1989, 'grad_norm': 0.8755645751953125, 'learning_rate': 4.008130493184839e-05, 'epoch': 3.4318181818181817}\n{'loss': 0.2132, 'grad_norm': 0.9573398232460022, 'learning_rate': 3.853971628062346e-05, 'epoch': 3.4545454545454546}\n{'loss': 0.1989, 'grad_norm': 1.2109925746917725, 'learning_rate': 3.699812762939852e-05, 'epoch': 3.4772727272727275}\n{'loss': 0.1787, 'grad_norm': 1.0349475145339966, 'learning_rate': 3.545653897817358e-05, 'epoch': 3.5}\n{'loss': 0.2005, 'grad_norm': 1.1549392938613892, 'learning_rate': 3.391495032694864e-05, 'epoch': 3.5227272727272725}\n{'loss': 0.2175, 'grad_norm': 1.0813227891921997, 'learning_rate': 3.23733616757237e-05, 'epoch': 3.5454545454545454}\n{'loss': 0.1802, 'grad_norm': 1.2032746076583862, 'learning_rate': 3.0831773024498766e-05, 'epoch': 3.5681818181818183}\n{'loss': 0.1815, 'grad_norm': 1.0504229068756104, 'learning_rate': 2.929018437327383e-05, 'epoch': 3.590909090909091}\n{'loss': 0.2472, 'grad_norm': 0.9681060910224915, 'learning_rate': 2.774859572204889e-05, 'epoch': 3.6136363636363638}\n{'loss': 0.1789, 'grad_norm': 0.8903617262840271, 'learning_rate': 2.6207007070823953e-05, 'epoch': 3.6363636363636362}\n{'loss': 0.2105, 'grad_norm': 1.1181873083114624, 'learning_rate': 2.4665418419599013e-05, 'epoch': 3.659090909090909}\n{'loss': 0.1621, 'grad_norm': 0.7139083743095398, 'learning_rate': 2.3123829768374076e-05, 'epoch': 3.6818181818181817}\n{'loss': 0.1769, 'grad_norm': 0.7569129467010498, 'learning_rate': 2.1582241117149136e-05, 'epoch': 3.7045454545454546}\n{'loss': 0.2121, 'grad_norm': 0.9256478548049927, 'learning_rate': 2.0040652465924196e-05, 'epoch': 3.7272727272727275}\n{'loss': 0.1703, 'grad_norm': 0.7799966931343079, 'learning_rate': 1.849906381469926e-05, 'epoch': 3.75}\n{'loss': 0.1561, 'grad_norm': 1.2341020107269287, 'learning_rate': 1.695747516347432e-05, 'epoch': 3.7727272727272725}\n{'loss': 0.1927, 'grad_norm': 1.063330888748169, 'learning_rate': 1.5415886512249383e-05, 'epoch': 3.7954545454545454}\n{'loss': 0.2012, 'grad_norm': 1.1630362272262573, 'learning_rate': 1.3874297861024445e-05, 'epoch': 3.8181818181818183}\n{'loss': 0.2169, 'grad_norm': 0.9322408437728882, 'learning_rate': 1.2332709209799506e-05, 'epoch': 3.840909090909091}\n{'loss': 0.1752, 'grad_norm': 0.80849689245224, 'learning_rate': 1.0791120558574568e-05, 'epoch': 3.8636363636363638}\n{'loss': 0.2058, 'grad_norm': 0.8181917667388916, 'learning_rate': 9.24953190734963e-06, 'epoch': 3.8863636363636362}\n{'loss': 0.1681, 'grad_norm': 0.7176665663719177, 'learning_rate': 7.707943256124691e-06, 'epoch': 3.909090909090909}\n{'loss': 0.2054, 'grad_norm': 1.0107654333114624, 'learning_rate': 6.166354604899753e-06, 'epoch': 3.9318181818181817}\n{'loss': 0.2154, 'grad_norm': 1.566332221031189, 'learning_rate': 4.624765953674815e-06, 'epoch': 3.9545454545454546}\n{'loss': 0.1946, 'grad_norm': 0.7718892693519592, 'learning_rate': 3.0831773024498766e-06, 'epoch': 3.9772727272727275}\n{'loss': 0.139, 'grad_norm': 1.9853498935699463, 'learning_rate': 1.5415886512249383e-06, 'epoch': 4.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8647    0.8808    0.8727      3221\n     PERCENT     0.7921    0.9195    0.8511        87\n        TYPE     0.9464    0.9662    0.9562     11501\n      VOLUME     0.6964    0.6610    0.6783        59\n\n   micro avg     0.9268    0.9462    0.9364     14868\n   macro avg     0.8249    0.8569    0.8395     14868\nweighted avg     0.9268    0.9462    0.9364     14868\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.23053233325481415, 'eval_f1_macro': 0.8395448124918784, 'eval_precision': 0.9268067725146584, 'eval_recall': 0.9461931665321496, 'eval_f1': 0.9363996405631178, 'eval_accuracy': 0.9314704286489419, 'eval_runtime': 1.5151, 'eval_samples_per_second': 3636.662, 'eval_steps_per_second': 7.26, 'epoch': 4.0}\n{'train_runtime': 25.0683, 'train_samples_per_second': 3517.108, 'train_steps_per_second': 7.021, 'train_loss': 0.4358482080173086, 'epoch': 4.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n[I 2025-09-26 18:01:26,952] Trial 1 finished with value: 0.8556681806898127 and parameters: {'learning_rate': 0.00024357100689354025, 'weight_decay': 0.019197443361117214, 'num_train_epochs': 4}. Best is trial 0 with value: 0.9174994186455976.\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8647    0.8808    0.8727      3221\n     PERCENT     0.7921    0.9195    0.8511        87\n        TYPE     0.9464    0.9662    0.9562     11501\n      VOLUME     0.6964    0.6610    0.6783        59\n\n   micro avg     0.9268    0.9462    0.9364     14868\n   macro avg     0.8249    0.8569    0.8395     14868\nweighted avg     0.9268    0.9462    0.9364     14868\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.23053233325481415, 'eval_f1_macro': 0.8395448124918784, 'eval_precision': 0.9268067725146584, 'eval_recall': 0.9461931665321496, 'eval_f1': 0.9363996405631178, 'eval_accuracy': 0.9314704286489419, 'eval_runtime': 1.5663, 'eval_samples_per_second': 3517.882, 'eval_steps_per_second': 7.023, 'epoch': 4.0}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/27552 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82184b77848c41e3810f0827bde54858"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nThe speedups for torchdynamo mostly come with GPU Ampere or higher and which is not detected here.\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/tmp/ipykernel_36/3364797558.py:66: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 2.3703, 'grad_norm': 7.477160930633545, 'learning_rate': 0.0, 'epoch': 0.022727272727272728}\n{'loss': 2.3834, 'grad_norm': 7.629173755645752, 'learning_rate': 2.1020573562092666e-05, 'epoch': 0.045454545454545456}\n{'loss': 2.3292, 'grad_norm': 7.429513931274414, 'learning_rate': 4.204114712418533e-05, 'epoch': 0.06818181818181818}\n{'loss': 2.2258, 'grad_norm': 7.456045627593994, 'learning_rate': 6.3061720686278e-05, 'epoch': 0.09090909090909091}\n{'loss': 2.0933, 'grad_norm': 6.78582763671875, 'learning_rate': 8.408229424837066e-05, 'epoch': 0.11363636363636363}\n{'loss': 1.9041, 'grad_norm': 6.4059600830078125, 'learning_rate': 0.00010510286781046334, 'epoch': 0.13636363636363635}\n{'loss': 1.7364, 'grad_norm': 5.532415390014648, 'learning_rate': 0.000126123441372556, 'epoch': 0.1590909090909091}\n{'loss': 1.4941, 'grad_norm': 4.839597225189209, 'learning_rate': 0.00014714401493464866, 'epoch': 0.18181818181818182}\n{'loss': 1.3577, 'grad_norm': 3.2357265949249268, 'learning_rate': 0.00016816458849674133, 'epoch': 0.20454545454545456}\n{'loss': 1.2303, 'grad_norm': 1.9887295961380005, 'learning_rate': 0.00018918516205883402, 'epoch': 0.22727272727272727}\n{'loss': 1.1256, 'grad_norm': 1.5156303644180298, 'learning_rate': 0.00021020573562092668, 'epoch': 0.25}\n{'loss': 1.1019, 'grad_norm': 1.6668795347213745, 'learning_rate': 0.00023122630918301931, 'epoch': 0.2727272727272727}\n{'loss': 1.0768, 'grad_norm': 1.717760682106018, 'learning_rate': 0.000252246882745112, 'epoch': 0.29545454545454547}\n{'loss': 1.0511, 'grad_norm': 1.1862272024154663, 'learning_rate': 0.0002732674563072047, 'epoch': 0.3181818181818182}\n{'loss': 1.0112, 'grad_norm': 1.4356108903884888, 'learning_rate': 0.00029428802986929733, 'epoch': 0.3409090909090909}\n{'loss': 1.0224, 'grad_norm': 2.1371519565582275, 'learning_rate': 0.00029179406351447277, 'epoch': 0.36363636363636365}\n{'loss': 0.9426, 'grad_norm': 2.897675037384033, 'learning_rate': 0.00028930009715964826, 'epoch': 0.38636363636363635}\n{'loss': 0.8925, 'grad_norm': 2.2635273933410645, 'learning_rate': 0.00028680613080482364, 'epoch': 0.4090909090909091}\n{'loss': 0.8373, 'grad_norm': 1.2578858137130737, 'learning_rate': 0.0002843121644499991, 'epoch': 0.4318181818181818}\n{'loss': 0.78, 'grad_norm': 1.2240004539489746, 'learning_rate': 0.00028181819809517457, 'epoch': 0.45454545454545453}\n{'loss': 0.7855, 'grad_norm': 1.2609001398086548, 'learning_rate': 0.00027932423174035, 'epoch': 0.4772727272727273}\n{'loss': 0.7016, 'grad_norm': 1.419707179069519, 'learning_rate': 0.00027683026538552545, 'epoch': 0.5}\n{'loss': 0.7114, 'grad_norm': 0.9432477951049805, 'learning_rate': 0.0002743362990307009, 'epoch': 0.5227272727272727}\n{'loss': 0.6875, 'grad_norm': 1.2437751293182373, 'learning_rate': 0.0002718423326758763, 'epoch': 0.5454545454545454}\n{'loss': 0.6827, 'grad_norm': 1.35843026638031, 'learning_rate': 0.0002693483663210518, 'epoch': 0.5681818181818182}\n{'loss': 0.6419, 'grad_norm': 1.4068632125854492, 'learning_rate': 0.00026685439996622726, 'epoch': 0.5909090909090909}\n{'loss': 0.542, 'grad_norm': 0.9588150382041931, 'learning_rate': 0.0002643604336114027, 'epoch': 0.6136363636363636}\n{'loss': 0.5685, 'grad_norm': 1.5013712644577026, 'learning_rate': 0.00026186646725657813, 'epoch': 0.6363636363636364}\n{'loss': 0.6334, 'grad_norm': 1.5589923858642578, 'learning_rate': 0.00025937250090175357, 'epoch': 0.6590909090909091}\n{'loss': 0.5563, 'grad_norm': 1.3708864450454712, 'learning_rate': 0.000256878534546929, 'epoch': 0.6818181818181818}\n{'loss': 0.5584, 'grad_norm': 1.0617932081222534, 'learning_rate': 0.0002543845681921045, 'epoch': 0.7045454545454546}\n{'loss': 0.5498, 'grad_norm': 1.1286426782608032, 'learning_rate': 0.00025189060183727994, 'epoch': 0.7272727272727273}\n{'loss': 0.5343, 'grad_norm': 0.8968172669410706, 'learning_rate': 0.0002493966354824554, 'epoch': 0.75}\n{'loss': 0.5334, 'grad_norm': 0.9111248254776001, 'learning_rate': 0.0002469026691276308, 'epoch': 0.7727272727272727}\n{'loss': 0.4882, 'grad_norm': 0.9936622977256775, 'learning_rate': 0.00024440870277280625, 'epoch': 0.7954545454545454}\n{'loss': 0.4713, 'grad_norm': 0.7203101515769958, 'learning_rate': 0.00024191473641798172, 'epoch': 0.8181818181818182}\n{'loss': 0.4824, 'grad_norm': 1.361295223236084, 'learning_rate': 0.00023942077006315716, 'epoch': 0.8409090909090909}\n{'loss': 0.4749, 'grad_norm': 1.2273768186569214, 'learning_rate': 0.0002369268037083326, 'epoch': 0.8636363636363636}\n{'loss': 0.4625, 'grad_norm': 0.6076984405517578, 'learning_rate': 0.00023443283735350803, 'epoch': 0.8863636363636364}\n{'loss': 0.4483, 'grad_norm': 0.7034622430801392, 'learning_rate': 0.0002319388709986835, 'epoch': 0.9090909090909091}\n{'loss': 0.5107, 'grad_norm': 1.2971856594085693, 'learning_rate': 0.00022944490464385894, 'epoch': 0.9318181818181818}\n{'loss': 0.4506, 'grad_norm': 1.4372551441192627, 'learning_rate': 0.0002269509382890344, 'epoch': 0.9545454545454546}\n{'loss': 0.4204, 'grad_norm': 0.8492826819419861, 'learning_rate': 0.0002244569719342098, 'epoch': 0.9772727272727273}\n{'loss': 0.4295, 'grad_norm': 3.302495241165161, 'learning_rate': 0.00022196300557938528, 'epoch': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.7997    0.7167    0.7560      3142\n     PERCENT     0.3422    0.9697    0.5059        66\n        TYPE     0.8930    0.9515    0.9213     11415\n      VOLUME     0.0000    0.0000    0.0000        70\n\n   micro avg     0.8689    0.8968    0.8826     14693\n   macro avg     0.5087    0.6595    0.5458     14693\nweighted avg     0.8663    0.8968    0.8797     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.4108641743659973, 'eval_f1_macro': 0.545792445144938, 'eval_precision': 0.8688513780825531, 'eval_recall': 0.8968216157353842, 'eval_f1': 0.8826149569643994, 'eval_accuracy': 0.8765915768854065, 'eval_runtime': 1.4872, 'eval_samples_per_second': 3705.509, 'eval_steps_per_second': 7.396, 'epoch': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.363, 'grad_norm': 0.8877320885658264, 'learning_rate': 0.00021946903922456072, 'epoch': 1.0227272727272727}\n{'loss': 0.4103, 'grad_norm': 0.9026186466217041, 'learning_rate': 0.00021697507286973618, 'epoch': 1.0454545454545454}\n{'loss': 0.4225, 'grad_norm': 0.9512041807174683, 'learning_rate': 0.0002144811065149116, 'epoch': 1.0681818181818181}\n{'loss': 0.3517, 'grad_norm': 0.834901750087738, 'learning_rate': 0.00021198714016008706, 'epoch': 1.0909090909090908}\n{'loss': 0.3307, 'grad_norm': 0.698990523815155, 'learning_rate': 0.0002094931738052625, 'epoch': 1.1136363636363635}\n{'loss': 0.3945, 'grad_norm': 0.9442896246910095, 'learning_rate': 0.00020699920745043796, 'epoch': 1.1363636363636362}\n{'loss': 0.4177, 'grad_norm': 1.064777135848999, 'learning_rate': 0.0002045052410956134, 'epoch': 1.1590909090909092}\n{'loss': 0.3859, 'grad_norm': 1.0482585430145264, 'learning_rate': 0.00020201127474078884, 'epoch': 1.1818181818181819}\n{'loss': 0.3738, 'grad_norm': 0.8862871527671814, 'learning_rate': 0.00019951730838596427, 'epoch': 1.2045454545454546}\n{'loss': 0.3836, 'grad_norm': 1.1182419061660767, 'learning_rate': 0.00019702334203113974, 'epoch': 1.2272727272727273}\n{'loss': 0.3535, 'grad_norm': 0.7586254477500916, 'learning_rate': 0.00019452937567631518, 'epoch': 1.25}\n{'loss': 0.3685, 'grad_norm': 0.7005501985549927, 'learning_rate': 0.00019203540932149064, 'epoch': 1.2727272727272727}\n{'loss': 0.3509, 'grad_norm': 1.3170360326766968, 'learning_rate': 0.00018954144296666605, 'epoch': 1.2954545454545454}\n{'loss': 0.3245, 'grad_norm': 1.3526999950408936, 'learning_rate': 0.00018704747661184152, 'epoch': 1.3181818181818181}\n{'loss': 0.338, 'grad_norm': 0.8364545106887817, 'learning_rate': 0.00018455351025701696, 'epoch': 1.3409090909090908}\n{'loss': 0.3668, 'grad_norm': 1.0627498626708984, 'learning_rate': 0.00018205954390219242, 'epoch': 1.3636363636363638}\n{'loss': 0.3763, 'grad_norm': 1.0639393329620361, 'learning_rate': 0.00017956557754736786, 'epoch': 1.3863636363636362}\n{'loss': 0.3145, 'grad_norm': 1.0407103300094604, 'learning_rate': 0.0001770716111925433, 'epoch': 1.4090909090909092}\n{'loss': 0.3387, 'grad_norm': 1.3458305597305298, 'learning_rate': 0.00017457764483771874, 'epoch': 1.4318181818181819}\n{'loss': 0.3594, 'grad_norm': 1.0793572664260864, 'learning_rate': 0.0001720836784828942, 'epoch': 1.4545454545454546}\n{'loss': 0.2977, 'grad_norm': 1.53606379032135, 'learning_rate': 0.00016958971212806964, 'epoch': 1.4772727272727273}\n{'loss': 0.3357, 'grad_norm': 1.1125893592834473, 'learning_rate': 0.0001670957457732451, 'epoch': 1.5}\n{'loss': 0.3771, 'grad_norm': 0.8468223810195923, 'learning_rate': 0.00016460177941842052, 'epoch': 1.5227272727272727}\n{'loss': 0.2867, 'grad_norm': 0.7280726432800293, 'learning_rate': 0.00016210781306359598, 'epoch': 1.5454545454545454}\n{'loss': 0.2789, 'grad_norm': 1.1807280778884888, 'learning_rate': 0.00015961384670877142, 'epoch': 1.5681818181818183}\n{'loss': 0.3027, 'grad_norm': 1.6454285383224487, 'learning_rate': 0.00015711988035394688, 'epoch': 1.5909090909090908}\n{'loss': 0.3054, 'grad_norm': 0.7912328243255615, 'learning_rate': 0.00015462591399912235, 'epoch': 1.6136363636363638}\n{'loss': 0.3344, 'grad_norm': 0.9845935106277466, 'learning_rate': 0.00015213194764429776, 'epoch': 1.6363636363636362}\n{'loss': 0.2712, 'grad_norm': 1.39248788356781, 'learning_rate': 0.0001496379812894732, 'epoch': 1.6590909090909092}\n{'loss': 0.34, 'grad_norm': 0.703874945640564, 'learning_rate': 0.00014714401493464866, 'epoch': 1.6818181818181817}\n{'loss': 0.2868, 'grad_norm': 1.6120567321777344, 'learning_rate': 0.00014465004857982413, 'epoch': 1.7045454545454546}\n{'loss': 0.2794, 'grad_norm': 1.1103800535202026, 'learning_rate': 0.00014215608222499954, 'epoch': 1.7272727272727273}\n{'loss': 0.375, 'grad_norm': 1.092174768447876, 'learning_rate': 0.000139662115870175, 'epoch': 1.75}\n{'loss': 0.3334, 'grad_norm': 0.9725164175033569, 'learning_rate': 0.00013716814951535044, 'epoch': 1.7727272727272727}\n{'loss': 0.3061, 'grad_norm': 1.0166032314300537, 'learning_rate': 0.0001346741831605259, 'epoch': 1.7954545454545454}\n{'loss': 0.3059, 'grad_norm': 0.9160690307617188, 'learning_rate': 0.00013218021680570135, 'epoch': 1.8181818181818183}\n{'loss': 0.2839, 'grad_norm': 0.8381499648094177, 'learning_rate': 0.00012968625045087679, 'epoch': 1.8409090909090908}\n{'loss': 0.3319, 'grad_norm': 1.6093926429748535, 'learning_rate': 0.00012719228409605225, 'epoch': 1.8636363636363638}\n{'loss': 0.2843, 'grad_norm': 1.370355486869812, 'learning_rate': 0.0001246983177412277, 'epoch': 1.8863636363636362}\n{'loss': 0.3597, 'grad_norm': 0.8884775638580322, 'learning_rate': 0.00012220435138640313, 'epoch': 1.9090909090909092}\n{'loss': 0.2396, 'grad_norm': 0.9806124567985535, 'learning_rate': 0.00011971038503157858, 'epoch': 1.9318181818181817}\n{'loss': 0.3351, 'grad_norm': 0.6144145131111145, 'learning_rate': 0.00011721641867675402, 'epoch': 1.9545454545454546}\n{'loss': 0.2746, 'grad_norm': 0.965435266494751, 'learning_rate': 0.00011472245232192947, 'epoch': 1.9772727272727273}\n{'loss': 0.4257, 'grad_norm': 3.7831733226776123, 'learning_rate': 0.0001122284859671049, 'epoch': 2.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8598    0.8004    0.8291      3142\n     PERCENT     0.6263    0.9394    0.7515        66\n        TYPE     0.9228    0.9645    0.9432     11415\n      VOLUME     0.5172    0.4286    0.4688        70\n\n   micro avg     0.9070    0.9268    0.9168     14693\n   macro avg     0.7315    0.7832    0.7481     14693\nweighted avg     0.9061    0.9268    0.9157     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.3098500669002533, 'eval_f1_macro': 0.7481356846690292, 'eval_precision': 0.9070139212682342, 'eval_recall': 0.9267678486354046, 'eval_f1': 0.9167844879822259, 'eval_accuracy': 0.9083687017085645, 'eval_runtime': 1.4927, 'eval_samples_per_second': 3692.012, 'eval_steps_per_second': 7.369, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.2606, 'grad_norm': 1.5470634698867798, 'learning_rate': 0.00010973451961228036, 'epoch': 2.022727272727273}\n{'loss': 0.2855, 'grad_norm': 1.2086597681045532, 'learning_rate': 0.0001072405532574558, 'epoch': 2.0454545454545454}\n{'loss': 0.3128, 'grad_norm': 1.3274955749511719, 'learning_rate': 0.00010474658690263125, 'epoch': 2.0681818181818183}\n{'loss': 0.2283, 'grad_norm': 0.7744278311729431, 'learning_rate': 0.0001022526205478067, 'epoch': 2.090909090909091}\n{'loss': 0.2202, 'grad_norm': 0.9072192311286926, 'learning_rate': 9.975865419298214e-05, 'epoch': 2.1136363636363638}\n{'loss': 0.1917, 'grad_norm': 0.8578634858131409, 'learning_rate': 9.726468783815759e-05, 'epoch': 2.1363636363636362}\n{'loss': 0.2434, 'grad_norm': 1.068355679512024, 'learning_rate': 9.477072148333303e-05, 'epoch': 2.159090909090909}\n{'loss': 0.2686, 'grad_norm': 0.8546217083930969, 'learning_rate': 9.227675512850848e-05, 'epoch': 2.1818181818181817}\n{'loss': 0.2709, 'grad_norm': 0.8321190476417542, 'learning_rate': 8.978278877368393e-05, 'epoch': 2.2045454545454546}\n{'loss': 0.2573, 'grad_norm': 0.6790217161178589, 'learning_rate': 8.728882241885937e-05, 'epoch': 2.227272727272727}\n{'loss': 0.2494, 'grad_norm': 0.7785236835479736, 'learning_rate': 8.479485606403482e-05, 'epoch': 2.25}\n{'loss': 0.2061, 'grad_norm': 0.7121747136116028, 'learning_rate': 8.230088970921026e-05, 'epoch': 2.2727272727272725}\n{'loss': 0.243, 'grad_norm': 1.1742497682571411, 'learning_rate': 7.980692335438571e-05, 'epoch': 2.2954545454545454}\n{'loss': 0.2556, 'grad_norm': 0.846938967704773, 'learning_rate': 7.731295699956117e-05, 'epoch': 2.3181818181818183}\n{'loss': 0.2173, 'grad_norm': 0.6930986046791077, 'learning_rate': 7.48189906447366e-05, 'epoch': 2.340909090909091}\n{'loss': 0.2477, 'grad_norm': 0.7259805202484131, 'learning_rate': 7.232502428991206e-05, 'epoch': 2.3636363636363638}\n{'loss': 0.2411, 'grad_norm': 0.7434989213943481, 'learning_rate': 6.98310579350875e-05, 'epoch': 2.3863636363636362}\n{'loss': 0.2972, 'grad_norm': 0.8489946126937866, 'learning_rate': 6.733709158026295e-05, 'epoch': 2.409090909090909}\n{'loss': 0.221, 'grad_norm': 0.7444751262664795, 'learning_rate': 6.484312522543839e-05, 'epoch': 2.4318181818181817}\n{'loss': 0.2485, 'grad_norm': 0.8100225925445557, 'learning_rate': 6.234915887061384e-05, 'epoch': 2.4545454545454546}\n{'loss': 0.2566, 'grad_norm': 0.7311607599258423, 'learning_rate': 5.985519251578929e-05, 'epoch': 2.4772727272727275}\n{'loss': 0.2172, 'grad_norm': 0.7709413766860962, 'learning_rate': 5.7361226160964734e-05, 'epoch': 2.5}\n{'loss': 0.2698, 'grad_norm': 0.824806809425354, 'learning_rate': 5.486725980614018e-05, 'epoch': 2.5227272727272725}\n{'loss': 0.2692, 'grad_norm': 0.8346193432807922, 'learning_rate': 5.2373293451315624e-05, 'epoch': 2.5454545454545454}\n{'loss': 0.2741, 'grad_norm': 0.867904543876648, 'learning_rate': 4.987932709649107e-05, 'epoch': 2.5681818181818183}\n{'loss': 0.3333, 'grad_norm': 1.231932282447815, 'learning_rate': 4.7385360741666513e-05, 'epoch': 2.590909090909091}\n{'loss': 0.2039, 'grad_norm': 0.9057144522666931, 'learning_rate': 4.4891394386841965e-05, 'epoch': 2.6136363636363638}\n{'loss': 0.231, 'grad_norm': 1.1344412565231323, 'learning_rate': 4.239742803201741e-05, 'epoch': 2.6363636363636362}\n{'loss': 0.2382, 'grad_norm': 1.290059208869934, 'learning_rate': 3.9903461677192855e-05, 'epoch': 2.659090909090909}\n{'loss': 0.1864, 'grad_norm': 0.796561062335968, 'learning_rate': 3.74094953223683e-05, 'epoch': 2.6818181818181817}\n{'loss': 0.2453, 'grad_norm': 0.9320436120033264, 'learning_rate': 3.491552896754375e-05, 'epoch': 2.7045454545454546}\n{'loss': 0.2295, 'grad_norm': 0.7178849577903748, 'learning_rate': 3.2421562612719196e-05, 'epoch': 2.7272727272727275}\n{'loss': 0.2495, 'grad_norm': 0.8061529397964478, 'learning_rate': 2.9927596257894645e-05, 'epoch': 2.75}\n{'loss': 0.2189, 'grad_norm': 0.6941757202148438, 'learning_rate': 2.743362990307009e-05, 'epoch': 2.7727272727272725}\n{'loss': 0.2733, 'grad_norm': 0.8574644327163696, 'learning_rate': 2.4939663548245534e-05, 'epoch': 2.7954545454545454}\n{'loss': 0.2566, 'grad_norm': 0.7775949239730835, 'learning_rate': 2.2445697193420983e-05, 'epoch': 2.8181818181818183}\n{'loss': 0.2579, 'grad_norm': 0.8797775506973267, 'learning_rate': 1.9951730838596427e-05, 'epoch': 2.840909090909091}\n{'loss': 0.2237, 'grad_norm': 0.8946062326431274, 'learning_rate': 1.7457764483771876e-05, 'epoch': 2.8636363636363638}\n{'loss': 0.2021, 'grad_norm': 1.1935737133026123, 'learning_rate': 1.4963798128947322e-05, 'epoch': 2.8863636363636362}\n{'loss': 0.2481, 'grad_norm': 1.2861524820327759, 'learning_rate': 1.2469831774122767e-05, 'epoch': 2.909090909090909}\n{'loss': 0.2248, 'grad_norm': 0.7578257322311401, 'learning_rate': 9.975865419298214e-06, 'epoch': 2.9318181818181817}\n{'loss': 0.281, 'grad_norm': 1.1146996021270752, 'learning_rate': 7.481899064473661e-06, 'epoch': 2.9545454545454546}\n{'loss': 0.2694, 'grad_norm': 1.1695303916931152, 'learning_rate': 4.987932709649107e-06, 'epoch': 2.9772727272727275}\n{'loss': 0.0951, 'grad_norm': 2.088757038116455, 'learning_rate': 2.4939663548245534e-06, 'epoch': 3.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8459    0.8561    0.8510      3142\n     PERCENT     0.7209    0.9394    0.8158        66\n        TYPE     0.9368    0.9657    0.9510     11415\n      VOLUME     0.6190    0.5571    0.5865        70\n\n   micro avg     0.9151    0.9402    0.9275     14693\n   macro avg     0.7807    0.8296    0.8011     14693\nweighted avg     0.9149    0.9402    0.9273     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.2777043282985687, 'eval_f1_macro': 0.8010734676963351, 'eval_precision': 0.9150824667152414, 'eval_recall': 0.9402436534404138, 'eval_f1': 0.9274924471299094, 'eval_accuracy': 0.9191424529328545, 'eval_runtime': 1.5077, 'eval_samples_per_second': 3655.349, 'eval_steps_per_second': 7.296, 'epoch': 3.0}\n{'train_runtime': 18.78, 'train_samples_per_second': 3520.93, 'train_steps_per_second': 7.029, 'train_loss': 0.5143068142122391, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nThe speedups for torchdynamo mostly come with GPU Ampere or higher and which is not detected here.\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8459    0.8561    0.8510      3142\n     PERCENT     0.7209    0.9394    0.8158        66\n        TYPE     0.9368    0.9657    0.9510     11415\n      VOLUME     0.6190    0.5571    0.5865        70\n\n   micro avg     0.9151    0.9402    0.9275     14693\n   macro avg     0.7807    0.8296    0.8011     14693\nweighted avg     0.9149    0.9402    0.9273     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.2777043282985687, 'eval_f1_macro': 0.8010734676963351, 'eval_precision': 0.9150824667152414, 'eval_recall': 0.9402436534404138, 'eval_f1': 0.9274924471299094, 'eval_accuracy': 0.9191424529328545, 'eval_runtime': 1.5639, 'eval_samples_per_second': 3523.986, 'eval_steps_per_second': 7.034, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/3364797558.py:66: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 2.2408, 'grad_norm': 7.283573150634766, 'learning_rate': 0.0, 'epoch': 0.022727272727272728}\n{'loss': 2.2636, 'grad_norm': 7.4505839347839355, 'learning_rate': 2.1020573562092666e-05, 'epoch': 0.045454545454545456}\n{'loss': 2.2195, 'grad_norm': 7.442996501922607, 'learning_rate': 4.204114712418533e-05, 'epoch': 0.06818181818181818}\n{'loss': 2.0982, 'grad_norm': 6.95726203918457, 'learning_rate': 6.3061720686278e-05, 'epoch': 0.09090909090909091}\n{'loss': 1.9812, 'grad_norm': 6.713710784912109, 'learning_rate': 8.408229424837066e-05, 'epoch': 0.11363636363636363}\n{'loss': 1.7939, 'grad_norm': 6.446583271026611, 'learning_rate': 0.00010510286781046334, 'epoch': 0.13636363636363635}\n{'loss': 1.6168, 'grad_norm': 5.503057956695557, 'learning_rate': 0.000126123441372556, 'epoch': 0.1590909090909091}\n{'loss': 1.4577, 'grad_norm': 3.7349021434783936, 'learning_rate': 0.00014714401493464866, 'epoch': 0.18181818181818182}\n{'loss': 1.2501, 'grad_norm': 2.5840721130371094, 'learning_rate': 0.00016816458849674133, 'epoch': 0.20454545454545456}\n{'loss': 1.2524, 'grad_norm': 2.385420799255371, 'learning_rate': 0.00018918516205883402, 'epoch': 0.22727272727272727}\n{'loss': 1.1435, 'grad_norm': 2.6537013053894043, 'learning_rate': 0.00021020573562092668, 'epoch': 0.25}\n{'loss': 1.2081, 'grad_norm': 3.2588467597961426, 'learning_rate': 0.00023122630918301931, 'epoch': 0.2727272727272727}\n{'loss': 1.0749, 'grad_norm': 2.1804680824279785, 'learning_rate': 0.000252246882745112, 'epoch': 0.29545454545454547}\n{'loss': 0.9561, 'grad_norm': 0.9566770195960999, 'learning_rate': 0.0002732674563072047, 'epoch': 0.3181818181818182}\n{'loss': 1.0172, 'grad_norm': 1.7148228883743286, 'learning_rate': 0.00029428802986929733, 'epoch': 0.3409090909090909}\n{'loss': 0.9777, 'grad_norm': 2.7872252464294434, 'learning_rate': 0.00029179406351447277, 'epoch': 0.36363636363636365}\n{'loss': 0.9056, 'grad_norm': 2.529149293899536, 'learning_rate': 0.00028930009715964826, 'epoch': 0.38636363636363635}\n{'loss': 0.8596, 'grad_norm': 1.641646146774292, 'learning_rate': 0.00028680613080482364, 'epoch': 0.4090909090909091}\n{'loss': 0.8153, 'grad_norm': 0.923757016658783, 'learning_rate': 0.0002843121644499991, 'epoch': 0.4318181818181818}\n{'loss': 0.792, 'grad_norm': 1.164458155632019, 'learning_rate': 0.00028181819809517457, 'epoch': 0.45454545454545453}\n{'loss': 0.7298, 'grad_norm': 1.0449775457382202, 'learning_rate': 0.00027932423174035, 'epoch': 0.4772727272727273}\n{'loss': 0.6298, 'grad_norm': 0.9124418497085571, 'learning_rate': 0.00027683026538552545, 'epoch': 0.5}\n{'loss': 0.6344, 'grad_norm': 1.0080465078353882, 'learning_rate': 0.0002743362990307009, 'epoch': 0.5227272727272727}\n{'loss': 0.6871, 'grad_norm': 0.7149638533592224, 'learning_rate': 0.0002718423326758763, 'epoch': 0.5454545454545454}\n{'loss': 0.5968, 'grad_norm': 0.7682139277458191, 'learning_rate': 0.0002693483663210518, 'epoch': 0.5681818181818182}\n{'loss': 0.5336, 'grad_norm': 0.7146570682525635, 'learning_rate': 0.00026685439996622726, 'epoch': 0.5909090909090909}\n{'loss': 0.6304, 'grad_norm': 0.9784286618232727, 'learning_rate': 0.0002643604336114027, 'epoch': 0.6136363636363636}\n{'loss': 0.5239, 'grad_norm': 1.189443826675415, 'learning_rate': 0.00026186646725657813, 'epoch': 0.6363636363636364}\n{'loss': 0.6198, 'grad_norm': 0.9326949119567871, 'learning_rate': 0.00025937250090175357, 'epoch': 0.6590909090909091}\n{'loss': 0.5531, 'grad_norm': 1.6971728801727295, 'learning_rate': 0.000256878534546929, 'epoch': 0.6818181818181818}\n{'loss': 0.5444, 'grad_norm': 2.1058599948883057, 'learning_rate': 0.0002543845681921045, 'epoch': 0.7045454545454546}\n{'loss': 0.5513, 'grad_norm': 1.2782584428787231, 'learning_rate': 0.00025189060183727994, 'epoch': 0.7272727272727273}\n{'loss': 0.5073, 'grad_norm': 1.359287142753601, 'learning_rate': 0.0002493966354824554, 'epoch': 0.75}\n{'loss': 0.5126, 'grad_norm': 1.0516705513000488, 'learning_rate': 0.0002469026691276308, 'epoch': 0.7727272727272727}\n{'loss': 0.4681, 'grad_norm': 1.7322922945022583, 'learning_rate': 0.00024440870277280625, 'epoch': 0.7954545454545454}\n{'loss': 0.4703, 'grad_norm': 1.7472562789916992, 'learning_rate': 0.00024191473641798172, 'epoch': 0.8181818181818182}\n{'loss': 0.4295, 'grad_norm': 0.8978191614151001, 'learning_rate': 0.00023942077006315716, 'epoch': 0.8409090909090909}\n{'loss': 0.4788, 'grad_norm': 0.8688552379608154, 'learning_rate': 0.0002369268037083326, 'epoch': 0.8636363636363636}\n{'loss': 0.4275, 'grad_norm': 1.5213332176208496, 'learning_rate': 0.00023443283735350803, 'epoch': 0.8863636363636364}\n{'loss': 0.4443, 'grad_norm': 1.5346851348876953, 'learning_rate': 0.0002319388709986835, 'epoch': 0.9090909090909091}\n{'loss': 0.4968, 'grad_norm': 0.8019891977310181, 'learning_rate': 0.00022944490464385894, 'epoch': 0.9318181818181818}\n{'loss': 0.4578, 'grad_norm': 0.9087729454040527, 'learning_rate': 0.0002269509382890344, 'epoch': 0.9545454545454546}\n{'loss': 0.441, 'grad_norm': 1.1519160270690918, 'learning_rate': 0.0002244569719342098, 'epoch': 0.9772727272727273}\n{'loss': 0.2607, 'grad_norm': 2.4505820274353027, 'learning_rate': 0.00022196300557938528, 'epoch': 1.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8318    0.6845    0.7510      3404\n     PERCENT     0.4667    0.9859    0.6335        71\n        TYPE     0.8789    0.9544    0.9151     11194\n      VOLUME     0.2222    0.0357    0.0615        56\n\n   micro avg     0.8657    0.8887    0.8770     14725\n   macro avg     0.5999    0.6651    0.5903     14725\nweighted avg     0.8635    0.8887    0.8726     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.4151645302772522, 'eval_f1_macro': 0.5902869124174039, 'eval_precision': 0.8657052130193172, 'eval_recall': 0.8886926994906621, 'eval_f1': 0.877048356288328, 'eval_accuracy': 0.8742531381899907, 'eval_runtime': 1.5078, 'eval_samples_per_second': 3654.985, 'eval_steps_per_second': 7.295, 'epoch': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.3792, 'grad_norm': 1.1909968852996826, 'learning_rate': 0.00021946903922456072, 'epoch': 1.0227272727272727}\n{'loss': 0.3449, 'grad_norm': 0.7981386184692383, 'learning_rate': 0.00021697507286973618, 'epoch': 1.0454545454545454}\n{'loss': 0.3413, 'grad_norm': 0.8129541873931885, 'learning_rate': 0.0002144811065149116, 'epoch': 1.0681818181818181}\n{'loss': 0.355, 'grad_norm': 0.8348253965377808, 'learning_rate': 0.00021198714016008706, 'epoch': 1.0909090909090908}\n{'loss': 0.3916, 'grad_norm': 0.9712562561035156, 'learning_rate': 0.0002094931738052625, 'epoch': 1.1136363636363635}\n{'loss': 0.2937, 'grad_norm': 1.2454073429107666, 'learning_rate': 0.00020699920745043796, 'epoch': 1.1363636363636362}\n{'loss': 0.341, 'grad_norm': 0.7883387207984924, 'learning_rate': 0.0002045052410956134, 'epoch': 1.1590909090909092}\n{'loss': 0.39, 'grad_norm': 0.9859524369239807, 'learning_rate': 0.00020201127474078884, 'epoch': 1.1818181818181819}\n{'loss': 0.3449, 'grad_norm': 1.0300096273422241, 'learning_rate': 0.00019951730838596427, 'epoch': 1.2045454545454546}\n{'loss': 0.4005, 'grad_norm': 1.4852125644683838, 'learning_rate': 0.00019702334203113974, 'epoch': 1.2272727272727273}\n{'loss': 0.3756, 'grad_norm': 0.9098193049430847, 'learning_rate': 0.00019452937567631518, 'epoch': 1.25}\n{'loss': 0.3249, 'grad_norm': 1.921177625656128, 'learning_rate': 0.00019203540932149064, 'epoch': 1.2727272727272727}\n{'loss': 0.2884, 'grad_norm': 1.6240670680999756, 'learning_rate': 0.00018954144296666605, 'epoch': 1.2954545454545454}\n{'loss': 0.3189, 'grad_norm': 1.1461730003356934, 'learning_rate': 0.00018704747661184152, 'epoch': 1.3181818181818181}\n{'loss': 0.3577, 'grad_norm': 0.932694137096405, 'learning_rate': 0.00018455351025701696, 'epoch': 1.3409090909090908}\n{'loss': 0.3628, 'grad_norm': 1.163557529449463, 'learning_rate': 0.00018205954390219242, 'epoch': 1.3636363636363638}\n{'loss': 0.302, 'grad_norm': 1.559288501739502, 'learning_rate': 0.00017956557754736786, 'epoch': 1.3863636363636362}\n{'loss': 0.3014, 'grad_norm': 1.0707299709320068, 'learning_rate': 0.0001770716111925433, 'epoch': 1.4090909090909092}\n{'loss': 0.3628, 'grad_norm': 1.0105606317520142, 'learning_rate': 0.00017457764483771874, 'epoch': 1.4318181818181819}\n{'loss': 0.3208, 'grad_norm': 0.7202736139297485, 'learning_rate': 0.0001720836784828942, 'epoch': 1.4545454545454546}\n{'loss': 0.3311, 'grad_norm': 2.250434160232544, 'learning_rate': 0.00016958971212806964, 'epoch': 1.4772727272727273}\n{'loss': 0.3548, 'grad_norm': 2.8445005416870117, 'learning_rate': 0.0001670957457732451, 'epoch': 1.5}\n{'loss': 0.3579, 'grad_norm': 1.4864076375961304, 'learning_rate': 0.00016460177941842052, 'epoch': 1.5227272727272727}\n{'loss': 0.2768, 'grad_norm': 0.6483047604560852, 'learning_rate': 0.00016210781306359598, 'epoch': 1.5454545454545454}\n{'loss': 0.3232, 'grad_norm': 1.1292917728424072, 'learning_rate': 0.00015961384670877142, 'epoch': 1.5681818181818183}\n{'loss': 0.2925, 'grad_norm': 0.8724044561386108, 'learning_rate': 0.00015711988035394688, 'epoch': 1.5909090909090908}\n{'loss': 0.2841, 'grad_norm': 0.9743534326553345, 'learning_rate': 0.00015462591399912235, 'epoch': 1.6136363636363638}\n{'loss': 0.3208, 'grad_norm': 1.0480509996414185, 'learning_rate': 0.00015213194764429776, 'epoch': 1.6363636363636362}\n{'loss': 0.306, 'grad_norm': 0.879987895488739, 'learning_rate': 0.0001496379812894732, 'epoch': 1.6590909090909092}\n{'loss': 0.3228, 'grad_norm': 0.765023410320282, 'learning_rate': 0.00014714401493464866, 'epoch': 1.6818181818181817}\n{'loss': 0.2963, 'grad_norm': 0.8729733228683472, 'learning_rate': 0.00014465004857982413, 'epoch': 1.7045454545454546}\n{'loss': 0.335, 'grad_norm': 1.0527235269546509, 'learning_rate': 0.00014215608222499954, 'epoch': 1.7272727272727273}\n{'loss': 0.2764, 'grad_norm': 0.7512578964233398, 'learning_rate': 0.000139662115870175, 'epoch': 1.75}\n{'loss': 0.2303, 'grad_norm': 0.877648115158081, 'learning_rate': 0.00013716814951535044, 'epoch': 1.7727272727272727}\n{'loss': 0.3375, 'grad_norm': 1.3638116121292114, 'learning_rate': 0.0001346741831605259, 'epoch': 1.7954545454545454}\n{'loss': 0.3131, 'grad_norm': 0.7447211146354675, 'learning_rate': 0.00013218021680570135, 'epoch': 1.8181818181818183}\n{'loss': 0.3376, 'grad_norm': 0.6828750371932983, 'learning_rate': 0.00012968625045087679, 'epoch': 1.8409090909090908}\n{'loss': 0.2787, 'grad_norm': 0.7485587000846863, 'learning_rate': 0.00012719228409605225, 'epoch': 1.8636363636363638}\n{'loss': 0.3188, 'grad_norm': 0.726165235042572, 'learning_rate': 0.0001246983177412277, 'epoch': 1.8863636363636362}\n{'loss': 0.3003, 'grad_norm': 1.1533089876174927, 'learning_rate': 0.00012220435138640313, 'epoch': 1.9090909090909092}\n{'loss': 0.2511, 'grad_norm': 0.6893157362937927, 'learning_rate': 0.00011971038503157858, 'epoch': 1.9318181818181817}\n{'loss': 0.2912, 'grad_norm': 0.9458146095275879, 'learning_rate': 0.00011721641867675402, 'epoch': 1.9545454545454546}\n{'loss': 0.2715, 'grad_norm': 0.8375453352928162, 'learning_rate': 0.00011472245232192947, 'epoch': 1.9772727272727273}\n{'loss': 0.1836, 'grad_norm': 2.909635543823242, 'learning_rate': 0.0001122284859671049, 'epoch': 2.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8389    0.8334    0.8361      3404\n     PERCENT     0.8846    0.9718    0.9262        71\n        TYPE     0.9219    0.9661    0.9435     11194\n      VOLUME     0.5079    0.5714    0.5378        56\n\n   micro avg     0.9016    0.9340    0.9175     14725\n   macro avg     0.7883    0.8357    0.8109     14725\nweighted avg     0.9010    0.9340    0.9171     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.2985810339450836, 'eval_f1_macro': 0.8109085721299888, 'eval_precision': 0.9015995804379179, 'eval_recall': 0.9339898132427844, 'eval_f1': 0.9175089229127055, 'eval_accuracy': 0.911911418078167, 'eval_runtime': 1.4749, 'eval_samples_per_second': 3736.434, 'eval_steps_per_second': 7.458, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.222, 'grad_norm': 0.862048327922821, 'learning_rate': 0.00010973451961228036, 'epoch': 2.022727272727273}\n{'loss': 0.2586, 'grad_norm': 0.6582316160202026, 'learning_rate': 0.0001072405532574558, 'epoch': 2.0454545454545454}\n{'loss': 0.2149, 'grad_norm': 0.809230387210846, 'learning_rate': 0.00010474658690263125, 'epoch': 2.0681818181818183}\n{'loss': 0.274, 'grad_norm': 0.9246962070465088, 'learning_rate': 0.0001022526205478067, 'epoch': 2.090909090909091}\n{'loss': 0.2528, 'grad_norm': 0.7521098256111145, 'learning_rate': 9.975865419298214e-05, 'epoch': 2.1136363636363638}\n{'loss': 0.2377, 'grad_norm': 0.6847628951072693, 'learning_rate': 9.726468783815759e-05, 'epoch': 2.1363636363636362}\n{'loss': 0.2478, 'grad_norm': 1.3446084260940552, 'learning_rate': 9.477072148333303e-05, 'epoch': 2.159090909090909}\n{'loss': 0.293, 'grad_norm': 0.730758547782898, 'learning_rate': 9.227675512850848e-05, 'epoch': 2.1818181818181817}\n{'loss': 0.2661, 'grad_norm': 1.0424572229385376, 'learning_rate': 8.978278877368393e-05, 'epoch': 2.2045454545454546}\n{'loss': 0.2293, 'grad_norm': 1.1187759637832642, 'learning_rate': 8.728882241885937e-05, 'epoch': 2.227272727272727}\n{'loss': 0.2807, 'grad_norm': 1.5022015571594238, 'learning_rate': 8.479485606403482e-05, 'epoch': 2.25}\n{'loss': 0.2437, 'grad_norm': 0.7551407814025879, 'learning_rate': 8.230088970921026e-05, 'epoch': 2.2727272727272725}\n{'loss': 0.2085, 'grad_norm': 1.0713108777999878, 'learning_rate': 7.980692335438571e-05, 'epoch': 2.2954545454545454}\n{'loss': 0.2146, 'grad_norm': 0.897497296333313, 'learning_rate': 7.731295699956117e-05, 'epoch': 2.3181818181818183}\n{'loss': 0.2085, 'grad_norm': 0.9309154748916626, 'learning_rate': 7.48189906447366e-05, 'epoch': 2.340909090909091}\n{'loss': 0.2623, 'grad_norm': 1.3357982635498047, 'learning_rate': 7.232502428991206e-05, 'epoch': 2.3636363636363638}\n{'loss': 0.2337, 'grad_norm': 0.639102041721344, 'learning_rate': 6.98310579350875e-05, 'epoch': 2.3863636363636362}\n{'loss': 0.2371, 'grad_norm': 0.844129204750061, 'learning_rate': 6.733709158026295e-05, 'epoch': 2.409090909090909}\n{'loss': 0.1982, 'grad_norm': 0.6404356360435486, 'learning_rate': 6.484312522543839e-05, 'epoch': 2.4318181818181817}\n{'loss': 0.2326, 'grad_norm': 0.8090794086456299, 'learning_rate': 6.234915887061384e-05, 'epoch': 2.4545454545454546}\n{'loss': 0.2182, 'grad_norm': 0.9864354133605957, 'learning_rate': 5.985519251578929e-05, 'epoch': 2.4772727272727275}\n{'loss': 0.2103, 'grad_norm': 0.7084989547729492, 'learning_rate': 5.7361226160964734e-05, 'epoch': 2.5}\n{'loss': 0.238, 'grad_norm': 0.7574012279510498, 'learning_rate': 5.486725980614018e-05, 'epoch': 2.5227272727272725}\n{'loss': 0.2129, 'grad_norm': 1.0982095003128052, 'learning_rate': 5.2373293451315624e-05, 'epoch': 2.5454545454545454}\n{'loss': 0.2729, 'grad_norm': 0.9534264802932739, 'learning_rate': 4.987932709649107e-05, 'epoch': 2.5681818181818183}\n{'loss': 0.2248, 'grad_norm': 1.105644702911377, 'learning_rate': 4.7385360741666513e-05, 'epoch': 2.590909090909091}\n{'loss': 0.2607, 'grad_norm': 0.811312735080719, 'learning_rate': 4.4891394386841965e-05, 'epoch': 2.6136363636363638}\n{'loss': 0.1687, 'grad_norm': 0.7304725646972656, 'learning_rate': 4.239742803201741e-05, 'epoch': 2.6363636363636362}\n{'loss': 0.1887, 'grad_norm': 0.700244665145874, 'learning_rate': 3.9903461677192855e-05, 'epoch': 2.659090909090909}\n{'loss': 0.1997, 'grad_norm': 0.7236558198928833, 'learning_rate': 3.74094953223683e-05, 'epoch': 2.6818181818181817}\n{'loss': 0.2526, 'grad_norm': 0.7156515717506409, 'learning_rate': 3.491552896754375e-05, 'epoch': 2.7045454545454546}\n{'loss': 0.1896, 'grad_norm': 0.8244599103927612, 'learning_rate': 3.2421562612719196e-05, 'epoch': 2.7272727272727275}\n{'loss': 0.242, 'grad_norm': 0.9470564723014832, 'learning_rate': 2.9927596257894645e-05, 'epoch': 2.75}\n{'loss': 0.2423, 'grad_norm': 0.6960398554801941, 'learning_rate': 2.743362990307009e-05, 'epoch': 2.7727272727272725}\n{'loss': 0.2087, 'grad_norm': 0.6876685619354248, 'learning_rate': 2.4939663548245534e-05, 'epoch': 2.7954545454545454}\n{'loss': 0.2258, 'grad_norm': 0.9867485761642456, 'learning_rate': 2.2445697193420983e-05, 'epoch': 2.8181818181818183}\n{'loss': 0.2337, 'grad_norm': 0.9968451857566833, 'learning_rate': 1.9951730838596427e-05, 'epoch': 2.840909090909091}\n{'loss': 0.2672, 'grad_norm': 1.0190181732177734, 'learning_rate': 1.7457764483771876e-05, 'epoch': 2.8636363636363638}\n{'loss': 0.2499, 'grad_norm': 0.793419599533081, 'learning_rate': 1.4963798128947322e-05, 'epoch': 2.8863636363636362}\n{'loss': 0.1918, 'grad_norm': 0.8241267800331116, 'learning_rate': 1.2469831774122767e-05, 'epoch': 2.909090909090909}\n{'loss': 0.2059, 'grad_norm': 1.3881404399871826, 'learning_rate': 9.975865419298214e-06, 'epoch': 2.9318181818181817}\n{'loss': 0.2366, 'grad_norm': 1.0808991193771362, 'learning_rate': 7.481899064473661e-06, 'epoch': 2.9545454545454546}\n{'loss': 0.2056, 'grad_norm': 1.4844359159469604, 'learning_rate': 4.987932709649107e-06, 'epoch': 2.9772727272727275}\n{'loss': 0.1439, 'grad_norm': 3.005307912826538, 'learning_rate': 2.4939663548245534e-06, 'epoch': 3.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8562    0.8499    0.8530      3404\n     PERCENT     0.9583    0.9718    0.9650        71\n        TYPE     0.9276    0.9665    0.9466     11194\n      VOLUME     0.5758    0.6786    0.6230        56\n\n   micro avg     0.9103    0.9385    0.9242     14725\n   macro avg     0.8295    0.8667    0.8469     14725\nweighted avg     0.9099    0.9385    0.9238     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.2824018895626068, 'eval_f1_macro': 0.8469069190902834, 'eval_precision': 0.9102825900797049, 'eval_recall': 0.9384719864176571, 'eval_f1': 0.924162375443055, 'eval_accuracy': 0.917776681466864, 'eval_runtime': 1.4743, 'eval_samples_per_second': 3738.044, 'eval_steps_per_second': 7.461, 'epoch': 3.0}\n{'train_runtime': 18.6377, 'train_samples_per_second': 3547.817, 'train_steps_per_second': 7.082, 'train_loss': 0.4905203619238102, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nThe speedups for torchdynamo mostly come with GPU Ampere or higher and which is not detected here.\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8562    0.8499    0.8530      3404\n     PERCENT     0.9583    0.9718    0.9650        71\n        TYPE     0.9276    0.9665    0.9466     11194\n      VOLUME     0.5758    0.6786    0.6230        56\n\n   micro avg     0.9103    0.9385    0.9242     14725\n   macro avg     0.8295    0.8667    0.8469     14725\nweighted avg     0.9099    0.9385    0.9238     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.2824018895626068, 'eval_f1_macro': 0.8469069190902834, 'eval_precision': 0.9102825900797049, 'eval_recall': 0.9384719864176571, 'eval_f1': 0.924162375443055, 'eval_accuracy': 0.917776681466864, 'eval_runtime': 1.5465, 'eval_samples_per_second': 3563.445, 'eval_steps_per_second': 7.113, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/3364797558.py:66: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 2.1429, 'grad_norm': 6.842605113983154, 'learning_rate': 0.0, 'epoch': 0.022727272727272728}\n{'loss': 2.1359, 'grad_norm': 6.875817775726318, 'learning_rate': 2.1020573562092666e-05, 'epoch': 0.045454545454545456}\n{'loss': 2.0975, 'grad_norm': 6.80778169631958, 'learning_rate': 4.204114712418533e-05, 'epoch': 0.06818181818181818}\n{'loss': 2.0197, 'grad_norm': 6.410953998565674, 'learning_rate': 6.3061720686278e-05, 'epoch': 0.09090909090909091}\n{'loss': 1.8873, 'grad_norm': 6.14910364151001, 'learning_rate': 8.408229424837066e-05, 'epoch': 0.11363636363636363}\n{'loss': 1.7204, 'grad_norm': 6.033485412597656, 'learning_rate': 0.00010510286781046334, 'epoch': 0.13636363636363635}\n{'loss': 1.6079, 'grad_norm': 4.690435409545898, 'learning_rate': 0.000126123441372556, 'epoch': 0.1590909090909091}\n{'loss': 1.422, 'grad_norm': 4.050168037414551, 'learning_rate': 0.00014714401493464866, 'epoch': 0.18181818181818182}\n{'loss': 1.2907, 'grad_norm': 2.889741897583008, 'learning_rate': 0.00016816458849674133, 'epoch': 0.20454545454545456}\n{'loss': 1.2059, 'grad_norm': 2.010035276412964, 'learning_rate': 0.00018918516205883402, 'epoch': 0.22727272727272727}\n{'loss': 1.187, 'grad_norm': 2.1088743209838867, 'learning_rate': 0.00021020573562092668, 'epoch': 0.25}\n{'loss': 1.1339, 'grad_norm': 1.9942888021469116, 'learning_rate': 0.00023122630918301931, 'epoch': 0.2727272727272727}\n{'loss': 1.0222, 'grad_norm': 1.395026445388794, 'learning_rate': 0.000252246882745112, 'epoch': 0.29545454545454547}\n{'loss': 1.0002, 'grad_norm': 1.4517494440078735, 'learning_rate': 0.0002732674563072047, 'epoch': 0.3181818181818182}\n{'loss': 0.9858, 'grad_norm': 1.9716781377792358, 'learning_rate': 0.00029428802986929733, 'epoch': 0.3409090909090909}\n{'loss': 0.9042, 'grad_norm': 2.4250497817993164, 'learning_rate': 0.00029179406351447277, 'epoch': 0.36363636363636365}\n{'loss': 0.8357, 'grad_norm': 1.4513890743255615, 'learning_rate': 0.00028930009715964826, 'epoch': 0.38636363636363635}\n{'loss': 0.7845, 'grad_norm': 1.2164866924285889, 'learning_rate': 0.00028680613080482364, 'epoch': 0.4090909090909091}\n{'loss': 0.7468, 'grad_norm': 1.5374220609664917, 'learning_rate': 0.0002843121644499991, 'epoch': 0.4318181818181818}\n{'loss': 0.745, 'grad_norm': 1.494385838508606, 'learning_rate': 0.00028181819809517457, 'epoch': 0.45454545454545453}\n{'loss': 0.7095, 'grad_norm': 0.8449129462242126, 'learning_rate': 0.00027932423174035, 'epoch': 0.4772727272727273}\n{'loss': 0.7116, 'grad_norm': 1.3305246829986572, 'learning_rate': 0.00027683026538552545, 'epoch': 0.5}\n{'loss': 0.6358, 'grad_norm': 1.9753307104110718, 'learning_rate': 0.0002743362990307009, 'epoch': 0.5227272727272727}\n{'loss': 0.6146, 'grad_norm': 1.2771261930465698, 'learning_rate': 0.0002718423326758763, 'epoch': 0.5454545454545454}\n{'loss': 0.5434, 'grad_norm': 0.7848759293556213, 'learning_rate': 0.0002693483663210518, 'epoch': 0.5681818181818182}\n{'loss': 0.626, 'grad_norm': 1.1826611757278442, 'learning_rate': 0.00026685439996622726, 'epoch': 0.5909090909090909}\n{'loss': 0.6067, 'grad_norm': 1.1292403936386108, 'learning_rate': 0.0002643604336114027, 'epoch': 0.6136363636363636}\n{'loss': 0.6424, 'grad_norm': 0.8732590675354004, 'learning_rate': 0.00026186646725657813, 'epoch': 0.6363636363636364}\n{'loss': 0.4989, 'grad_norm': 1.6972935199737549, 'learning_rate': 0.00025937250090175357, 'epoch': 0.6590909090909091}\n{'loss': 0.5418, 'grad_norm': 1.0502493381500244, 'learning_rate': 0.000256878534546929, 'epoch': 0.6818181818181818}\n{'loss': 0.5616, 'grad_norm': 1.7689296007156372, 'learning_rate': 0.0002543845681921045, 'epoch': 0.7045454545454546}\n{'loss': 0.5734, 'grad_norm': 1.0125401020050049, 'learning_rate': 0.00025189060183727994, 'epoch': 0.7272727272727273}\n{'loss': 0.5343, 'grad_norm': 1.1932162046432495, 'learning_rate': 0.0002493966354824554, 'epoch': 0.75}\n{'loss': 0.4949, 'grad_norm': 1.1292880773544312, 'learning_rate': 0.0002469026691276308, 'epoch': 0.7727272727272727}\n{'loss': 0.5561, 'grad_norm': 1.0657405853271484, 'learning_rate': 0.00024440870277280625, 'epoch': 0.7954545454545454}\n{'loss': 0.5313, 'grad_norm': 1.1153700351715088, 'learning_rate': 0.00024191473641798172, 'epoch': 0.8181818181818182}\n{'loss': 0.4314, 'grad_norm': 1.3097138404846191, 'learning_rate': 0.00023942077006315716, 'epoch': 0.8409090909090909}\n{'loss': 0.4717, 'grad_norm': 0.9802197217941284, 'learning_rate': 0.0002369268037083326, 'epoch': 0.8636363636363636}\n{'loss': 0.4614, 'grad_norm': 0.8139058947563171, 'learning_rate': 0.00023443283735350803, 'epoch': 0.8863636363636364}\n{'loss': 0.4142, 'grad_norm': 0.6601833701133728, 'learning_rate': 0.0002319388709986835, 'epoch': 0.9090909090909091}\n{'loss': 0.4072, 'grad_norm': 1.1098105907440186, 'learning_rate': 0.00022944490464385894, 'epoch': 0.9318181818181818}\n{'loss': 0.4223, 'grad_norm': 1.0723477602005005, 'learning_rate': 0.0002269509382890344, 'epoch': 0.9545454545454546}\n{'loss': 0.4478, 'grad_norm': 0.9648287296295166, 'learning_rate': 0.0002244569719342098, 'epoch': 0.9772727272727273}\n{'loss': 0.3848, 'grad_norm': 2.671964406967163, 'learning_rate': 0.00022196300557938528, 'epoch': 1.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.7824    0.7690    0.7756      3311\n     PERCENT     0.4793    0.9419    0.6353        86\n        TYPE     0.9091    0.9442    0.9263     11299\n      VOLUME     0.0000    0.0000    0.0000        42\n\n   micro avg     0.8769    0.9022    0.8894     14738\n   macro avg     0.5427    0.6638    0.5843     14738\nweighted avg     0.8755    0.9022    0.8881     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.38085493445396423, 'eval_f1_macro': 0.5843129870750345, 'eval_precision': 0.8769291650178077, 'eval_recall': 0.9021576876102592, 'eval_f1': 0.8893645484949834, 'eval_accuracy': 0.8859625157095241, 'eval_runtime': 1.4976, 'eval_samples_per_second': 3679.107, 'eval_steps_per_second': 7.345, 'epoch': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.3528, 'grad_norm': 0.8694474697113037, 'learning_rate': 0.00021946903922456072, 'epoch': 1.0227272727272727}\n{'loss': 0.3739, 'grad_norm': 0.744333803653717, 'learning_rate': 0.00021697507286973618, 'epoch': 1.0454545454545454}\n{'loss': 0.3904, 'grad_norm': 1.1745939254760742, 'learning_rate': 0.0002144811065149116, 'epoch': 1.0681818181818181}\n{'loss': 0.3646, 'grad_norm': 0.7210742235183716, 'learning_rate': 0.00021198714016008706, 'epoch': 1.0909090909090908}\n{'loss': 0.3597, 'grad_norm': 1.5664979219436646, 'learning_rate': 0.0002094931738052625, 'epoch': 1.1136363636363635}\n{'loss': 0.4288, 'grad_norm': 0.8521194458007812, 'learning_rate': 0.00020699920745043796, 'epoch': 1.1363636363636362}\n{'loss': 0.3093, 'grad_norm': 0.7649602890014648, 'learning_rate': 0.0002045052410956134, 'epoch': 1.1590909090909092}\n{'loss': 0.3421, 'grad_norm': 1.4194362163543701, 'learning_rate': 0.00020201127474078884, 'epoch': 1.1818181818181819}\n{'loss': 0.3421, 'grad_norm': 0.7332205176353455, 'learning_rate': 0.00019951730838596427, 'epoch': 1.2045454545454546}\n{'loss': 0.3924, 'grad_norm': 0.866263210773468, 'learning_rate': 0.00019702334203113974, 'epoch': 1.2272727272727273}\n{'loss': 0.376, 'grad_norm': 0.8364346027374268, 'learning_rate': 0.00019452937567631518, 'epoch': 1.25}\n{'loss': 0.3102, 'grad_norm': 0.8616718649864197, 'learning_rate': 0.00019203540932149064, 'epoch': 1.2727272727272727}\n{'loss': 0.3187, 'grad_norm': 1.1192258596420288, 'learning_rate': 0.00018954144296666605, 'epoch': 1.2954545454545454}\n{'loss': 0.3812, 'grad_norm': 0.8852147459983826, 'learning_rate': 0.00018704747661184152, 'epoch': 1.3181818181818181}\n{'loss': 0.2892, 'grad_norm': 1.0619298219680786, 'learning_rate': 0.00018455351025701696, 'epoch': 1.3409090909090908}\n{'loss': 0.3618, 'grad_norm': 1.5715157985687256, 'learning_rate': 0.00018205954390219242, 'epoch': 1.3636363636363638}\n{'loss': 0.3171, 'grad_norm': 2.0546374320983887, 'learning_rate': 0.00017956557754736786, 'epoch': 1.3863636363636362}\n{'loss': 0.3542, 'grad_norm': 1.0413542985916138, 'learning_rate': 0.0001770716111925433, 'epoch': 1.4090909090909092}\n{'loss': 0.3015, 'grad_norm': 0.8016534447669983, 'learning_rate': 0.00017457764483771874, 'epoch': 1.4318181818181819}\n{'loss': 0.3116, 'grad_norm': 1.0669631958007812, 'learning_rate': 0.0001720836784828942, 'epoch': 1.4545454545454546}\n{'loss': 0.2643, 'grad_norm': 0.8845372200012207, 'learning_rate': 0.00016958971212806964, 'epoch': 1.4772727272727273}\n{'loss': 0.3453, 'grad_norm': 1.2696466445922852, 'learning_rate': 0.0001670957457732451, 'epoch': 1.5}\n{'loss': 0.3176, 'grad_norm': 1.016905665397644, 'learning_rate': 0.00016460177941842052, 'epoch': 1.5227272727272727}\n{'loss': 0.2838, 'grad_norm': 0.8762774467468262, 'learning_rate': 0.00016210781306359598, 'epoch': 1.5454545454545454}\n{'loss': 0.286, 'grad_norm': 1.0705941915512085, 'learning_rate': 0.00015961384670877142, 'epoch': 1.5681818181818183}\n{'loss': 0.3009, 'grad_norm': 0.8819915652275085, 'learning_rate': 0.00015711988035394688, 'epoch': 1.5909090909090908}\n{'loss': 0.2924, 'grad_norm': 1.598740816116333, 'learning_rate': 0.00015462591399912235, 'epoch': 1.6136363636363638}\n{'loss': 0.3218, 'grad_norm': 1.0864312648773193, 'learning_rate': 0.00015213194764429776, 'epoch': 1.6363636363636362}\n{'loss': 0.2886, 'grad_norm': 1.0705955028533936, 'learning_rate': 0.0001496379812894732, 'epoch': 1.6590909090909092}\n{'loss': 0.3061, 'grad_norm': 0.6579791307449341, 'learning_rate': 0.00014714401493464866, 'epoch': 1.6818181818181817}\n{'loss': 0.2954, 'grad_norm': 1.1555399894714355, 'learning_rate': 0.00014465004857982413, 'epoch': 1.7045454545454546}\n{'loss': 0.2916, 'grad_norm': 0.9259047508239746, 'learning_rate': 0.00014215608222499954, 'epoch': 1.7272727272727273}\n{'loss': 0.2811, 'grad_norm': 0.9037377834320068, 'learning_rate': 0.000139662115870175, 'epoch': 1.75}\n{'loss': 0.2996, 'grad_norm': 1.1764625310897827, 'learning_rate': 0.00013716814951535044, 'epoch': 1.7727272727272727}\n{'loss': 0.2466, 'grad_norm': 0.9690900444984436, 'learning_rate': 0.0001346741831605259, 'epoch': 1.7954545454545454}\n{'loss': 0.2937, 'grad_norm': 0.8838911056518555, 'learning_rate': 0.00013218021680570135, 'epoch': 1.8181818181818183}\n{'loss': 0.2457, 'grad_norm': 1.0920193195343018, 'learning_rate': 0.00012968625045087679, 'epoch': 1.8409090909090908}\n{'loss': 0.3113, 'grad_norm': 0.8715301752090454, 'learning_rate': 0.00012719228409605225, 'epoch': 1.8636363636363638}\n{'loss': 0.3092, 'grad_norm': 0.9896939992904663, 'learning_rate': 0.0001246983177412277, 'epoch': 1.8863636363636362}\n{'loss': 0.3073, 'grad_norm': 0.9259383678436279, 'learning_rate': 0.00012220435138640313, 'epoch': 1.9090909090909092}\n{'loss': 0.3502, 'grad_norm': 0.8189594149589539, 'learning_rate': 0.00011971038503157858, 'epoch': 1.9318181818181817}\n{'loss': 0.317, 'grad_norm': 0.925153374671936, 'learning_rate': 0.00011721641867675402, 'epoch': 1.9545454545454546}\n{'loss': 0.2386, 'grad_norm': 0.729075014591217, 'learning_rate': 0.00011472245232192947, 'epoch': 1.9772727272727273}\n{'loss': 0.2188, 'grad_norm': 5.112240314483643, 'learning_rate': 0.0001122284859671049, 'epoch': 2.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8411    0.8550    0.8480      3311\n     PERCENT     0.7117    0.9186    0.8020        86\n        TYPE     0.9334    0.9657    0.9492     11299\n      VOLUME     0.4242    0.3333    0.3733        42\n\n   micro avg     0.9102    0.9387    0.9242     14738\n   macro avg     0.7276    0.7682    0.7431     14738\nweighted avg     0.9099    0.9387    0.9240     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.2844521403312683, 'eval_f1_macro': 0.7431465009712168, 'eval_precision': 0.9101973684210526, 'eval_recall': 0.9387298140860361, 'eval_f1': 0.9242434364352995, 'eval_accuracy': 0.91738156384897, 'eval_runtime': 1.4786, 'eval_samples_per_second': 3726.612, 'eval_steps_per_second': 7.44, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.236, 'grad_norm': 0.8359770774841309, 'learning_rate': 0.00010973451961228036, 'epoch': 2.022727272727273}\n{'loss': 0.2174, 'grad_norm': 0.8569695949554443, 'learning_rate': 0.0001072405532574558, 'epoch': 2.0454545454545454}\n{'loss': 0.2532, 'grad_norm': 1.0329877138137817, 'learning_rate': 0.00010474658690263125, 'epoch': 2.0681818181818183}\n{'loss': 0.2689, 'grad_norm': 1.1647413969039917, 'learning_rate': 0.0001022526205478067, 'epoch': 2.090909090909091}\n{'loss': 0.2361, 'grad_norm': 0.8136911392211914, 'learning_rate': 9.975865419298214e-05, 'epoch': 2.1136363636363638}\n{'loss': 0.1811, 'grad_norm': 0.6907818913459778, 'learning_rate': 9.726468783815759e-05, 'epoch': 2.1363636363636362}\n{'loss': 0.2426, 'grad_norm': 0.9581597447395325, 'learning_rate': 9.477072148333303e-05, 'epoch': 2.159090909090909}\n{'loss': 0.2523, 'grad_norm': 1.0315414667129517, 'learning_rate': 9.227675512850848e-05, 'epoch': 2.1818181818181817}\n{'loss': 0.2631, 'grad_norm': 0.9638235569000244, 'learning_rate': 8.978278877368393e-05, 'epoch': 2.2045454545454546}\n{'loss': 0.2835, 'grad_norm': 1.0533864498138428, 'learning_rate': 8.728882241885937e-05, 'epoch': 2.227272727272727}\n{'loss': 0.2666, 'grad_norm': 0.7969104647636414, 'learning_rate': 8.479485606403482e-05, 'epoch': 2.25}\n{'loss': 0.2112, 'grad_norm': 1.251808762550354, 'learning_rate': 8.230088970921026e-05, 'epoch': 2.2727272727272725}\n{'loss': 0.2909, 'grad_norm': 0.8123745918273926, 'learning_rate': 7.980692335438571e-05, 'epoch': 2.2954545454545454}\n{'loss': 0.2576, 'grad_norm': 1.3836385011672974, 'learning_rate': 7.731295699956117e-05, 'epoch': 2.3181818181818183}\n{'loss': 0.169, 'grad_norm': 1.178705096244812, 'learning_rate': 7.48189906447366e-05, 'epoch': 2.340909090909091}\n{'loss': 0.2287, 'grad_norm': 0.9660494327545166, 'learning_rate': 7.232502428991206e-05, 'epoch': 2.3636363636363638}\n{'loss': 0.2065, 'grad_norm': 1.0998162031173706, 'learning_rate': 6.98310579350875e-05, 'epoch': 2.3863636363636362}\n{'loss': 0.2752, 'grad_norm': 1.0353422164916992, 'learning_rate': 6.733709158026295e-05, 'epoch': 2.409090909090909}\n{'loss': 0.2278, 'grad_norm': 0.7198585867881775, 'learning_rate': 6.484312522543839e-05, 'epoch': 2.4318181818181817}\n{'loss': 0.1863, 'grad_norm': 1.0461360216140747, 'learning_rate': 6.234915887061384e-05, 'epoch': 2.4545454545454546}\n{'loss': 0.2212, 'grad_norm': 0.9210683107376099, 'learning_rate': 5.985519251578929e-05, 'epoch': 2.4772727272727275}\n{'loss': 0.199, 'grad_norm': 1.060178518295288, 'learning_rate': 5.7361226160964734e-05, 'epoch': 2.5}\n{'loss': 0.2131, 'grad_norm': 0.7375576496124268, 'learning_rate': 5.486725980614018e-05, 'epoch': 2.5227272727272725}\n{'loss': 0.2144, 'grad_norm': 0.6912103295326233, 'learning_rate': 5.2373293451315624e-05, 'epoch': 2.5454545454545454}\n{'loss': 0.2144, 'grad_norm': 1.1431082487106323, 'learning_rate': 4.987932709649107e-05, 'epoch': 2.5681818181818183}\n{'loss': 0.2391, 'grad_norm': 1.0122580528259277, 'learning_rate': 4.7385360741666513e-05, 'epoch': 2.590909090909091}\n{'loss': 0.2399, 'grad_norm': 0.8322534561157227, 'learning_rate': 4.4891394386841965e-05, 'epoch': 2.6136363636363638}\n{'loss': 0.2585, 'grad_norm': 0.70539790391922, 'learning_rate': 4.239742803201741e-05, 'epoch': 2.6363636363636362}\n{'loss': 0.1958, 'grad_norm': 0.7159880995750427, 'learning_rate': 3.9903461677192855e-05, 'epoch': 2.659090909090909}\n{'loss': 0.178, 'grad_norm': 0.6653509140014648, 'learning_rate': 3.74094953223683e-05, 'epoch': 2.6818181818181817}\n{'loss': 0.1949, 'grad_norm': 0.7044500708580017, 'learning_rate': 3.491552896754375e-05, 'epoch': 2.7045454545454546}\n{'loss': 0.2881, 'grad_norm': 1.006360650062561, 'learning_rate': 3.2421562612719196e-05, 'epoch': 2.7272727272727275}\n{'loss': 0.2292, 'grad_norm': 0.9534394145011902, 'learning_rate': 2.9927596257894645e-05, 'epoch': 2.75}\n{'loss': 0.2361, 'grad_norm': 1.291739583015442, 'learning_rate': 2.743362990307009e-05, 'epoch': 2.7727272727272725}\n{'loss': 0.2137, 'grad_norm': 0.6933306455612183, 'learning_rate': 2.4939663548245534e-05, 'epoch': 2.7954545454545454}\n{'loss': 0.2271, 'grad_norm': 0.9000587463378906, 'learning_rate': 2.2445697193420983e-05, 'epoch': 2.8181818181818183}\n{'loss': 0.2656, 'grad_norm': 1.1503373384475708, 'learning_rate': 1.9951730838596427e-05, 'epoch': 2.840909090909091}\n{'loss': 0.1963, 'grad_norm': 0.763420581817627, 'learning_rate': 1.7457764483771876e-05, 'epoch': 2.8636363636363638}\n{'loss': 0.2336, 'grad_norm': 0.9415919184684753, 'learning_rate': 1.4963798128947322e-05, 'epoch': 2.8863636363636362}\n{'loss': 0.1994, 'grad_norm': 0.748389720916748, 'learning_rate': 1.2469831774122767e-05, 'epoch': 2.909090909090909}\n{'loss': 0.2392, 'grad_norm': 1.0842779874801636, 'learning_rate': 9.975865419298214e-06, 'epoch': 2.9318181818181817}\n{'loss': 0.2386, 'grad_norm': 0.8533337712287903, 'learning_rate': 7.481899064473661e-06, 'epoch': 2.9545454545454546}\n{'loss': 0.2311, 'grad_norm': 0.7550435066223145, 'learning_rate': 4.987932709649107e-06, 'epoch': 2.9772727272727275}\n{'loss': 0.1318, 'grad_norm': 2.1878600120544434, 'learning_rate': 2.4939663548245534e-06, 'epoch': 3.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8433    0.8744    0.8585      3311\n     PERCENT     0.8172    0.8837    0.8492        86\n        TYPE     0.9397    0.9657    0.9525     11299\n      VOLUME     0.6923    0.6429    0.6667        42\n\n   micro avg     0.9165    0.9438    0.9299     14738\n   macro avg     0.8231    0.8416    0.8317     14738\nweighted avg     0.9166    0.9438    0.9300     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.2696182131767273, 'eval_f1_macro': 0.8317198560364006, 'eval_precision': 0.9165129151291513, 'eval_recall': 0.9437508481476455, 'eval_f1': 0.9299324730895233, 'eval_accuracy': 0.9225178951969838, 'eval_runtime': 1.4735, 'eval_samples_per_second': 3739.483, 'eval_steps_per_second': 7.465, 'epoch': 3.0}\n{'train_runtime': 18.75, 'train_samples_per_second': 3526.717, 'train_steps_per_second': 7.04, 'train_loss': 0.48288502058748045, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nThe speedups for torchdynamo mostly come with GPU Ampere or higher and which is not detected here.\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8433    0.8744    0.8585      3311\n     PERCENT     0.8172    0.8837    0.8492        86\n        TYPE     0.9397    0.9657    0.9525     11299\n      VOLUME     0.6923    0.6429    0.6667        42\n\n   micro avg     0.9165    0.9438    0.9299     14738\n   macro avg     0.8231    0.8416    0.8317     14738\nweighted avg     0.9166    0.9438    0.9300     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.2696182131767273, 'eval_f1_macro': 0.8317198560364006, 'eval_precision': 0.9165129151291513, 'eval_recall': 0.9437508481476455, 'eval_f1': 0.9299324730895233, 'eval_accuracy': 0.9225178951969838, 'eval_runtime': 1.5362, 'eval_samples_per_second': 3586.813, 'eval_steps_per_second': 7.161, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/3364797558.py:66: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 2.3038, 'grad_norm': 7.201316833496094, 'learning_rate': 0.0, 'epoch': 0.022727272727272728}\n{'loss': 2.3049, 'grad_norm': 7.560040473937988, 'learning_rate': 2.1020573562092666e-05, 'epoch': 0.045454545454545456}\n{'loss': 2.2571, 'grad_norm': 7.085115909576416, 'learning_rate': 4.204114712418533e-05, 'epoch': 0.06818181818181818}\n{'loss': 2.1473, 'grad_norm': 7.42470645904541, 'learning_rate': 6.3061720686278e-05, 'epoch': 0.09090909090909091}\n{'loss': 2.0534, 'grad_norm': 6.704670429229736, 'learning_rate': 8.408229424837066e-05, 'epoch': 0.11363636363636363}\n{'loss': 1.8865, 'grad_norm': 5.910343647003174, 'learning_rate': 0.00010510286781046334, 'epoch': 0.13636363636363635}\n{'loss': 1.6892, 'grad_norm': 5.4183807373046875, 'learning_rate': 0.000126123441372556, 'epoch': 0.1590909090909091}\n{'loss': 1.5309, 'grad_norm': 4.211606979370117, 'learning_rate': 0.00014714401493464866, 'epoch': 0.18181818181818182}\n{'loss': 1.3685, 'grad_norm': 3.0454280376434326, 'learning_rate': 0.00016816458849674133, 'epoch': 0.20454545454545456}\n{'loss': 1.1978, 'grad_norm': 1.9376773834228516, 'learning_rate': 0.00018918516205883402, 'epoch': 0.22727272727272727}\n{'loss': 1.1219, 'grad_norm': 1.9684447050094604, 'learning_rate': 0.00021020573562092668, 'epoch': 0.25}\n{'loss': 1.1193, 'grad_norm': 2.3073160648345947, 'learning_rate': 0.00023122630918301931, 'epoch': 0.2727272727272727}\n{'loss': 1.1242, 'grad_norm': 1.8859108686447144, 'learning_rate': 0.000252246882745112, 'epoch': 0.29545454545454547}\n{'loss': 1.0767, 'grad_norm': 1.0904158353805542, 'learning_rate': 0.0002732674563072047, 'epoch': 0.3181818181818182}\n{'loss': 1.0118, 'grad_norm': 1.7665609121322632, 'learning_rate': 0.00029428802986929733, 'epoch': 0.3409090909090909}\n{'loss': 0.9662, 'grad_norm': 2.7032010555267334, 'learning_rate': 0.00029179406351447277, 'epoch': 0.36363636363636365}\n{'loss': 0.9159, 'grad_norm': 1.7431145906448364, 'learning_rate': 0.00028930009715964826, 'epoch': 0.38636363636363635}\n{'loss': 0.8217, 'grad_norm': 1.2376028299331665, 'learning_rate': 0.00028680613080482364, 'epoch': 0.4090909090909091}\n{'loss': 0.7911, 'grad_norm': 1.3818776607513428, 'learning_rate': 0.0002843121644499991, 'epoch': 0.4318181818181818}\n{'loss': 0.7618, 'grad_norm': 1.3615871667861938, 'learning_rate': 0.00028181819809517457, 'epoch': 0.45454545454545453}\n{'loss': 0.7101, 'grad_norm': 0.8259408473968506, 'learning_rate': 0.00027932423174035, 'epoch': 0.4772727272727273}\n{'loss': 0.6866, 'grad_norm': 1.0817492008209229, 'learning_rate': 0.00027683026538552545, 'epoch': 0.5}\n{'loss': 0.6685, 'grad_norm': 1.1410596370697021, 'learning_rate': 0.0002743362990307009, 'epoch': 0.5227272727272727}\n{'loss': 0.6206, 'grad_norm': 0.9125118851661682, 'learning_rate': 0.0002718423326758763, 'epoch': 0.5454545454545454}\n{'loss': 0.5924, 'grad_norm': 1.2936196327209473, 'learning_rate': 0.0002693483663210518, 'epoch': 0.5681818181818182}\n{'loss': 0.6155, 'grad_norm': 1.4975463151931763, 'learning_rate': 0.00026685439996622726, 'epoch': 0.5909090909090909}\n{'loss': 0.7043, 'grad_norm': 1.614203929901123, 'learning_rate': 0.0002643604336114027, 'epoch': 0.6136363636363636}\n{'loss': 0.6121, 'grad_norm': 0.7049008011817932, 'learning_rate': 0.00026186646725657813, 'epoch': 0.6363636363636364}\n{'loss': 0.6711, 'grad_norm': 0.9745535850524902, 'learning_rate': 0.00025937250090175357, 'epoch': 0.6590909090909091}\n{'loss': 0.6156, 'grad_norm': 1.1715093851089478, 'learning_rate': 0.000256878534546929, 'epoch': 0.6818181818181818}\n{'loss': 0.5641, 'grad_norm': 1.2208759784698486, 'learning_rate': 0.0002543845681921045, 'epoch': 0.7045454545454546}\n{'loss': 0.5537, 'grad_norm': 1.4187872409820557, 'learning_rate': 0.00025189060183727994, 'epoch': 0.7272727272727273}\n{'loss': 0.5002, 'grad_norm': 0.7947198748588562, 'learning_rate': 0.0002493966354824554, 'epoch': 0.75}\n{'loss': 0.5449, 'grad_norm': 1.3012059926986694, 'learning_rate': 0.0002469026691276308, 'epoch': 0.7727272727272727}\n{'loss': 0.5231, 'grad_norm': 1.7805386781692505, 'learning_rate': 0.00024440870277280625, 'epoch': 0.7954545454545454}\n{'loss': 0.4788, 'grad_norm': 1.0706090927124023, 'learning_rate': 0.00024191473641798172, 'epoch': 0.8181818181818182}\n{'loss': 0.4967, 'grad_norm': 0.6784432530403137, 'learning_rate': 0.00023942077006315716, 'epoch': 0.8409090909090909}\n{'loss': 0.4516, 'grad_norm': 1.337576985359192, 'learning_rate': 0.0002369268037083326, 'epoch': 0.8636363636363636}\n{'loss': 0.4409, 'grad_norm': 1.8293907642364502, 'learning_rate': 0.00023443283735350803, 'epoch': 0.8863636363636364}\n{'loss': 0.4161, 'grad_norm': 1.052087426185608, 'learning_rate': 0.0002319388709986835, 'epoch': 0.9090909090909091}\n{'loss': 0.4378, 'grad_norm': 0.9262012839317322, 'learning_rate': 0.00022944490464385894, 'epoch': 0.9318181818181818}\n{'loss': 0.4255, 'grad_norm': 0.8690332770347595, 'learning_rate': 0.0002269509382890344, 'epoch': 0.9545454545454546}\n{'loss': 0.4482, 'grad_norm': 1.36471688747406, 'learning_rate': 0.0002244569719342098, 'epoch': 0.9772727272727273}\n{'loss': 0.4304, 'grad_norm': 3.738400459289551, 'learning_rate': 0.00022196300557938528, 'epoch': 1.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8267    0.6927    0.7538      3456\n     PERCENT     0.5435    0.9740    0.6977        77\n        TYPE     0.8894    0.9426    0.9152     11282\n      VOLUME     0.0000    0.0000    0.0000        41\n\n   micro avg     0.8732    0.8820    0.8776     14856\n   macro avg     0.5649    0.6523    0.5917     14856\nweighted avg     0.8705    0.8820    0.8740     14856\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.42129331827163696, 'eval_f1_macro': 0.5916596088455581, 'eval_precision': 0.8732422525824725, 'eval_recall': 0.8820005385029618, 'eval_f1': 0.8775995445564448, 'eval_accuracy': 0.8741840731070496, 'eval_runtime': 1.4856, 'eval_samples_per_second': 3708.892, 'eval_steps_per_second': 7.404, 'epoch': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.4097, 'grad_norm': 1.8145742416381836, 'learning_rate': 0.00021946903922456072, 'epoch': 1.0227272727272727}\n{'loss': 0.4299, 'grad_norm': 1.1487343311309814, 'learning_rate': 0.00021697507286973618, 'epoch': 1.0454545454545454}\n{'loss': 0.4037, 'grad_norm': 1.2011101245880127, 'learning_rate': 0.0002144811065149116, 'epoch': 1.0681818181818181}\n{'loss': 0.3946, 'grad_norm': 3.2511441707611084, 'learning_rate': 0.00021198714016008706, 'epoch': 1.0909090909090908}\n{'loss': 0.3582, 'grad_norm': 1.686730980873108, 'learning_rate': 0.0002094931738052625, 'epoch': 1.1136363636363635}\n{'loss': 0.3837, 'grad_norm': 1.5283066034317017, 'learning_rate': 0.00020699920745043796, 'epoch': 1.1363636363636362}\n{'loss': 0.3742, 'grad_norm': 1.1316450834274292, 'learning_rate': 0.0002045052410956134, 'epoch': 1.1590909090909092}\n{'loss': 0.3024, 'grad_norm': 1.076912522315979, 'learning_rate': 0.00020201127474078884, 'epoch': 1.1818181818181819}\n{'loss': 0.3651, 'grad_norm': 0.9184483885765076, 'learning_rate': 0.00019951730838596427, 'epoch': 1.2045454545454546}\n{'loss': 0.3101, 'grad_norm': 0.8471822738647461, 'learning_rate': 0.00019702334203113974, 'epoch': 1.2272727272727273}\n{'loss': 0.3719, 'grad_norm': 1.0788052082061768, 'learning_rate': 0.00019452937567631518, 'epoch': 1.25}\n{'loss': 0.3113, 'grad_norm': 1.3515545129776, 'learning_rate': 0.00019203540932149064, 'epoch': 1.2727272727272727}\n{'loss': 0.3612, 'grad_norm': 1.3841955661773682, 'learning_rate': 0.00018954144296666605, 'epoch': 1.2954545454545454}\n{'loss': 0.3846, 'grad_norm': 0.7291477918624878, 'learning_rate': 0.00018704747661184152, 'epoch': 1.3181818181818181}\n{'loss': 0.3452, 'grad_norm': 1.2718751430511475, 'learning_rate': 0.00018455351025701696, 'epoch': 1.3409090909090908}\n{'loss': 0.3405, 'grad_norm': 0.7933705449104309, 'learning_rate': 0.00018205954390219242, 'epoch': 1.3636363636363638}\n{'loss': 0.3241, 'grad_norm': 0.9294999241828918, 'learning_rate': 0.00017956557754736786, 'epoch': 1.3863636363636362}\n{'loss': 0.3844, 'grad_norm': 1.246856451034546, 'learning_rate': 0.0001770716111925433, 'epoch': 1.4090909090909092}\n{'loss': 0.3401, 'grad_norm': 0.8357996344566345, 'learning_rate': 0.00017457764483771874, 'epoch': 1.4318181818181819}\n{'loss': 0.2832, 'grad_norm': 1.5897597074508667, 'learning_rate': 0.0001720836784828942, 'epoch': 1.4545454545454546}\n{'loss': 0.2978, 'grad_norm': 1.9197503328323364, 'learning_rate': 0.00016958971212806964, 'epoch': 1.4772727272727273}\n{'loss': 0.3404, 'grad_norm': 0.8269006609916687, 'learning_rate': 0.0001670957457732451, 'epoch': 1.5}\n{'loss': 0.3166, 'grad_norm': 0.8622393012046814, 'learning_rate': 0.00016460177941842052, 'epoch': 1.5227272727272727}\n{'loss': 0.2777, 'grad_norm': 0.7590110301971436, 'learning_rate': 0.00016210781306359598, 'epoch': 1.5454545454545454}\n{'loss': 0.3429, 'grad_norm': 1.2062737941741943, 'learning_rate': 0.00015961384670877142, 'epoch': 1.5681818181818183}\n{'loss': 0.364, 'grad_norm': 1.059412956237793, 'learning_rate': 0.00015711988035394688, 'epoch': 1.5909090909090908}\n{'loss': 0.3208, 'grad_norm': 1.0762858390808105, 'learning_rate': 0.00015462591399912235, 'epoch': 1.6136363636363638}\n{'loss': 0.3344, 'grad_norm': 1.555176854133606, 'learning_rate': 0.00015213194764429776, 'epoch': 1.6363636363636362}\n{'loss': 0.2832, 'grad_norm': 1.333797812461853, 'learning_rate': 0.0001496379812894732, 'epoch': 1.6590909090909092}\n{'loss': 0.3065, 'grad_norm': 1.113945484161377, 'learning_rate': 0.00014714401493464866, 'epoch': 1.6818181818181817}\n{'loss': 0.3542, 'grad_norm': 1.2904952764511108, 'learning_rate': 0.00014465004857982413, 'epoch': 1.7045454545454546}\n{'loss': 0.3537, 'grad_norm': 0.9937655329704285, 'learning_rate': 0.00014215608222499954, 'epoch': 1.7272727272727273}\n{'loss': 0.3443, 'grad_norm': 0.7168304324150085, 'learning_rate': 0.000139662115870175, 'epoch': 1.75}\n{'loss': 0.2759, 'grad_norm': 0.8185266852378845, 'learning_rate': 0.00013716814951535044, 'epoch': 1.7727272727272727}\n{'loss': 0.2843, 'grad_norm': 0.8836145997047424, 'learning_rate': 0.0001346741831605259, 'epoch': 1.7954545454545454}\n{'loss': 0.2793, 'grad_norm': 0.9353247880935669, 'learning_rate': 0.00013218021680570135, 'epoch': 1.8181818181818183}\n{'loss': 0.3373, 'grad_norm': 1.1514315605163574, 'learning_rate': 0.00012968625045087679, 'epoch': 1.8409090909090908}\n{'loss': 0.2821, 'grad_norm': 0.9724804162979126, 'learning_rate': 0.00012719228409605225, 'epoch': 1.8636363636363638}\n{'loss': 0.3017, 'grad_norm': 0.6759523153305054, 'learning_rate': 0.0001246983177412277, 'epoch': 1.8863636363636362}\n{'loss': 0.2924, 'grad_norm': 0.880372166633606, 'learning_rate': 0.00012220435138640313, 'epoch': 1.9090909090909092}\n{'loss': 0.3115, 'grad_norm': 0.8976892232894897, 'learning_rate': 0.00011971038503157858, 'epoch': 1.9318181818181817}\n{'loss': 0.252, 'grad_norm': 0.8403673768043518, 'learning_rate': 0.00011721641867675402, 'epoch': 1.9545454545454546}\n{'loss': 0.3116, 'grad_norm': 0.7969104051589966, 'learning_rate': 0.00011472245232192947, 'epoch': 1.9772727272727273}\n{'loss': 0.3186, 'grad_norm': 3.5625905990600586, 'learning_rate': 0.0001122284859671049, 'epoch': 2.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8480    0.8267    0.8372      3456\n     PERCENT     0.8132    0.9610    0.8810        77\n        TYPE     0.9258    0.9661    0.9455     11282\n      VOLUME     0.6750    0.6585    0.6667        41\n\n   micro avg     0.9073    0.9328    0.9198     14856\n   macro avg     0.8155    0.8531    0.8326     14856\nweighted avg     0.9064    0.9328    0.9192     14856\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.28890490531921387, 'eval_f1_macro': 0.832578342282311, 'eval_precision': 0.9072873698683952, 'eval_recall': 0.9327544426494345, 'eval_f1': 0.91984466792791, 'eval_accuracy': 0.9155243690165361, 'eval_runtime': 1.5107, 'eval_samples_per_second': 3647.352, 'eval_steps_per_second': 7.281, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.232, 'grad_norm': 0.6670588254928589, 'learning_rate': 0.00010973451961228036, 'epoch': 2.022727272727273}\n{'loss': 0.2576, 'grad_norm': 0.9082595109939575, 'learning_rate': 0.0001072405532574558, 'epoch': 2.0454545454545454}\n{'loss': 0.2615, 'grad_norm': 0.7013043165206909, 'learning_rate': 0.00010474658690263125, 'epoch': 2.0681818181818183}\n{'loss': 0.2403, 'grad_norm': 0.9650198817253113, 'learning_rate': 0.0001022526205478067, 'epoch': 2.090909090909091}\n{'loss': 0.2283, 'grad_norm': 0.8252509236335754, 'learning_rate': 9.975865419298214e-05, 'epoch': 2.1136363636363638}\n{'loss': 0.2365, 'grad_norm': 0.7172549366950989, 'learning_rate': 9.726468783815759e-05, 'epoch': 2.1363636363636362}\n{'loss': 0.2059, 'grad_norm': 0.6717449426651001, 'learning_rate': 9.477072148333303e-05, 'epoch': 2.159090909090909}\n{'loss': 0.2011, 'grad_norm': 0.7792878746986389, 'learning_rate': 9.227675512850848e-05, 'epoch': 2.1818181818181817}\n{'loss': 0.2386, 'grad_norm': 0.8931635618209839, 'learning_rate': 8.978278877368393e-05, 'epoch': 2.2045454545454546}\n{'loss': 0.2283, 'grad_norm': 0.601076602935791, 'learning_rate': 8.728882241885937e-05, 'epoch': 2.227272727272727}\n{'loss': 0.2775, 'grad_norm': 0.826482892036438, 'learning_rate': 8.479485606403482e-05, 'epoch': 2.25}\n{'loss': 0.2126, 'grad_norm': 0.7725400924682617, 'learning_rate': 8.230088970921026e-05, 'epoch': 2.2727272727272725}\n{'loss': 0.2035, 'grad_norm': 0.6628443002700806, 'learning_rate': 7.980692335438571e-05, 'epoch': 2.2954545454545454}\n{'loss': 0.2387, 'grad_norm': 0.7187168002128601, 'learning_rate': 7.731295699956117e-05, 'epoch': 2.3181818181818183}\n{'loss': 0.2464, 'grad_norm': 0.8876037001609802, 'learning_rate': 7.48189906447366e-05, 'epoch': 2.340909090909091}\n{'loss': 0.238, 'grad_norm': 0.8623815774917603, 'learning_rate': 7.232502428991206e-05, 'epoch': 2.3636363636363638}\n{'loss': 0.2009, 'grad_norm': 0.5525551438331604, 'learning_rate': 6.98310579350875e-05, 'epoch': 2.3863636363636362}\n{'loss': 0.2067, 'grad_norm': 1.1998745203018188, 'learning_rate': 6.733709158026295e-05, 'epoch': 2.409090909090909}\n{'loss': 0.2235, 'grad_norm': 0.8971049189567566, 'learning_rate': 6.484312522543839e-05, 'epoch': 2.4318181818181817}\n{'loss': 0.2161, 'grad_norm': 0.8646625876426697, 'learning_rate': 6.234915887061384e-05, 'epoch': 2.4545454545454546}\n{'loss': 0.2764, 'grad_norm': 1.0303184986114502, 'learning_rate': 5.985519251578929e-05, 'epoch': 2.4772727272727275}\n{'loss': 0.2032, 'grad_norm': 0.742882251739502, 'learning_rate': 5.7361226160964734e-05, 'epoch': 2.5}\n{'loss': 0.1951, 'grad_norm': 0.6897192001342773, 'learning_rate': 5.486725980614018e-05, 'epoch': 2.5227272727272725}\n{'loss': 0.2531, 'grad_norm': 0.9519094228744507, 'learning_rate': 5.2373293451315624e-05, 'epoch': 2.5454545454545454}\n{'loss': 0.2079, 'grad_norm': 0.758071780204773, 'learning_rate': 4.987932709649107e-05, 'epoch': 2.5681818181818183}\n{'loss': 0.2673, 'grad_norm': 0.8665472865104675, 'learning_rate': 4.7385360741666513e-05, 'epoch': 2.590909090909091}\n{'loss': 0.1915, 'grad_norm': 0.9988605976104736, 'learning_rate': 4.4891394386841965e-05, 'epoch': 2.6136363636363638}\n{'loss': 0.2819, 'grad_norm': 1.1208173036575317, 'learning_rate': 4.239742803201741e-05, 'epoch': 2.6363636363636362}\n{'loss': 0.2569, 'grad_norm': 1.1048033237457275, 'learning_rate': 3.9903461677192855e-05, 'epoch': 2.659090909090909}\n{'loss': 0.2337, 'grad_norm': 0.9610354900360107, 'learning_rate': 3.74094953223683e-05, 'epoch': 2.6818181818181817}\n{'loss': 0.2192, 'grad_norm': 0.728183388710022, 'learning_rate': 3.491552896754375e-05, 'epoch': 2.7045454545454546}\n{'loss': 0.1835, 'grad_norm': 0.9111127853393555, 'learning_rate': 3.2421562612719196e-05, 'epoch': 2.7272727272727275}\n{'loss': 0.216, 'grad_norm': 1.0159014463424683, 'learning_rate': 2.9927596257894645e-05, 'epoch': 2.75}\n{'loss': 0.3114, 'grad_norm': 0.9373725652694702, 'learning_rate': 2.743362990307009e-05, 'epoch': 2.7727272727272725}\n{'loss': 0.2122, 'grad_norm': 1.012434720993042, 'learning_rate': 2.4939663548245534e-05, 'epoch': 2.7954545454545454}\n{'loss': 0.2349, 'grad_norm': 0.7260094881057739, 'learning_rate': 2.2445697193420983e-05, 'epoch': 2.8181818181818183}\n{'loss': 0.2462, 'grad_norm': 1.6067464351654053, 'learning_rate': 1.9951730838596427e-05, 'epoch': 2.840909090909091}\n{'loss': 0.2706, 'grad_norm': 1.148145079612732, 'learning_rate': 1.7457764483771876e-05, 'epoch': 2.8636363636363638}\n{'loss': 0.2137, 'grad_norm': 0.9498087763786316, 'learning_rate': 1.4963798128947322e-05, 'epoch': 2.8863636363636362}\n{'loss': 0.2288, 'grad_norm': 0.7384145855903625, 'learning_rate': 1.2469831774122767e-05, 'epoch': 2.909090909090909}\n{'loss': 0.2442, 'grad_norm': 0.9076061844825745, 'learning_rate': 9.975865419298214e-06, 'epoch': 2.9318181818181817}\n{'loss': 0.2752, 'grad_norm': 0.8780331015586853, 'learning_rate': 7.481899064473661e-06, 'epoch': 2.9545454545454546}\n{'loss': 0.2419, 'grad_norm': 0.8674900531768799, 'learning_rate': 4.987932709649107e-06, 'epoch': 2.9772727272727275}\n{'loss': 0.1555, 'grad_norm': 2.7849605083465576, 'learning_rate': 2.4939663548245534e-06, 'epoch': 3.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8466    0.8524    0.8495      3456\n     PERCENT     0.8132    0.9610    0.8810        77\n        TYPE     0.9342    0.9629    0.9483     11282\n      VOLUME     0.7179    0.6829    0.7000        41\n\n   micro avg     0.9129    0.9364    0.9245     14856\n   macro avg     0.8280    0.8648    0.8447     14856\nweighted avg     0.9126    0.9364    0.9243     14856\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.27339425683021545, 'eval_f1_macro': 0.844688215235256, 'eval_precision': 0.9129150807192545, 'eval_recall': 0.936389337641357, 'eval_f1': 0.9245032232338672, 'eval_accuracy': 0.9195496083550914, 'eval_runtime': 1.5319, 'eval_samples_per_second': 3596.799, 'eval_steps_per_second': 7.181, 'epoch': 3.0}\n{'train_runtime': 18.8279, 'train_samples_per_second': 3512.128, 'train_steps_per_second': 7.011, 'train_loss': 0.5040522606083842, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nThe speedups for torchdynamo mostly come with GPU Ampere or higher and which is not detected here.\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8466    0.8524    0.8495      3456\n     PERCENT     0.8132    0.9610    0.8810        77\n        TYPE     0.9342    0.9629    0.9483     11282\n      VOLUME     0.7179    0.6829    0.7000        41\n\n   micro avg     0.9129    0.9364    0.9245     14856\n   macro avg     0.8280    0.8648    0.8447     14856\nweighted avg     0.9126    0.9364    0.9243     14856\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.27339425683021545, 'eval_f1_macro': 0.844688215235256, 'eval_precision': 0.9129150807192545, 'eval_recall': 0.936389337641357, 'eval_f1': 0.9245032232338672, 'eval_accuracy': 0.9195496083550914, 'eval_runtime': 1.55, 'eval_samples_per_second': 3554.896, 'eval_steps_per_second': 7.097, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/3364797558.py:66: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 2.1798, 'grad_norm': 7.4479780197143555, 'learning_rate': 0.0, 'epoch': 0.022727272727272728}\n{'loss': 2.1728, 'grad_norm': 7.209383487701416, 'learning_rate': 2.1020573562092666e-05, 'epoch': 0.045454545454545456}\n{'loss': 2.1322, 'grad_norm': 6.964213848114014, 'learning_rate': 4.204114712418533e-05, 'epoch': 0.06818181818181818}\n{'loss': 2.0415, 'grad_norm': 7.082885265350342, 'learning_rate': 6.3061720686278e-05, 'epoch': 0.09090909090909091}\n{'loss': 1.9165, 'grad_norm': 6.639068603515625, 'learning_rate': 8.408229424837066e-05, 'epoch': 0.11363636363636363}\n{'loss': 1.7547, 'grad_norm': 6.047759056091309, 'learning_rate': 0.00010510286781046334, 'epoch': 0.13636363636363635}\n{'loss': 1.6079, 'grad_norm': 4.8523783683776855, 'learning_rate': 0.000126123441372556, 'epoch': 0.1590909090909091}\n{'loss': 1.4244, 'grad_norm': 3.8932924270629883, 'learning_rate': 0.00014714401493464866, 'epoch': 0.18181818181818182}\n{'loss': 1.2607, 'grad_norm': 2.6040384769439697, 'learning_rate': 0.00016816458849674133, 'epoch': 0.20454545454545456}\n{'loss': 1.2551, 'grad_norm': 2.207441806793213, 'learning_rate': 0.00018918516205883402, 'epoch': 0.22727272727272727}\n{'loss': 1.1412, 'grad_norm': 2.005016803741455, 'learning_rate': 0.00021020573562092668, 'epoch': 0.25}\n{'loss': 1.1576, 'grad_norm': 2.5046496391296387, 'learning_rate': 0.00023122630918301931, 'epoch': 0.2727272727272727}\n{'loss': 1.0524, 'grad_norm': 1.4856430292129517, 'learning_rate': 0.000252246882745112, 'epoch': 0.29545454545454547}\n{'loss': 1.0169, 'grad_norm': 1.138508915901184, 'learning_rate': 0.0002732674563072047, 'epoch': 0.3181818181818182}\n{'loss': 1.0582, 'grad_norm': 1.7790155410766602, 'learning_rate': 0.00029428802986929733, 'epoch': 0.3409090909090909}\n{'loss': 0.9154, 'grad_norm': 2.569438934326172, 'learning_rate': 0.00029179406351447277, 'epoch': 0.36363636363636365}\n{'loss': 0.8605, 'grad_norm': 1.6331323385238647, 'learning_rate': 0.00028930009715964826, 'epoch': 0.38636363636363635}\n{'loss': 0.8042, 'grad_norm': 1.2229372262954712, 'learning_rate': 0.00028680613080482364, 'epoch': 0.4090909090909091}\n{'loss': 0.804, 'grad_norm': 1.080991506576538, 'learning_rate': 0.0002843121644499991, 'epoch': 0.4318181818181818}\n{'loss': 0.7183, 'grad_norm': 1.0140786170959473, 'learning_rate': 0.00028181819809517457, 'epoch': 0.45454545454545453}\n{'loss': 0.7128, 'grad_norm': 0.7201377749443054, 'learning_rate': 0.00027932423174035, 'epoch': 0.4772727272727273}\n{'loss': 0.6929, 'grad_norm': 1.227471947669983, 'learning_rate': 0.00027683026538552545, 'epoch': 0.5}\n{'loss': 0.6423, 'grad_norm': 0.9544108510017395, 'learning_rate': 0.0002743362990307009, 'epoch': 0.5227272727272727}\n{'loss': 0.6387, 'grad_norm': 0.6863664984703064, 'learning_rate': 0.0002718423326758763, 'epoch': 0.5454545454545454}\n{'loss': 0.5764, 'grad_norm': 1.1880908012390137, 'learning_rate': 0.0002693483663210518, 'epoch': 0.5681818181818182}\n{'loss': 0.5991, 'grad_norm': 1.0152615308761597, 'learning_rate': 0.00026685439996622726, 'epoch': 0.5909090909090909}\n{'loss': 0.605, 'grad_norm': 0.8675507307052612, 'learning_rate': 0.0002643604336114027, 'epoch': 0.6136363636363636}\n{'loss': 0.5679, 'grad_norm': 1.0453189611434937, 'learning_rate': 0.00026186646725657813, 'epoch': 0.6363636363636364}\n{'loss': 0.5883, 'grad_norm': 1.1049468517303467, 'learning_rate': 0.00025937250090175357, 'epoch': 0.6590909090909091}\n{'loss': 0.5106, 'grad_norm': 0.6030974388122559, 'learning_rate': 0.000256878534546929, 'epoch': 0.6818181818181818}\n{'loss': 0.51, 'grad_norm': 0.7416666746139526, 'learning_rate': 0.0002543845681921045, 'epoch': 0.7045454545454546}\n{'loss': 0.4935, 'grad_norm': 0.9795421361923218, 'learning_rate': 0.00025189060183727994, 'epoch': 0.7272727272727273}\n{'loss': 0.4685, 'grad_norm': 0.7951454520225525, 'learning_rate': 0.0002493966354824554, 'epoch': 0.75}\n{'loss': 0.466, 'grad_norm': 1.2481372356414795, 'learning_rate': 0.0002469026691276308, 'epoch': 0.7727272727272727}\n{'loss': 0.4782, 'grad_norm': 1.110190749168396, 'learning_rate': 0.00024440870277280625, 'epoch': 0.7954545454545454}\n{'loss': 0.4036, 'grad_norm': 0.6795696020126343, 'learning_rate': 0.00024191473641798172, 'epoch': 0.8181818181818182}\n{'loss': 0.484, 'grad_norm': 0.7829402089118958, 'learning_rate': 0.00023942077006315716, 'epoch': 0.8409090909090909}\n{'loss': 0.4344, 'grad_norm': 0.7020255923271179, 'learning_rate': 0.0002369268037083326, 'epoch': 0.8636363636363636}\n{'loss': 0.4739, 'grad_norm': 0.8153892755508423, 'learning_rate': 0.00023443283735350803, 'epoch': 0.8863636363636364}\n{'loss': 0.4464, 'grad_norm': 1.1106008291244507, 'learning_rate': 0.0002319388709986835, 'epoch': 0.9090909090909091}\n{'loss': 0.4015, 'grad_norm': 0.8778890371322632, 'learning_rate': 0.00022944490464385894, 'epoch': 0.9318181818181818}\n{'loss': 0.4018, 'grad_norm': 1.5803908109664917, 'learning_rate': 0.0002269509382890344, 'epoch': 0.9545454545454546}\n{'loss': 0.4627, 'grad_norm': 1.1338768005371094, 'learning_rate': 0.0002244569719342098, 'epoch': 0.9772727272727273}\n{'loss': 0.4459, 'grad_norm': 3.2056710720062256, 'learning_rate': 0.00022196300557938528, 'epoch': 1.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8183    0.7538    0.7847      3221\n     PERCENT     0.4909    0.9310    0.6429        87\n        TYPE     0.9060    0.9575    0.9311     11501\n      VOLUME     0.0000    0.0000    0.0000        59\n\n   micro avg     0.8838    0.9094    0.8964     14868\n   macro avg     0.5538    0.6606    0.5897     14868\nweighted avg     0.8810    0.9094    0.8940     14868\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.35994768142700195, 'eval_f1_macro': 0.589663081953917, 'eval_precision': 0.8838410249705844, 'eval_recall': 0.9094027441485069, 'eval_f1': 0.896439700324869, 'eval_accuracy': 0.8940314704286489, 'eval_runtime': 1.4851, 'eval_samples_per_second': 3710.239, 'eval_steps_per_second': 7.407, 'epoch': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.3822, 'grad_norm': 1.0906782150268555, 'learning_rate': 0.00021946903922456072, 'epoch': 1.0227272727272727}\n{'loss': 0.4222, 'grad_norm': 1.6206425428390503, 'learning_rate': 0.00021697507286973618, 'epoch': 1.0454545454545454}\n{'loss': 0.3393, 'grad_norm': 1.1295452117919922, 'learning_rate': 0.0002144811065149116, 'epoch': 1.0681818181818181}\n{'loss': 0.3878, 'grad_norm': 0.8198389410972595, 'learning_rate': 0.00021198714016008706, 'epoch': 1.0909090909090908}\n{'loss': 0.3899, 'grad_norm': 1.0789521932601929, 'learning_rate': 0.0002094931738052625, 'epoch': 1.1136363636363635}\n{'loss': 0.3566, 'grad_norm': 0.9371389746665955, 'learning_rate': 0.00020699920745043796, 'epoch': 1.1363636363636362}\n{'loss': 0.3462, 'grad_norm': 0.7608686089515686, 'learning_rate': 0.0002045052410956134, 'epoch': 1.1590909090909092}\n{'loss': 0.3521, 'grad_norm': 1.6064045429229736, 'learning_rate': 0.00020201127474078884, 'epoch': 1.1818181818181819}\n{'loss': 0.3613, 'grad_norm': 0.9639249444007874, 'learning_rate': 0.00019951730838596427, 'epoch': 1.2045454545454546}\n{'loss': 0.3317, 'grad_norm': 0.6752218008041382, 'learning_rate': 0.00019702334203113974, 'epoch': 1.2272727272727273}\n{'loss': 0.3506, 'grad_norm': 1.4281107187271118, 'learning_rate': 0.00019452937567631518, 'epoch': 1.25}\n{'loss': 0.3551, 'grad_norm': 0.7218686938285828, 'learning_rate': 0.00019203540932149064, 'epoch': 1.2727272727272727}\n{'loss': 0.2917, 'grad_norm': 0.8527167439460754, 'learning_rate': 0.00018954144296666605, 'epoch': 1.2954545454545454}\n{'loss': 0.3737, 'grad_norm': 1.049091100692749, 'learning_rate': 0.00018704747661184152, 'epoch': 1.3181818181818181}\n{'loss': 0.3045, 'grad_norm': 1.386057734489441, 'learning_rate': 0.00018455351025701696, 'epoch': 1.3409090909090908}\n{'loss': 0.3595, 'grad_norm': 0.8324089050292969, 'learning_rate': 0.00018205954390219242, 'epoch': 1.3636363636363638}\n{'loss': 0.2957, 'grad_norm': 1.1718388795852661, 'learning_rate': 0.00017956557754736786, 'epoch': 1.3863636363636362}\n{'loss': 0.304, 'grad_norm': 1.0544037818908691, 'learning_rate': 0.0001770716111925433, 'epoch': 1.4090909090909092}\n{'loss': 0.2995, 'grad_norm': 0.908234715461731, 'learning_rate': 0.00017457764483771874, 'epoch': 1.4318181818181819}\n{'loss': 0.3047, 'grad_norm': 1.089228630065918, 'learning_rate': 0.0001720836784828942, 'epoch': 1.4545454545454546}\n{'loss': 0.2802, 'grad_norm': 0.9678342938423157, 'learning_rate': 0.00016958971212806964, 'epoch': 1.4772727272727273}\n{'loss': 0.3012, 'grad_norm': 1.2325465679168701, 'learning_rate': 0.0001670957457732451, 'epoch': 1.5}\n{'loss': 0.3346, 'grad_norm': 1.8133997917175293, 'learning_rate': 0.00016460177941842052, 'epoch': 1.5227272727272727}\n{'loss': 0.339, 'grad_norm': 0.7827146053314209, 'learning_rate': 0.00016210781306359598, 'epoch': 1.5454545454545454}\n{'loss': 0.3119, 'grad_norm': 1.3591134548187256, 'learning_rate': 0.00015961384670877142, 'epoch': 1.5681818181818183}\n{'loss': 0.3394, 'grad_norm': 0.9202604293823242, 'learning_rate': 0.00015711988035394688, 'epoch': 1.5909090909090908}\n{'loss': 0.3141, 'grad_norm': 1.624963641166687, 'learning_rate': 0.00015462591399912235, 'epoch': 1.6136363636363638}\n{'loss': 0.3014, 'grad_norm': 0.732850968837738, 'learning_rate': 0.00015213194764429776, 'epoch': 1.6363636363636362}\n{'loss': 0.3441, 'grad_norm': 1.2933400869369507, 'learning_rate': 0.0001496379812894732, 'epoch': 1.6590909090909092}\n{'loss': 0.3009, 'grad_norm': 1.0571058988571167, 'learning_rate': 0.00014714401493464866, 'epoch': 1.6818181818181817}\n{'loss': 0.3099, 'grad_norm': 0.8086962103843689, 'learning_rate': 0.00014465004857982413, 'epoch': 1.7045454545454546}\n{'loss': 0.2661, 'grad_norm': 0.7365233302116394, 'learning_rate': 0.00014215608222499954, 'epoch': 1.7272727272727273}\n{'loss': 0.3249, 'grad_norm': 0.9502747058868408, 'learning_rate': 0.000139662115870175, 'epoch': 1.75}\n{'loss': 0.3201, 'grad_norm': 1.1702070236206055, 'learning_rate': 0.00013716814951535044, 'epoch': 1.7727272727272727}\n{'loss': 0.2401, 'grad_norm': 1.042069911956787, 'learning_rate': 0.0001346741831605259, 'epoch': 1.7954545454545454}\n{'loss': 0.3075, 'grad_norm': 0.7632204294204712, 'learning_rate': 0.00013218021680570135, 'epoch': 1.8181818181818183}\n{'loss': 0.2958, 'grad_norm': 0.8394380807876587, 'learning_rate': 0.00012968625045087679, 'epoch': 1.8409090909090908}\n{'loss': 0.3052, 'grad_norm': 0.9315024614334106, 'learning_rate': 0.00012719228409605225, 'epoch': 1.8636363636363638}\n{'loss': 0.2871, 'grad_norm': 1.1344366073608398, 'learning_rate': 0.0001246983177412277, 'epoch': 1.8863636363636362}\n{'loss': 0.3015, 'grad_norm': 0.9909946918487549, 'learning_rate': 0.00012220435138640313, 'epoch': 1.9090909090909092}\n{'loss': 0.2637, 'grad_norm': 0.9374959468841553, 'learning_rate': 0.00011971038503157858, 'epoch': 1.9318181818181817}\n{'loss': 0.3217, 'grad_norm': 1.0194019079208374, 'learning_rate': 0.00011721641867675402, 'epoch': 1.9545454545454546}\n{'loss': 0.3046, 'grad_norm': 0.7437947392463684, 'learning_rate': 0.00011472245232192947, 'epoch': 1.9772727272727273}\n{'loss': 0.1432, 'grad_norm': 1.7641901969909668, 'learning_rate': 0.0001122284859671049, 'epoch': 2.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8225    0.8733    0.8472      3221\n     PERCENT     0.6694    0.9310    0.7788        87\n        TYPE     0.9407    0.9621    0.9513     11501\n      VOLUME     0.5581    0.4068    0.4706        59\n\n   micro avg     0.9112    0.9405    0.9256     14868\n   macro avg     0.7477    0.7933    0.7620     14868\nweighted avg     0.9120    0.9405    0.9258     14868\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.26115021109580994, 'eval_f1_macro': 0.7619730026783429, 'eval_precision': 0.9111820669881402, 'eval_recall': 0.9404761904761905, 'eval_f1': 0.9255974051764082, 'eval_accuracy': 0.9206185567010309, 'eval_runtime': 1.6021, 'eval_samples_per_second': 3439.158, 'eval_steps_per_second': 6.866, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.3068, 'grad_norm': 0.982410728931427, 'learning_rate': 0.00010973451961228036, 'epoch': 2.022727272727273}\n{'loss': 0.2826, 'grad_norm': 1.882012963294983, 'learning_rate': 0.0001072405532574558, 'epoch': 2.0454545454545454}\n{'loss': 0.2746, 'grad_norm': 0.7839115858078003, 'learning_rate': 0.00010474658690263125, 'epoch': 2.0681818181818183}\n{'loss': 0.259, 'grad_norm': 0.9128214716911316, 'learning_rate': 0.0001022526205478067, 'epoch': 2.090909090909091}\n{'loss': 0.2841, 'grad_norm': 1.4817852973937988, 'learning_rate': 9.975865419298214e-05, 'epoch': 2.1136363636363638}\n{'loss': 0.2332, 'grad_norm': 0.8199845552444458, 'learning_rate': 9.726468783815759e-05, 'epoch': 2.1363636363636362}\n{'loss': 0.2148, 'grad_norm': 1.025994896888733, 'learning_rate': 9.477072148333303e-05, 'epoch': 2.159090909090909}\n{'loss': 0.2271, 'grad_norm': 0.68271404504776, 'learning_rate': 9.227675512850848e-05, 'epoch': 2.1818181818181817}\n{'loss': 0.2823, 'grad_norm': 0.7684205174446106, 'learning_rate': 8.978278877368393e-05, 'epoch': 2.2045454545454546}\n{'loss': 0.1877, 'grad_norm': 1.3317219018936157, 'learning_rate': 8.728882241885937e-05, 'epoch': 2.227272727272727}\n{'loss': 0.2345, 'grad_norm': 1.2051498889923096, 'learning_rate': 8.479485606403482e-05, 'epoch': 2.25}\n{'loss': 0.2089, 'grad_norm': 1.1517504453659058, 'learning_rate': 8.230088970921026e-05, 'epoch': 2.2727272727272725}\n{'loss': 0.1957, 'grad_norm': 0.9485087394714355, 'learning_rate': 7.980692335438571e-05, 'epoch': 2.2954545454545454}\n{'loss': 0.2187, 'grad_norm': 0.8520923852920532, 'learning_rate': 7.731295699956117e-05, 'epoch': 2.3181818181818183}\n{'loss': 0.2121, 'grad_norm': 0.7679528594017029, 'learning_rate': 7.48189906447366e-05, 'epoch': 2.340909090909091}\n{'loss': 0.2645, 'grad_norm': 1.7955443859100342, 'learning_rate': 7.232502428991206e-05, 'epoch': 2.3636363636363638}\n{'loss': 0.3048, 'grad_norm': 2.6737587451934814, 'learning_rate': 6.98310579350875e-05, 'epoch': 2.3863636363636362}\n{'loss': 0.2258, 'grad_norm': 1.216414213180542, 'learning_rate': 6.733709158026295e-05, 'epoch': 2.409090909090909}\n{'loss': 0.1974, 'grad_norm': 0.889703631401062, 'learning_rate': 6.484312522543839e-05, 'epoch': 2.4318181818181817}\n{'loss': 0.228, 'grad_norm': 0.854835569858551, 'learning_rate': 6.234915887061384e-05, 'epoch': 2.4545454545454546}\n{'loss': 0.2021, 'grad_norm': 1.2641676664352417, 'learning_rate': 5.985519251578929e-05, 'epoch': 2.4772727272727275}\n{'loss': 0.2258, 'grad_norm': 1.6615628004074097, 'learning_rate': 5.7361226160964734e-05, 'epoch': 2.5}\n{'loss': 0.2702, 'grad_norm': 1.2901114225387573, 'learning_rate': 5.486725980614018e-05, 'epoch': 2.5227272727272725}\n{'loss': 0.2164, 'grad_norm': 1.582764983177185, 'learning_rate': 5.2373293451315624e-05, 'epoch': 2.5454545454545454}\n{'loss': 0.2119, 'grad_norm': 1.6556787490844727, 'learning_rate': 4.987932709649107e-05, 'epoch': 2.5681818181818183}\n{'loss': 0.2275, 'grad_norm': 1.567234754562378, 'learning_rate': 4.7385360741666513e-05, 'epoch': 2.590909090909091}\n{'loss': 0.2209, 'grad_norm': 0.817344069480896, 'learning_rate': 4.4891394386841965e-05, 'epoch': 2.6136363636363638}\n{'loss': 0.2677, 'grad_norm': 0.8095134496688843, 'learning_rate': 4.239742803201741e-05, 'epoch': 2.6363636363636362}\n{'loss': 0.2047, 'grad_norm': 0.7489029765129089, 'learning_rate': 3.9903461677192855e-05, 'epoch': 2.659090909090909}\n{'loss': 0.2937, 'grad_norm': 1.7208651304244995, 'learning_rate': 3.74094953223683e-05, 'epoch': 2.6818181818181817}\n{'loss': 0.2146, 'grad_norm': 0.6889008283615112, 'learning_rate': 3.491552896754375e-05, 'epoch': 2.7045454545454546}\n{'loss': 0.2627, 'grad_norm': 0.9643822908401489, 'learning_rate': 3.2421562612719196e-05, 'epoch': 2.7272727272727275}\n{'loss': 0.241, 'grad_norm': 0.9658985137939453, 'learning_rate': 2.9927596257894645e-05, 'epoch': 2.75}\n{'loss': 0.2041, 'grad_norm': 1.362508773803711, 'learning_rate': 2.743362990307009e-05, 'epoch': 2.7727272727272725}\n{'loss': 0.2653, 'grad_norm': 0.8985149264335632, 'learning_rate': 2.4939663548245534e-05, 'epoch': 2.7954545454545454}\n{'loss': 0.2269, 'grad_norm': 1.0341641902923584, 'learning_rate': 2.2445697193420983e-05, 'epoch': 2.8181818181818183}\n{'loss': 0.241, 'grad_norm': 0.8110300302505493, 'learning_rate': 1.9951730838596427e-05, 'epoch': 2.840909090909091}\n{'loss': 0.2038, 'grad_norm': 0.7001954317092896, 'learning_rate': 1.7457764483771876e-05, 'epoch': 2.8636363636363638}\n{'loss': 0.2243, 'grad_norm': 0.9654006958007812, 'learning_rate': 1.4963798128947322e-05, 'epoch': 2.8863636363636362}\n{'loss': 0.2149, 'grad_norm': 0.7659714818000793, 'learning_rate': 1.2469831774122767e-05, 'epoch': 2.909090909090909}\n{'loss': 0.2059, 'grad_norm': 1.1170096397399902, 'learning_rate': 9.975865419298214e-06, 'epoch': 2.9318181818181817}\n{'loss': 0.2169, 'grad_norm': 0.8682363629341125, 'learning_rate': 7.481899064473661e-06, 'epoch': 2.9545454545454546}\n{'loss': 0.2084, 'grad_norm': 0.7156714797019958, 'learning_rate': 4.987932709649107e-06, 'epoch': 2.9772727272727275}\n{'loss': 0.123, 'grad_norm': 2.869962215423584, 'learning_rate': 2.4939663548245534e-06, 'epoch': 3.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8564    0.8702    0.8633      3221\n     PERCENT     0.7547    0.9195    0.8290        87\n        TYPE     0.9439    0.9657    0.9547     11501\n      VOLUME     0.6531    0.5424    0.5926        59\n\n   micro avg     0.9228    0.9431    0.9328     14868\n   macro avg     0.8020    0.8245    0.8099     14868\nweighted avg     0.9227    0.9431    0.9327     14868\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.23943054676055908, 'eval_f1_macro': 0.8098920663216019, 'eval_precision': 0.9228035538005923, 'eval_recall': 0.9430992736077481, 'eval_f1': 0.9328410338289592, 'eval_accuracy': 0.9277265328269126, 'eval_runtime': 1.4785, 'eval_samples_per_second': 3726.728, 'eval_steps_per_second': 7.44, 'epoch': 3.0}\n{'train_runtime': 18.7892, 'train_samples_per_second': 3519.365, 'train_steps_per_second': 7.025, 'train_loss': 0.48546728548226936, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n[I 2025-09-26 18:03:13,312] Trial 2 finished with value: 0.8268561048759754 and parameters: {'learning_rate': 0.00029428802986929733, 'weight_decay': 0.008654053108004656, 'num_train_epochs': 3}. Best is trial 0 with value: 0.9174994186455976.\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8564    0.8702    0.8633      3221\n     PERCENT     0.7547    0.9195    0.8290        87\n        TYPE     0.9439    0.9657    0.9547     11501\n      VOLUME     0.6531    0.5424    0.5926        59\n\n   micro avg     0.9228    0.9431    0.9328     14868\n   macro avg     0.8020    0.8245    0.8099     14868\nweighted avg     0.9227    0.9431    0.9327     14868\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.23943054676055908, 'eval_f1_macro': 0.8098920663216019, 'eval_precision': 0.9228035538005923, 'eval_recall': 0.9430992736077481, 'eval_f1': 0.9328410338289592, 'eval_accuracy': 0.9277265328269126, 'eval_runtime': 1.5575, 'eval_samples_per_second': 3537.803, 'eval_steps_per_second': 7.063, 'epoch': 3.0}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/27552 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b7313bbae6d47ff8a7095f46ea75ad5"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nThe speedups for torchdynamo mostly come with GPU Ampere or higher and which is not detected here.\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/tmp/ipykernel_36/3364797558.py:66: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 2.145, 'grad_norm': 6.811957359313965, 'learning_rate': 0.0, 'epoch': 0.022727272727272728}\n{'loss': 2.1254, 'grad_norm': 6.948701858520508, 'learning_rate': 2.0217240434015375e-05, 'epoch': 0.045454545454545456}\n{'loss': 2.0888, 'grad_norm': 6.728906154632568, 'learning_rate': 4.043448086803075e-05, 'epoch': 0.06818181818181818}\n{'loss': 2.0012, 'grad_norm': 6.806797981262207, 'learning_rate': 6.065172130204612e-05, 'epoch': 0.09090909090909091}\n{'loss': 1.8891, 'grad_norm': 6.137043476104736, 'learning_rate': 8.08689617360615e-05, 'epoch': 0.11363636363636363}\n{'loss': 1.7491, 'grad_norm': 5.765679836273193, 'learning_rate': 0.00010108620217007686, 'epoch': 0.13636363636363635}\n{'loss': 1.6068, 'grad_norm': 4.831759929656982, 'learning_rate': 0.00012130344260409224, 'epoch': 0.1590909090909091}\n{'loss': 1.4172, 'grad_norm': 4.24608850479126, 'learning_rate': 0.00014152068303810761, 'epoch': 0.18181818181818182}\n{'loss': 1.3236, 'grad_norm': 2.7962350845336914, 'learning_rate': 0.000161737923472123, 'epoch': 0.20454545454545456}\n{'loss': 1.1924, 'grad_norm': 1.8449757099151611, 'learning_rate': 0.0001819551639061384, 'epoch': 0.22727272727272727}\n{'loss': 1.0913, 'grad_norm': 1.4622821807861328, 'learning_rate': 0.00020217240434015373, 'epoch': 0.25}\n{'loss': 1.071, 'grad_norm': 1.6606123447418213, 'learning_rate': 0.00022238964477416912, 'epoch': 0.2727272727272727}\n{'loss': 1.0428, 'grad_norm': 1.624211311340332, 'learning_rate': 0.00024260688520818448, 'epoch': 0.29545454545454547}\n{'loss': 1.0233, 'grad_norm': 1.1364872455596924, 'learning_rate': 0.00026282412564219987, 'epoch': 0.3181818181818182}\n{'loss': 0.9798, 'grad_norm': 1.648229718208313, 'learning_rate': 0.00028304136607621523, 'epoch': 0.3409090909090909}\n{'loss': 0.9785, 'grad_norm': 2.0854146480560303, 'learning_rate': 0.0003032586065102306, 'epoch': 0.36363636363636365}\n{'loss': 0.8705, 'grad_norm': 2.3030874729156494, 'learning_rate': 0.000323475846944246, 'epoch': 0.38636363636363635}\n{'loss': 0.8044, 'grad_norm': 1.3314015865325928, 'learning_rate': 0.00034369308737826137, 'epoch': 0.4090909090909091}\n{'loss': 0.7858, 'grad_norm': 1.6065962314605713, 'learning_rate': 0.0003639103278122768, 'epoch': 0.4318181818181818}\n{'loss': 0.7177, 'grad_norm': 1.603874921798706, 'learning_rate': 0.0003841275682462921, 'epoch': 0.45454545454545453}\n{'loss': 0.7254, 'grad_norm': 1.0230804681777954, 'learning_rate': 0.00040434480868030745, 'epoch': 0.4772727272727273}\n{'loss': 0.6311, 'grad_norm': 1.6649049520492554, 'learning_rate': 0.00042456204911432287, 'epoch': 0.5}\n{'loss': 0.6558, 'grad_norm': 0.79639732837677, 'learning_rate': 0.00044477928954833823, 'epoch': 0.5227272727272727}\n{'loss': 0.6565, 'grad_norm': 1.22929048538208, 'learning_rate': 0.0004425329295001143, 'epoch': 0.5454545454545454}\n{'loss': 0.6307, 'grad_norm': 1.0022304058074951, 'learning_rate': 0.0004402865694518904, 'epoch': 0.5681818181818182}\n{'loss': 0.5893, 'grad_norm': 1.2000510692596436, 'learning_rate': 0.00043804020940366645, 'epoch': 0.5909090909090909}\n{'loss': 0.4837, 'grad_norm': 1.2337232828140259, 'learning_rate': 0.0004357938493554425, 'epoch': 0.6136363636363636}\n{'loss': 0.5023, 'grad_norm': 1.4823665618896484, 'learning_rate': 0.00043354748930721855, 'epoch': 0.6363636363636364}\n{'loss': 0.5435, 'grad_norm': 0.9306398630142212, 'learning_rate': 0.00043130112925899466, 'epoch': 0.6590909090909091}\n{'loss': 0.4997, 'grad_norm': 0.8427582383155823, 'learning_rate': 0.0004290547692107707, 'epoch': 0.6818181818181818}\n{'loss': 0.4975, 'grad_norm': 1.8513164520263672, 'learning_rate': 0.00042680840916254676, 'epoch': 0.7045454545454546}\n{'loss': 0.4998, 'grad_norm': 1.246281623840332, 'learning_rate': 0.00042456204911432287, 'epoch': 0.7272727272727273}\n{'loss': 0.489, 'grad_norm': 1.2264890670776367, 'learning_rate': 0.0004223156890660989, 'epoch': 0.75}\n{'loss': 0.4698, 'grad_norm': 1.476261854171753, 'learning_rate': 0.000420069329017875, 'epoch': 0.7727272727272727}\n{'loss': 0.4507, 'grad_norm': 2.0073695182800293, 'learning_rate': 0.0004178229689696511, 'epoch': 0.7954545454545454}\n{'loss': 0.429, 'grad_norm': 1.8699129819869995, 'learning_rate': 0.00041557660892142714, 'epoch': 0.8181818181818182}\n{'loss': 0.4244, 'grad_norm': 1.1355007886886597, 'learning_rate': 0.0004133302488732032, 'epoch': 0.8409090909090909}\n{'loss': 0.4082, 'grad_norm': 1.1445120573043823, 'learning_rate': 0.00041108388882497924, 'epoch': 0.8636363636363636}\n{'loss': 0.4228, 'grad_norm': 1.798990249633789, 'learning_rate': 0.00040883752877675535, 'epoch': 0.8863636363636364}\n{'loss': 0.4044, 'grad_norm': 1.5711610317230225, 'learning_rate': 0.0004065911687285314, 'epoch': 0.9090909090909091}\n{'loss': 0.4405, 'grad_norm': 1.0546717643737793, 'learning_rate': 0.00040434480868030745, 'epoch': 0.9318181818181818}\n{'loss': 0.4014, 'grad_norm': 1.8631986379623413, 'learning_rate': 0.00040209844863208356, 'epoch': 0.9545454545454546}\n{'loss': 0.3906, 'grad_norm': 1.8816086053848267, 'learning_rate': 0.0003998520885838596, 'epoch': 0.9772727272727273}\n{'loss': 0.3638, 'grad_norm': 3.692019462585449, 'learning_rate': 0.00039760572853563567, 'epoch': 1.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.7957    0.7673    0.7813      3142\n     PERCENT     0.4000    0.9697    0.5664        66\n        TYPE     0.9125    0.9551    0.9333     11415\n      VOLUME     0.2143    0.0429    0.0714        70\n\n   micro avg     0.8831    0.9106    0.8967     14693\n   macro avg     0.5806    0.6837    0.5881     14693\nweighted avg     0.8819    0.9106    0.8950     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.3607209026813507, 'eval_f1_macro': 0.5880952524591316, 'eval_precision': 0.8831100257408752, 'eval_recall': 0.91063771864153, 'eval_f1': 0.8966626457579412, 'eval_accuracy': 0.8904668625530525, 'eval_runtime': 1.4882, 'eval_samples_per_second': 3703.148, 'eval_steps_per_second': 7.392, 'epoch': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.3186, 'grad_norm': 0.6855791211128235, 'learning_rate': 0.0003953593684874117, 'epoch': 1.0227272727272727}\n{'loss': 0.3499, 'grad_norm': 1.2424699068069458, 'learning_rate': 0.0003931130084391878, 'epoch': 1.0454545454545454}\n{'loss': 0.3749, 'grad_norm': 1.6695252656936646, 'learning_rate': 0.0003908666483909639, 'epoch': 1.0681818181818181}\n{'loss': 0.3105, 'grad_norm': 1.372079610824585, 'learning_rate': 0.00038862028834273993, 'epoch': 1.0909090909090908}\n{'loss': 0.2743, 'grad_norm': 0.6595227718353271, 'learning_rate': 0.00038637392829451604, 'epoch': 1.1136363636363635}\n{'loss': 0.3269, 'grad_norm': 1.3178266286849976, 'learning_rate': 0.0003841275682462921, 'epoch': 1.1363636363636362}\n{'loss': 0.3403, 'grad_norm': 0.9101893305778503, 'learning_rate': 0.00038188120819806815, 'epoch': 1.1590909090909092}\n{'loss': 0.3087, 'grad_norm': 1.147055983543396, 'learning_rate': 0.0003796348481498442, 'epoch': 1.1818181818181819}\n{'loss': 0.3174, 'grad_norm': 0.9010559320449829, 'learning_rate': 0.0003773884881016203, 'epoch': 1.2045454545454546}\n{'loss': 0.329, 'grad_norm': 1.3478373289108276, 'learning_rate': 0.00037514212805339636, 'epoch': 1.2272727272727273}\n{'loss': 0.2933, 'grad_norm': 0.8956289291381836, 'learning_rate': 0.00037289576800517247, 'epoch': 1.25}\n{'loss': 0.3138, 'grad_norm': 0.6454845666885376, 'learning_rate': 0.0003706494079569485, 'epoch': 1.2727272727272727}\n{'loss': 0.3052, 'grad_norm': 1.1186257600784302, 'learning_rate': 0.0003684030479087246, 'epoch': 1.2954545454545454}\n{'loss': 0.2699, 'grad_norm': 1.5606210231781006, 'learning_rate': 0.0003661566878605007, 'epoch': 1.3181818181818181}\n{'loss': 0.2924, 'grad_norm': 0.9519201517105103, 'learning_rate': 0.0003639103278122768, 'epoch': 1.3409090909090908}\n{'loss': 0.3114, 'grad_norm': 1.041176676750183, 'learning_rate': 0.00036166396776405284, 'epoch': 1.3636363636363638}\n{'loss': 0.3056, 'grad_norm': 1.1968384981155396, 'learning_rate': 0.0003594176077158289, 'epoch': 1.3863636363636362}\n{'loss': 0.2723, 'grad_norm': 1.7418887615203857, 'learning_rate': 0.00035717124766760494, 'epoch': 1.4090909090909092}\n{'loss': 0.2966, 'grad_norm': 1.3657913208007812, 'learning_rate': 0.00035492488761938105, 'epoch': 1.4318181818181819}\n{'loss': 0.2895, 'grad_norm': 0.9821799993515015, 'learning_rate': 0.0003526785275711571, 'epoch': 1.4545454545454546}\n{'loss': 0.237, 'grad_norm': 0.9163258075714111, 'learning_rate': 0.00035043216752293316, 'epoch': 1.4772727272727273}\n{'loss': 0.2657, 'grad_norm': 0.9666980504989624, 'learning_rate': 0.00034818580747470926, 'epoch': 1.5}\n{'loss': 0.3317, 'grad_norm': 1.2010650634765625, 'learning_rate': 0.0003459394474264853, 'epoch': 1.5227272727272727}\n{'loss': 0.2632, 'grad_norm': 0.8648939728736877, 'learning_rate': 0.00034369308737826137, 'epoch': 1.5454545454545454}\n{'loss': 0.2315, 'grad_norm': 1.1344923973083496, 'learning_rate': 0.0003414467273300374, 'epoch': 1.5681818181818183}\n{'loss': 0.2402, 'grad_norm': 0.9230234026908875, 'learning_rate': 0.00033920036728181353, 'epoch': 1.5909090909090908}\n{'loss': 0.2449, 'grad_norm': 0.948706865310669, 'learning_rate': 0.0003369540072335896, 'epoch': 1.6136363636363638}\n{'loss': 0.2866, 'grad_norm': 0.7773196697235107, 'learning_rate': 0.00033470764718536563, 'epoch': 1.6363636363636362}\n{'loss': 0.2374, 'grad_norm': 0.9941582083702087, 'learning_rate': 0.00033246128713714174, 'epoch': 1.6590909090909092}\n{'loss': 0.2841, 'grad_norm': 1.171669840812683, 'learning_rate': 0.0003302149270889178, 'epoch': 1.6818181818181817}\n{'loss': 0.2378, 'grad_norm': 1.0084904432296753, 'learning_rate': 0.00032796856704069385, 'epoch': 1.7045454545454546}\n{'loss': 0.2273, 'grad_norm': 0.6537784337997437, 'learning_rate': 0.00032572220699246995, 'epoch': 1.7272727272727273}\n{'loss': 0.3087, 'grad_norm': 1.3606470823287964, 'learning_rate': 0.000323475846944246, 'epoch': 1.75}\n{'loss': 0.2716, 'grad_norm': 0.9279383420944214, 'learning_rate': 0.00032122948689602206, 'epoch': 1.7727272727272727}\n{'loss': 0.2349, 'grad_norm': 0.8477986454963684, 'learning_rate': 0.0003189831268477981, 'epoch': 1.7954545454545454}\n{'loss': 0.2387, 'grad_norm': 0.953758955001831, 'learning_rate': 0.0003167367667995742, 'epoch': 1.8181818181818183}\n{'loss': 0.241, 'grad_norm': 1.0079938173294067, 'learning_rate': 0.00031449040675135027, 'epoch': 1.8409090909090908}\n{'loss': 0.266, 'grad_norm': 1.2511534690856934, 'learning_rate': 0.0003122440467031263, 'epoch': 1.8636363636363638}\n{'loss': 0.2151, 'grad_norm': 0.7297940254211426, 'learning_rate': 0.00030999768665490243, 'epoch': 1.8863636363636362}\n{'loss': 0.3074, 'grad_norm': 1.3203349113464355, 'learning_rate': 0.0003077513266066785, 'epoch': 1.9090909090909092}\n{'loss': 0.1952, 'grad_norm': 0.9412040114402771, 'learning_rate': 0.00030550496655845454, 'epoch': 1.9318181818181817}\n{'loss': 0.3123, 'grad_norm': 1.262204647064209, 'learning_rate': 0.0003032586065102306, 'epoch': 1.9545454545454546}\n{'loss': 0.2293, 'grad_norm': 0.6384424567222595, 'learning_rate': 0.0003010122464620067, 'epoch': 1.9772727272727273}\n{'loss': 0.3468, 'grad_norm': 5.033311367034912, 'learning_rate': 0.00029876588641378275, 'epoch': 2.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8462    0.8682    0.8571      3142\n     PERCENT     0.7949    0.9394    0.8611        66\n        TYPE     0.9423    0.9605    0.9513     11415\n      VOLUME     0.7869    0.6857    0.7328        70\n\n   micro avg     0.9203    0.9394    0.9297     14693\n   macro avg     0.8426    0.8635    0.8506     14693\nweighted avg     0.9204    0.9394    0.9297     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.2614676058292389, 'eval_f1_macro': 0.8505779608922627, 'eval_precision': 0.920256034137885, 'eval_recall': 0.9393588783774587, 'eval_f1': 0.9297093395304974, 'eval_accuracy': 0.9209380781369029, 'eval_runtime': 1.5137, 'eval_samples_per_second': 3640.642, 'eval_steps_per_second': 7.267, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.1819, 'grad_norm': 0.8485177159309387, 'learning_rate': 0.0002965195263655588, 'epoch': 2.022727272727273}\n{'loss': 0.1934, 'grad_norm': 0.8484665751457214, 'learning_rate': 0.0002942731663173349, 'epoch': 2.0454545454545454}\n{'loss': 0.2046, 'grad_norm': 1.0724478960037231, 'learning_rate': 0.00029202680626911096, 'epoch': 2.0681818181818183}\n{'loss': 0.1581, 'grad_norm': 0.8177210688591003, 'learning_rate': 0.000289780446220887, 'epoch': 2.090909090909091}\n{'loss': 0.1449, 'grad_norm': 0.7741323709487915, 'learning_rate': 0.0002875340861726631, 'epoch': 2.1136363636363638}\n{'loss': 0.1215, 'grad_norm': 0.6585100889205933, 'learning_rate': 0.0002852877261244392, 'epoch': 2.1363636363636362}\n{'loss': 0.1457, 'grad_norm': 0.6020282506942749, 'learning_rate': 0.00028304136607621523, 'epoch': 2.159090909090909}\n{'loss': 0.1652, 'grad_norm': 0.9582378268241882, 'learning_rate': 0.0002807950060279913, 'epoch': 2.1818181818181817}\n{'loss': 0.1692, 'grad_norm': 0.8157197833061218, 'learning_rate': 0.0002785486459797674, 'epoch': 2.2045454545454546}\n{'loss': 0.1804, 'grad_norm': 0.753364086151123, 'learning_rate': 0.00027630228593154344, 'epoch': 2.227272727272727}\n{'loss': 0.1684, 'grad_norm': 0.742530345916748, 'learning_rate': 0.0002740559258833195, 'epoch': 2.25}\n{'loss': 0.1405, 'grad_norm': 0.9451786279678345, 'learning_rate': 0.0002718095658350956, 'epoch': 2.2727272727272725}\n{'loss': 0.1685, 'grad_norm': 0.9753872752189636, 'learning_rate': 0.00026956320578687165, 'epoch': 2.2954545454545454}\n{'loss': 0.1787, 'grad_norm': 0.7871395945549011, 'learning_rate': 0.0002673168457386477, 'epoch': 2.3181818181818183}\n{'loss': 0.1578, 'grad_norm': 0.8274424076080322, 'learning_rate': 0.00026507048569042376, 'epoch': 2.340909090909091}\n{'loss': 0.1616, 'grad_norm': 0.7315302491188049, 'learning_rate': 0.00026282412564219987, 'epoch': 2.3636363636363638}\n{'loss': 0.1628, 'grad_norm': 0.8419927954673767, 'learning_rate': 0.0002605777655939759, 'epoch': 2.3863636363636362}\n{'loss': 0.2124, 'grad_norm': 1.41020929813385, 'learning_rate': 0.00025833140554575197, 'epoch': 2.409090909090909}\n{'loss': 0.1469, 'grad_norm': 0.9472149610519409, 'learning_rate': 0.0002560850454975281, 'epoch': 2.4318181818181817}\n{'loss': 0.1745, 'grad_norm': 0.9708103537559509, 'learning_rate': 0.00025383868544930413, 'epoch': 2.4545454545454546}\n{'loss': 0.1827, 'grad_norm': 1.708305835723877, 'learning_rate': 0.0002515923254010802, 'epoch': 2.4772727272727275}\n{'loss': 0.1374, 'grad_norm': 1.0552005767822266, 'learning_rate': 0.00024934596535285624, 'epoch': 2.5}\n{'loss': 0.2031, 'grad_norm': 1.3695513010025024, 'learning_rate': 0.00024709960530463235, 'epoch': 2.5227272727272725}\n{'loss': 0.1628, 'grad_norm': 0.7035878896713257, 'learning_rate': 0.0002448532452564084, 'epoch': 2.5454545454545454}\n{'loss': 0.1939, 'grad_norm': 1.4943879842758179, 'learning_rate': 0.00024260688520818448, 'epoch': 2.5681818181818183}\n{'loss': 0.231, 'grad_norm': 2.4304301738739014, 'learning_rate': 0.00024036052515996059, 'epoch': 2.590909090909091}\n{'loss': 0.1316, 'grad_norm': 0.7570332288742065, 'learning_rate': 0.00023811416511173664, 'epoch': 2.6136363636363638}\n{'loss': 0.1638, 'grad_norm': 1.6050513982772827, 'learning_rate': 0.0002358678050635127, 'epoch': 2.6363636363636362}\n{'loss': 0.1386, 'grad_norm': 0.9201776385307312, 'learning_rate': 0.0002336214450152888, 'epoch': 2.659090909090909}\n{'loss': 0.1318, 'grad_norm': 1.1843510866165161, 'learning_rate': 0.00023137508496706485, 'epoch': 2.6818181818181817}\n{'loss': 0.1727, 'grad_norm': 1.1120326519012451, 'learning_rate': 0.0002291287249188409, 'epoch': 2.7045454545454546}\n{'loss': 0.1453, 'grad_norm': 0.8974106311798096, 'learning_rate': 0.00022688236487061696, 'epoch': 2.7272727272727275}\n{'loss': 0.1639, 'grad_norm': 0.8652623295783997, 'learning_rate': 0.00022463600482239306, 'epoch': 2.75}\n{'loss': 0.1326, 'grad_norm': 0.6728793978691101, 'learning_rate': 0.00022238964477416912, 'epoch': 2.7727272727272725}\n{'loss': 0.1987, 'grad_norm': 1.0004956722259521, 'learning_rate': 0.0002201432847259452, 'epoch': 2.7954545454545454}\n{'loss': 0.1786, 'grad_norm': 0.9580503106117249, 'learning_rate': 0.00021789692467772125, 'epoch': 2.8181818181818183}\n{'loss': 0.1703, 'grad_norm': 1.1282726526260376, 'learning_rate': 0.00021565056462949733, 'epoch': 2.840909090909091}\n{'loss': 0.1579, 'grad_norm': 1.464242935180664, 'learning_rate': 0.00021340420458127338, 'epoch': 2.8636363636363638}\n{'loss': 0.1508, 'grad_norm': 1.4443806409835815, 'learning_rate': 0.00021115784453304946, 'epoch': 2.8863636363636362}\n{'loss': 0.1879, 'grad_norm': 1.055427074432373, 'learning_rate': 0.00020891148448482554, 'epoch': 2.909090909090909}\n{'loss': 0.1512, 'grad_norm': 1.0188556909561157, 'learning_rate': 0.0002066651244366016, 'epoch': 2.9318181818181817}\n{'loss': 0.2027, 'grad_norm': 1.2784754037857056, 'learning_rate': 0.00020441876438837767, 'epoch': 2.9545454545454546}\n{'loss': 0.1861, 'grad_norm': 1.2163857221603394, 'learning_rate': 0.00020217240434015373, 'epoch': 2.9772727272727275}\n{'loss': 0.0876, 'grad_norm': 3.8812694549560547, 'learning_rate': 0.0001999260442919298, 'epoch': 3.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8813    0.8864    0.8838      3142\n     PERCENT     0.8986    0.9394    0.9185        66\n        TYPE     0.9444    0.9717    0.9579     11415\n      VOLUME     0.8955    0.8571    0.8759        70\n\n   micro avg     0.9307    0.9528    0.9416     14693\n   macro avg     0.9050    0.9137    0.9090     14693\nweighted avg     0.9305    0.9528    0.9415     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.23767471313476562, 'eval_f1_macro': 0.909033925439442, 'eval_precision': 0.9307226913104182, 'eval_recall': 0.9527666235622405, 'eval_f1': 0.94161565884173, 'eval_accuracy': 0.9312765262814234, 'eval_runtime': 1.4875, 'eval_samples_per_second': 3704.775, 'eval_steps_per_second': 7.395, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.1138, 'grad_norm': 0.5463120341300964, 'learning_rate': 0.00019767968424370586, 'epoch': 3.022727272727273}\n{'loss': 0.1189, 'grad_norm': 0.704689621925354, 'learning_rate': 0.00019543332419548194, 'epoch': 3.0454545454545454}\n{'loss': 0.0997, 'grad_norm': 0.7479150891304016, 'learning_rate': 0.00019318696414725802, 'epoch': 3.0681818181818183}\n{'loss': 0.1075, 'grad_norm': 0.5482929348945618, 'learning_rate': 0.00019094060409903407, 'epoch': 3.090909090909091}\n{'loss': 0.126, 'grad_norm': 0.8925636410713196, 'learning_rate': 0.00018869424405081015, 'epoch': 3.1136363636363638}\n{'loss': 0.0958, 'grad_norm': 0.6123635172843933, 'learning_rate': 0.00018644788400258623, 'epoch': 3.1363636363636362}\n{'loss': 0.1059, 'grad_norm': 0.9086713194847107, 'learning_rate': 0.0001842015239543623, 'epoch': 3.159090909090909}\n{'loss': 0.1237, 'grad_norm': 0.8199287056922913, 'learning_rate': 0.0001819551639061384, 'epoch': 3.1818181818181817}\n{'loss': 0.142, 'grad_norm': 0.5792131423950195, 'learning_rate': 0.00017970880385791445, 'epoch': 3.2045454545454546}\n{'loss': 0.0948, 'grad_norm': 0.7145644426345825, 'learning_rate': 0.00017746244380969053, 'epoch': 3.227272727272727}\n{'loss': 0.1267, 'grad_norm': 0.885180652141571, 'learning_rate': 0.00017521608376146658, 'epoch': 3.25}\n{'loss': 0.1032, 'grad_norm': 0.8416895866394043, 'learning_rate': 0.00017296972371324266, 'epoch': 3.2727272727272725}\n{'loss': 0.1249, 'grad_norm': 1.0349847078323364, 'learning_rate': 0.0001707233636650187, 'epoch': 3.2954545454545454}\n{'loss': 0.112, 'grad_norm': 0.9885420203208923, 'learning_rate': 0.0001684770036167948, 'epoch': 3.3181818181818183}\n{'loss': 0.1237, 'grad_norm': 1.6533348560333252, 'learning_rate': 0.00016623064356857087, 'epoch': 3.340909090909091}\n{'loss': 0.1011, 'grad_norm': 0.6025145053863525, 'learning_rate': 0.00016398428352034692, 'epoch': 3.3636363636363638}\n{'loss': 0.1017, 'grad_norm': 0.6264121532440186, 'learning_rate': 0.000161737923472123, 'epoch': 3.3863636363636362}\n{'loss': 0.104, 'grad_norm': 0.8433336615562439, 'learning_rate': 0.00015949156342389906, 'epoch': 3.409090909090909}\n{'loss': 0.09, 'grad_norm': 0.585209846496582, 'learning_rate': 0.00015724520337567514, 'epoch': 3.4318181818181817}\n{'loss': 0.1582, 'grad_norm': 1.4951601028442383, 'learning_rate': 0.00015499884332745122, 'epoch': 3.4545454545454546}\n{'loss': 0.1148, 'grad_norm': 1.1196058988571167, 'learning_rate': 0.00015275248327922727, 'epoch': 3.4772727272727275}\n{'loss': 0.1314, 'grad_norm': 0.8802433013916016, 'learning_rate': 0.00015050612323100335, 'epoch': 3.5}\n{'loss': 0.1279, 'grad_norm': 0.9511740803718567, 'learning_rate': 0.0001482597631827794, 'epoch': 3.5227272727272725}\n{'loss': 0.0822, 'grad_norm': 0.8209432363510132, 'learning_rate': 0.00014601340313455548, 'epoch': 3.5454545454545454}\n{'loss': 0.0844, 'grad_norm': 0.7343682050704956, 'learning_rate': 0.00014376704308633156, 'epoch': 3.5681818181818183}\n{'loss': 0.1354, 'grad_norm': 0.76936936378479, 'learning_rate': 0.00014152068303810761, 'epoch': 3.590909090909091}\n{'loss': 0.1066, 'grad_norm': 0.7430223822593689, 'learning_rate': 0.0001392743229898837, 'epoch': 3.6136363636363638}\n{'loss': 0.0815, 'grad_norm': 0.6224049925804138, 'learning_rate': 0.00013702796294165975, 'epoch': 3.6363636363636362}\n{'loss': 0.0855, 'grad_norm': 0.8940582871437073, 'learning_rate': 0.00013478160289343583, 'epoch': 3.659090909090909}\n{'loss': 0.1019, 'grad_norm': 0.9915065169334412, 'learning_rate': 0.00013253524284521188, 'epoch': 3.6818181818181817}\n{'loss': 0.0927, 'grad_norm': 0.7673042416572571, 'learning_rate': 0.00013028888279698796, 'epoch': 3.7045454545454546}\n{'loss': 0.1184, 'grad_norm': 0.7363068461418152, 'learning_rate': 0.00012804252274876404, 'epoch': 3.7272727272727275}\n{'loss': 0.0753, 'grad_norm': 0.6453484296798706, 'learning_rate': 0.0001257961627005401, 'epoch': 3.75}\n{'loss': 0.1362, 'grad_norm': 1.231589436531067, 'learning_rate': 0.00012354980265231617, 'epoch': 3.7727272727272725}\n{'loss': 0.115, 'grad_norm': 1.2293814420700073, 'learning_rate': 0.00012130344260409224, 'epoch': 3.7954545454545454}\n{'loss': 0.1221, 'grad_norm': 1.138185739517212, 'learning_rate': 0.00011905708255586832, 'epoch': 3.8181818181818183}\n{'loss': 0.091, 'grad_norm': 0.6642882227897644, 'learning_rate': 0.0001168107225076444, 'epoch': 3.840909090909091}\n{'loss': 0.1564, 'grad_norm': 0.7801181077957153, 'learning_rate': 0.00011456436245942045, 'epoch': 3.8636363636363638}\n{'loss': 0.1155, 'grad_norm': 0.7760655879974365, 'learning_rate': 0.00011231800241119653, 'epoch': 3.8863636363636362}\n{'loss': 0.1126, 'grad_norm': 1.312148928642273, 'learning_rate': 0.0001100716423629726, 'epoch': 3.909090909090909}\n{'loss': 0.1141, 'grad_norm': 1.6926660537719727, 'learning_rate': 0.00010782528231474866, 'epoch': 3.9318181818181817}\n{'loss': 0.1235, 'grad_norm': 1.0131621360778809, 'learning_rate': 0.00010557892226652473, 'epoch': 3.9545454545454546}\n{'loss': 0.1475, 'grad_norm': 1.0720230340957642, 'learning_rate': 0.0001033325622183008, 'epoch': 3.9772727272727275}\n{'loss': 0.1323, 'grad_norm': 3.2320034503936768, 'learning_rate': 0.00010108620217007686, 'epoch': 4.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8735    0.8947    0.8840      3142\n     PERCENT     0.8857    0.9394    0.9118        66\n        TYPE     0.9531    0.9639    0.9584     11415\n      VOLUME     0.8824    0.8571    0.8696        70\n\n   micro avg     0.9352    0.9485    0.9418     14693\n   macro avg     0.8987    0.9138    0.9059     14693\nweighted avg     0.9354    0.9485    0.9419     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.24592813849449158, 'eval_f1_macro': 0.9059354161941292, 'eval_precision': 0.9352392456882088, 'eval_recall': 0.9484788674879194, 'eval_f1': 0.9418125295668041, 'eval_accuracy': 0.9311677005114811, 'eval_runtime': 1.518, 'eval_samples_per_second': 3630.389, 'eval_steps_per_second': 7.246, 'epoch': 4.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.0832, 'grad_norm': 0.545717716217041, 'learning_rate': 9.883984212185293e-05, 'epoch': 4.0227272727272725}\n{'loss': 0.0907, 'grad_norm': 0.8460813164710999, 'learning_rate': 9.659348207362901e-05, 'epoch': 4.045454545454546}\n{'loss': 0.1024, 'grad_norm': 0.5718956589698792, 'learning_rate': 9.434712202540508e-05, 'epoch': 4.068181818181818}\n{'loss': 0.0924, 'grad_norm': 0.6352217793464661, 'learning_rate': 9.210076197718116e-05, 'epoch': 4.090909090909091}\n{'loss': 0.0997, 'grad_norm': 0.8031524419784546, 'learning_rate': 8.985440192895722e-05, 'epoch': 4.113636363636363}\n{'loss': 0.1022, 'grad_norm': 0.632877767086029, 'learning_rate': 8.760804188073329e-05, 'epoch': 4.136363636363637}\n{'loss': 0.0727, 'grad_norm': 0.7264042496681213, 'learning_rate': 8.536168183250936e-05, 'epoch': 4.159090909090909}\n{'loss': 0.0922, 'grad_norm': 1.0483781099319458, 'learning_rate': 8.311532178428544e-05, 'epoch': 4.181818181818182}\n{'loss': 0.082, 'grad_norm': 1.1524324417114258, 'learning_rate': 8.08689617360615e-05, 'epoch': 4.204545454545454}\n{'loss': 0.0771, 'grad_norm': 0.7256417274475098, 'learning_rate': 7.862260168783757e-05, 'epoch': 4.2272727272727275}\n{'loss': 0.1049, 'grad_norm': 1.1006861925125122, 'learning_rate': 7.637624163961363e-05, 'epoch': 4.25}\n{'loss': 0.1038, 'grad_norm': 0.880882203578949, 'learning_rate': 7.41298815913897e-05, 'epoch': 4.2727272727272725}\n{'loss': 0.0827, 'grad_norm': 1.0725764036178589, 'learning_rate': 7.188352154316578e-05, 'epoch': 4.295454545454546}\n{'loss': 0.0719, 'grad_norm': 0.6072583198547363, 'learning_rate': 6.963716149494185e-05, 'epoch': 4.318181818181818}\n{'loss': 0.0806, 'grad_norm': 0.9217545390129089, 'learning_rate': 6.739080144671791e-05, 'epoch': 4.340909090909091}\n{'loss': 0.0812, 'grad_norm': 0.7124804854393005, 'learning_rate': 6.514444139849398e-05, 'epoch': 4.363636363636363}\n{'loss': 0.0854, 'grad_norm': 0.6188772916793823, 'learning_rate': 6.289808135027005e-05, 'epoch': 4.386363636363637}\n{'loss': 0.0537, 'grad_norm': 0.6719185709953308, 'learning_rate': 6.065172130204612e-05, 'epoch': 4.409090909090909}\n{'loss': 0.077, 'grad_norm': 0.6988651752471924, 'learning_rate': 5.84053612538222e-05, 'epoch': 4.431818181818182}\n{'loss': 0.0866, 'grad_norm': 0.6131272315979004, 'learning_rate': 5.6159001205598266e-05, 'epoch': 4.454545454545454}\n{'loss': 0.097, 'grad_norm': 1.5927811861038208, 'learning_rate': 5.391264115737433e-05, 'epoch': 4.4772727272727275}\n{'loss': 0.0836, 'grad_norm': 0.7569100260734558, 'learning_rate': 5.16662811091504e-05, 'epoch': 4.5}\n{'loss': 0.0849, 'grad_norm': 0.8343040347099304, 'learning_rate': 4.9419921060926465e-05, 'epoch': 4.5227272727272725}\n{'loss': 0.0626, 'grad_norm': 0.8603061437606812, 'learning_rate': 4.717356101270254e-05, 'epoch': 4.545454545454545}\n{'loss': 0.0965, 'grad_norm': 1.0925158262252808, 'learning_rate': 4.492720096447861e-05, 'epoch': 4.568181818181818}\n{'loss': 0.0962, 'grad_norm': 1.3969762325286865, 'learning_rate': 4.268084091625468e-05, 'epoch': 4.590909090909091}\n{'loss': 0.0881, 'grad_norm': 0.8717213273048401, 'learning_rate': 4.043448086803075e-05, 'epoch': 4.613636363636363}\n{'loss': 0.107, 'grad_norm': 0.7245488166809082, 'learning_rate': 3.818812081980682e-05, 'epoch': 4.636363636363637}\n{'loss': 0.0704, 'grad_norm': 0.5788330435752869, 'learning_rate': 3.594176077158289e-05, 'epoch': 4.659090909090909}\n{'loss': 0.0501, 'grad_norm': 0.723563551902771, 'learning_rate': 3.369540072335896e-05, 'epoch': 4.681818181818182}\n{'loss': 0.0541, 'grad_norm': 0.49430158734321594, 'learning_rate': 3.144904067513502e-05, 'epoch': 4.704545454545455}\n{'loss': 0.0952, 'grad_norm': 0.9934214353561401, 'learning_rate': 2.92026806269111e-05, 'epoch': 4.7272727272727275}\n{'loss': 0.0757, 'grad_norm': 0.7982940077781677, 'learning_rate': 2.6956320578687166e-05, 'epoch': 4.75}\n{'loss': 0.0671, 'grad_norm': 0.5682552456855774, 'learning_rate': 2.4709960530463233e-05, 'epoch': 4.7727272727272725}\n{'loss': 0.077, 'grad_norm': 0.6352450251579285, 'learning_rate': 2.2463600482239306e-05, 'epoch': 4.795454545454545}\n{'loss': 0.0665, 'grad_norm': 1.2981244325637817, 'learning_rate': 2.0217240434015375e-05, 'epoch': 4.818181818181818}\n{'loss': 0.0897, 'grad_norm': 0.8134286999702454, 'learning_rate': 1.7970880385791445e-05, 'epoch': 4.840909090909091}\n{'loss': 0.0941, 'grad_norm': 0.8102434277534485, 'learning_rate': 1.572452033756751e-05, 'epoch': 4.863636363636363}\n{'loss': 0.1293, 'grad_norm': 0.8610274195671082, 'learning_rate': 1.3478160289343583e-05, 'epoch': 4.886363636363637}\n{'loss': 0.0661, 'grad_norm': 0.6556738615036011, 'learning_rate': 1.1231800241119653e-05, 'epoch': 4.909090909090909}\n{'loss': 0.0787, 'grad_norm': 0.9428843259811401, 'learning_rate': 8.985440192895723e-06, 'epoch': 4.931818181818182}\n{'loss': 0.0659, 'grad_norm': 0.6285836696624756, 'learning_rate': 6.7390801446717915e-06, 'epoch': 4.954545454545455}\n{'loss': 0.0828, 'grad_norm': 0.6170090436935425, 'learning_rate': 4.492720096447861e-06, 'epoch': 4.9772727272727275}\n{'loss': 0.11, 'grad_norm': 3.444451093673706, 'learning_rate': 2.2463600482239307e-06, 'epoch': 5.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8688    0.8997    0.8840      3142\n     PERCENT     0.8857    0.9394    0.9118        66\n        TYPE     0.9524    0.9623    0.9573     11415\n      VOLUME     0.8824    0.8571    0.8696        70\n\n   micro avg     0.9335    0.9483    0.9409     14693\n   macro avg     0.8973    0.9147    0.9057     14693\nweighted avg     0.9339    0.9483    0.9410     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.260527104139328, 'eval_f1_macro': 0.9056650265336856, 'eval_precision': 0.9335387913707625, 'eval_recall': 0.9483427482474648, 'eval_f1': 0.9408825416118033, 'eval_accuracy': 0.9308412232016542, 'eval_runtime': 1.5268, 'eval_samples_per_second': 3609.447, 'eval_steps_per_second': 7.204, 'epoch': 5.0}\n{'train_runtime': 31.0921, 'train_samples_per_second': 3544.472, 'train_steps_per_second': 7.076, 'train_loss': 0.3061687542294914, 'epoch': 5.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nThe speedups for torchdynamo mostly come with GPU Ampere or higher and which is not detected here.\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8813    0.8864    0.8838      3142\n     PERCENT     0.8986    0.9394    0.9185        66\n        TYPE     0.9444    0.9717    0.9579     11415\n      VOLUME     0.8955    0.8571    0.8759        70\n\n   micro avg     0.9307    0.9528    0.9416     14693\n   macro avg     0.9050    0.9137    0.9090     14693\nweighted avg     0.9305    0.9528    0.9415     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.23767471313476562, 'eval_f1_macro': 0.909033925439442, 'eval_precision': 0.9307226913104182, 'eval_recall': 0.9527666235622405, 'eval_f1': 0.94161565884173, 'eval_accuracy': 0.9312765262814234, 'eval_runtime': 1.9699, 'eval_samples_per_second': 2797.674, 'eval_steps_per_second': 5.584, 'epoch': 5.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/3364797558.py:66: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 2.352, 'grad_norm': 7.783750057220459, 'learning_rate': 0.0, 'epoch': 0.022727272727272728}\n{'loss': 2.3704, 'grad_norm': 7.948930263519287, 'learning_rate': 2.0217240434015375e-05, 'epoch': 0.045454545454545456}\n{'loss': 2.3253, 'grad_norm': 7.917801380157471, 'learning_rate': 4.043448086803075e-05, 'epoch': 0.06818181818181818}\n{'loss': 2.2079, 'grad_norm': 7.4389729499816895, 'learning_rate': 6.065172130204612e-05, 'epoch': 0.09090909090909091}\n{'loss': 2.0599, 'grad_norm': 7.05297327041626, 'learning_rate': 8.08689617360615e-05, 'epoch': 0.11363636363636363}\n{'loss': 1.8726, 'grad_norm': 6.719592571258545, 'learning_rate': 0.00010108620217007686, 'epoch': 0.13636363636363635}\n{'loss': 1.6925, 'grad_norm': 5.787282466888428, 'learning_rate': 0.00012130344260409224, 'epoch': 0.1590909090909091}\n{'loss': 1.5455, 'grad_norm': 4.253681182861328, 'learning_rate': 0.00014152068303810761, 'epoch': 0.18181818181818182}\n{'loss': 1.3044, 'grad_norm': 3.1956095695495605, 'learning_rate': 0.000161737923472123, 'epoch': 0.20454545454545456}\n{'loss': 1.2815, 'grad_norm': 2.212311267852783, 'learning_rate': 0.0001819551639061384, 'epoch': 0.22727272727272727}\n{'loss': 1.1482, 'grad_norm': 2.3269143104553223, 'learning_rate': 0.00020217240434015373, 'epoch': 0.25}\n{'loss': 1.205, 'grad_norm': 2.987593650817871, 'learning_rate': 0.00022238964477416912, 'epoch': 0.2727272727272727}\n{'loss': 1.0724, 'grad_norm': 1.9952586889266968, 'learning_rate': 0.00024260688520818448, 'epoch': 0.29545454545454547}\n{'loss': 0.964, 'grad_norm': 1.2519773244857788, 'learning_rate': 0.00026282412564219987, 'epoch': 0.3181818181818182}\n{'loss': 1.021, 'grad_norm': 1.9794774055480957, 'learning_rate': 0.00028304136607621523, 'epoch': 0.3409090909090909}\n{'loss': 0.9573, 'grad_norm': 2.4546122550964355, 'learning_rate': 0.0003032586065102306, 'epoch': 0.36363636363636365}\n{'loss': 0.8712, 'grad_norm': 1.7826049327850342, 'learning_rate': 0.000323475846944246, 'epoch': 0.38636363636363635}\n{'loss': 0.8205, 'grad_norm': 0.941317081451416, 'learning_rate': 0.00034369308737826137, 'epoch': 0.4090909090909091}\n{'loss': 0.8011, 'grad_norm': 1.7029510736465454, 'learning_rate': 0.0003639103278122768, 'epoch': 0.4318181818181818}\n{'loss': 0.7579, 'grad_norm': 1.393951177597046, 'learning_rate': 0.0003841275682462921, 'epoch': 0.45454545454545453}\n{'loss': 0.6974, 'grad_norm': 0.9651477932929993, 'learning_rate': 0.00040434480868030745, 'epoch': 0.4772727272727273}\n{'loss': 0.6088, 'grad_norm': 1.3270379304885864, 'learning_rate': 0.00042456204911432287, 'epoch': 0.5}\n{'loss': 0.6021, 'grad_norm': 0.8615707159042358, 'learning_rate': 0.00044477928954833823, 'epoch': 0.5227272727272727}\n{'loss': 0.6463, 'grad_norm': 1.3984798192977905, 'learning_rate': 0.0004425329295001143, 'epoch': 0.5454545454545454}\n{'loss': 0.5736, 'grad_norm': 0.9040384888648987, 'learning_rate': 0.0004402865694518904, 'epoch': 0.5681818181818182}\n{'loss': 0.4869, 'grad_norm': 0.7311362624168396, 'learning_rate': 0.00043804020940366645, 'epoch': 0.5909090909090909}\n{'loss': 0.594, 'grad_norm': 1.2351312637329102, 'learning_rate': 0.0004357938493554425, 'epoch': 0.6136363636363636}\n{'loss': 0.4656, 'grad_norm': 0.9468465447425842, 'learning_rate': 0.00043354748930721855, 'epoch': 0.6363636363636364}\n{'loss': 0.565, 'grad_norm': 1.0313680171966553, 'learning_rate': 0.00043130112925899466, 'epoch': 0.6590909090909091}\n{'loss': 0.4821, 'grad_norm': 1.5805835723876953, 'learning_rate': 0.0004290547692107707, 'epoch': 0.6818181818181818}\n{'loss': 0.4895, 'grad_norm': 1.7009273767471313, 'learning_rate': 0.00042680840916254676, 'epoch': 0.7045454545454546}\n{'loss': 0.4852, 'grad_norm': 1.0901248455047607, 'learning_rate': 0.00042456204911432287, 'epoch': 0.7272727272727273}\n{'loss': 0.4363, 'grad_norm': 1.1326117515563965, 'learning_rate': 0.0004223156890660989, 'epoch': 0.75}\n{'loss': 0.4459, 'grad_norm': 0.8453853726387024, 'learning_rate': 0.000420069329017875, 'epoch': 0.7727272727272727}\n{'loss': 0.4107, 'grad_norm': 1.2698434591293335, 'learning_rate': 0.0004178229689696511, 'epoch': 0.7954545454545454}\n{'loss': 0.4067, 'grad_norm': 0.9332733750343323, 'learning_rate': 0.00041557660892142714, 'epoch': 0.8181818181818182}\n{'loss': 0.4131, 'grad_norm': 2.308293104171753, 'learning_rate': 0.0004133302488732032, 'epoch': 0.8409090909090909}\n{'loss': 0.45, 'grad_norm': 2.0164291858673096, 'learning_rate': 0.00041108388882497924, 'epoch': 0.8636363636363636}\n{'loss': 0.366, 'grad_norm': 1.3637971878051758, 'learning_rate': 0.00040883752877675535, 'epoch': 0.8863636363636364}\n{'loss': 0.3914, 'grad_norm': 0.805012047290802, 'learning_rate': 0.0004065911687285314, 'epoch': 0.9090909090909091}\n{'loss': 0.4532, 'grad_norm': 2.2423996925354004, 'learning_rate': 0.00040434480868030745, 'epoch': 0.9318181818181818}\n{'loss': 0.4379, 'grad_norm': 2.0819807052612305, 'learning_rate': 0.00040209844863208356, 'epoch': 0.9545454545454546}\n{'loss': 0.4043, 'grad_norm': 1.8678312301635742, 'learning_rate': 0.0003998520885838596, 'epoch': 0.9772727272727273}\n{'loss': 0.2317, 'grad_norm': 2.3946962356567383, 'learning_rate': 0.00039760572853563567, 'epoch': 1.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8014    0.7729    0.7869      3404\n     PERCENT     0.7609    0.9859    0.8589        71\n        TYPE     0.9045    0.9560    0.9295     11194\n      VOLUME     0.5227    0.4107    0.4600        56\n\n   micro avg     0.8803    0.9117    0.8957     14725\n   macro avg     0.7474    0.7814    0.7588     14725\nweighted avg     0.8785    0.9117    0.8944     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.3570951223373413, 'eval_f1_macro': 0.7588267653274428, 'eval_precision': 0.8803278688524591, 'eval_recall': 0.9117147707979627, 'eval_f1': 0.8957464553794829, 'eval_accuracy': 0.8922874527215918, 'eval_runtime': 1.5293, 'eval_samples_per_second': 3603.575, 'eval_steps_per_second': 7.193, 'epoch': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.3077, 'grad_norm': 0.7014375329017639, 'learning_rate': 0.0003953593684874117, 'epoch': 1.0227272727272727}\n{'loss': 0.2798, 'grad_norm': 1.5063633918762207, 'learning_rate': 0.0003931130084391878, 'epoch': 1.0454545454545454}\n{'loss': 0.2805, 'grad_norm': 0.9555522799491882, 'learning_rate': 0.0003908666483909639, 'epoch': 1.0681818181818181}\n{'loss': 0.2865, 'grad_norm': 0.6662066578865051, 'learning_rate': 0.00038862028834273993, 'epoch': 1.0909090909090908}\n{'loss': 0.3127, 'grad_norm': 1.2417590618133545, 'learning_rate': 0.00038637392829451604, 'epoch': 1.1136363636363635}\n{'loss': 0.2383, 'grad_norm': 0.9037220478057861, 'learning_rate': 0.0003841275682462921, 'epoch': 1.1363636363636362}\n{'loss': 0.294, 'grad_norm': 0.8363068103790283, 'learning_rate': 0.00038188120819806815, 'epoch': 1.1590909090909092}\n{'loss': 0.3376, 'grad_norm': 0.7470033168792725, 'learning_rate': 0.0003796348481498442, 'epoch': 1.1818181818181819}\n{'loss': 0.2888, 'grad_norm': 1.4527223110198975, 'learning_rate': 0.0003773884881016203, 'epoch': 1.2045454545454546}\n{'loss': 0.3427, 'grad_norm': 1.3624438047409058, 'learning_rate': 0.00037514212805339636, 'epoch': 1.2272727272727273}\n{'loss': 0.3048, 'grad_norm': 1.2079875469207764, 'learning_rate': 0.00037289576800517247, 'epoch': 1.25}\n{'loss': 0.274, 'grad_norm': 0.7331960201263428, 'learning_rate': 0.0003706494079569485, 'epoch': 1.2727272727272727}\n{'loss': 0.2324, 'grad_norm': 0.9242087006568909, 'learning_rate': 0.0003684030479087246, 'epoch': 1.2954545454545454}\n{'loss': 0.2684, 'grad_norm': 0.9210774302482605, 'learning_rate': 0.0003661566878605007, 'epoch': 1.3181818181818181}\n{'loss': 0.3067, 'grad_norm': 1.2989788055419922, 'learning_rate': 0.0003639103278122768, 'epoch': 1.3409090909090908}\n{'loss': 0.3116, 'grad_norm': 0.8296862244606018, 'learning_rate': 0.00036166396776405284, 'epoch': 1.3636363636363638}\n{'loss': 0.236, 'grad_norm': 0.6452436447143555, 'learning_rate': 0.0003594176077158289, 'epoch': 1.3863636363636362}\n{'loss': 0.2478, 'grad_norm': 0.6188422441482544, 'learning_rate': 0.00035717124766760494, 'epoch': 1.4090909090909092}\n{'loss': 0.2874, 'grad_norm': 0.8927099108695984, 'learning_rate': 0.00035492488761938105, 'epoch': 1.4318181818181819}\n{'loss': 0.2709, 'grad_norm': 1.4544410705566406, 'learning_rate': 0.0003526785275711571, 'epoch': 1.4545454545454546}\n{'loss': 0.2493, 'grad_norm': 1.0445185899734497, 'learning_rate': 0.00035043216752293316, 'epoch': 1.4772727272727273}\n{'loss': 0.2752, 'grad_norm': 1.0348854064941406, 'learning_rate': 0.00034818580747470926, 'epoch': 1.5}\n{'loss': 0.307, 'grad_norm': 0.7479889988899231, 'learning_rate': 0.0003459394474264853, 'epoch': 1.5227272727272727}\n{'loss': 0.2114, 'grad_norm': 1.0359514951705933, 'learning_rate': 0.00034369308737826137, 'epoch': 1.5454545454545454}\n{'loss': 0.2614, 'grad_norm': 1.136646032333374, 'learning_rate': 0.0003414467273300374, 'epoch': 1.5681818181818183}\n{'loss': 0.25, 'grad_norm': 1.0536494255065918, 'learning_rate': 0.00033920036728181353, 'epoch': 1.5909090909090908}\n{'loss': 0.2279, 'grad_norm': 0.9764122366905212, 'learning_rate': 0.0003369540072335896, 'epoch': 1.6136363636363638}\n{'loss': 0.2825, 'grad_norm': 0.7590253949165344, 'learning_rate': 0.00033470764718536563, 'epoch': 1.6363636363636362}\n{'loss': 0.2492, 'grad_norm': 0.9347151517868042, 'learning_rate': 0.00033246128713714174, 'epoch': 1.6590909090909092}\n{'loss': 0.2773, 'grad_norm': 1.6595990657806396, 'learning_rate': 0.0003302149270889178, 'epoch': 1.6818181818181817}\n{'loss': 0.2361, 'grad_norm': 0.90348219871521, 'learning_rate': 0.00032796856704069385, 'epoch': 1.7045454545454546}\n{'loss': 0.2662, 'grad_norm': 0.9698200225830078, 'learning_rate': 0.00032572220699246995, 'epoch': 1.7272727272727273}\n{'loss': 0.2155, 'grad_norm': 1.4475454092025757, 'learning_rate': 0.000323475846944246, 'epoch': 1.75}\n{'loss': 0.1978, 'grad_norm': 1.080614686012268, 'learning_rate': 0.00032122948689602206, 'epoch': 1.7727272727272727}\n{'loss': 0.2586, 'grad_norm': 0.7129232287406921, 'learning_rate': 0.0003189831268477981, 'epoch': 1.7954545454545454}\n{'loss': 0.255, 'grad_norm': 0.7790781855583191, 'learning_rate': 0.0003167367667995742, 'epoch': 1.8181818181818183}\n{'loss': 0.2869, 'grad_norm': 0.9472772479057312, 'learning_rate': 0.00031449040675135027, 'epoch': 1.8409090909090908}\n{'loss': 0.2272, 'grad_norm': 0.778872549533844, 'learning_rate': 0.0003122440467031263, 'epoch': 1.8636363636363638}\n{'loss': 0.2574, 'grad_norm': 0.766369104385376, 'learning_rate': 0.00030999768665490243, 'epoch': 1.8863636363636362}\n{'loss': 0.2483, 'grad_norm': 0.8024981617927551, 'learning_rate': 0.0003077513266066785, 'epoch': 1.9090909090909092}\n{'loss': 0.2014, 'grad_norm': 1.4241509437561035, 'learning_rate': 0.00030550496655845454, 'epoch': 1.9318181818181817}\n{'loss': 0.2446, 'grad_norm': 1.4667109251022339, 'learning_rate': 0.0003032586065102306, 'epoch': 1.9545454545454546}\n{'loss': 0.217, 'grad_norm': 0.7753912210464478, 'learning_rate': 0.0003010122464620067, 'epoch': 1.9772727272727273}\n{'loss': 0.1212, 'grad_norm': 2.5997157096862793, 'learning_rate': 0.00029876588641378275, 'epoch': 2.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8803    0.8581    0.8691      3404\n     PERCENT     0.9571    0.9437    0.9504        71\n        TYPE     0.9263    0.9726    0.9489     11194\n      VOLUME     0.8600    0.7679    0.8113        56\n\n   micro avg     0.9162    0.9452    0.9305     14725\n   macro avg     0.9060    0.8856    0.8949     14725\nweighted avg     0.9156    0.9452    0.9299     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.2719194293022156, 'eval_f1_macro': 0.8949110382112678, 'eval_precision': 0.9162003818050162, 'eval_recall': 0.9451952461799661, 'eval_f1': 0.9304719882337212, 'eval_accuracy': 0.9238063914926273, 'eval_runtime': 1.4597, 'eval_samples_per_second': 3775.455, 'eval_steps_per_second': 7.536, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.1522, 'grad_norm': 0.5473377704620361, 'learning_rate': 0.0002965195263655588, 'epoch': 2.022727272727273}\n{'loss': 0.1862, 'grad_norm': 0.9775122404098511, 'learning_rate': 0.0002942731663173349, 'epoch': 2.0454545454545454}\n{'loss': 0.1513, 'grad_norm': 0.9837369918823242, 'learning_rate': 0.00029202680626911096, 'epoch': 2.0681818181818183}\n{'loss': 0.1775, 'grad_norm': 1.1734068393707275, 'learning_rate': 0.000289780446220887, 'epoch': 2.090909090909091}\n{'loss': 0.1676, 'grad_norm': 0.6491696834564209, 'learning_rate': 0.0002875340861726631, 'epoch': 2.1136363636363638}\n{'loss': 0.1739, 'grad_norm': 1.2154078483581543, 'learning_rate': 0.0002852877261244392, 'epoch': 2.1363636363636362}\n{'loss': 0.1559, 'grad_norm': 0.9362673759460449, 'learning_rate': 0.00028304136607621523, 'epoch': 2.159090909090909}\n{'loss': 0.2072, 'grad_norm': 0.8900330066680908, 'learning_rate': 0.0002807950060279913, 'epoch': 2.1818181818181817}\n{'loss': 0.166, 'grad_norm': 1.3013256788253784, 'learning_rate': 0.0002785486459797674, 'epoch': 2.2045454545454546}\n{'loss': 0.135, 'grad_norm': 0.7086580395698547, 'learning_rate': 0.00027630228593154344, 'epoch': 2.227272727272727}\n{'loss': 0.225, 'grad_norm': 2.338002920150757, 'learning_rate': 0.0002740559258833195, 'epoch': 2.25}\n{'loss': 0.1734, 'grad_norm': 1.2240591049194336, 'learning_rate': 0.0002718095658350956, 'epoch': 2.2727272727272725}\n{'loss': 0.1431, 'grad_norm': 1.065468668937683, 'learning_rate': 0.00026956320578687165, 'epoch': 2.2954545454545454}\n{'loss': 0.1555, 'grad_norm': 1.0456212759017944, 'learning_rate': 0.0002673168457386477, 'epoch': 2.3181818181818183}\n{'loss': 0.1151, 'grad_norm': 0.6955319046974182, 'learning_rate': 0.00026507048569042376, 'epoch': 2.340909090909091}\n{'loss': 0.1731, 'grad_norm': 0.8405917286872864, 'learning_rate': 0.00026282412564219987, 'epoch': 2.3636363636363638}\n{'loss': 0.1722, 'grad_norm': 1.5382236242294312, 'learning_rate': 0.0002605777655939759, 'epoch': 2.3863636363636362}\n{'loss': 0.153, 'grad_norm': 0.9890551567077637, 'learning_rate': 0.00025833140554575197, 'epoch': 2.409090909090909}\n{'loss': 0.1333, 'grad_norm': 1.239859938621521, 'learning_rate': 0.0002560850454975281, 'epoch': 2.4318181818181817}\n{'loss': 0.175, 'grad_norm': 0.9383814930915833, 'learning_rate': 0.00025383868544930413, 'epoch': 2.4545454545454546}\n{'loss': 0.1661, 'grad_norm': 2.0101404190063477, 'learning_rate': 0.0002515923254010802, 'epoch': 2.4772727272727275}\n{'loss': 0.1551, 'grad_norm': 1.022376298904419, 'learning_rate': 0.00024934596535285624, 'epoch': 2.5}\n{'loss': 0.1693, 'grad_norm': 1.4156370162963867, 'learning_rate': 0.00024709960530463235, 'epoch': 2.5227272727272725}\n{'loss': 0.1561, 'grad_norm': 0.8690310120582581, 'learning_rate': 0.0002448532452564084, 'epoch': 2.5454545454545454}\n{'loss': 0.1947, 'grad_norm': 1.2757207155227661, 'learning_rate': 0.00024260688520818448, 'epoch': 2.5681818181818183}\n{'loss': 0.131, 'grad_norm': 0.7034133076667786, 'learning_rate': 0.00024036052515996059, 'epoch': 2.590909090909091}\n{'loss': 0.2061, 'grad_norm': 1.5685056447982788, 'learning_rate': 0.00023811416511173664, 'epoch': 2.6136363636363638}\n{'loss': 0.148, 'grad_norm': 2.214132785797119, 'learning_rate': 0.0002358678050635127, 'epoch': 2.6363636363636362}\n{'loss': 0.1328, 'grad_norm': 1.12517511844635, 'learning_rate': 0.0002336214450152888, 'epoch': 2.659090909090909}\n{'loss': 0.1318, 'grad_norm': 1.1268467903137207, 'learning_rate': 0.00023137508496706485, 'epoch': 2.6818181818181817}\n{'loss': 0.1717, 'grad_norm': 0.7824668884277344, 'learning_rate': 0.0002291287249188409, 'epoch': 2.7045454545454546}\n{'loss': 0.1364, 'grad_norm': 0.7479436993598938, 'learning_rate': 0.00022688236487061696, 'epoch': 2.7272727272727275}\n{'loss': 0.2091, 'grad_norm': 1.3213391304016113, 'learning_rate': 0.00022463600482239306, 'epoch': 2.75}\n{'loss': 0.1805, 'grad_norm': 0.9338337779045105, 'learning_rate': 0.00022238964477416912, 'epoch': 2.7727272727272725}\n{'loss': 0.1384, 'grad_norm': 0.9041760563850403, 'learning_rate': 0.0002201432847259452, 'epoch': 2.7954545454545454}\n{'loss': 0.1357, 'grad_norm': 0.8800744414329529, 'learning_rate': 0.00021789692467772125, 'epoch': 2.8181818181818183}\n{'loss': 0.1549, 'grad_norm': 0.9640636444091797, 'learning_rate': 0.00021565056462949733, 'epoch': 2.840909090909091}\n{'loss': 0.175, 'grad_norm': 0.9049058556556702, 'learning_rate': 0.00021340420458127338, 'epoch': 2.8636363636363638}\n{'loss': 0.1576, 'grad_norm': 1.1044557094573975, 'learning_rate': 0.00021115784453304946, 'epoch': 2.8863636363636362}\n{'loss': 0.1335, 'grad_norm': 0.8528215885162354, 'learning_rate': 0.00020891148448482554, 'epoch': 2.909090909090909}\n{'loss': 0.1571, 'grad_norm': 1.8465932607650757, 'learning_rate': 0.0002066651244366016, 'epoch': 2.9318181818181817}\n{'loss': 0.1473, 'grad_norm': 0.782477617263794, 'learning_rate': 0.00020441876438837767, 'epoch': 2.9545454545454546}\n{'loss': 0.1337, 'grad_norm': 1.105499029159546, 'learning_rate': 0.00020217240434015373, 'epoch': 2.9772727272727275}\n{'loss': 0.072, 'grad_norm': 1.20492684841156, 'learning_rate': 0.0001999260442919298, 'epoch': 3.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.9184    0.8261    0.8698      3404\n     PERCENT     0.9571    0.9437    0.9504        71\n        TYPE     0.9248    0.9769    0.9501     11194\n      VOLUME     0.8431    0.7679    0.8037        56\n\n   micro avg     0.9234    0.9411    0.9321     14725\n   macro avg     0.9109    0.8786    0.8935     14725\nweighted avg     0.9232    0.9411    0.9310     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.28521087765693665, 'eval_f1_macro': 0.8934998264434814, 'eval_precision': 0.9233690944226027, 'eval_recall': 0.9410526315789474, 'eval_f1': 0.9321270012108166, 'eval_accuracy': 0.924793071315025, 'eval_runtime': 1.4995, 'eval_samples_per_second': 3675.221, 'eval_steps_per_second': 7.336, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.1119, 'grad_norm': 1.4250574111938477, 'learning_rate': 0.00019767968424370586, 'epoch': 3.022727272727273}\n{'loss': 0.1361, 'grad_norm': 1.8114632368087769, 'learning_rate': 0.00019543332419548194, 'epoch': 3.0454545454545454}\n{'loss': 0.1317, 'grad_norm': 2.0704686641693115, 'learning_rate': 0.00019318696414725802, 'epoch': 3.0681818181818183}\n{'loss': 0.0787, 'grad_norm': 0.8872584700584412, 'learning_rate': 0.00019094060409903407, 'epoch': 3.090909090909091}\n{'loss': 0.0824, 'grad_norm': 0.7274918556213379, 'learning_rate': 0.00018869424405081015, 'epoch': 3.1136363636363638}\n{'loss': 0.1476, 'grad_norm': 0.906924307346344, 'learning_rate': 0.00018644788400258623, 'epoch': 3.1363636363636362}\n{'loss': 0.1278, 'grad_norm': 0.8674200177192688, 'learning_rate': 0.0001842015239543623, 'epoch': 3.159090909090909}\n{'loss': 0.1273, 'grad_norm': 1.8383214473724365, 'learning_rate': 0.0001819551639061384, 'epoch': 3.1818181818181817}\n{'loss': 0.0933, 'grad_norm': 1.0951659679412842, 'learning_rate': 0.00017970880385791445, 'epoch': 3.2045454545454546}\n{'loss': 0.0995, 'grad_norm': 1.0646547079086304, 'learning_rate': 0.00017746244380969053, 'epoch': 3.227272727272727}\n{'loss': 0.0875, 'grad_norm': 1.1871874332427979, 'learning_rate': 0.00017521608376146658, 'epoch': 3.25}\n{'loss': 0.1003, 'grad_norm': 0.6170650720596313, 'learning_rate': 0.00017296972371324266, 'epoch': 3.2727272727272725}\n{'loss': 0.1136, 'grad_norm': 1.0567889213562012, 'learning_rate': 0.0001707233636650187, 'epoch': 3.2954545454545454}\n{'loss': 0.1371, 'grad_norm': 1.812025547027588, 'learning_rate': 0.0001684770036167948, 'epoch': 3.3181818181818183}\n{'loss': 0.0959, 'grad_norm': 0.9395237565040588, 'learning_rate': 0.00016623064356857087, 'epoch': 3.340909090909091}\n{'loss': 0.1268, 'grad_norm': 1.5709892511367798, 'learning_rate': 0.00016398428352034692, 'epoch': 3.3636363636363638}\n{'loss': 0.1432, 'grad_norm': 2.0438685417175293, 'learning_rate': 0.000161737923472123, 'epoch': 3.3863636363636362}\n{'loss': 0.092, 'grad_norm': 0.7201539874076843, 'learning_rate': 0.00015949156342389906, 'epoch': 3.409090909090909}\n{'loss': 0.1229, 'grad_norm': 1.045285940170288, 'learning_rate': 0.00015724520337567514, 'epoch': 3.4318181818181817}\n{'loss': 0.1135, 'grad_norm': 1.5897103548049927, 'learning_rate': 0.00015499884332745122, 'epoch': 3.4545454545454546}\n{'loss': 0.106, 'grad_norm': 1.5109682083129883, 'learning_rate': 0.00015275248327922727, 'epoch': 3.4772727272727275}\n{'loss': 0.0939, 'grad_norm': 1.8779675960540771, 'learning_rate': 0.00015050612323100335, 'epoch': 3.5}\n{'loss': 0.1242, 'grad_norm': 2.510303258895874, 'learning_rate': 0.0001482597631827794, 'epoch': 3.5227272727272725}\n{'loss': 0.1133, 'grad_norm': 0.7939941883087158, 'learning_rate': 0.00014601340313455548, 'epoch': 3.5454545454545454}\n{'loss': 0.1329, 'grad_norm': 1.308922529220581, 'learning_rate': 0.00014376704308633156, 'epoch': 3.5681818181818183}\n{'loss': 0.143, 'grad_norm': 0.8290942907333374, 'learning_rate': 0.00014152068303810761, 'epoch': 3.590909090909091}\n{'loss': 0.1013, 'grad_norm': 1.0198590755462646, 'learning_rate': 0.0001392743229898837, 'epoch': 3.6136363636363638}\n{'loss': 0.0773, 'grad_norm': 0.9936516284942627, 'learning_rate': 0.00013702796294165975, 'epoch': 3.6363636363636362}\n{'loss': 0.1105, 'grad_norm': 0.7756006121635437, 'learning_rate': 0.00013478160289343583, 'epoch': 3.659090909090909}\n{'loss': 0.1266, 'grad_norm': 1.5702235698699951, 'learning_rate': 0.00013253524284521188, 'epoch': 3.6818181818181817}\n{'loss': 0.1006, 'grad_norm': 1.2254269123077393, 'learning_rate': 0.00013028888279698796, 'epoch': 3.7045454545454546}\n{'loss': 0.1007, 'grad_norm': 1.1793993711471558, 'learning_rate': 0.00012804252274876404, 'epoch': 3.7272727272727275}\n{'loss': 0.1045, 'grad_norm': 0.8441619277000427, 'learning_rate': 0.0001257961627005401, 'epoch': 3.75}\n{'loss': 0.1118, 'grad_norm': 0.9065975546836853, 'learning_rate': 0.00012354980265231617, 'epoch': 3.7727272727272725}\n{'loss': 0.1049, 'grad_norm': 1.0981019735336304, 'learning_rate': 0.00012130344260409224, 'epoch': 3.7954545454545454}\n{'loss': 0.1173, 'grad_norm': 1.3239482641220093, 'learning_rate': 0.00011905708255586832, 'epoch': 3.8181818181818183}\n{'loss': 0.0886, 'grad_norm': 1.1091175079345703, 'learning_rate': 0.0001168107225076444, 'epoch': 3.840909090909091}\n{'loss': 0.1229, 'grad_norm': 1.1093403100967407, 'learning_rate': 0.00011456436245942045, 'epoch': 3.8636363636363638}\n{'loss': 0.1056, 'grad_norm': 1.633799433708191, 'learning_rate': 0.00011231800241119653, 'epoch': 3.8863636363636362}\n{'loss': 0.1157, 'grad_norm': 1.475299596786499, 'learning_rate': 0.0001100716423629726, 'epoch': 3.909090909090909}\n{'loss': 0.1003, 'grad_norm': 1.5746474266052246, 'learning_rate': 0.00010782528231474866, 'epoch': 3.9318181818181817}\n{'loss': 0.1158, 'grad_norm': 1.0096943378448486, 'learning_rate': 0.00010557892226652473, 'epoch': 3.9545454545454546}\n{'loss': 0.1061, 'grad_norm': 0.7068389654159546, 'learning_rate': 0.0001033325622183008, 'epoch': 3.9772727272727275}\n{'loss': 0.0679, 'grad_norm': 3.1099441051483154, 'learning_rate': 0.00010108620217007686, 'epoch': 4.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.9027    0.8772    0.8897      3404\n     PERCENT     0.9571    0.9437    0.9504        71\n        TYPE     0.9400    0.9700    0.9548     11194\n      VOLUME     0.8431    0.7679    0.8037        56\n\n   micro avg     0.9315    0.9476    0.9395     14725\n   macro avg     0.9107    0.8897    0.8997     14725\nweighted avg     0.9311    0.9476    0.9391     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.2693703770637512, 'eval_f1_macro': 0.8996504793669649, 'eval_precision': 0.9315086782376502, 'eval_recall': 0.9476400679117147, 'eval_f1': 0.9395051338158559, 'eval_accuracy': 0.9316998300718083, 'eval_runtime': 1.5237, 'eval_samples_per_second': 3616.851, 'eval_steps_per_second': 7.219, 'epoch': 4.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.0884, 'grad_norm': 1.2853386402130127, 'learning_rate': 9.883984212185293e-05, 'epoch': 4.0227272727272725}\n{'loss': 0.0808, 'grad_norm': 0.9540750980377197, 'learning_rate': 9.659348207362901e-05, 'epoch': 4.045454545454546}\n{'loss': 0.0838, 'grad_norm': 0.936713695526123, 'learning_rate': 9.434712202540508e-05, 'epoch': 4.068181818181818}\n{'loss': 0.0753, 'grad_norm': 1.015130877494812, 'learning_rate': 9.210076197718116e-05, 'epoch': 4.090909090909091}\n{'loss': 0.0596, 'grad_norm': 0.5388286709785461, 'learning_rate': 8.985440192895722e-05, 'epoch': 4.113636363636363}\n{'loss': 0.0668, 'grad_norm': 0.5535487532615662, 'learning_rate': 8.760804188073329e-05, 'epoch': 4.136363636363637}\n{'loss': 0.0936, 'grad_norm': 0.9346482753753662, 'learning_rate': 8.536168183250936e-05, 'epoch': 4.159090909090909}\n{'loss': 0.0903, 'grad_norm': 0.9989129304885864, 'learning_rate': 8.311532178428544e-05, 'epoch': 4.181818181818182}\n{'loss': 0.0935, 'grad_norm': 0.8621132373809814, 'learning_rate': 8.08689617360615e-05, 'epoch': 4.204545454545454}\n{'loss': 0.0884, 'grad_norm': 0.7255060076713562, 'learning_rate': 7.862260168783757e-05, 'epoch': 4.2272727272727275}\n{'loss': 0.0755, 'grad_norm': 0.5996091961860657, 'learning_rate': 7.637624163961363e-05, 'epoch': 4.25}\n{'loss': 0.0634, 'grad_norm': 1.1055879592895508, 'learning_rate': 7.41298815913897e-05, 'epoch': 4.2727272727272725}\n{'loss': 0.0898, 'grad_norm': 0.9770657420158386, 'learning_rate': 7.188352154316578e-05, 'epoch': 4.295454545454546}\n{'loss': 0.0861, 'grad_norm': 0.644340991973877, 'learning_rate': 6.963716149494185e-05, 'epoch': 4.318181818181818}\n{'loss': 0.0629, 'grad_norm': 0.6255559325218201, 'learning_rate': 6.739080144671791e-05, 'epoch': 4.340909090909091}\n{'loss': 0.0867, 'grad_norm': 1.6054826974868774, 'learning_rate': 6.514444139849398e-05, 'epoch': 4.363636363636363}\n{'loss': 0.0786, 'grad_norm': 0.7853363156318665, 'learning_rate': 6.289808135027005e-05, 'epoch': 4.386363636363637}\n{'loss': 0.0803, 'grad_norm': 0.9705774188041687, 'learning_rate': 6.065172130204612e-05, 'epoch': 4.409090909090909}\n{'loss': 0.0822, 'grad_norm': 0.7460286021232605, 'learning_rate': 5.84053612538222e-05, 'epoch': 4.431818181818182}\n{'loss': 0.0857, 'grad_norm': 1.1409039497375488, 'learning_rate': 5.6159001205598266e-05, 'epoch': 4.454545454545454}\n{'loss': 0.0878, 'grad_norm': 0.7357409000396729, 'learning_rate': 5.391264115737433e-05, 'epoch': 4.4772727272727275}\n{'loss': 0.0753, 'grad_norm': 0.8037949800491333, 'learning_rate': 5.16662811091504e-05, 'epoch': 4.5}\n{'loss': 0.0651, 'grad_norm': 0.5885531306266785, 'learning_rate': 4.9419921060926465e-05, 'epoch': 4.5227272727272725}\n{'loss': 0.0854, 'grad_norm': 0.8111551403999329, 'learning_rate': 4.717356101270254e-05, 'epoch': 4.545454545454545}\n{'loss': 0.0836, 'grad_norm': 0.6970586180686951, 'learning_rate': 4.492720096447861e-05, 'epoch': 4.568181818181818}\n{'loss': 0.1343, 'grad_norm': 1.3318504095077515, 'learning_rate': 4.268084091625468e-05, 'epoch': 4.590909090909091}\n{'loss': 0.0711, 'grad_norm': 0.513367235660553, 'learning_rate': 4.043448086803075e-05, 'epoch': 4.613636363636363}\n{'loss': 0.0885, 'grad_norm': 0.7895944714546204, 'learning_rate': 3.818812081980682e-05, 'epoch': 4.636363636363637}\n{'loss': 0.0771, 'grad_norm': 0.9362363815307617, 'learning_rate': 3.594176077158289e-05, 'epoch': 4.659090909090909}\n{'loss': 0.1015, 'grad_norm': 1.130558729171753, 'learning_rate': 3.369540072335896e-05, 'epoch': 4.681818181818182}\n{'loss': 0.0717, 'grad_norm': 0.9622006416320801, 'learning_rate': 3.144904067513502e-05, 'epoch': 4.704545454545455}\n{'loss': 0.0857, 'grad_norm': 0.9151111841201782, 'learning_rate': 2.92026806269111e-05, 'epoch': 4.7272727272727275}\n{'loss': 0.1077, 'grad_norm': 0.8825057744979858, 'learning_rate': 2.6956320578687166e-05, 'epoch': 4.75}\n{'loss': 0.0713, 'grad_norm': 0.9411681294441223, 'learning_rate': 2.4709960530463233e-05, 'epoch': 4.7727272727272725}\n{'loss': 0.0932, 'grad_norm': 0.7261417508125305, 'learning_rate': 2.2463600482239306e-05, 'epoch': 4.795454545454545}\n{'loss': 0.1168, 'grad_norm': 0.7267905473709106, 'learning_rate': 2.0217240434015375e-05, 'epoch': 4.818181818181818}\n{'loss': 0.0846, 'grad_norm': 0.8168027997016907, 'learning_rate': 1.7970880385791445e-05, 'epoch': 4.840909090909091}\n{'loss': 0.093, 'grad_norm': 0.7199776768684387, 'learning_rate': 1.572452033756751e-05, 'epoch': 4.863636363636363}\n{'loss': 0.0524, 'grad_norm': 0.9384613037109375, 'learning_rate': 1.3478160289343583e-05, 'epoch': 4.886363636363637}\n{'loss': 0.1082, 'grad_norm': 0.7385264039039612, 'learning_rate': 1.1231800241119653e-05, 'epoch': 4.909090909090909}\n{'loss': 0.0596, 'grad_norm': 0.5643245577812195, 'learning_rate': 8.985440192895723e-06, 'epoch': 4.931818181818182}\n{'loss': 0.0764, 'grad_norm': 0.6020060181617737, 'learning_rate': 6.7390801446717915e-06, 'epoch': 4.954545454545455}\n{'loss': 0.116, 'grad_norm': 0.7657778859138489, 'learning_rate': 4.492720096447861e-06, 'epoch': 4.9772727272727275}\n{'loss': 0.0191, 'grad_norm': 1.0498073101043701, 'learning_rate': 2.2463600482239307e-06, 'epoch': 5.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8944    0.8837    0.8890      3404\n     PERCENT     0.9437    0.9437    0.9437        71\n        TYPE     0.9436    0.9694    0.9563     11194\n      VOLUME     0.8627    0.7857    0.8224        56\n\n   micro avg     0.9323    0.9487    0.9404     14725\n   macro avg     0.9111    0.8956    0.9029     14725\nweighted avg     0.9319    0.9487    0.9402     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.27056023478507996, 'eval_f1_macro': 0.902850032330353, 'eval_precision': 0.9322655989322656, 'eval_recall': 0.9487266553480476, 'eval_f1': 0.9404240996297542, 'eval_accuracy': 0.9327961409855835, 'eval_runtime': 1.4697, 'eval_samples_per_second': 3749.642, 'eval_steps_per_second': 7.484, 'epoch': 5.0}\n{'train_runtime': 30.9648, 'train_samples_per_second': 3559.044, 'train_steps_per_second': 7.105, 'train_loss': 0.3054121165282347, 'epoch': 5.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nThe speedups for torchdynamo mostly come with GPU Ampere or higher and which is not detected here.\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8944    0.8837    0.8890      3404\n     PERCENT     0.9437    0.9437    0.9437        71\n        TYPE     0.9436    0.9694    0.9563     11194\n      VOLUME     0.8627    0.7857    0.8224        56\n\n   micro avg     0.9323    0.9487    0.9404     14725\n   macro avg     0.9111    0.8956    0.9029     14725\nweighted avg     0.9319    0.9487    0.9402     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.27056023478507996, 'eval_f1_macro': 0.902850032330353, 'eval_precision': 0.9322655989322656, 'eval_recall': 0.9487266553480476, 'eval_f1': 0.9404240996297542, 'eval_accuracy': 0.9327961409855835, 'eval_runtime': 1.984, 'eval_samples_per_second': 2777.729, 'eval_steps_per_second': 5.544, 'epoch': 5.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/3364797558.py:66: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 2.1125, 'grad_norm': 6.846538066864014, 'learning_rate': 0.0, 'epoch': 0.022727272727272728}\n{'loss': 2.109, 'grad_norm': 6.948081016540527, 'learning_rate': 2.0217240434015375e-05, 'epoch': 0.045454545454545456}\n{'loss': 2.0408, 'grad_norm': 6.78020715713501, 'learning_rate': 4.043448086803075e-05, 'epoch': 0.06818181818181818}\n{'loss': 1.9756, 'grad_norm': 6.414498805999756, 'learning_rate': 6.065172130204612e-05, 'epoch': 0.09090909090909091}\n{'loss': 1.8463, 'grad_norm': 6.0612874031066895, 'learning_rate': 8.08689617360615e-05, 'epoch': 0.11363636363636363}\n{'loss': 1.6672, 'grad_norm': 5.772058486938477, 'learning_rate': 0.00010108620217007686, 'epoch': 0.13636363636363635}\n{'loss': 1.5799, 'grad_norm': 4.258321285247803, 'learning_rate': 0.00012130344260409224, 'epoch': 0.1590909090909091}\n{'loss': 1.3884, 'grad_norm': 3.424044370651245, 'learning_rate': 0.00014152068303810761, 'epoch': 0.18181818181818182}\n{'loss': 1.2968, 'grad_norm': 2.3415048122406006, 'learning_rate': 0.000161737923472123, 'epoch': 0.20454545454545456}\n{'loss': 1.2135, 'grad_norm': 1.9065550565719604, 'learning_rate': 0.0001819551639061384, 'epoch': 0.22727272727272727}\n{'loss': 1.2179, 'grad_norm': 2.4977567195892334, 'learning_rate': 0.00020217240434015373, 'epoch': 0.25}\n{'loss': 1.1613, 'grad_norm': 2.402719736099243, 'learning_rate': 0.00022238964477416912, 'epoch': 0.2727272727272727}\n{'loss': 1.061, 'grad_norm': 1.4782682657241821, 'learning_rate': 0.00024260688520818448, 'epoch': 0.29545454545454547}\n{'loss': 1.0476, 'grad_norm': 1.3440208435058594, 'learning_rate': 0.00026282412564219987, 'epoch': 0.3181818181818182}\n{'loss': 1.031, 'grad_norm': 2.050492763519287, 'learning_rate': 0.00028304136607621523, 'epoch': 0.3409090909090909}\n{'loss': 0.9493, 'grad_norm': 2.7445273399353027, 'learning_rate': 0.0003032586065102306, 'epoch': 0.36363636363636365}\n{'loss': 0.8768, 'grad_norm': 1.8623905181884766, 'learning_rate': 0.000323475846944246, 'epoch': 0.38636363636363635}\n{'loss': 0.846, 'grad_norm': 1.4436651468276978, 'learning_rate': 0.00034369308737826137, 'epoch': 0.4090909090909091}\n{'loss': 0.8001, 'grad_norm': 1.461560606956482, 'learning_rate': 0.0003639103278122768, 'epoch': 0.4318181818181818}\n{'loss': 0.8058, 'grad_norm': 1.577987790107727, 'learning_rate': 0.0003841275682462921, 'epoch': 0.45454545454545453}\n{'loss': 0.7253, 'grad_norm': 1.180367112159729, 'learning_rate': 0.00040434480868030745, 'epoch': 0.4772727272727273}\n{'loss': 0.7202, 'grad_norm': 1.5199100971221924, 'learning_rate': 0.00042456204911432287, 'epoch': 0.5}\n{'loss': 0.6628, 'grad_norm': 2.2079124450683594, 'learning_rate': 0.00044477928954833823, 'epoch': 0.5227272727272727}\n{'loss': 0.5966, 'grad_norm': 1.6625714302062988, 'learning_rate': 0.0004425329295001143, 'epoch': 0.5454545454545454}\n{'loss': 0.5432, 'grad_norm': 1.047460913658142, 'learning_rate': 0.0004402865694518904, 'epoch': 0.5681818181818182}\n{'loss': 0.6228, 'grad_norm': 1.490531086921692, 'learning_rate': 0.00043804020940366645, 'epoch': 0.5909090909090909}\n{'loss': 0.5905, 'grad_norm': 1.1943162679672241, 'learning_rate': 0.0004357938493554425, 'epoch': 0.6136363636363636}\n{'loss': 0.6246, 'grad_norm': 1.44585382938385, 'learning_rate': 0.00043354748930721855, 'epoch': 0.6363636363636364}\n{'loss': 0.4774, 'grad_norm': 1.836166262626648, 'learning_rate': 0.00043130112925899466, 'epoch': 0.6590909090909091}\n{'loss': 0.5169, 'grad_norm': 0.783498227596283, 'learning_rate': 0.0004290547692107707, 'epoch': 0.6818181818181818}\n{'loss': 0.5151, 'grad_norm': 0.7650974988937378, 'learning_rate': 0.00042680840916254676, 'epoch': 0.7045454545454546}\n{'loss': 0.5461, 'grad_norm': 1.307688593864441, 'learning_rate': 0.00042456204911432287, 'epoch': 0.7272727272727273}\n{'loss': 0.4946, 'grad_norm': 1.4286037683486938, 'learning_rate': 0.0004223156890660989, 'epoch': 0.75}\n{'loss': 0.4428, 'grad_norm': 0.8837493658065796, 'learning_rate': 0.000420069329017875, 'epoch': 0.7727272727272727}\n{'loss': 0.5244, 'grad_norm': 1.5011682510375977, 'learning_rate': 0.0004178229689696511, 'epoch': 0.7954545454545454}\n{'loss': 0.4919, 'grad_norm': 1.896561622619629, 'learning_rate': 0.00041557660892142714, 'epoch': 0.8181818181818182}\n{'loss': 0.3974, 'grad_norm': 1.2612353563308716, 'learning_rate': 0.0004133302488732032, 'epoch': 0.8409090909090909}\n{'loss': 0.4463, 'grad_norm': 1.2631306648254395, 'learning_rate': 0.00041108388882497924, 'epoch': 0.8636363636363636}\n{'loss': 0.425, 'grad_norm': 1.4144549369812012, 'learning_rate': 0.00040883752877675535, 'epoch': 0.8863636363636364}\n{'loss': 0.3799, 'grad_norm': 0.9551413655281067, 'learning_rate': 0.0004065911687285314, 'epoch': 0.9090909090909091}\n{'loss': 0.3627, 'grad_norm': 0.8814662098884583, 'learning_rate': 0.00040434480868030745, 'epoch': 0.9318181818181818}\n{'loss': 0.363, 'grad_norm': 0.8238357305526733, 'learning_rate': 0.00040209844863208356, 'epoch': 0.9545454545454546}\n{'loss': 0.4052, 'grad_norm': 1.0830514430999756, 'learning_rate': 0.0003998520885838596, 'epoch': 0.9772727272727273}\n{'loss': 0.3868, 'grad_norm': 3.564356803894043, 'learning_rate': 0.00039760572853563567, 'epoch': 1.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8066    0.8149    0.8107      3311\n     PERCENT     0.5000    0.9302    0.6504        86\n        TYPE     0.9171    0.9532    0.9348     11299\n      VOLUME     0.1000    0.0238    0.0385        42\n\n   micro avg     0.8879    0.9193    0.9034     14738\n   macro avg     0.5809    0.6805    0.6086     14738\nweighted avg     0.8875    0.9193    0.9027     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.34659138321876526, 'eval_f1_macro': 0.6085848189661465, 'eval_precision': 0.8879349891867094, 'eval_recall': 0.919324195956032, 'eval_f1': 0.9033570023669034, 'eval_accuracy': 0.8977105076225342, 'eval_runtime': 1.5095, 'eval_samples_per_second': 3650.119, 'eval_steps_per_second': 7.287, 'epoch': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.3205, 'grad_norm': 1.5138953924179077, 'learning_rate': 0.0003953593684874117, 'epoch': 1.0227272727272727}\n{'loss': 0.3375, 'grad_norm': 1.020003318786621, 'learning_rate': 0.0003931130084391878, 'epoch': 1.0454545454545454}\n{'loss': 0.3653, 'grad_norm': 1.4894492626190186, 'learning_rate': 0.0003908666483909639, 'epoch': 1.0681818181818181}\n{'loss': 0.3217, 'grad_norm': 0.7484637498855591, 'learning_rate': 0.00038862028834273993, 'epoch': 1.0909090909090908}\n{'loss': 0.317, 'grad_norm': 1.3988906145095825, 'learning_rate': 0.00038637392829451604, 'epoch': 1.1136363636363635}\n{'loss': 0.3688, 'grad_norm': 1.2910683155059814, 'learning_rate': 0.0003841275682462921, 'epoch': 1.1363636363636362}\n{'loss': 0.2624, 'grad_norm': 0.851360023021698, 'learning_rate': 0.00038188120819806815, 'epoch': 1.1590909090909092}\n{'loss': 0.3052, 'grad_norm': 1.7322752475738525, 'learning_rate': 0.0003796348481498442, 'epoch': 1.1818181818181819}\n{'loss': 0.3, 'grad_norm': 1.0971988439559937, 'learning_rate': 0.0003773884881016203, 'epoch': 1.2045454545454546}\n{'loss': 0.3258, 'grad_norm': 0.9595935344696045, 'learning_rate': 0.00037514212805339636, 'epoch': 1.2272727272727273}\n{'loss': 0.3034, 'grad_norm': 1.0662425756454468, 'learning_rate': 0.00037289576800517247, 'epoch': 1.25}\n{'loss': 0.2638, 'grad_norm': 1.3622735738754272, 'learning_rate': 0.0003706494079569485, 'epoch': 1.2727272727272727}\n{'loss': 0.2716, 'grad_norm': 1.3209172487258911, 'learning_rate': 0.0003684030479087246, 'epoch': 1.2954545454545454}\n{'loss': 0.3236, 'grad_norm': 1.0697685480117798, 'learning_rate': 0.0003661566878605007, 'epoch': 1.3181818181818181}\n{'loss': 0.2581, 'grad_norm': 0.653820276260376, 'learning_rate': 0.0003639103278122768, 'epoch': 1.3409090909090908}\n{'loss': 0.3216, 'grad_norm': 1.4614087343215942, 'learning_rate': 0.00036166396776405284, 'epoch': 1.3636363636363638}\n{'loss': 0.2821, 'grad_norm': 1.5745261907577515, 'learning_rate': 0.0003594176077158289, 'epoch': 1.3863636363636362}\n{'loss': 0.3186, 'grad_norm': 0.9894313216209412, 'learning_rate': 0.00035717124766760494, 'epoch': 1.4090909090909092}\n{'loss': 0.2646, 'grad_norm': 2.1306028366088867, 'learning_rate': 0.00035492488761938105, 'epoch': 1.4318181818181819}\n{'loss': 0.2683, 'grad_norm': 1.2909084558486938, 'learning_rate': 0.0003526785275711571, 'epoch': 1.4545454545454546}\n{'loss': 0.2235, 'grad_norm': 0.7858564257621765, 'learning_rate': 0.00035043216752293316, 'epoch': 1.4772727272727273}\n{'loss': 0.2993, 'grad_norm': 1.0489386320114136, 'learning_rate': 0.00034818580747470926, 'epoch': 1.5}\n{'loss': 0.2515, 'grad_norm': 0.6707305908203125, 'learning_rate': 0.0003459394474264853, 'epoch': 1.5227272727272727}\n{'loss': 0.2262, 'grad_norm': 0.6718675494194031, 'learning_rate': 0.00034369308737826137, 'epoch': 1.5454545454545454}\n{'loss': 0.2245, 'grad_norm': 0.941990077495575, 'learning_rate': 0.0003414467273300374, 'epoch': 1.5681818181818183}\n{'loss': 0.2581, 'grad_norm': 1.0311239957809448, 'learning_rate': 0.00033920036728181353, 'epoch': 1.5909090909090908}\n{'loss': 0.2417, 'grad_norm': 1.1887824535369873, 'learning_rate': 0.0003369540072335896, 'epoch': 1.6136363636363638}\n{'loss': 0.2721, 'grad_norm': 0.8438599109649658, 'learning_rate': 0.00033470764718536563, 'epoch': 1.6363636363636362}\n{'loss': 0.2357, 'grad_norm': 1.6411105394363403, 'learning_rate': 0.00033246128713714174, 'epoch': 1.6590909090909092}\n{'loss': 0.2746, 'grad_norm': 1.1436636447906494, 'learning_rate': 0.0003302149270889178, 'epoch': 1.6818181818181817}\n{'loss': 0.2561, 'grad_norm': 0.9965160489082336, 'learning_rate': 0.00032796856704069385, 'epoch': 1.7045454545454546}\n{'loss': 0.245, 'grad_norm': 1.2451270818710327, 'learning_rate': 0.00032572220699246995, 'epoch': 1.7272727272727273}\n{'loss': 0.2298, 'grad_norm': 0.7287924289703369, 'learning_rate': 0.000323475846944246, 'epoch': 1.75}\n{'loss': 0.2583, 'grad_norm': 1.3973910808563232, 'learning_rate': 0.00032122948689602206, 'epoch': 1.7727272727272727}\n{'loss': 0.2125, 'grad_norm': 1.0861849784851074, 'learning_rate': 0.0003189831268477981, 'epoch': 1.7954545454545454}\n{'loss': 0.2605, 'grad_norm': 0.777121901512146, 'learning_rate': 0.0003167367667995742, 'epoch': 1.8181818181818183}\n{'loss': 0.2036, 'grad_norm': 0.6196991205215454, 'learning_rate': 0.00031449040675135027, 'epoch': 1.8409090909090908}\n{'loss': 0.2508, 'grad_norm': 0.8026944398880005, 'learning_rate': 0.0003122440467031263, 'epoch': 1.8636363636363638}\n{'loss': 0.2226, 'grad_norm': 0.8616043925285339, 'learning_rate': 0.00030999768665490243, 'epoch': 1.8863636363636362}\n{'loss': 0.2444, 'grad_norm': 0.8193431496620178, 'learning_rate': 0.0003077513266066785, 'epoch': 1.9090909090909092}\n{'loss': 0.2765, 'grad_norm': 0.7106210589408875, 'learning_rate': 0.00030550496655845454, 'epoch': 1.9318181818181817}\n{'loss': 0.2579, 'grad_norm': 0.973000705242157, 'learning_rate': 0.0003032586065102306, 'epoch': 1.9545454545454546}\n{'loss': 0.211, 'grad_norm': 0.8966994881629944, 'learning_rate': 0.0003010122464620067, 'epoch': 1.9772727272727273}\n{'loss': 0.1998, 'grad_norm': 3.0472676753997803, 'learning_rate': 0.00029876588641378275, 'epoch': 2.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8634    0.8931    0.8780      3311\n     PERCENT     0.8144    0.9186    0.8634        86\n        TYPE     0.9452    0.9653    0.9552     11299\n      VOLUME     0.6667    0.5714    0.6154        42\n\n   micro avg     0.9252    0.9477    0.9363     14738\n   macro avg     0.8224    0.8371    0.8280     14738\nweighted avg     0.9253    0.9477    0.9363     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.23708239197731018, 'eval_f1_macro': 0.8279760408043739, 'eval_precision': 0.9251506921905014, 'eval_recall': 0.9476862532229611, 'eval_f1': 0.9362828892240656, 'eval_accuracy': 0.9285831375334681, 'eval_runtime': 1.4631, 'eval_samples_per_second': 3766.071, 'eval_steps_per_second': 7.518, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.1531, 'grad_norm': 0.8257977366447449, 'learning_rate': 0.0002965195263655588, 'epoch': 2.022727272727273}\n{'loss': 0.1728, 'grad_norm': 0.9784489870071411, 'learning_rate': 0.0002942731663173349, 'epoch': 2.0454545454545454}\n{'loss': 0.175, 'grad_norm': 0.893501341342926, 'learning_rate': 0.00029202680626911096, 'epoch': 2.0681818181818183}\n{'loss': 0.1817, 'grad_norm': 0.9519100785255432, 'learning_rate': 0.000289780446220887, 'epoch': 2.090909090909091}\n{'loss': 0.1685, 'grad_norm': 0.7142960429191589, 'learning_rate': 0.0002875340861726631, 'epoch': 2.1136363636363638}\n{'loss': 0.12, 'grad_norm': 0.5806414484977722, 'learning_rate': 0.0002852877261244392, 'epoch': 2.1363636363636362}\n{'loss': 0.1634, 'grad_norm': 0.8416401147842407, 'learning_rate': 0.00028304136607621523, 'epoch': 2.159090909090909}\n{'loss': 0.1645, 'grad_norm': 0.7083094120025635, 'learning_rate': 0.0002807950060279913, 'epoch': 2.1818181818181817}\n{'loss': 0.2, 'grad_norm': 0.9581568837165833, 'learning_rate': 0.0002785486459797674, 'epoch': 2.2045454545454546}\n{'loss': 0.1826, 'grad_norm': 0.8005905747413635, 'learning_rate': 0.00027630228593154344, 'epoch': 2.227272727272727}\n{'loss': 0.2054, 'grad_norm': 1.8453651666641235, 'learning_rate': 0.0002740559258833195, 'epoch': 2.25}\n{'loss': 0.1494, 'grad_norm': 1.8212777376174927, 'learning_rate': 0.0002718095658350956, 'epoch': 2.2727272727272725}\n{'loss': 0.1862, 'grad_norm': 0.9448937177658081, 'learning_rate': 0.00026956320578687165, 'epoch': 2.2954545454545454}\n{'loss': 0.1776, 'grad_norm': 1.4839472770690918, 'learning_rate': 0.0002673168457386477, 'epoch': 2.3181818181818183}\n{'loss': 0.115, 'grad_norm': 0.9818533658981323, 'learning_rate': 0.00026507048569042376, 'epoch': 2.340909090909091}\n{'loss': 0.159, 'grad_norm': 0.7992745041847229, 'learning_rate': 0.00026282412564219987, 'epoch': 2.3636363636363638}\n{'loss': 0.1511, 'grad_norm': 0.8978822827339172, 'learning_rate': 0.0002605777655939759, 'epoch': 2.3863636363636362}\n{'loss': 0.217, 'grad_norm': 1.3751037120819092, 'learning_rate': 0.00025833140554575197, 'epoch': 2.409090909090909}\n{'loss': 0.1761, 'grad_norm': 1.0103598833084106, 'learning_rate': 0.0002560850454975281, 'epoch': 2.4318181818181817}\n{'loss': 0.1278, 'grad_norm': 1.258885145187378, 'learning_rate': 0.00025383868544930413, 'epoch': 2.4545454545454546}\n{'loss': 0.1785, 'grad_norm': 1.5767900943756104, 'learning_rate': 0.0002515923254010802, 'epoch': 2.4772727272727275}\n{'loss': 0.1351, 'grad_norm': 1.0271281003952026, 'learning_rate': 0.00024934596535285624, 'epoch': 2.5}\n{'loss': 0.1417, 'grad_norm': 1.1201406717300415, 'learning_rate': 0.00024709960530463235, 'epoch': 2.5227272727272725}\n{'loss': 0.1469, 'grad_norm': 1.4897887706756592, 'learning_rate': 0.0002448532452564084, 'epoch': 2.5454545454545454}\n{'loss': 0.1814, 'grad_norm': 1.9414814710617065, 'learning_rate': 0.00024260688520818448, 'epoch': 2.5681818181818183}\n{'loss': 0.1893, 'grad_norm': 1.5225533246994019, 'learning_rate': 0.00024036052515996059, 'epoch': 2.590909090909091}\n{'loss': 0.1797, 'grad_norm': 0.987919270992279, 'learning_rate': 0.00023811416511173664, 'epoch': 2.6136363636363638}\n{'loss': 0.2122, 'grad_norm': 0.8590908050537109, 'learning_rate': 0.0002358678050635127, 'epoch': 2.6363636363636362}\n{'loss': 0.1458, 'grad_norm': 1.0716195106506348, 'learning_rate': 0.0002336214450152888, 'epoch': 2.659090909090909}\n{'loss': 0.1203, 'grad_norm': 1.0065885782241821, 'learning_rate': 0.00023137508496706485, 'epoch': 2.6818181818181817}\n{'loss': 0.1392, 'grad_norm': 0.8513127565383911, 'learning_rate': 0.0002291287249188409, 'epoch': 2.7045454545454546}\n{'loss': 0.2115, 'grad_norm': 0.9052950739860535, 'learning_rate': 0.00022688236487061696, 'epoch': 2.7272727272727275}\n{'loss': 0.17, 'grad_norm': 1.1223175525665283, 'learning_rate': 0.00022463600482239306, 'epoch': 2.75}\n{'loss': 0.1687, 'grad_norm': 1.4482309818267822, 'learning_rate': 0.00022238964477416912, 'epoch': 2.7727272727272725}\n{'loss': 0.151, 'grad_norm': 1.0530856847763062, 'learning_rate': 0.0002201432847259452, 'epoch': 2.7954545454545454}\n{'loss': 0.1456, 'grad_norm': 0.8392202258110046, 'learning_rate': 0.00021789692467772125, 'epoch': 2.8181818181818183}\n{'loss': 0.1913, 'grad_norm': 1.3359928131103516, 'learning_rate': 0.00021565056462949733, 'epoch': 2.840909090909091}\n{'loss': 0.1266, 'grad_norm': 1.2462414503097534, 'learning_rate': 0.00021340420458127338, 'epoch': 2.8636363636363638}\n{'loss': 0.1588, 'grad_norm': 1.1992424726486206, 'learning_rate': 0.00021115784453304946, 'epoch': 2.8863636363636362}\n{'loss': 0.1438, 'grad_norm': 0.8938936591148376, 'learning_rate': 0.00020891148448482554, 'epoch': 2.909090909090909}\n{'loss': 0.1704, 'grad_norm': 1.1327931880950928, 'learning_rate': 0.0002066651244366016, 'epoch': 2.9318181818181817}\n{'loss': 0.1596, 'grad_norm': 0.8929866552352905, 'learning_rate': 0.00020441876438837767, 'epoch': 2.9545454545454546}\n{'loss': 0.1683, 'grad_norm': 0.7921090722084045, 'learning_rate': 0.00020217240434015373, 'epoch': 2.9772727272727275}\n{'loss': 0.0866, 'grad_norm': 2.8319766521453857, 'learning_rate': 0.0001999260442919298, 'epoch': 3.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8544    0.9091    0.8809      3311\n     PERCENT     0.8667    0.9070    0.8864        86\n        TYPE     0.9466    0.9606    0.9536     11299\n      VOLUME     0.7500    0.7143    0.7317        42\n\n   micro avg     0.9241    0.9480    0.9359     14738\n   macro avg     0.8544    0.8727    0.8631     14738\nweighted avg     0.9249    0.9480    0.9362     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.244726300239563, 'eval_f1_macro': 0.8631324245180767, 'eval_precision': 0.9241351941265957, 'eval_recall': 0.9480255122811779, 'eval_f1': 0.9359279231001105, 'eval_accuracy': 0.9281460029506584, 'eval_runtime': 1.4852, 'eval_samples_per_second': 3709.878, 'eval_steps_per_second': 7.406, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.0946, 'grad_norm': 0.9496968388557434, 'learning_rate': 0.00019767968424370586, 'epoch': 3.022727272727273}\n{'loss': 0.1394, 'grad_norm': 1.5339101552963257, 'learning_rate': 0.00019543332419548194, 'epoch': 3.0454545454545454}\n{'loss': 0.1223, 'grad_norm': 1.1758114099502563, 'learning_rate': 0.00019318696414725802, 'epoch': 3.0681818181818183}\n{'loss': 0.1208, 'grad_norm': 1.2366719245910645, 'learning_rate': 0.00019094060409903407, 'epoch': 3.090909090909091}\n{'loss': 0.1215, 'grad_norm': 0.8459974527359009, 'learning_rate': 0.00018869424405081015, 'epoch': 3.1136363636363638}\n{'loss': 0.0925, 'grad_norm': 0.719356894493103, 'learning_rate': 0.00018644788400258623, 'epoch': 3.1363636363636362}\n{'loss': 0.1468, 'grad_norm': 1.4192081689834595, 'learning_rate': 0.0001842015239543623, 'epoch': 3.159090909090909}\n{'loss': 0.0781, 'grad_norm': 0.740576446056366, 'learning_rate': 0.0001819551639061384, 'epoch': 3.1818181818181817}\n{'loss': 0.1044, 'grad_norm': 0.8353723287582397, 'learning_rate': 0.00017970880385791445, 'epoch': 3.2045454545454546}\n{'loss': 0.1178, 'grad_norm': 0.7990037202835083, 'learning_rate': 0.00017746244380969053, 'epoch': 3.227272727272727}\n{'loss': 0.1312, 'grad_norm': 1.207390546798706, 'learning_rate': 0.00017521608376146658, 'epoch': 3.25}\n{'loss': 0.134, 'grad_norm': 1.4884415864944458, 'learning_rate': 0.00017296972371324266, 'epoch': 3.2727272727272725}\n{'loss': 0.1116, 'grad_norm': 1.122546911239624, 'learning_rate': 0.0001707233636650187, 'epoch': 3.2954545454545454}\n{'loss': 0.1202, 'grad_norm': 1.0150272846221924, 'learning_rate': 0.0001684770036167948, 'epoch': 3.3181818181818183}\n{'loss': 0.0891, 'grad_norm': 0.6922440528869629, 'learning_rate': 0.00016623064356857087, 'epoch': 3.340909090909091}\n{'loss': 0.0844, 'grad_norm': 0.9892697930335999, 'learning_rate': 0.00016398428352034692, 'epoch': 3.3636363636363638}\n{'loss': 0.0798, 'grad_norm': 1.1414597034454346, 'learning_rate': 0.000161737923472123, 'epoch': 3.3863636363636362}\n{'loss': 0.1052, 'grad_norm': 1.1879299879074097, 'learning_rate': 0.00015949156342389906, 'epoch': 3.409090909090909}\n{'loss': 0.0929, 'grad_norm': 1.1450337171554565, 'learning_rate': 0.00015724520337567514, 'epoch': 3.4318181818181817}\n{'loss': 0.1207, 'grad_norm': 0.8139514923095703, 'learning_rate': 0.00015499884332745122, 'epoch': 3.4545454545454546}\n{'loss': 0.0963, 'grad_norm': 0.8552694320678711, 'learning_rate': 0.00015275248327922727, 'epoch': 3.4772727272727275}\n{'loss': 0.1238, 'grad_norm': 1.5760475397109985, 'learning_rate': 0.00015050612323100335, 'epoch': 3.5}\n{'loss': 0.0991, 'grad_norm': 0.9639792442321777, 'learning_rate': 0.0001482597631827794, 'epoch': 3.5227272727272725}\n{'loss': 0.0785, 'grad_norm': 1.4414587020874023, 'learning_rate': 0.00014601340313455548, 'epoch': 3.5454545454545454}\n{'loss': 0.1328, 'grad_norm': 1.1413859128952026, 'learning_rate': 0.00014376704308633156, 'epoch': 3.5681818181818183}\n{'loss': 0.1205, 'grad_norm': 0.6987881064414978, 'learning_rate': 0.00014152068303810761, 'epoch': 3.590909090909091}\n{'loss': 0.1241, 'grad_norm': 1.012144684791565, 'learning_rate': 0.0001392743229898837, 'epoch': 3.6136363636363638}\n{'loss': 0.1122, 'grad_norm': 1.0340983867645264, 'learning_rate': 0.00013702796294165975, 'epoch': 3.6363636363636362}\n{'loss': 0.0985, 'grad_norm': 0.665569543838501, 'learning_rate': 0.00013478160289343583, 'epoch': 3.659090909090909}\n{'loss': 0.1133, 'grad_norm': 1.0411670207977295, 'learning_rate': 0.00013253524284521188, 'epoch': 3.6818181818181817}\n{'loss': 0.0863, 'grad_norm': 0.6937991976737976, 'learning_rate': 0.00013028888279698796, 'epoch': 3.7045454545454546}\n{'loss': 0.1173, 'grad_norm': 0.6937951445579529, 'learning_rate': 0.00012804252274876404, 'epoch': 3.7272727272727275}\n{'loss': 0.136, 'grad_norm': 0.936755359172821, 'learning_rate': 0.0001257961627005401, 'epoch': 3.75}\n{'loss': 0.0952, 'grad_norm': 0.731763482093811, 'learning_rate': 0.00012354980265231617, 'epoch': 3.7727272727272725}\n{'loss': 0.1022, 'grad_norm': 0.8641891479492188, 'learning_rate': 0.00012130344260409224, 'epoch': 3.7954545454545454}\n{'loss': 0.1099, 'grad_norm': 0.6040621399879456, 'learning_rate': 0.00011905708255586832, 'epoch': 3.8181818181818183}\n{'loss': 0.149, 'grad_norm': 0.8259685039520264, 'learning_rate': 0.0001168107225076444, 'epoch': 3.840909090909091}\n{'loss': 0.1175, 'grad_norm': 0.9689567685127258, 'learning_rate': 0.00011456436245942045, 'epoch': 3.8636363636363638}\n{'loss': 0.0838, 'grad_norm': 0.6159664392471313, 'learning_rate': 0.00011231800241119653, 'epoch': 3.8863636363636362}\n{'loss': 0.1109, 'grad_norm': 0.7939480543136597, 'learning_rate': 0.0001100716423629726, 'epoch': 3.909090909090909}\n{'loss': 0.1044, 'grad_norm': 0.790865957736969, 'learning_rate': 0.00010782528231474866, 'epoch': 3.9318181818181817}\n{'loss': 0.1095, 'grad_norm': 0.6915451884269714, 'learning_rate': 0.00010557892226652473, 'epoch': 3.9545454545454546}\n{'loss': 0.1195, 'grad_norm': 0.6262756586074829, 'learning_rate': 0.0001033325622183008, 'epoch': 3.9772727272727275}\n{'loss': 0.0127, 'grad_norm': 0.7495555877685547, 'learning_rate': 0.00010108620217007686, 'epoch': 4.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8878    0.8961    0.8919      3311\n     PERCENT     0.8652    0.8953    0.8800        86\n        TYPE     0.9457    0.9670    0.9562     11299\n      VOLUME     0.7442    0.7619    0.7529        42\n\n   micro avg     0.9318    0.9501    0.9408     14738\n   macro avg     0.8607    0.8801    0.8703     14738\nweighted avg     0.9317    0.9501    0.9408     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.2399638444185257, 'eval_f1_macro': 0.8702774459586428, 'eval_precision': 0.9317894456644706, 'eval_recall': 0.9500610666304791, 'eval_f1': 0.9408365529984883, 'eval_accuracy': 0.9322987814873505, 'eval_runtime': 1.4894, 'eval_samples_per_second': 3699.543, 'eval_steps_per_second': 7.386, 'epoch': 4.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.1017, 'grad_norm': 0.8256344199180603, 'learning_rate': 9.883984212185293e-05, 'epoch': 4.0227272727272725}\n{'loss': 0.063, 'grad_norm': 0.4527645409107208, 'learning_rate': 9.659348207362901e-05, 'epoch': 4.045454545454546}\n{'loss': 0.1081, 'grad_norm': 0.900696337223053, 'learning_rate': 9.434712202540508e-05, 'epoch': 4.068181818181818}\n{'loss': 0.0813, 'grad_norm': 0.6872433423995972, 'learning_rate': 9.210076197718116e-05, 'epoch': 4.090909090909091}\n{'loss': 0.0799, 'grad_norm': 0.6165143847465515, 'learning_rate': 8.985440192895722e-05, 'epoch': 4.113636363636363}\n{'loss': 0.0726, 'grad_norm': 0.6486866474151611, 'learning_rate': 8.760804188073329e-05, 'epoch': 4.136363636363637}\n{'loss': 0.0753, 'grad_norm': 0.8585337996482849, 'learning_rate': 8.536168183250936e-05, 'epoch': 4.159090909090909}\n{'loss': 0.0786, 'grad_norm': 0.7621782422065735, 'learning_rate': 8.311532178428544e-05, 'epoch': 4.181818181818182}\n{'loss': 0.072, 'grad_norm': 0.554237961769104, 'learning_rate': 8.08689617360615e-05, 'epoch': 4.204545454545454}\n{'loss': 0.1038, 'grad_norm': 0.6966859102249146, 'learning_rate': 7.862260168783757e-05, 'epoch': 4.2272727272727275}\n{'loss': 0.086, 'grad_norm': 0.45531991124153137, 'learning_rate': 7.637624163961363e-05, 'epoch': 4.25}\n{'loss': 0.0946, 'grad_norm': 0.6656998991966248, 'learning_rate': 7.41298815913897e-05, 'epoch': 4.2727272727272725}\n{'loss': 0.0834, 'grad_norm': 0.7073277831077576, 'learning_rate': 7.188352154316578e-05, 'epoch': 4.295454545454546}\n{'loss': 0.1026, 'grad_norm': 0.9064696431159973, 'learning_rate': 6.963716149494185e-05, 'epoch': 4.318181818181818}\n{'loss': 0.0924, 'grad_norm': 0.8328092098236084, 'learning_rate': 6.739080144671791e-05, 'epoch': 4.340909090909091}\n{'loss': 0.0701, 'grad_norm': 0.4596281349658966, 'learning_rate': 6.514444139849398e-05, 'epoch': 4.363636363636363}\n{'loss': 0.1137, 'grad_norm': 0.7144079208374023, 'learning_rate': 6.289808135027005e-05, 'epoch': 4.386363636363637}\n{'loss': 0.1022, 'grad_norm': 0.7021552920341492, 'learning_rate': 6.065172130204612e-05, 'epoch': 4.409090909090909}\n{'loss': 0.0677, 'grad_norm': 0.6023500561714172, 'learning_rate': 5.84053612538222e-05, 'epoch': 4.431818181818182}\n{'loss': 0.0631, 'grad_norm': 0.6313793063163757, 'learning_rate': 5.6159001205598266e-05, 'epoch': 4.454545454545454}\n{'loss': 0.1039, 'grad_norm': 0.6945484280586243, 'learning_rate': 5.391264115737433e-05, 'epoch': 4.4772727272727275}\n{'loss': 0.114, 'grad_norm': 1.1566816568374634, 'learning_rate': 5.16662811091504e-05, 'epoch': 4.5}\n{'loss': 0.077, 'grad_norm': 0.6230624914169312, 'learning_rate': 4.9419921060926465e-05, 'epoch': 4.5227272727272725}\n{'loss': 0.0814, 'grad_norm': 1.3133491277694702, 'learning_rate': 4.717356101270254e-05, 'epoch': 4.545454545454545}\n{'loss': 0.0908, 'grad_norm': 0.6055175065994263, 'learning_rate': 4.492720096447861e-05, 'epoch': 4.568181818181818}\n{'loss': 0.0771, 'grad_norm': 0.5599241256713867, 'learning_rate': 4.268084091625468e-05, 'epoch': 4.590909090909091}\n{'loss': 0.1174, 'grad_norm': 0.7628793120384216, 'learning_rate': 4.043448086803075e-05, 'epoch': 4.613636363636363}\n{'loss': 0.0824, 'grad_norm': 0.7139708995819092, 'learning_rate': 3.818812081980682e-05, 'epoch': 4.636363636363637}\n{'loss': 0.0974, 'grad_norm': 0.6894019246101379, 'learning_rate': 3.594176077158289e-05, 'epoch': 4.659090909090909}\n{'loss': 0.0764, 'grad_norm': 0.8668765425682068, 'learning_rate': 3.369540072335896e-05, 'epoch': 4.681818181818182}\n{'loss': 0.0857, 'grad_norm': 0.7280609607696533, 'learning_rate': 3.144904067513502e-05, 'epoch': 4.704545454545455}\n{'loss': 0.0735, 'grad_norm': 0.6469992399215698, 'learning_rate': 2.92026806269111e-05, 'epoch': 4.7272727272727275}\n{'loss': 0.0935, 'grad_norm': 0.6960899233818054, 'learning_rate': 2.6956320578687166e-05, 'epoch': 4.75}\n{'loss': 0.0835, 'grad_norm': 1.5311152935028076, 'learning_rate': 2.4709960530463233e-05, 'epoch': 4.7727272727272725}\n{'loss': 0.0687, 'grad_norm': 0.6097765564918518, 'learning_rate': 2.2463600482239306e-05, 'epoch': 4.795454545454545}\n{'loss': 0.0747, 'grad_norm': 0.9288850426673889, 'learning_rate': 2.0217240434015375e-05, 'epoch': 4.818181818181818}\n{'loss': 0.0616, 'grad_norm': 0.7422559857368469, 'learning_rate': 1.7970880385791445e-05, 'epoch': 4.840909090909091}\n{'loss': 0.0918, 'grad_norm': 1.0221531391143799, 'learning_rate': 1.572452033756751e-05, 'epoch': 4.863636363636363}\n{'loss': 0.0994, 'grad_norm': 1.170586347579956, 'learning_rate': 1.3478160289343583e-05, 'epoch': 4.886363636363637}\n{'loss': 0.0727, 'grad_norm': 0.6657191514968872, 'learning_rate': 1.1231800241119653e-05, 'epoch': 4.909090909090909}\n{'loss': 0.092, 'grad_norm': 0.6645092368125916, 'learning_rate': 8.985440192895723e-06, 'epoch': 4.931818181818182}\n{'loss': 0.0514, 'grad_norm': 0.5887698531150818, 'learning_rate': 6.7390801446717915e-06, 'epoch': 4.954545454545455}\n{'loss': 0.069, 'grad_norm': 0.7709276080131531, 'learning_rate': 4.492720096447861e-06, 'epoch': 4.9772727272727275}\n{'loss': 0.0697, 'grad_norm': 7.146169662475586, 'learning_rate': 2.2463600482239307e-06, 'epoch': 5.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8994    0.8910    0.8952      3311\n     PERCENT     0.8652    0.8953    0.8800        86\n        TYPE     0.9475    0.9696    0.9584     11299\n      VOLUME     0.8000    0.8571    0.8276        42\n\n   micro avg     0.9360    0.9511    0.9435     14738\n   macro avg     0.8780    0.9033    0.8903     14738\nweighted avg     0.9358    0.9511    0.9434     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.2529582977294922, 'eval_f1_macro': 0.890286760881084, 'eval_precision': 0.9360309829059829, 'eval_recall': 0.951146695616773, 'eval_f1': 0.9435283031567611, 'eval_accuracy': 0.9341566034642915, 'eval_runtime': 1.4606, 'eval_samples_per_second': 3772.372, 'eval_steps_per_second': 7.531, 'epoch': 5.0}\n{'train_runtime': 30.9078, 'train_samples_per_second': 3565.766, 'train_steps_per_second': 7.118, 'train_loss': 0.3039588194255802, 'epoch': 5.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nThe speedups for torchdynamo mostly come with GPU Ampere or higher and which is not detected here.\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8994    0.8910    0.8952      3311\n     PERCENT     0.8652    0.8953    0.8800        86\n        TYPE     0.9475    0.9696    0.9584     11299\n      VOLUME     0.8000    0.8571    0.8276        42\n\n   micro avg     0.9360    0.9511    0.9435     14738\n   macro avg     0.8780    0.9033    0.8903     14738\nweighted avg     0.9358    0.9511    0.9434     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.2529582977294922, 'eval_f1_macro': 0.890286760881084, 'eval_precision': 0.9360309829059829, 'eval_recall': 0.951146695616773, 'eval_f1': 0.9435283031567611, 'eval_accuracy': 0.9341566034642915, 'eval_runtime': 1.945, 'eval_samples_per_second': 2832.932, 'eval_steps_per_second': 5.656, 'epoch': 5.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/3364797558.py:66: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 2.2283, 'grad_norm': 6.8190693855285645, 'learning_rate': 0.0, 'epoch': 0.022727272727272728}\n{'loss': 2.2231, 'grad_norm': 7.124982833862305, 'learning_rate': 2.0217240434015375e-05, 'epoch': 0.045454545454545456}\n{'loss': 2.1818, 'grad_norm': 6.6742353439331055, 'learning_rate': 4.043448086803075e-05, 'epoch': 0.06818181818181818}\n{'loss': 2.0595, 'grad_norm': 6.9263200759887695, 'learning_rate': 6.065172130204612e-05, 'epoch': 0.09090909090909091}\n{'loss': 1.9755, 'grad_norm': 6.295759201049805, 'learning_rate': 8.08689617360615e-05, 'epoch': 0.11363636363636363}\n{'loss': 1.8581, 'grad_norm': 5.578288555145264, 'learning_rate': 0.00010108620217007686, 'epoch': 0.13636363636363635}\n{'loss': 1.6625, 'grad_norm': 5.1212029457092285, 'learning_rate': 0.00012130344260409224, 'epoch': 0.1590909090909091}\n{'loss': 1.5244, 'grad_norm': 4.023754119873047, 'learning_rate': 0.00014152068303810761, 'epoch': 0.18181818181818182}\n{'loss': 1.3618, 'grad_norm': 3.0255682468414307, 'learning_rate': 0.000161737923472123, 'epoch': 0.20454545454545456}\n{'loss': 1.206, 'grad_norm': 2.021172285079956, 'learning_rate': 0.0001819551639061384, 'epoch': 0.22727272727272727}\n{'loss': 1.1363, 'grad_norm': 1.711435079574585, 'learning_rate': 0.00020217240434015373, 'epoch': 0.25}\n{'loss': 1.1258, 'grad_norm': 2.088050603866577, 'learning_rate': 0.00022238964477416912, 'epoch': 0.2727272727272727}\n{'loss': 1.1522, 'grad_norm': 2.072779417037964, 'learning_rate': 0.00024260688520818448, 'epoch': 0.29545454545454547}\n{'loss': 1.105, 'grad_norm': 1.4588619470596313, 'learning_rate': 0.00026282412564219987, 'epoch': 0.3181818181818182}\n{'loss': 1.0201, 'grad_norm': 1.2950433492660522, 'learning_rate': 0.00028304136607621523, 'epoch': 0.3409090909090909}\n{'loss': 0.9848, 'grad_norm': 2.7121481895446777, 'learning_rate': 0.0003032586065102306, 'epoch': 0.36363636363636365}\n{'loss': 0.9419, 'grad_norm': 2.720205307006836, 'learning_rate': 0.000323475846944246, 'epoch': 0.38636363636363635}\n{'loss': 0.8586, 'grad_norm': 2.598510980606079, 'learning_rate': 0.00034369308737826137, 'epoch': 0.4090909090909091}\n{'loss': 0.7919, 'grad_norm': 1.6543149948120117, 'learning_rate': 0.0003639103278122768, 'epoch': 0.4318181818181818}\n{'loss': 0.7559, 'grad_norm': 1.2437812089920044, 'learning_rate': 0.0003841275682462921, 'epoch': 0.45454545454545453}\n{'loss': 0.6964, 'grad_norm': 0.9610570669174194, 'learning_rate': 0.00040434480868030745, 'epoch': 0.4772727272727273}\n{'loss': 0.6791, 'grad_norm': 1.2990425825119019, 'learning_rate': 0.00042456204911432287, 'epoch': 0.5}\n{'loss': 0.6465, 'grad_norm': 0.9758591055870056, 'learning_rate': 0.00044477928954833823, 'epoch': 0.5227272727272727}\n{'loss': 0.5851, 'grad_norm': 0.9999863505363464, 'learning_rate': 0.0004425329295001143, 'epoch': 0.5454545454545454}\n{'loss': 0.5769, 'grad_norm': 1.7279515266418457, 'learning_rate': 0.0004402865694518904, 'epoch': 0.5681818181818182}\n{'loss': 0.5632, 'grad_norm': 1.2818154096603394, 'learning_rate': 0.00043804020940366645, 'epoch': 0.5909090909090909}\n{'loss': 0.6481, 'grad_norm': 1.133413314819336, 'learning_rate': 0.0004357938493554425, 'epoch': 0.6136363636363636}\n{'loss': 0.5764, 'grad_norm': 2.00905442237854, 'learning_rate': 0.00043354748930721855, 'epoch': 0.6363636363636364}\n{'loss': 0.6295, 'grad_norm': 1.1229965686798096, 'learning_rate': 0.00043130112925899466, 'epoch': 0.6590909090909091}\n{'loss': 0.5913, 'grad_norm': 0.8787376880645752, 'learning_rate': 0.0004290547692107707, 'epoch': 0.6818181818181818}\n{'loss': 0.5216, 'grad_norm': 0.7935212850570679, 'learning_rate': 0.00042680840916254676, 'epoch': 0.7045454545454546}\n{'loss': 0.5066, 'grad_norm': 0.844150722026825, 'learning_rate': 0.00042456204911432287, 'epoch': 0.7272727272727273}\n{'loss': 0.4498, 'grad_norm': 0.722350537776947, 'learning_rate': 0.0004223156890660989, 'epoch': 0.75}\n{'loss': 0.4887, 'grad_norm': 1.041193962097168, 'learning_rate': 0.000420069329017875, 'epoch': 0.7727272727272727}\n{'loss': 0.4645, 'grad_norm': 1.1711249351501465, 'learning_rate': 0.0004178229689696511, 'epoch': 0.7954545454545454}\n{'loss': 0.4356, 'grad_norm': 1.0120879411697388, 'learning_rate': 0.00041557660892142714, 'epoch': 0.8181818181818182}\n{'loss': 0.4543, 'grad_norm': 0.9041584730148315, 'learning_rate': 0.0004133302488732032, 'epoch': 0.8409090909090909}\n{'loss': 0.4252, 'grad_norm': 0.9176735281944275, 'learning_rate': 0.00041108388882497924, 'epoch': 0.8636363636363636}\n{'loss': 0.3735, 'grad_norm': 0.6239238977432251, 'learning_rate': 0.00040883752877675535, 'epoch': 0.8863636363636364}\n{'loss': 0.3732, 'grad_norm': 2.163945436477661, 'learning_rate': 0.0004065911687285314, 'epoch': 0.9090909090909091}\n{'loss': 0.3909, 'grad_norm': 1.2524402141571045, 'learning_rate': 0.00040434480868030745, 'epoch': 0.9318181818181818}\n{'loss': 0.3689, 'grad_norm': 1.2116239070892334, 'learning_rate': 0.00040209844863208356, 'epoch': 0.9545454545454546}\n{'loss': 0.3923, 'grad_norm': 0.7461003065109253, 'learning_rate': 0.0003998520885838596, 'epoch': 0.9772727272727273}\n{'loss': 0.3558, 'grad_norm': 3.7069919109344482, 'learning_rate': 0.00039760572853563567, 'epoch': 1.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.7516    0.8316    0.7896      3456\n     PERCENT     0.7115    0.9610    0.8177        77\n        TYPE     0.9206    0.9406    0.9305     11282\n      VOLUME     0.1887    0.2439    0.2128        41\n\n   micro avg     0.8750    0.9134    0.8938     14856\n   macro avg     0.6431    0.7443    0.6876     14856\nweighted avg     0.8782    0.9134    0.8952     14856\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.35668709874153137, 'eval_f1_macro': 0.6876289603666574, 'eval_precision': 0.8750322414237812, 'eval_recall': 0.9134356488960689, 'eval_f1': 0.8938216308786721, 'eval_accuracy': 0.8927872062663186, 'eval_runtime': 1.4803, 'eval_samples_per_second': 3722.175, 'eval_steps_per_second': 7.431, 'epoch': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.3315, 'grad_norm': 1.7707610130310059, 'learning_rate': 0.0003953593684874117, 'epoch': 1.0227272727272727}\n{'loss': 0.3654, 'grad_norm': 1.9451618194580078, 'learning_rate': 0.0003931130084391878, 'epoch': 1.0454545454545454}\n{'loss': 0.334, 'grad_norm': 1.1603245735168457, 'learning_rate': 0.0003908666483909639, 'epoch': 1.0681818181818181}\n{'loss': 0.3213, 'grad_norm': 1.5613001585006714, 'learning_rate': 0.00038862028834273993, 'epoch': 1.0909090909090908}\n{'loss': 0.3048, 'grad_norm': 1.9391433000564575, 'learning_rate': 0.00038637392829451604, 'epoch': 1.1136363636363635}\n{'loss': 0.3354, 'grad_norm': 1.9781732559204102, 'learning_rate': 0.0003841275682462921, 'epoch': 1.1363636363636362}\n{'loss': 0.3539, 'grad_norm': 2.451107978820801, 'learning_rate': 0.00038188120819806815, 'epoch': 1.1590909090909092}\n{'loss': 0.2343, 'grad_norm': 0.8510087132453918, 'learning_rate': 0.0003796348481498442, 'epoch': 1.1818181818181819}\n{'loss': 0.3182, 'grad_norm': 1.1158392429351807, 'learning_rate': 0.0003773884881016203, 'epoch': 1.2045454545454546}\n{'loss': 0.2572, 'grad_norm': 1.35805344581604, 'learning_rate': 0.00037514212805339636, 'epoch': 1.2272727272727273}\n{'loss': 0.3059, 'grad_norm': 0.7964277863502502, 'learning_rate': 0.00037289576800517247, 'epoch': 1.25}\n{'loss': 0.2569, 'grad_norm': 0.8173269033432007, 'learning_rate': 0.0003706494079569485, 'epoch': 1.2727272727272727}\n{'loss': 0.3109, 'grad_norm': 0.9598614573478699, 'learning_rate': 0.0003684030479087246, 'epoch': 1.2954545454545454}\n{'loss': 0.3393, 'grad_norm': 0.8381555676460266, 'learning_rate': 0.0003661566878605007, 'epoch': 1.3181818181818181}\n{'loss': 0.2834, 'grad_norm': 1.1707028150558472, 'learning_rate': 0.0003639103278122768, 'epoch': 1.3409090909090908}\n{'loss': 0.2827, 'grad_norm': 1.1642259359359741, 'learning_rate': 0.00036166396776405284, 'epoch': 1.3636363636363638}\n{'loss': 0.2851, 'grad_norm': 1.2748692035675049, 'learning_rate': 0.0003594176077158289, 'epoch': 1.3863636363636362}\n{'loss': 0.3303, 'grad_norm': 0.8311554789543152, 'learning_rate': 0.00035717124766760494, 'epoch': 1.4090909090909092}\n{'loss': 0.2767, 'grad_norm': 1.0648216009140015, 'learning_rate': 0.00035492488761938105, 'epoch': 1.4318181818181819}\n{'loss': 0.225, 'grad_norm': 0.9449158906936646, 'learning_rate': 0.0003526785275711571, 'epoch': 1.4545454545454546}\n{'loss': 0.2389, 'grad_norm': 0.9380794167518616, 'learning_rate': 0.00035043216752293316, 'epoch': 1.4772727272727273}\n{'loss': 0.2794, 'grad_norm': 1.2455295324325562, 'learning_rate': 0.00034818580747470926, 'epoch': 1.5}\n{'loss': 0.2772, 'grad_norm': 1.2880468368530273, 'learning_rate': 0.0003459394474264853, 'epoch': 1.5227272727272727}\n{'loss': 0.2205, 'grad_norm': 0.9159183502197266, 'learning_rate': 0.00034369308737826137, 'epoch': 1.5454545454545454}\n{'loss': 0.3094, 'grad_norm': 0.6577581167221069, 'learning_rate': 0.0003414467273300374, 'epoch': 1.5681818181818183}\n{'loss': 0.3267, 'grad_norm': 0.9108017086982727, 'learning_rate': 0.00033920036728181353, 'epoch': 1.5909090909090908}\n{'loss': 0.2756, 'grad_norm': 2.1489694118499756, 'learning_rate': 0.0003369540072335896, 'epoch': 1.6136363636363638}\n{'loss': 0.2893, 'grad_norm': 1.9103679656982422, 'learning_rate': 0.00033470764718536563, 'epoch': 1.6363636363636362}\n{'loss': 0.2327, 'grad_norm': 1.1246286630630493, 'learning_rate': 0.00033246128713714174, 'epoch': 1.6590909090909092}\n{'loss': 0.2441, 'grad_norm': 1.573106288909912, 'learning_rate': 0.0003302149270889178, 'epoch': 1.6818181818181817}\n{'loss': 0.322, 'grad_norm': 1.7979445457458496, 'learning_rate': 0.00032796856704069385, 'epoch': 1.7045454545454546}\n{'loss': 0.2952, 'grad_norm': 1.2944765090942383, 'learning_rate': 0.00032572220699246995, 'epoch': 1.7272727272727273}\n{'loss': 0.2993, 'grad_norm': 0.9916534423828125, 'learning_rate': 0.000323475846944246, 'epoch': 1.75}\n{'loss': 0.2375, 'grad_norm': 1.370248794555664, 'learning_rate': 0.00032122948689602206, 'epoch': 1.7727272727272727}\n{'loss': 0.2296, 'grad_norm': 1.3704875707626343, 'learning_rate': 0.0003189831268477981, 'epoch': 1.7954545454545454}\n{'loss': 0.2354, 'grad_norm': 1.2856990098953247, 'learning_rate': 0.0003167367667995742, 'epoch': 1.8181818181818183}\n{'loss': 0.2875, 'grad_norm': 1.6965131759643555, 'learning_rate': 0.00031449040675135027, 'epoch': 1.8409090909090908}\n{'loss': 0.2291, 'grad_norm': 0.7139750719070435, 'learning_rate': 0.0003122440467031263, 'epoch': 1.8636363636363638}\n{'loss': 0.261, 'grad_norm': 1.3831137418746948, 'learning_rate': 0.00030999768665490243, 'epoch': 1.8863636363636362}\n{'loss': 0.2331, 'grad_norm': 1.144313097000122, 'learning_rate': 0.0003077513266066785, 'epoch': 1.9090909090909092}\n{'loss': 0.2534, 'grad_norm': 0.8446553349494934, 'learning_rate': 0.00030550496655845454, 'epoch': 1.9318181818181817}\n{'loss': 0.1993, 'grad_norm': 1.1921937465667725, 'learning_rate': 0.0003032586065102306, 'epoch': 1.9545454545454546}\n{'loss': 0.2544, 'grad_norm': 1.0685874223709106, 'learning_rate': 0.0003010122464620067, 'epoch': 1.9772727272727273}\n{'loss': 0.2414, 'grad_norm': 4.749355792999268, 'learning_rate': 0.00029876588641378275, 'epoch': 2.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8673    0.8736    0.8704      3456\n     PERCENT     0.8675    0.9351    0.9000        77\n        TYPE     0.9369    0.9676    0.9520     11282\n      VOLUME     0.7500    0.8049    0.7765        41\n\n   micro avg     0.9201    0.9451    0.9325     14856\n   macro avg     0.8554    0.8953    0.8747     14856\nweighted avg     0.9198    0.9451    0.9323     14856\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.25494301319122314, 'eval_f1_macro': 0.8747279851225134, 'eval_precision': 0.9201179554390564, 'eval_recall': 0.9451400107700593, 'eval_f1': 0.9324611502191527, 'eval_accuracy': 0.9259138381201044, 'eval_runtime': 1.4905, 'eval_samples_per_second': 3696.868, 'eval_steps_per_second': 7.38, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.1814, 'grad_norm': 0.7539137005805969, 'learning_rate': 0.0002965195263655588, 'epoch': 2.022727272727273}\n{'loss': 0.201, 'grad_norm': 0.7295065522193909, 'learning_rate': 0.0002942731663173349, 'epoch': 2.0454545454545454}\n{'loss': 0.196, 'grad_norm': 1.210203766822815, 'learning_rate': 0.00029202680626911096, 'epoch': 2.0681818181818183}\n{'loss': 0.1792, 'grad_norm': 1.5429850816726685, 'learning_rate': 0.000289780446220887, 'epoch': 2.090909090909091}\n{'loss': 0.1589, 'grad_norm': 0.7774763703346252, 'learning_rate': 0.0002875340861726631, 'epoch': 2.1136363636363638}\n{'loss': 0.1742, 'grad_norm': 0.6193075776100159, 'learning_rate': 0.0002852877261244392, 'epoch': 2.1363636363636362}\n{'loss': 0.1508, 'grad_norm': 0.840966522693634, 'learning_rate': 0.00028304136607621523, 'epoch': 2.159090909090909}\n{'loss': 0.1362, 'grad_norm': 0.9354507327079773, 'learning_rate': 0.0002807950060279913, 'epoch': 2.1818181818181817}\n{'loss': 0.1738, 'grad_norm': 0.7502527832984924, 'learning_rate': 0.0002785486459797674, 'epoch': 2.2045454545454546}\n{'loss': 0.1614, 'grad_norm': 0.6245264410972595, 'learning_rate': 0.00027630228593154344, 'epoch': 2.227272727272727}\n{'loss': 0.2212, 'grad_norm': 1.005120038986206, 'learning_rate': 0.0002740559258833195, 'epoch': 2.25}\n{'loss': 0.1254, 'grad_norm': 0.7717239856719971, 'learning_rate': 0.0002718095658350956, 'epoch': 2.2727272727272725}\n{'loss': 0.1467, 'grad_norm': 0.8641291856765747, 'learning_rate': 0.00026956320578687165, 'epoch': 2.2954545454545454}\n{'loss': 0.1928, 'grad_norm': 0.9644705653190613, 'learning_rate': 0.0002673168457386477, 'epoch': 2.3181818181818183}\n{'loss': 0.1716, 'grad_norm': 0.8946455121040344, 'learning_rate': 0.00026507048569042376, 'epoch': 2.340909090909091}\n{'loss': 0.1795, 'grad_norm': 0.7798893451690674, 'learning_rate': 0.00026282412564219987, 'epoch': 2.3636363636363638}\n{'loss': 0.1489, 'grad_norm': 0.5898085832595825, 'learning_rate': 0.0002605777655939759, 'epoch': 2.3863636363636362}\n{'loss': 0.1391, 'grad_norm': 0.8702929019927979, 'learning_rate': 0.00025833140554575197, 'epoch': 2.409090909090909}\n{'loss': 0.1539, 'grad_norm': 1.0645805597305298, 'learning_rate': 0.0002560850454975281, 'epoch': 2.4318181818181817}\n{'loss': 0.1773, 'grad_norm': 0.8859149217605591, 'learning_rate': 0.00025383868544930413, 'epoch': 2.4545454545454546}\n{'loss': 0.2169, 'grad_norm': 0.826386034488678, 'learning_rate': 0.0002515923254010802, 'epoch': 2.4772727272727275}\n{'loss': 0.1301, 'grad_norm': 0.7119952440261841, 'learning_rate': 0.00024934596535285624, 'epoch': 2.5}\n{'loss': 0.1167, 'grad_norm': 0.865851104259491, 'learning_rate': 0.00024709960530463235, 'epoch': 2.5227272727272725}\n{'loss': 0.1661, 'grad_norm': 0.7505338788032532, 'learning_rate': 0.0002448532452564084, 'epoch': 2.5454545454545454}\n{'loss': 0.1362, 'grad_norm': 1.1196215152740479, 'learning_rate': 0.00024260688520818448, 'epoch': 2.5681818181818183}\n{'loss': 0.1786, 'grad_norm': 1.0109726190567017, 'learning_rate': 0.00024036052515996059, 'epoch': 2.590909090909091}\n{'loss': 0.1336, 'grad_norm': 0.6722946166992188, 'learning_rate': 0.00023811416511173664, 'epoch': 2.6136363636363638}\n{'loss': 0.2152, 'grad_norm': 1.021810531616211, 'learning_rate': 0.0002358678050635127, 'epoch': 2.6363636363636362}\n{'loss': 0.2016, 'grad_norm': 1.0426480770111084, 'learning_rate': 0.0002336214450152888, 'epoch': 2.659090909090909}\n{'loss': 0.1607, 'grad_norm': 0.9258399605751038, 'learning_rate': 0.00023137508496706485, 'epoch': 2.6818181818181817}\n{'loss': 0.1684, 'grad_norm': 1.2788273096084595, 'learning_rate': 0.0002291287249188409, 'epoch': 2.7045454545454546}\n{'loss': 0.1272, 'grad_norm': 1.3689976930618286, 'learning_rate': 0.00022688236487061696, 'epoch': 2.7272727272727275}\n{'loss': 0.147, 'grad_norm': 0.6095042824745178, 'learning_rate': 0.00022463600482239306, 'epoch': 2.75}\n{'loss': 0.2206, 'grad_norm': 1.1894187927246094, 'learning_rate': 0.00022238964477416912, 'epoch': 2.7727272727272725}\n{'loss': 0.1603, 'grad_norm': 1.1297589540481567, 'learning_rate': 0.0002201432847259452, 'epoch': 2.7954545454545454}\n{'loss': 0.1731, 'grad_norm': 1.3341875076293945, 'learning_rate': 0.00021789692467772125, 'epoch': 2.8181818181818183}\n{'loss': 0.1747, 'grad_norm': 0.8225605487823486, 'learning_rate': 0.00021565056462949733, 'epoch': 2.840909090909091}\n{'loss': 0.1845, 'grad_norm': 0.9301201701164246, 'learning_rate': 0.00021340420458127338, 'epoch': 2.8636363636363638}\n{'loss': 0.1537, 'grad_norm': 1.348670482635498, 'learning_rate': 0.00021115784453304946, 'epoch': 2.8863636363636362}\n{'loss': 0.1653, 'grad_norm': 1.3358957767486572, 'learning_rate': 0.00020891148448482554, 'epoch': 2.909090909090909}\n{'loss': 0.1959, 'grad_norm': 1.536284327507019, 'learning_rate': 0.0002066651244366016, 'epoch': 2.9318181818181817}\n{'loss': 0.1974, 'grad_norm': 1.2569743394851685, 'learning_rate': 0.00020441876438837767, 'epoch': 2.9545454545454546}\n{'loss': 0.1809, 'grad_norm': 0.858074426651001, 'learning_rate': 0.00020217240434015373, 'epoch': 2.9772727272727275}\n{'loss': 0.0725, 'grad_norm': 2.5674479007720947, 'learning_rate': 0.0001999260442919298, 'epoch': 3.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8647    0.8895    0.8769      3456\n     PERCENT     0.8506    0.9610    0.9024        77\n        TYPE     0.9447    0.9655    0.9550     11282\n      VOLUME     0.8537    0.8537    0.8537        41\n\n   micro avg     0.9252    0.9475    0.9362     14856\n   macro avg     0.8784    0.9174    0.8970     14856\nweighted avg     0.9253    0.9475    0.9363     14856\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.24400193989276886, 'eval_f1_macro': 0.8969967715097721, 'eval_precision': 0.9252004732483239, 'eval_recall': 0.9474959612277868, 'eval_f1': 0.9362154971732625, 'eval_accuracy': 0.9301566579634465, 'eval_runtime': 1.4968, 'eval_samples_per_second': 3681.16, 'eval_steps_per_second': 7.349, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.1255, 'grad_norm': 0.835783064365387, 'learning_rate': 0.00019767968424370586, 'epoch': 3.022727272727273}\n{'loss': 0.1221, 'grad_norm': 1.0228768587112427, 'learning_rate': 0.00019543332419548194, 'epoch': 3.0454545454545454}\n{'loss': 0.1133, 'grad_norm': 0.642482340335846, 'learning_rate': 0.00019318696414725802, 'epoch': 3.0681818181818183}\n{'loss': 0.1686, 'grad_norm': 1.142799973487854, 'learning_rate': 0.00019094060409903407, 'epoch': 3.090909090909091}\n{'loss': 0.105, 'grad_norm': 0.6059867143630981, 'learning_rate': 0.00018869424405081015, 'epoch': 3.1136363636363638}\n{'loss': 0.1236, 'grad_norm': 1.4073991775512695, 'learning_rate': 0.00018644788400258623, 'epoch': 3.1363636363636362}\n{'loss': 0.0976, 'grad_norm': 1.0295172929763794, 'learning_rate': 0.0001842015239543623, 'epoch': 3.159090909090909}\n{'loss': 0.1354, 'grad_norm': 0.8071901798248291, 'learning_rate': 0.0001819551639061384, 'epoch': 3.1818181818181817}\n{'loss': 0.0866, 'grad_norm': 0.7351928949356079, 'learning_rate': 0.00017970880385791445, 'epoch': 3.2045454545454546}\n{'loss': 0.0938, 'grad_norm': 1.4807608127593994, 'learning_rate': 0.00017746244380969053, 'epoch': 3.227272727272727}\n{'loss': 0.1031, 'grad_norm': 0.8807450532913208, 'learning_rate': 0.00017521608376146658, 'epoch': 3.25}\n{'loss': 0.1075, 'grad_norm': 1.1816442012786865, 'learning_rate': 0.00017296972371324266, 'epoch': 3.2727272727272725}\n{'loss': 0.1335, 'grad_norm': 0.864728569984436, 'learning_rate': 0.0001707233636650187, 'epoch': 3.2954545454545454}\n{'loss': 0.111, 'grad_norm': 0.6576362252235413, 'learning_rate': 0.0001684770036167948, 'epoch': 3.3181818181818183}\n{'loss': 0.0836, 'grad_norm': 0.6448509693145752, 'learning_rate': 0.00016623064356857087, 'epoch': 3.340909090909091}\n{'loss': 0.1437, 'grad_norm': 1.5377126932144165, 'learning_rate': 0.00016398428352034692, 'epoch': 3.3636363636363638}\n{'loss': 0.0891, 'grad_norm': 0.7432857155799866, 'learning_rate': 0.000161737923472123, 'epoch': 3.3863636363636362}\n{'loss': 0.1143, 'grad_norm': 1.1671732664108276, 'learning_rate': 0.00015949156342389906, 'epoch': 3.409090909090909}\n{'loss': 0.1048, 'grad_norm': 0.6278855204582214, 'learning_rate': 0.00015724520337567514, 'epoch': 3.4318181818181817}\n{'loss': 0.1275, 'grad_norm': 0.6930059194564819, 'learning_rate': 0.00015499884332745122, 'epoch': 3.4545454545454546}\n{'loss': 0.0983, 'grad_norm': 0.6239232420921326, 'learning_rate': 0.00015275248327922727, 'epoch': 3.4772727272727275}\n{'loss': 0.1077, 'grad_norm': 0.8385506868362427, 'learning_rate': 0.00015050612323100335, 'epoch': 3.5}\n{'loss': 0.1085, 'grad_norm': 0.9699475765228271, 'learning_rate': 0.0001482597631827794, 'epoch': 3.5227272727272725}\n{'loss': 0.1236, 'grad_norm': 0.8168294429779053, 'learning_rate': 0.00014601340313455548, 'epoch': 3.5454545454545454}\n{'loss': 0.1375, 'grad_norm': 0.9601005911827087, 'learning_rate': 0.00014376704308633156, 'epoch': 3.5681818181818183}\n{'loss': 0.1229, 'grad_norm': 1.124356985092163, 'learning_rate': 0.00014152068303810761, 'epoch': 3.590909090909091}\n{'loss': 0.1259, 'grad_norm': 0.7435693144798279, 'learning_rate': 0.0001392743229898837, 'epoch': 3.6136363636363638}\n{'loss': 0.0866, 'grad_norm': 1.1548967361450195, 'learning_rate': 0.00013702796294165975, 'epoch': 3.6363636363636362}\n{'loss': 0.1191, 'grad_norm': 0.8552321791648865, 'learning_rate': 0.00013478160289343583, 'epoch': 3.659090909090909}\n{'loss': 0.1202, 'grad_norm': 1.0444426536560059, 'learning_rate': 0.00013253524284521188, 'epoch': 3.6818181818181817}\n{'loss': 0.0667, 'grad_norm': 0.6947011947631836, 'learning_rate': 0.00013028888279698796, 'epoch': 3.7045454545454546}\n{'loss': 0.1215, 'grad_norm': 0.8642310500144958, 'learning_rate': 0.00012804252274876404, 'epoch': 3.7272727272727275}\n{'loss': 0.0853, 'grad_norm': 1.0277888774871826, 'learning_rate': 0.0001257961627005401, 'epoch': 3.75}\n{'loss': 0.1175, 'grad_norm': 1.495671272277832, 'learning_rate': 0.00012354980265231617, 'epoch': 3.7727272727272725}\n{'loss': 0.1431, 'grad_norm': 0.9970443248748779, 'learning_rate': 0.00012130344260409224, 'epoch': 3.7954545454545454}\n{'loss': 0.1378, 'grad_norm': 0.8680253028869629, 'learning_rate': 0.00011905708255586832, 'epoch': 3.8181818181818183}\n{'loss': 0.1238, 'grad_norm': 0.9240207076072693, 'learning_rate': 0.0001168107225076444, 'epoch': 3.840909090909091}\n{'loss': 0.0769, 'grad_norm': 0.7406793832778931, 'learning_rate': 0.00011456436245942045, 'epoch': 3.8636363636363638}\n{'loss': 0.1241, 'grad_norm': 0.9858764410018921, 'learning_rate': 0.00011231800241119653, 'epoch': 3.8863636363636362}\n{'loss': 0.154, 'grad_norm': 1.0100407600402832, 'learning_rate': 0.0001100716423629726, 'epoch': 3.909090909090909}\n{'loss': 0.1769, 'grad_norm': 0.7533231377601624, 'learning_rate': 0.00010782528231474866, 'epoch': 3.9318181818181817}\n{'loss': 0.1523, 'grad_norm': 1.2941070795059204, 'learning_rate': 0.00010557892226652473, 'epoch': 3.9545454545454546}\n{'loss': 0.1003, 'grad_norm': 0.8591474294662476, 'learning_rate': 0.0001033325622183008, 'epoch': 3.9772727272727275}\n{'loss': 0.1434, 'grad_norm': 6.113189697265625, 'learning_rate': 0.00010108620217007686, 'epoch': 4.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.9023    0.8872    0.8947      3456\n     PERCENT     0.8506    0.9610    0.9024        77\n        TYPE     0.9482    0.9658    0.9569     11282\n      VOLUME     0.8537    0.8537    0.8537        41\n\n   micro avg     0.9370    0.9472    0.9421     14856\n   macro avg     0.8887    0.9169    0.9019     14856\nweighted avg     0.9368    0.9472    0.9419     14856\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.24693220853805542, 'eval_f1_macro': 0.9019200712711136, 'eval_precision': 0.9370047279749617, 'eval_recall': 0.9471593968766828, 'eval_f1': 0.9420546982224751, 'eval_accuracy': 0.9338011314186249, 'eval_runtime': 1.503, 'eval_samples_per_second': 3666.039, 'eval_steps_per_second': 7.319, 'epoch': 4.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.1028, 'grad_norm': 0.7635178565979004, 'learning_rate': 9.883984212185293e-05, 'epoch': 4.0227272727272725}\n{'loss': 0.1062, 'grad_norm': 0.6853004693984985, 'learning_rate': 9.659348207362901e-05, 'epoch': 4.045454545454546}\n{'loss': 0.0993, 'grad_norm': 0.6345758438110352, 'learning_rate': 9.434712202540508e-05, 'epoch': 4.068181818181818}\n{'loss': 0.1157, 'grad_norm': 0.6037748456001282, 'learning_rate': 9.210076197718116e-05, 'epoch': 4.090909090909091}\n{'loss': 0.0854, 'grad_norm': 0.5767059922218323, 'learning_rate': 8.985440192895722e-05, 'epoch': 4.113636363636363}\n{'loss': 0.0863, 'grad_norm': 0.7147723436355591, 'learning_rate': 8.760804188073329e-05, 'epoch': 4.136363636363637}\n{'loss': 0.086, 'grad_norm': 0.7393363118171692, 'learning_rate': 8.536168183250936e-05, 'epoch': 4.159090909090909}\n{'loss': 0.0907, 'grad_norm': 0.8884299993515015, 'learning_rate': 8.311532178428544e-05, 'epoch': 4.181818181818182}\n{'loss': 0.1014, 'grad_norm': 0.6756694316864014, 'learning_rate': 8.08689617360615e-05, 'epoch': 4.204545454545454}\n{'loss': 0.1242, 'grad_norm': 0.6577962636947632, 'learning_rate': 7.862260168783757e-05, 'epoch': 4.2272727272727275}\n{'loss': 0.086, 'grad_norm': 0.5492188930511475, 'learning_rate': 7.637624163961363e-05, 'epoch': 4.25}\n{'loss': 0.0842, 'grad_norm': 0.7827395796775818, 'learning_rate': 7.41298815913897e-05, 'epoch': 4.2727272727272725}\n{'loss': 0.0858, 'grad_norm': 0.5692095160484314, 'learning_rate': 7.188352154316578e-05, 'epoch': 4.295454545454546}\n{'loss': 0.0859, 'grad_norm': 0.6408799290657043, 'learning_rate': 6.963716149494185e-05, 'epoch': 4.318181818181818}\n{'loss': 0.0712, 'grad_norm': 0.7032099962234497, 'learning_rate': 6.739080144671791e-05, 'epoch': 4.340909090909091}\n{'loss': 0.0867, 'grad_norm': 0.7369582056999207, 'learning_rate': 6.514444139849398e-05, 'epoch': 4.363636363636363}\n{'loss': 0.0852, 'grad_norm': 0.7788645625114441, 'learning_rate': 6.289808135027005e-05, 'epoch': 4.386363636363637}\n{'loss': 0.0835, 'grad_norm': 0.8190192580223083, 'learning_rate': 6.065172130204612e-05, 'epoch': 4.409090909090909}\n{'loss': 0.0811, 'grad_norm': 0.857892632484436, 'learning_rate': 5.84053612538222e-05, 'epoch': 4.431818181818182}\n{'loss': 0.0881, 'grad_norm': 0.5693013668060303, 'learning_rate': 5.6159001205598266e-05, 'epoch': 4.454545454545454}\n{'loss': 0.0672, 'grad_norm': 0.6835988759994507, 'learning_rate': 5.391264115737433e-05, 'epoch': 4.4772727272727275}\n{'loss': 0.0881, 'grad_norm': 0.6997755765914917, 'learning_rate': 5.16662811091504e-05, 'epoch': 4.5}\n{'loss': 0.1, 'grad_norm': 0.9918030500411987, 'learning_rate': 4.9419921060926465e-05, 'epoch': 4.5227272727272725}\n{'loss': 0.062, 'grad_norm': 0.4655049443244934, 'learning_rate': 4.717356101270254e-05, 'epoch': 4.545454545454545}\n{'loss': 0.0791, 'grad_norm': 0.6492592692375183, 'learning_rate': 4.492720096447861e-05, 'epoch': 4.568181818181818}\n{'loss': 0.0729, 'grad_norm': 0.802383542060852, 'learning_rate': 4.268084091625468e-05, 'epoch': 4.590909090909091}\n{'loss': 0.0856, 'grad_norm': 0.5868303179740906, 'learning_rate': 4.043448086803075e-05, 'epoch': 4.613636363636363}\n{'loss': 0.0705, 'grad_norm': 0.7911444306373596, 'learning_rate': 3.818812081980682e-05, 'epoch': 4.636363636363637}\n{'loss': 0.0753, 'grad_norm': 0.7370786070823669, 'learning_rate': 3.594176077158289e-05, 'epoch': 4.659090909090909}\n{'loss': 0.1008, 'grad_norm': 0.7483471035957336, 'learning_rate': 3.369540072335896e-05, 'epoch': 4.681818181818182}\n{'loss': 0.1097, 'grad_norm': 0.7950105667114258, 'learning_rate': 3.144904067513502e-05, 'epoch': 4.704545454545455}\n{'loss': 0.0826, 'grad_norm': 0.5262358784675598, 'learning_rate': 2.92026806269111e-05, 'epoch': 4.7272727272727275}\n{'loss': 0.0726, 'grad_norm': 0.7126099467277527, 'learning_rate': 2.6956320578687166e-05, 'epoch': 4.75}\n{'loss': 0.0809, 'grad_norm': 0.9295995831489563, 'learning_rate': 2.4709960530463233e-05, 'epoch': 4.7727272727272725}\n{'loss': 0.1296, 'grad_norm': 1.1714597940444946, 'learning_rate': 2.2463600482239306e-05, 'epoch': 4.795454545454545}\n{'loss': 0.0574, 'grad_norm': 0.5903576016426086, 'learning_rate': 2.0217240434015375e-05, 'epoch': 4.818181818181818}\n{'loss': 0.0824, 'grad_norm': 0.7201191782951355, 'learning_rate': 1.7970880385791445e-05, 'epoch': 4.840909090909091}\n{'loss': 0.0766, 'grad_norm': 0.6085730195045471, 'learning_rate': 1.572452033756751e-05, 'epoch': 4.863636363636363}\n{'loss': 0.1047, 'grad_norm': 0.6789780259132385, 'learning_rate': 1.3478160289343583e-05, 'epoch': 4.886363636363637}\n{'loss': 0.0907, 'grad_norm': 0.7371042370796204, 'learning_rate': 1.1231800241119653e-05, 'epoch': 4.909090909090909}\n{'loss': 0.0988, 'grad_norm': 0.8263288736343384, 'learning_rate': 8.985440192895723e-06, 'epoch': 4.931818181818182}\n{'loss': 0.103, 'grad_norm': 0.7977622151374817, 'learning_rate': 6.7390801446717915e-06, 'epoch': 4.954545454545455}\n{'loss': 0.0689, 'grad_norm': 0.5707715153694153, 'learning_rate': 4.492720096447861e-06, 'epoch': 4.9772727272727275}\n{'loss': 0.1641, 'grad_norm': 6.5439605712890625, 'learning_rate': 2.2463600482239307e-06, 'epoch': 5.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8933    0.8886    0.8909      3456\n     PERCENT     0.8795    0.9481    0.9125        77\n        TYPE     0.9486    0.9655    0.9570     11282\n      VOLUME     0.8444    0.9268    0.8837        41\n\n   micro avg     0.9353    0.9474    0.9413     14856\n   macro avg     0.8915    0.9323    0.9110     14856\nweighted avg     0.9351    0.9474    0.9412     14856\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.2623521387577057, 'eval_f1_macro': 0.9110339895389867, 'eval_precision': 0.935278091567546, 'eval_recall': 0.947428648357566, 'eval_f1': 0.941314161511453, 'eval_accuracy': 0.9331483899042646, 'eval_runtime': 1.4717, 'eval_samples_per_second': 3743.948, 'eval_steps_per_second': 7.474, 'epoch': 5.0}\n{'train_runtime': 31.0506, 'train_samples_per_second': 3549.364, 'train_steps_per_second': 7.085, 'train_loss': 0.3142745094712485, 'epoch': 5.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nThe speedups for torchdynamo mostly come with GPU Ampere or higher and which is not detected here.\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8933    0.8886    0.8909      3456\n     PERCENT     0.8795    0.9481    0.9125        77\n        TYPE     0.9486    0.9655    0.9570     11282\n      VOLUME     0.8444    0.9268    0.8837        41\n\n   micro avg     0.9353    0.9474    0.9413     14856\n   macro avg     0.8915    0.9323    0.9110     14856\nweighted avg     0.9351    0.9474    0.9412     14856\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.2623521387577057, 'eval_f1_macro': 0.9110339895389867, 'eval_precision': 0.935278091567546, 'eval_recall': 0.947428648357566, 'eval_f1': 0.941314161511453, 'eval_accuracy': 0.9331483899042646, 'eval_runtime': 1.9531, 'eval_samples_per_second': 2821.139, 'eval_steps_per_second': 5.632, 'epoch': 5.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/3364797558.py:66: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 2.1488, 'grad_norm': 7.094794750213623, 'learning_rate': 0.0, 'epoch': 0.022727272727272728}\n{'loss': 2.1595, 'grad_norm': 6.867481231689453, 'learning_rate': 2.0217240434015375e-05, 'epoch': 0.045454545454545456}\n{'loss': 2.111, 'grad_norm': 6.556026935577393, 'learning_rate': 4.043448086803075e-05, 'epoch': 0.06818181818181818}\n{'loss': 2.0007, 'grad_norm': 6.645452499389648, 'learning_rate': 6.065172130204612e-05, 'epoch': 0.09090909090909091}\n{'loss': 1.8766, 'grad_norm': 6.254143238067627, 'learning_rate': 8.08689617360615e-05, 'epoch': 0.11363636363636363}\n{'loss': 1.7273, 'grad_norm': 5.609859943389893, 'learning_rate': 0.00010108620217007686, 'epoch': 0.13636363636363635}\n{'loss': 1.607, 'grad_norm': 4.560237407684326, 'learning_rate': 0.00012130344260409224, 'epoch': 0.1590909090909091}\n{'loss': 1.4317, 'grad_norm': 3.6384212970733643, 'learning_rate': 0.00014152068303810761, 'epoch': 0.18181818181818182}\n{'loss': 1.2798, 'grad_norm': 2.5507655143737793, 'learning_rate': 0.000161737923472123, 'epoch': 0.20454545454545456}\n{'loss': 1.2668, 'grad_norm': 2.3027663230895996, 'learning_rate': 0.0001819551639061384, 'epoch': 0.22727272727272727}\n{'loss': 1.1407, 'grad_norm': 1.9272221326828003, 'learning_rate': 0.00020217240434015373, 'epoch': 0.25}\n{'loss': 1.1773, 'grad_norm': 2.347400426864624, 'learning_rate': 0.00022238964477416912, 'epoch': 0.2727272727272727}\n{'loss': 1.063, 'grad_norm': 1.2096832990646362, 'learning_rate': 0.00024260688520818448, 'epoch': 0.29545454545454547}\n{'loss': 1.0456, 'grad_norm': 1.3089489936828613, 'learning_rate': 0.00026282412564219987, 'epoch': 0.3181818181818182}\n{'loss': 1.0867, 'grad_norm': 1.967342734336853, 'learning_rate': 0.00028304136607621523, 'epoch': 0.3409090909090909}\n{'loss': 0.9315, 'grad_norm': 1.9453610181808472, 'learning_rate': 0.0003032586065102306, 'epoch': 0.36363636363636365}\n{'loss': 0.8866, 'grad_norm': 0.9726309776306152, 'learning_rate': 0.000323475846944246, 'epoch': 0.38636363636363635}\n{'loss': 0.8348, 'grad_norm': 1.4091206789016724, 'learning_rate': 0.00034369308737826137, 'epoch': 0.4090909090909091}\n{'loss': 0.8134, 'grad_norm': 0.9211626648902893, 'learning_rate': 0.0003639103278122768, 'epoch': 0.4318181818181818}\n{'loss': 0.7288, 'grad_norm': 1.1341243982315063, 'learning_rate': 0.0003841275682462921, 'epoch': 0.45454545454545453}\n{'loss': 0.7032, 'grad_norm': 0.6439857482910156, 'learning_rate': 0.00040434480868030745, 'epoch': 0.4772727272727273}\n{'loss': 0.6712, 'grad_norm': 0.7485561966896057, 'learning_rate': 0.00042456204911432287, 'epoch': 0.5}\n{'loss': 0.6254, 'grad_norm': 0.8065736293792725, 'learning_rate': 0.00044477928954833823, 'epoch': 0.5227272727272727}\n{'loss': 0.6382, 'grad_norm': 0.8219874501228333, 'learning_rate': 0.0004425329295001143, 'epoch': 0.5454545454545454}\n{'loss': 0.5423, 'grad_norm': 1.3059239387512207, 'learning_rate': 0.0004402865694518904, 'epoch': 0.5681818181818182}\n{'loss': 0.5678, 'grad_norm': 1.212558627128601, 'learning_rate': 0.00043804020940366645, 'epoch': 0.5909090909090909}\n{'loss': 0.5516, 'grad_norm': 1.0482947826385498, 'learning_rate': 0.0004357938493554425, 'epoch': 0.6136363636363636}\n{'loss': 0.5409, 'grad_norm': 1.0517348051071167, 'learning_rate': 0.00043354748930721855, 'epoch': 0.6363636363636364}\n{'loss': 0.5581, 'grad_norm': 2.238178014755249, 'learning_rate': 0.00043130112925899466, 'epoch': 0.6590909090909091}\n{'loss': 0.4943, 'grad_norm': 0.9463807344436646, 'learning_rate': 0.0004290547692107707, 'epoch': 0.6818181818181818}\n{'loss': 0.458, 'grad_norm': 1.0255448818206787, 'learning_rate': 0.00042680840916254676, 'epoch': 0.7045454545454546}\n{'loss': 0.4429, 'grad_norm': 1.5640275478363037, 'learning_rate': 0.00042456204911432287, 'epoch': 0.7272727272727273}\n{'loss': 0.4264, 'grad_norm': 1.1651030778884888, 'learning_rate': 0.0004223156890660989, 'epoch': 0.75}\n{'loss': 0.4148, 'grad_norm': 1.386358618736267, 'learning_rate': 0.000420069329017875, 'epoch': 0.7727272727272727}\n{'loss': 0.4463, 'grad_norm': 1.996259331703186, 'learning_rate': 0.0004178229689696511, 'epoch': 0.7954545454545454}\n{'loss': 0.3762, 'grad_norm': 1.0733524560928345, 'learning_rate': 0.00041557660892142714, 'epoch': 0.8181818181818182}\n{'loss': 0.4515, 'grad_norm': 1.0037578344345093, 'learning_rate': 0.0004133302488732032, 'epoch': 0.8409090909090909}\n{'loss': 0.4072, 'grad_norm': 2.3664557933807373, 'learning_rate': 0.00041108388882497924, 'epoch': 0.8636363636363636}\n{'loss': 0.4493, 'grad_norm': 1.8866010904312134, 'learning_rate': 0.00040883752877675535, 'epoch': 0.8863636363636364}\n{'loss': 0.4041, 'grad_norm': 1.1636104583740234, 'learning_rate': 0.0004065911687285314, 'epoch': 0.9090909090909091}\n{'loss': 0.3639, 'grad_norm': 1.2777024507522583, 'learning_rate': 0.00040434480868030745, 'epoch': 0.9318181818181818}\n{'loss': 0.3542, 'grad_norm': 0.8744118213653564, 'learning_rate': 0.00040209844863208356, 'epoch': 0.9545454545454546}\n{'loss': 0.4306, 'grad_norm': 1.2555110454559326, 'learning_rate': 0.0003998520885838596, 'epoch': 0.9772727272727273}\n{'loss': 0.4008, 'grad_norm': 4.563475131988525, 'learning_rate': 0.00039760572853563567, 'epoch': 1.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.7895    0.8395    0.8137      3221\n     PERCENT     0.5704    0.9310    0.7074        87\n        TYPE     0.9277    0.9528    0.9401     11501\n      VOLUME     0.0909    0.0847    0.0877        59\n\n   micro avg     0.8908    0.9247    0.9074     14868\n   macro avg     0.5946    0.7020    0.6372     14868\nweighted avg     0.8923    0.9247    0.9080     14868\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.3225070834159851, 'eval_f1_macro': 0.6372354427887665, 'eval_precision': 0.8907606582868991, 'eval_recall': 0.9246704331450094, 'eval_f1': 0.907398851560953, 'eval_accuracy': 0.9052631578947369, 'eval_runtime': 1.485, 'eval_samples_per_second': 3710.346, 'eval_steps_per_second': 7.407, 'epoch': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.317, 'grad_norm': 1.7015786170959473, 'learning_rate': 0.0003953593684874117, 'epoch': 1.0227272727272727}\n{'loss': 0.366, 'grad_norm': 0.687509298324585, 'learning_rate': 0.0003931130084391878, 'epoch': 1.0454545454545454}\n{'loss': 0.2841, 'grad_norm': 1.0690706968307495, 'learning_rate': 0.0003908666483909639, 'epoch': 1.0681818181818181}\n{'loss': 0.3473, 'grad_norm': 0.9416095018386841, 'learning_rate': 0.00038862028834273993, 'epoch': 1.0909090909090908}\n{'loss': 0.3549, 'grad_norm': 1.0078731775283813, 'learning_rate': 0.00038637392829451604, 'epoch': 1.1136363636363635}\n{'loss': 0.3106, 'grad_norm': 0.9957927465438843, 'learning_rate': 0.0003841275682462921, 'epoch': 1.1363636363636362}\n{'loss': 0.306, 'grad_norm': 1.1532615423202515, 'learning_rate': 0.00038188120819806815, 'epoch': 1.1590909090909092}\n{'loss': 0.3133, 'grad_norm': 0.7948090434074402, 'learning_rate': 0.0003796348481498442, 'epoch': 1.1818181818181819}\n{'loss': 0.2846, 'grad_norm': 0.793550968170166, 'learning_rate': 0.0003773884881016203, 'epoch': 1.2045454545454546}\n{'loss': 0.2922, 'grad_norm': 0.6741313934326172, 'learning_rate': 0.00037514212805339636, 'epoch': 1.2272727272727273}\n{'loss': 0.3103, 'grad_norm': 1.123923897743225, 'learning_rate': 0.00037289576800517247, 'epoch': 1.25}\n{'loss': 0.3269, 'grad_norm': 1.5425286293029785, 'learning_rate': 0.0003706494079569485, 'epoch': 1.2727272727272727}\n{'loss': 0.2656, 'grad_norm': 1.0189210176467896, 'learning_rate': 0.0003684030479087246, 'epoch': 1.2954545454545454}\n{'loss': 0.3339, 'grad_norm': 0.9281553626060486, 'learning_rate': 0.0003661566878605007, 'epoch': 1.3181818181818181}\n{'loss': 0.2526, 'grad_norm': 1.1978366374969482, 'learning_rate': 0.0003639103278122768, 'epoch': 1.3409090909090908}\n{'loss': 0.2857, 'grad_norm': 1.031690239906311, 'learning_rate': 0.00036166396776405284, 'epoch': 1.3636363636363638}\n{'loss': 0.2455, 'grad_norm': 1.0167241096496582, 'learning_rate': 0.0003594176077158289, 'epoch': 1.3863636363636362}\n{'loss': 0.2592, 'grad_norm': 1.0101902484893799, 'learning_rate': 0.00035717124766760494, 'epoch': 1.4090909090909092}\n{'loss': 0.2523, 'grad_norm': 1.3539084196090698, 'learning_rate': 0.00035492488761938105, 'epoch': 1.4318181818181819}\n{'loss': 0.2412, 'grad_norm': 1.272322416305542, 'learning_rate': 0.0003526785275711571, 'epoch': 1.4545454545454546}\n{'loss': 0.2217, 'grad_norm': 1.3039692640304565, 'learning_rate': 0.00035043216752293316, 'epoch': 1.4772727272727273}\n{'loss': 0.2461, 'grad_norm': 1.5014005899429321, 'learning_rate': 0.00034818580747470926, 'epoch': 1.5}\n{'loss': 0.2684, 'grad_norm': 1.8448033332824707, 'learning_rate': 0.0003459394474264853, 'epoch': 1.5227272727272727}\n{'loss': 0.3016, 'grad_norm': 1.1081024408340454, 'learning_rate': 0.00034369308737826137, 'epoch': 1.5454545454545454}\n{'loss': 0.2457, 'grad_norm': 2.1002888679504395, 'learning_rate': 0.0003414467273300374, 'epoch': 1.5681818181818183}\n{'loss': 0.2833, 'grad_norm': 1.163581371307373, 'learning_rate': 0.00033920036728181353, 'epoch': 1.5909090909090908}\n{'loss': 0.2819, 'grad_norm': 2.250436305999756, 'learning_rate': 0.0003369540072335896, 'epoch': 1.6136363636363638}\n{'loss': 0.2787, 'grad_norm': 1.0919419527053833, 'learning_rate': 0.00033470764718536563, 'epoch': 1.6363636363636362}\n{'loss': 0.2893, 'grad_norm': 1.6314340829849243, 'learning_rate': 0.00033246128713714174, 'epoch': 1.6590909090909092}\n{'loss': 0.2518, 'grad_norm': 1.04293692111969, 'learning_rate': 0.0003302149270889178, 'epoch': 1.6818181818181817}\n{'loss': 0.2567, 'grad_norm': 0.7565209865570068, 'learning_rate': 0.00032796856704069385, 'epoch': 1.7045454545454546}\n{'loss': 0.233, 'grad_norm': 0.7338718771934509, 'learning_rate': 0.00032572220699246995, 'epoch': 1.7272727272727273}\n{'loss': 0.2572, 'grad_norm': 1.5174875259399414, 'learning_rate': 0.000323475846944246, 'epoch': 1.75}\n{'loss': 0.2639, 'grad_norm': 0.9183081984519958, 'learning_rate': 0.00032122948689602206, 'epoch': 1.7727272727272727}\n{'loss': 0.2024, 'grad_norm': 0.9309149384498596, 'learning_rate': 0.0003189831268477981, 'epoch': 1.7954545454545454}\n{'loss': 0.2569, 'grad_norm': 0.7958230972290039, 'learning_rate': 0.0003167367667995742, 'epoch': 1.8181818181818183}\n{'loss': 0.2411, 'grad_norm': 0.7624235153198242, 'learning_rate': 0.00031449040675135027, 'epoch': 1.8409090909090908}\n{'loss': 0.2423, 'grad_norm': 0.9072467088699341, 'learning_rate': 0.0003122440467031263, 'epoch': 1.8636363636363638}\n{'loss': 0.2318, 'grad_norm': 0.6438409686088562, 'learning_rate': 0.00030999768665490243, 'epoch': 1.8863636363636362}\n{'loss': 0.2627, 'grad_norm': 0.6922308206558228, 'learning_rate': 0.0003077513266066785, 'epoch': 1.9090909090909092}\n{'loss': 0.2179, 'grad_norm': 1.2701269388198853, 'learning_rate': 0.00030550496655845454, 'epoch': 1.9318181818181817}\n{'loss': 0.28, 'grad_norm': 0.7230876088142395, 'learning_rate': 0.0003032586065102306, 'epoch': 1.9545454545454546}\n{'loss': 0.269, 'grad_norm': 0.8263677358627319, 'learning_rate': 0.0003010122464620067, 'epoch': 1.9772727272727273}\n{'loss': 0.1377, 'grad_norm': 2.5733182430267334, 'learning_rate': 0.00029876588641378275, 'epoch': 2.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8368    0.9010    0.8677      3221\n     PERCENT     0.8163    0.9195    0.8649        87\n        TYPE     0.9516    0.9606    0.9561     11501\n      VOLUME     0.7966    0.7966    0.7966        59\n\n   micro avg     0.9240    0.9468    0.9353     14868\n   macro avg     0.8503    0.8944    0.8713     14868\nweighted avg     0.9253    0.9468    0.9358     14868\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.234804168343544, 'eval_f1_macro': 0.8713124454165821, 'eval_precision': 0.9239908106334099, 'eval_recall': 0.946798493408663, 'eval_f1': 0.9352556223632197, 'eval_accuracy': 0.9300596852957135, 'eval_runtime': 1.4783, 'eval_samples_per_second': 3727.184, 'eval_steps_per_second': 7.441, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.2347, 'grad_norm': 1.1372236013412476, 'learning_rate': 0.0002965195263655588, 'epoch': 2.022727272727273}\n{'loss': 0.2172, 'grad_norm': 2.3546650409698486, 'learning_rate': 0.0002942731663173349, 'epoch': 2.0454545454545454}\n{'loss': 0.1854, 'grad_norm': 1.4137471914291382, 'learning_rate': 0.00029202680626911096, 'epoch': 2.0681818181818183}\n{'loss': 0.189, 'grad_norm': 0.9691827297210693, 'learning_rate': 0.000289780446220887, 'epoch': 2.090909090909091}\n{'loss': 0.1905, 'grad_norm': 1.0402902364730835, 'learning_rate': 0.0002875340861726631, 'epoch': 2.1136363636363638}\n{'loss': 0.1697, 'grad_norm': 1.211182951927185, 'learning_rate': 0.0002852877261244392, 'epoch': 2.1363636363636362}\n{'loss': 0.1663, 'grad_norm': 1.6057488918304443, 'learning_rate': 0.00028304136607621523, 'epoch': 2.159090909090909}\n{'loss': 0.1571, 'grad_norm': 1.208337426185608, 'learning_rate': 0.0002807950060279913, 'epoch': 2.1818181818181817}\n{'loss': 0.2238, 'grad_norm': 0.7630105018615723, 'learning_rate': 0.0002785486459797674, 'epoch': 2.2045454545454546}\n{'loss': 0.1195, 'grad_norm': 1.9067531824111938, 'learning_rate': 0.00027630228593154344, 'epoch': 2.227272727272727}\n{'loss': 0.1846, 'grad_norm': 1.5832871198654175, 'learning_rate': 0.0002740559258833195, 'epoch': 2.25}\n{'loss': 0.1574, 'grad_norm': 2.2858195304870605, 'learning_rate': 0.0002718095658350956, 'epoch': 2.2727272727272725}\n{'loss': 0.1485, 'grad_norm': 1.071919322013855, 'learning_rate': 0.00026956320578687165, 'epoch': 2.2954545454545454}\n{'loss': 0.1468, 'grad_norm': 0.7804126739501953, 'learning_rate': 0.0002673168457386477, 'epoch': 2.3181818181818183}\n{'loss': 0.1551, 'grad_norm': 1.3556843996047974, 'learning_rate': 0.00026507048569042376, 'epoch': 2.340909090909091}\n{'loss': 0.191, 'grad_norm': 2.866514205932617, 'learning_rate': 0.00026282412564219987, 'epoch': 2.3636363636363638}\n{'loss': 0.1979, 'grad_norm': 2.674285411834717, 'learning_rate': 0.0002605777655939759, 'epoch': 2.3863636363636362}\n{'loss': 0.1617, 'grad_norm': 1.2159456014633179, 'learning_rate': 0.00025833140554575197, 'epoch': 2.409090909090909}\n{'loss': 0.1517, 'grad_norm': 0.6595417261123657, 'learning_rate': 0.0002560850454975281, 'epoch': 2.4318181818181817}\n{'loss': 0.1874, 'grad_norm': 1.073535442352295, 'learning_rate': 0.00025383868544930413, 'epoch': 2.4545454545454546}\n{'loss': 0.1527, 'grad_norm': 2.469419479370117, 'learning_rate': 0.0002515923254010802, 'epoch': 2.4772727272727275}\n{'loss': 0.1849, 'grad_norm': 2.2498369216918945, 'learning_rate': 0.00024934596535285624, 'epoch': 2.5}\n{'loss': 0.2018, 'grad_norm': 2.1602416038513184, 'learning_rate': 0.00024709960530463235, 'epoch': 2.5227272727272725}\n{'loss': 0.136, 'grad_norm': 1.2236889600753784, 'learning_rate': 0.0002448532452564084, 'epoch': 2.5454545454545454}\n{'loss': 0.1438, 'grad_norm': 0.6196364760398865, 'learning_rate': 0.00024260688520818448, 'epoch': 2.5681818181818183}\n{'loss': 0.1706, 'grad_norm': 0.7845349311828613, 'learning_rate': 0.00024036052515996059, 'epoch': 2.590909090909091}\n{'loss': 0.1846, 'grad_norm': 1.8212257623672485, 'learning_rate': 0.00023811416511173664, 'epoch': 2.6136363636363638}\n{'loss': 0.2083, 'grad_norm': 2.3019816875457764, 'learning_rate': 0.0002358678050635127, 'epoch': 2.6363636363636362}\n{'loss': 0.1607, 'grad_norm': 1.178971767425537, 'learning_rate': 0.0002336214450152888, 'epoch': 2.659090909090909}\n{'loss': 0.2249, 'grad_norm': 1.8001991510391235, 'learning_rate': 0.00023137508496706485, 'epoch': 2.6818181818181817}\n{'loss': 0.1535, 'grad_norm': 0.9678982496261597, 'learning_rate': 0.0002291287249188409, 'epoch': 2.7045454545454546}\n{'loss': 0.1784, 'grad_norm': 0.9524877071380615, 'learning_rate': 0.00022688236487061696, 'epoch': 2.7272727272727275}\n{'loss': 0.2109, 'grad_norm': 1.466016411781311, 'learning_rate': 0.00022463600482239306, 'epoch': 2.75}\n{'loss': 0.1381, 'grad_norm': 0.8109409213066101, 'learning_rate': 0.00022238964477416912, 'epoch': 2.7727272727272725}\n{'loss': 0.198, 'grad_norm': 0.8838520646095276, 'learning_rate': 0.0002201432847259452, 'epoch': 2.7954545454545454}\n{'loss': 0.1654, 'grad_norm': 0.8593682050704956, 'learning_rate': 0.00021789692467772125, 'epoch': 2.8181818181818183}\n{'loss': 0.1803, 'grad_norm': 1.0285353660583496, 'learning_rate': 0.00021565056462949733, 'epoch': 2.840909090909091}\n{'loss': 0.1342, 'grad_norm': 0.8490967750549316, 'learning_rate': 0.00021340420458127338, 'epoch': 2.8636363636363638}\n{'loss': 0.1408, 'grad_norm': 1.1098010540008545, 'learning_rate': 0.00021115784453304946, 'epoch': 2.8863636363636362}\n{'loss': 0.1644, 'grad_norm': 1.1050227880477905, 'learning_rate': 0.00020891148448482554, 'epoch': 2.909090909090909}\n{'loss': 0.149, 'grad_norm': 1.153220295906067, 'learning_rate': 0.0002066651244366016, 'epoch': 2.9318181818181817}\n{'loss': 0.1697, 'grad_norm': 0.6774477362632751, 'learning_rate': 0.00020441876438837767, 'epoch': 2.9545454545454546}\n{'loss': 0.1511, 'grad_norm': 1.2873880863189697, 'learning_rate': 0.00020217240434015373, 'epoch': 2.9772727272727275}\n{'loss': 0.1017, 'grad_norm': 3.931971311569214, 'learning_rate': 0.0001999260442919298, 'epoch': 3.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8701    0.9003    0.8850      3221\n     PERCENT     0.7921    0.9195    0.8511        87\n        TYPE     0.9542    0.9615    0.9578     11501\n      VOLUME     0.8214    0.7797    0.8000        59\n\n   micro avg     0.9340    0.9473    0.9406     14868\n   macro avg     0.8594    0.8903    0.8735     14868\nweighted avg     0.9345    0.9473    0.9408     14868\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.21986693143844604, 'eval_f1_macro': 0.8734592047246785, 'eval_precision': 0.9340141919225413, 'eval_recall': 0.9472693032015066, 'eval_f1': 0.9405950512572211, 'eval_accuracy': 0.9351058057514922, 'eval_runtime': 1.6717, 'eval_samples_per_second': 3296.099, 'eval_steps_per_second': 6.58, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.128, 'grad_norm': 0.6086179614067078, 'learning_rate': 0.00019767968424370586, 'epoch': 3.022727272727273}\n{'loss': 0.1319, 'grad_norm': 0.5882104635238647, 'learning_rate': 0.00019543332419548194, 'epoch': 3.0454545454545454}\n{'loss': 0.1421, 'grad_norm': 0.7881052494049072, 'learning_rate': 0.00019318696414725802, 'epoch': 3.0681818181818183}\n{'loss': 0.1211, 'grad_norm': 0.6826174855232239, 'learning_rate': 0.00019094060409903407, 'epoch': 3.090909090909091}\n{'loss': 0.1146, 'grad_norm': 0.829780101776123, 'learning_rate': 0.00018869424405081015, 'epoch': 3.1136363636363638}\n{'loss': 0.0984, 'grad_norm': 0.6895084977149963, 'learning_rate': 0.00018644788400258623, 'epoch': 3.1363636363636362}\n{'loss': 0.1278, 'grad_norm': 1.2376121282577515, 'learning_rate': 0.0001842015239543623, 'epoch': 3.159090909090909}\n{'loss': 0.1531, 'grad_norm': 1.3574419021606445, 'learning_rate': 0.0001819551639061384, 'epoch': 3.1818181818181817}\n{'loss': 0.1252, 'grad_norm': 0.9068769812583923, 'learning_rate': 0.00017970880385791445, 'epoch': 3.2045454545454546}\n{'loss': 0.1354, 'grad_norm': 0.7523699998855591, 'learning_rate': 0.00017746244380969053, 'epoch': 3.227272727272727}\n{'loss': 0.088, 'grad_norm': 1.1102263927459717, 'learning_rate': 0.00017521608376146658, 'epoch': 3.25}\n{'loss': 0.1, 'grad_norm': 1.1410448551177979, 'learning_rate': 0.00017296972371324266, 'epoch': 3.2727272727272725}\n{'loss': 0.1286, 'grad_norm': 1.6448450088500977, 'learning_rate': 0.0001707233636650187, 'epoch': 3.2954545454545454}\n{'loss': 0.1082, 'grad_norm': 1.043933391571045, 'learning_rate': 0.0001684770036167948, 'epoch': 3.3181818181818183}\n{'loss': 0.0928, 'grad_norm': 0.591170072555542, 'learning_rate': 0.00016623064356857087, 'epoch': 3.340909090909091}\n{'loss': 0.1229, 'grad_norm': 1.1380928754806519, 'learning_rate': 0.00016398428352034692, 'epoch': 3.3636363636363638}\n{'loss': 0.1335, 'grad_norm': 1.1871780157089233, 'learning_rate': 0.000161737923472123, 'epoch': 3.3863636363636362}\n{'loss': 0.0963, 'grad_norm': 0.7794029116630554, 'learning_rate': 0.00015949156342389906, 'epoch': 3.409090909090909}\n{'loss': 0.1111, 'grad_norm': 0.8116347193717957, 'learning_rate': 0.00015724520337567514, 'epoch': 3.4318181818181817}\n{'loss': 0.1363, 'grad_norm': 1.089085340499878, 'learning_rate': 0.00015499884332745122, 'epoch': 3.4545454545454546}\n{'loss': 0.1224, 'grad_norm': 0.9404123425483704, 'learning_rate': 0.00015275248327922727, 'epoch': 3.4772727272727275}\n{'loss': 0.1083, 'grad_norm': 1.3063843250274658, 'learning_rate': 0.00015050612323100335, 'epoch': 3.5}\n{'loss': 0.128, 'grad_norm': 1.859339714050293, 'learning_rate': 0.0001482597631827794, 'epoch': 3.5227272727272725}\n{'loss': 0.1558, 'grad_norm': 1.0768911838531494, 'learning_rate': 0.00014601340313455548, 'epoch': 3.5454545454545454}\n{'loss': 0.0928, 'grad_norm': 0.7796067595481873, 'learning_rate': 0.00014376704308633156, 'epoch': 3.5681818181818183}\n{'loss': 0.1129, 'grad_norm': 0.6207191348075867, 'learning_rate': 0.00014152068303810761, 'epoch': 3.590909090909091}\n{'loss': 0.134, 'grad_norm': 0.6381623148918152, 'learning_rate': 0.0001392743229898837, 'epoch': 3.6136363636363638}\n{'loss': 0.1084, 'grad_norm': 0.8628906011581421, 'learning_rate': 0.00013702796294165975, 'epoch': 3.6363636363636362}\n{'loss': 0.1263, 'grad_norm': 1.0722365379333496, 'learning_rate': 0.00013478160289343583, 'epoch': 3.659090909090909}\n{'loss': 0.1135, 'grad_norm': 0.8278673887252808, 'learning_rate': 0.00013253524284521188, 'epoch': 3.6818181818181817}\n{'loss': 0.1046, 'grad_norm': 0.8140605688095093, 'learning_rate': 0.00013028888279698796, 'epoch': 3.7045454545454546}\n{'loss': 0.1201, 'grad_norm': 0.9097493290901184, 'learning_rate': 0.00012804252274876404, 'epoch': 3.7272727272727275}\n{'loss': 0.118, 'grad_norm': 0.8235632181167603, 'learning_rate': 0.0001257961627005401, 'epoch': 3.75}\n{'loss': 0.0838, 'grad_norm': 0.9334874153137207, 'learning_rate': 0.00012354980265231617, 'epoch': 3.7727272727272725}\n{'loss': 0.1051, 'grad_norm': 1.0815951824188232, 'learning_rate': 0.00012130344260409224, 'epoch': 3.7954545454545454}\n{'loss': 0.1396, 'grad_norm': 1.3341851234436035, 'learning_rate': 0.00011905708255586832, 'epoch': 3.8181818181818183}\n{'loss': 0.1073, 'grad_norm': 1.2251431941986084, 'learning_rate': 0.0001168107225076444, 'epoch': 3.840909090909091}\n{'loss': 0.1007, 'grad_norm': 0.937336266040802, 'learning_rate': 0.00011456436245942045, 'epoch': 3.8636363636363638}\n{'loss': 0.1279, 'grad_norm': 1.0713722705841064, 'learning_rate': 0.00011231800241119653, 'epoch': 3.8863636363636362}\n{'loss': 0.1125, 'grad_norm': 0.9871569275856018, 'learning_rate': 0.0001100716423629726, 'epoch': 3.909090909090909}\n{'loss': 0.1629, 'grad_norm': 1.0134375095367432, 'learning_rate': 0.00010782528231474866, 'epoch': 3.9318181818181817}\n{'loss': 0.116, 'grad_norm': 0.9684950709342957, 'learning_rate': 0.00010557892226652473, 'epoch': 3.9545454545454546}\n{'loss': 0.1285, 'grad_norm': 0.790116548538208, 'learning_rate': 0.0001033325622183008, 'epoch': 3.9772727272727275}\n{'loss': 0.0855, 'grad_norm': 2.242149591445923, 'learning_rate': 0.00010108620217007686, 'epoch': 4.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.9034    0.8830    0.8931      3221\n     PERCENT     0.8696    0.9195    0.8939        87\n        TYPE     0.9525    0.9677    0.9600     11501\n      VOLUME     0.8387    0.8814    0.8595        59\n\n   micro avg     0.9412    0.9487    0.9449     14868\n   macro avg     0.8911    0.9129    0.9016     14868\nweighted avg     0.9409    0.9487    0.9447     14868\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.21508091688156128, 'eval_f1_macro': 0.901612992362435, 'eval_precision': 0.9412117976778327, 'eval_recall': 0.9486817325800376, 'eval_f1': 0.9449320024117371, 'eval_accuracy': 0.9392837764514379, 'eval_runtime': 1.4716, 'eval_samples_per_second': 3744.264, 'eval_steps_per_second': 7.475, 'epoch': 4.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.0836, 'grad_norm': 0.6001855731010437, 'learning_rate': 9.883984212185293e-05, 'epoch': 4.0227272727272725}\n{'loss': 0.0991, 'grad_norm': 0.7722114324569702, 'learning_rate': 9.659348207362901e-05, 'epoch': 4.045454545454546}\n{'loss': 0.0659, 'grad_norm': 0.6486440300941467, 'learning_rate': 9.434712202540508e-05, 'epoch': 4.068181818181818}\n{'loss': 0.0994, 'grad_norm': 0.5008188486099243, 'learning_rate': 9.210076197718116e-05, 'epoch': 4.090909090909091}\n{'loss': 0.0968, 'grad_norm': 0.9350308179855347, 'learning_rate': 8.985440192895722e-05, 'epoch': 4.113636363636363}\n{'loss': 0.0927, 'grad_norm': 0.622024416923523, 'learning_rate': 8.760804188073329e-05, 'epoch': 4.136363636363637}\n{'loss': 0.0738, 'grad_norm': 0.5560495853424072, 'learning_rate': 8.536168183250936e-05, 'epoch': 4.159090909090909}\n{'loss': 0.0988, 'grad_norm': 0.7524245977401733, 'learning_rate': 8.311532178428544e-05, 'epoch': 4.181818181818182}\n{'loss': 0.1005, 'grad_norm': 0.7317755818367004, 'learning_rate': 8.08689617360615e-05, 'epoch': 4.204545454545454}\n{'loss': 0.0845, 'grad_norm': 0.6246488094329834, 'learning_rate': 7.862260168783757e-05, 'epoch': 4.2272727272727275}\n{'loss': 0.074, 'grad_norm': 0.6443082094192505, 'learning_rate': 7.637624163961363e-05, 'epoch': 4.25}\n{'loss': 0.0709, 'grad_norm': 0.5262709259986877, 'learning_rate': 7.41298815913897e-05, 'epoch': 4.2727272727272725}\n{'loss': 0.1011, 'grad_norm': 0.819042980670929, 'learning_rate': 7.188352154316578e-05, 'epoch': 4.295454545454546}\n{'loss': 0.0706, 'grad_norm': 0.594526469707489, 'learning_rate': 6.963716149494185e-05, 'epoch': 4.318181818181818}\n{'loss': 0.0939, 'grad_norm': 0.7997772097587585, 'learning_rate': 6.739080144671791e-05, 'epoch': 4.340909090909091}\n{'loss': 0.0765, 'grad_norm': 0.9075923562049866, 'learning_rate': 6.514444139849398e-05, 'epoch': 4.363636363636363}\n{'loss': 0.075, 'grad_norm': 0.5489298701286316, 'learning_rate': 6.289808135027005e-05, 'epoch': 4.386363636363637}\n{'loss': 0.081, 'grad_norm': 0.621694803237915, 'learning_rate': 6.065172130204612e-05, 'epoch': 4.409090909090909}\n{'loss': 0.1084, 'grad_norm': 0.8993936777114868, 'learning_rate': 5.84053612538222e-05, 'epoch': 4.431818181818182}\n{'loss': 0.0981, 'grad_norm': 1.5654176473617554, 'learning_rate': 5.6159001205598266e-05, 'epoch': 4.454545454545454}\n{'loss': 0.1037, 'grad_norm': 0.794005274772644, 'learning_rate': 5.391264115737433e-05, 'epoch': 4.4772727272727275}\n{'loss': 0.0727, 'grad_norm': 0.7116584181785583, 'learning_rate': 5.16662811091504e-05, 'epoch': 4.5}\n{'loss': 0.1055, 'grad_norm': 0.9343400001525879, 'learning_rate': 4.9419921060926465e-05, 'epoch': 4.5227272727272725}\n{'loss': 0.1123, 'grad_norm': 0.7407302260398865, 'learning_rate': 4.717356101270254e-05, 'epoch': 4.545454545454545}\n{'loss': 0.0991, 'grad_norm': 0.6936690807342529, 'learning_rate': 4.492720096447861e-05, 'epoch': 4.568181818181818}\n{'loss': 0.093, 'grad_norm': 0.6453016996383667, 'learning_rate': 4.268084091625468e-05, 'epoch': 4.590909090909091}\n{'loss': 0.0743, 'grad_norm': 0.8017131090164185, 'learning_rate': 4.043448086803075e-05, 'epoch': 4.613636363636363}\n{'loss': 0.1027, 'grad_norm': 1.0582693815231323, 'learning_rate': 3.818812081980682e-05, 'epoch': 4.636363636363637}\n{'loss': 0.0724, 'grad_norm': 0.7584515810012817, 'learning_rate': 3.594176077158289e-05, 'epoch': 4.659090909090909}\n{'loss': 0.1083, 'grad_norm': 0.6626902222633362, 'learning_rate': 3.369540072335896e-05, 'epoch': 4.681818181818182}\n{'loss': 0.1015, 'grad_norm': 0.752653956413269, 'learning_rate': 3.144904067513502e-05, 'epoch': 4.704545454545455}\n{'loss': 0.0805, 'grad_norm': 0.9770737886428833, 'learning_rate': 2.92026806269111e-05, 'epoch': 4.7272727272727275}\n{'loss': 0.086, 'grad_norm': 0.5909948945045471, 'learning_rate': 2.6956320578687166e-05, 'epoch': 4.75}\n{'loss': 0.0724, 'grad_norm': 0.6603612899780273, 'learning_rate': 2.4709960530463233e-05, 'epoch': 4.7727272727272725}\n{'loss': 0.1033, 'grad_norm': 0.8941614031791687, 'learning_rate': 2.2463600482239306e-05, 'epoch': 4.795454545454545}\n{'loss': 0.0799, 'grad_norm': 0.7649222016334534, 'learning_rate': 2.0217240434015375e-05, 'epoch': 4.818181818181818}\n{'loss': 0.0836, 'grad_norm': 0.6580600738525391, 'learning_rate': 1.7970880385791445e-05, 'epoch': 4.840909090909091}\n{'loss': 0.0628, 'grad_norm': 0.7904033660888672, 'learning_rate': 1.572452033756751e-05, 'epoch': 4.863636363636363}\n{'loss': 0.0848, 'grad_norm': 0.5615953207015991, 'learning_rate': 1.3478160289343583e-05, 'epoch': 4.886363636363637}\n{'loss': 0.0759, 'grad_norm': 0.64492267370224, 'learning_rate': 1.1231800241119653e-05, 'epoch': 4.909090909090909}\n{'loss': 0.1058, 'grad_norm': 0.6824681758880615, 'learning_rate': 8.985440192895723e-06, 'epoch': 4.931818181818182}\n{'loss': 0.0935, 'grad_norm': 0.842284619808197, 'learning_rate': 6.7390801446717915e-06, 'epoch': 4.954545454545455}\n{'loss': 0.1076, 'grad_norm': 0.7631726264953613, 'learning_rate': 4.492720096447861e-06, 'epoch': 4.9772727272727275}\n{'loss': 0.024, 'grad_norm': 1.4696964025497437, 'learning_rate': 2.2463600482239307e-06, 'epoch': 5.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8937    0.8975    0.8956      3221\n     PERCENT     0.8247    0.9195    0.8696        87\n        TYPE     0.9540    0.9658    0.9599     11501\n      VOLUME     0.8596    0.8305    0.8448        59\n\n   micro avg     0.9399    0.9502    0.9450     14868\n   macro avg     0.8830    0.9034    0.8925     14868\nweighted avg     0.9398    0.9502    0.9450     14868\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.2228260338306427, 'eval_f1_macro': 0.8924742523950413, 'eval_precision': 0.9398616285258116, 'eval_recall': 0.9502286790422384, 'eval_f1': 0.9450167224080268, 'eval_accuracy': 0.9393922951709169, 'eval_runtime': 1.4666, 'eval_samples_per_second': 3756.957, 'eval_steps_per_second': 7.5, 'epoch': 5.0}\n{'train_runtime': 31.0966, 'train_samples_per_second': 3544.12, 'train_steps_per_second': 7.075, 'train_loss': 0.307301503165879, 'epoch': 5.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n[I 2025-09-26 18:06:03,055] Trial 3 finished with value: 0.9029635401104601 and parameters: {'learning_rate': 0.00044477928954833823, 'weight_decay': 0.01042507684654507, 'num_train_epochs': 5}. Best is trial 0 with value: 0.9174994186455976.\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.9034    0.8830    0.8931      3221\n     PERCENT     0.8696    0.9195    0.8939        87\n        TYPE     0.9525    0.9677    0.9600     11501\n      VOLUME     0.8387    0.8814    0.8595        59\n\n   micro avg     0.9412    0.9487    0.9449     14868\n   macro avg     0.8911    0.9129    0.9016     14868\nweighted avg     0.9409    0.9487    0.9447     14868\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.21508091688156128, 'eval_f1_macro': 0.901612992362435, 'eval_precision': 0.9412117976778327, 'eval_recall': 0.9486817325800376, 'eval_f1': 0.9449320024117371, 'eval_accuracy': 0.9392837764514379, 'eval_runtime': 1.9296, 'eval_samples_per_second': 2855.525, 'eval_steps_per_second': 5.701, 'epoch': 5.0}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/27552 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5479ace386a045cdbab1d364af179ac3"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nThe speedups for torchdynamo mostly come with GPU Ampere or higher and which is not detected here.\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/tmp/ipykernel_36/3364797558.py:66: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 2.2533, 'grad_norm': 7.151388645172119, 'learning_rate': 0.0, 'epoch': 0.022727272727272728}\n{'loss': 2.2519, 'grad_norm': 7.316790580749512, 'learning_rate': 5.539636162535749e-07, 'epoch': 0.045454545454545456}\n{'loss': 2.2627, 'grad_norm': 7.230693340301514, 'learning_rate': 1.1079272325071499e-06, 'epoch': 0.06818181818181818}\n{'loss': 2.2388, 'grad_norm': 7.5041117668151855, 'learning_rate': 1.6618908487607247e-06, 'epoch': 0.09090909090909091}\n{'loss': 2.242, 'grad_norm': 7.156303882598877, 'learning_rate': 2.2158544650142997e-06, 'epoch': 0.11363636363636363}\n{'loss': 2.245, 'grad_norm': 7.292089939117432, 'learning_rate': 2.7698180812678746e-06, 'epoch': 0.13636363636363635}\n{'loss': 2.2392, 'grad_norm': 7.045125484466553, 'learning_rate': 3.3237816975214494e-06, 'epoch': 0.1590909090909091}\n{'loss': 2.2349, 'grad_norm': 7.408904552459717, 'learning_rate': 3.877745313775024e-06, 'epoch': 0.18181818181818182}\n{'loss': 2.2186, 'grad_norm': 7.095512866973877, 'learning_rate': 4.4317089300285995e-06, 'epoch': 0.20454545454545456}\n{'loss': 2.212, 'grad_norm': 7.362390041351318, 'learning_rate': 4.985672546282174e-06, 'epoch': 0.22727272727272727}\n{'loss': 2.1963, 'grad_norm': 7.534672737121582, 'learning_rate': 5.539636162535749e-06, 'epoch': 0.25}\n{'loss': 2.1837, 'grad_norm': 7.488317012786865, 'learning_rate': 6.093599778789325e-06, 'epoch': 0.2727272727272727}\n{'loss': 2.1639, 'grad_norm': 7.344156265258789, 'learning_rate': 6.647563395042899e-06, 'epoch': 0.29545454545454547}\n{'loss': 2.1576, 'grad_norm': 7.144752502441406, 'learning_rate': 7.2015270112964745e-06, 'epoch': 0.3181818181818182}\n{'loss': 2.132, 'grad_norm': 7.109100818634033, 'learning_rate': 7.755490627550048e-06, 'epoch': 0.3409090909090909}\n{'loss': 2.1184, 'grad_norm': 6.924496173858643, 'learning_rate': 8.309454243803624e-06, 'epoch': 0.36363636363636365}\n{'loss': 2.1149, 'grad_norm': 6.961133003234863, 'learning_rate': 8.863417860057199e-06, 'epoch': 0.38636363636363635}\n{'loss': 2.0914, 'grad_norm': 7.05643367767334, 'learning_rate': 9.417381476310774e-06, 'epoch': 0.4090909090909091}\n{'loss': 2.0601, 'grad_norm': 6.833683967590332, 'learning_rate': 9.971345092564349e-06, 'epoch': 0.4318181818181818}\n{'loss': 2.0474, 'grad_norm': 6.997602462768555, 'learning_rate': 1.0525308708817923e-05, 'epoch': 0.45454545454545453}\n{'loss': 2.0212, 'grad_norm': 6.765805244445801, 'learning_rate': 1.1079272325071498e-05, 'epoch': 0.4772727272727273}\n{'loss': 1.9967, 'grad_norm': 6.881976127624512, 'learning_rate': 1.1633235941325073e-05, 'epoch': 0.5}\n{'loss': 1.9822, 'grad_norm': 6.494560718536377, 'learning_rate': 1.218719955757865e-05, 'epoch': 0.5227272727272727}\n{'loss': 1.9409, 'grad_norm': 6.786870002746582, 'learning_rate': 1.2741163173832223e-05, 'epoch': 0.5454545454545454}\n{'loss': 1.9224, 'grad_norm': 6.3580121994018555, 'learning_rate': 1.3295126790085798e-05, 'epoch': 0.5681818181818182}\n{'loss': 1.8925, 'grad_norm': 6.3269572257995605, 'learning_rate': 1.3849090406339372e-05, 'epoch': 0.5909090909090909}\n{'loss': 1.8613, 'grad_norm': 6.40846061706543, 'learning_rate': 1.4403054022592949e-05, 'epoch': 0.6136363636363636}\n{'loss': 1.8361, 'grad_norm': 6.3452911376953125, 'learning_rate': 1.4957017638846524e-05, 'epoch': 0.6363636363636364}\n{'loss': 1.8013, 'grad_norm': 6.271238803863525, 'learning_rate': 1.5510981255100095e-05, 'epoch': 0.6590909090909091}\n{'loss': 1.7519, 'grad_norm': 6.345937728881836, 'learning_rate': 1.6064944871353672e-05, 'epoch': 0.6818181818181818}\n{'loss': 1.7316, 'grad_norm': 6.206079959869385, 'learning_rate': 1.6618908487607248e-05, 'epoch': 0.7045454545454546}\n{'loss': 1.6997, 'grad_norm': 6.057835102081299, 'learning_rate': 1.717287210386082e-05, 'epoch': 0.7272727272727273}\n{'loss': 1.6986, 'grad_norm': 5.4236555099487305, 'learning_rate': 1.7726835720114398e-05, 'epoch': 0.75}\n{'loss': 1.6722, 'grad_norm': 5.228732109069824, 'learning_rate': 1.828079933636797e-05, 'epoch': 0.7727272727272727}\n{'loss': 1.5882, 'grad_norm': 5.611064910888672, 'learning_rate': 1.8834762952621548e-05, 'epoch': 0.7954545454545454}\n{'loss': 1.5708, 'grad_norm': 5.237521648406982, 'learning_rate': 1.938872656887512e-05, 'epoch': 0.8181818181818182}\n{'loss': 1.543, 'grad_norm': 5.027527809143066, 'learning_rate': 1.9942690185128697e-05, 'epoch': 0.8409090909090909}\n{'loss': 1.5297, 'grad_norm': 4.714285850524902, 'learning_rate': 2.0496653801382274e-05, 'epoch': 0.8636363636363636}\n{'loss': 1.4748, 'grad_norm': 4.753297805786133, 'learning_rate': 2.1050617417635847e-05, 'epoch': 0.8863636363636364}\n{'loss': 1.4577, 'grad_norm': 4.109132289886475, 'learning_rate': 2.160458103388942e-05, 'epoch': 0.9090909090909091}\n{'loss': 1.4498, 'grad_norm': 3.9138288497924805, 'learning_rate': 2.2158544650142997e-05, 'epoch': 0.9318181818181818}\n{'loss': 1.376, 'grad_norm': 3.9322102069854736, 'learning_rate': 2.2096301547193157e-05, 'epoch': 0.9545454545454546}\n{'loss': 1.3907, 'grad_norm': 3.3515021800994873, 'learning_rate': 2.2034058444243317e-05, 'epoch': 0.9772727272727273}\n{'loss': 1.3263, 'grad_norm': 3.8181138038635254, 'learning_rate': 2.1971815341293477e-05, 'epoch': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.0000    0.0000    0.0000      3142\n     PERCENT     0.0000    0.0000    0.0000        66\n        TYPE     0.5735    0.9233    0.7075     11415\n      VOLUME     0.0000    0.0000    0.0000        70\n\n   micro avg     0.5735    0.7173    0.6374     14693\n   macro avg     0.1434    0.2308    0.1769     14693\nweighted avg     0.4456    0.7173    0.5497     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 1.2816380262374878, 'eval_f1_macro': 0.1768871882657, 'eval_precision': 0.5735118075960387, 'eval_recall': 0.7173483971959437, 'eval_f1': 0.6374164676000121, 'eval_accuracy': 0.6209598432908913, 'eval_runtime': 1.4732, 'eval_samples_per_second': 3740.782, 'eval_steps_per_second': 7.467, 'epoch': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 1.2803, 'grad_norm': 3.190800905227661, 'learning_rate': 2.1909572238343638e-05, 'epoch': 1.0227272727272727}\n{'loss': 1.2855, 'grad_norm': 2.731318235397339, 'learning_rate': 2.1847329135393798e-05, 'epoch': 1.0454545454545454}\n{'loss': 1.2898, 'grad_norm': 2.495297431945801, 'learning_rate': 2.1785086032443958e-05, 'epoch': 1.0681818181818181}\n{'loss': 1.2626, 'grad_norm': 2.315549850463867, 'learning_rate': 2.172284292949412e-05, 'epoch': 1.0909090909090908}\n{'loss': 1.2342, 'grad_norm': 2.1387062072753906, 'learning_rate': 2.1660599826544275e-05, 'epoch': 1.1136363636363635}\n{'loss': 1.2538, 'grad_norm': 1.9394605159759521, 'learning_rate': 2.159835672359444e-05, 'epoch': 1.1363636363636362}\n{'loss': 1.2196, 'grad_norm': 1.8726552724838257, 'learning_rate': 2.15361136206446e-05, 'epoch': 1.1590909090909092}\n{'loss': 1.2306, 'grad_norm': 1.839132308959961, 'learning_rate': 2.1473870517694756e-05, 'epoch': 1.1818181818181819}\n{'loss': 1.1963, 'grad_norm': 1.844221591949463, 'learning_rate': 2.141162741474492e-05, 'epoch': 1.2045454545454546}\n{'loss': 1.1468, 'grad_norm': 1.7083193063735962, 'learning_rate': 2.1349384311795076e-05, 'epoch': 1.2272727272727273}\n{'loss': 1.1734, 'grad_norm': 1.719446063041687, 'learning_rate': 2.128714120884524e-05, 'epoch': 1.25}\n{'loss': 1.1004, 'grad_norm': 1.4938238859176636, 'learning_rate': 2.1224898105895397e-05, 'epoch': 1.2727272727272727}\n{'loss': 1.1239, 'grad_norm': 1.5326521396636963, 'learning_rate': 2.1162655002945557e-05, 'epoch': 1.2954545454545454}\n{'loss': 1.0467, 'grad_norm': 1.364991545677185, 'learning_rate': 2.110041189999572e-05, 'epoch': 1.3181818181818181}\n{'loss': 1.0971, 'grad_norm': 1.3689320087432861, 'learning_rate': 2.1038168797045878e-05, 'epoch': 1.3409090909090908}\n{'loss': 1.1263, 'grad_norm': 1.3375918865203857, 'learning_rate': 2.097592569409604e-05, 'epoch': 1.3636363636363638}\n{'loss': 1.1146, 'grad_norm': 1.2624459266662598, 'learning_rate': 2.0913682591146198e-05, 'epoch': 1.3863636363636362}\n{'loss': 1.0116, 'grad_norm': 1.30275297164917, 'learning_rate': 2.085143948819636e-05, 'epoch': 1.4090909090909092}\n{'loss': 1.0572, 'grad_norm': 1.1427850723266602, 'learning_rate': 2.078919638524652e-05, 'epoch': 1.4318181818181819}\n{'loss': 1.1215, 'grad_norm': 1.346854567527771, 'learning_rate': 2.072695328229668e-05, 'epoch': 1.4545454545454546}\n{'loss': 1.0446, 'grad_norm': 1.195874810218811, 'learning_rate': 2.066471017934684e-05, 'epoch': 1.4772727272727273}\n{'loss': 1.0439, 'grad_norm': 1.1321660280227661, 'learning_rate': 2.0602467076397e-05, 'epoch': 1.5}\n{'loss': 1.0669, 'grad_norm': 1.1450380086898804, 'learning_rate': 2.054022397344716e-05, 'epoch': 1.5227272727272727}\n{'loss': 0.9858, 'grad_norm': 1.185473084449768, 'learning_rate': 2.047798087049732e-05, 'epoch': 1.5454545454545454}\n{'loss': 1.0013, 'grad_norm': 1.211102843284607, 'learning_rate': 2.041573776754748e-05, 'epoch': 1.5681818181818183}\n{'loss': 0.9993, 'grad_norm': 1.1609323024749756, 'learning_rate': 2.035349466459764e-05, 'epoch': 1.5909090909090908}\n{'loss': 1.0428, 'grad_norm': 1.1413182020187378, 'learning_rate': 2.02912515616478e-05, 'epoch': 1.6136363636363638}\n{'loss': 0.9896, 'grad_norm': 1.1117212772369385, 'learning_rate': 2.022900845869796e-05, 'epoch': 1.6363636363636362}\n{'loss': 0.9729, 'grad_norm': 1.0651296377182007, 'learning_rate': 2.016676535574812e-05, 'epoch': 1.6590909090909092}\n{'loss': 1.0119, 'grad_norm': 1.0898585319519043, 'learning_rate': 2.010452225279828e-05, 'epoch': 1.6818181818181817}\n{'loss': 0.9688, 'grad_norm': 1.1570043563842773, 'learning_rate': 2.004227914984844e-05, 'epoch': 1.7045454545454546}\n{'loss': 0.9494, 'grad_norm': 1.0358718633651733, 'learning_rate': 1.9980036046898602e-05, 'epoch': 1.7272727272727273}\n{'loss': 0.9819, 'grad_norm': 1.081168532371521, 'learning_rate': 1.9917792943948762e-05, 'epoch': 1.75}\n{'loss': 0.9434, 'grad_norm': 1.049304723739624, 'learning_rate': 1.9855549840998922e-05, 'epoch': 1.7727272727272727}\n{'loss': 0.8938, 'grad_norm': 0.9967783689498901, 'learning_rate': 1.979330673804908e-05, 'epoch': 1.7954545454545454}\n{'loss': 0.9005, 'grad_norm': 1.0624659061431885, 'learning_rate': 1.9731063635099243e-05, 'epoch': 1.8181818181818183}\n{'loss': 0.9364, 'grad_norm': 1.0211514234542847, 'learning_rate': 1.96688205321494e-05, 'epoch': 1.8409090909090908}\n{'loss': 0.9114, 'grad_norm': 0.992250919342041, 'learning_rate': 1.960657742919956e-05, 'epoch': 1.8636363636363638}\n{'loss': 0.8776, 'grad_norm': 1.0476161241531372, 'learning_rate': 1.9544334326249724e-05, 'epoch': 1.8863636363636362}\n{'loss': 0.9545, 'grad_norm': 0.9899595379829407, 'learning_rate': 1.948209122329988e-05, 'epoch': 1.9090909090909092}\n{'loss': 0.8671, 'grad_norm': 0.9984157085418701, 'learning_rate': 1.9419848120350044e-05, 'epoch': 1.9318181818181817}\n{'loss': 0.8755, 'grad_norm': 0.934513509273529, 'learning_rate': 1.93576050174002e-05, 'epoch': 1.9545454545454546}\n{'loss': 0.8577, 'grad_norm': 1.0351027250289917, 'learning_rate': 1.929536191445036e-05, 'epoch': 1.9772727272727273}\n{'loss': 0.8365, 'grad_norm': 2.320566177368164, 'learning_rate': 1.923311881150052e-05, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.6601    0.4258    0.5177      3142\n     PERCENT     0.0000    0.0000    0.0000        66\n        TYPE     0.6613    0.8985    0.7618     11415\n      VOLUME     0.0000    0.0000    0.0000        70\n\n   micro avg     0.6612    0.7891    0.7195     14693\n   macro avg     0.3303    0.3311    0.3199     14693\nweighted avg     0.6549    0.7891    0.7026     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.8113028407096863, 'eval_f1_macro': 0.3198874620791544, 'eval_precision': 0.661154197080292, 'eval_recall': 0.789083236915538, 'eval_f1': 0.7194762480995378, 'eval_accuracy': 0.7153661987158559, 'eval_runtime': 1.4495, 'eval_samples_per_second': 3801.923, 'eval_steps_per_second': 7.589, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.8427, 'grad_norm': 1.0211067199707031, 'learning_rate': 1.917087570855068e-05, 'epoch': 2.022727272727273}\n{'loss': 0.8194, 'grad_norm': 1.2515778541564941, 'learning_rate': 1.9108632605600845e-05, 'epoch': 2.0454545454545454}\n{'loss': 0.8622, 'grad_norm': 0.9478796720504761, 'learning_rate': 1.9046389502651002e-05, 'epoch': 2.0681818181818183}\n{'loss': 0.8008, 'grad_norm': 1.0194411277770996, 'learning_rate': 1.8984146399701162e-05, 'epoch': 2.090909090909091}\n{'loss': 0.8291, 'grad_norm': 0.988548994064331, 'learning_rate': 1.8921903296751323e-05, 'epoch': 2.1136363636363638}\n{'loss': 0.7751, 'grad_norm': 1.0126802921295166, 'learning_rate': 1.8859660193801483e-05, 'epoch': 2.1363636363636362}\n{'loss': 0.786, 'grad_norm': 0.9710772037506104, 'learning_rate': 1.8797417090851643e-05, 'epoch': 2.159090909090909}\n{'loss': 0.7834, 'grad_norm': 0.9649794697761536, 'learning_rate': 1.8735173987901803e-05, 'epoch': 2.1818181818181817}\n{'loss': 0.8208, 'grad_norm': 0.9330095648765564, 'learning_rate': 1.8672930884951964e-05, 'epoch': 2.2045454545454546}\n{'loss': 0.7917, 'grad_norm': 0.9437254071235657, 'learning_rate': 1.8610687782002124e-05, 'epoch': 2.227272727272727}\n{'loss': 0.7946, 'grad_norm': 0.8818357586860657, 'learning_rate': 1.8548444679052284e-05, 'epoch': 2.25}\n{'loss': 0.7555, 'grad_norm': 0.9948816299438477, 'learning_rate': 1.8486201576102444e-05, 'epoch': 2.2727272727272725}\n{'loss': 0.8332, 'grad_norm': 1.1940159797668457, 'learning_rate': 1.8423958473152605e-05, 'epoch': 2.2954545454545454}\n{'loss': 0.7221, 'grad_norm': 0.909180223941803, 'learning_rate': 1.8361715370202765e-05, 'epoch': 2.3181818181818183}\n{'loss': 0.7398, 'grad_norm': 0.9136362671852112, 'learning_rate': 1.8299472267252925e-05, 'epoch': 2.340909090909091}\n{'loss': 0.7762, 'grad_norm': 0.9843258261680603, 'learning_rate': 1.8237229164303082e-05, 'epoch': 2.3636363636363638}\n{'loss': 0.7393, 'grad_norm': 0.8684003949165344, 'learning_rate': 1.8174986061353246e-05, 'epoch': 2.3863636363636362}\n{'loss': 0.7739, 'grad_norm': 0.8952692151069641, 'learning_rate': 1.8112742958403406e-05, 'epoch': 2.409090909090909}\n{'loss': 0.7209, 'grad_norm': 0.8941017389297485, 'learning_rate': 1.8050499855453566e-05, 'epoch': 2.4318181818181817}\n{'loss': 0.7471, 'grad_norm': 1.0235095024108887, 'learning_rate': 1.7988256752503726e-05, 'epoch': 2.4545454545454546}\n{'loss': 0.7291, 'grad_norm': 0.9722160696983337, 'learning_rate': 1.7926013649553883e-05, 'epoch': 2.4772727272727275}\n{'loss': 0.717, 'grad_norm': 0.8634330630302429, 'learning_rate': 1.7863770546604047e-05, 'epoch': 2.5}\n{'loss': 0.7535, 'grad_norm': 0.8001314997673035, 'learning_rate': 1.7801527443654204e-05, 'epoch': 2.5227272727272725}\n{'loss': 0.7326, 'grad_norm': 0.8463662266731262, 'learning_rate': 1.7739284340704364e-05, 'epoch': 2.5454545454545454}\n{'loss': 0.7147, 'grad_norm': 0.8800860047340393, 'learning_rate': 1.7677041237754524e-05, 'epoch': 2.5681818181818183}\n{'loss': 0.7486, 'grad_norm': 0.982094407081604, 'learning_rate': 1.7614798134804684e-05, 'epoch': 2.590909090909091}\n{'loss': 0.6589, 'grad_norm': 0.9079577922821045, 'learning_rate': 1.7552555031854848e-05, 'epoch': 2.6136363636363638}\n{'loss': 0.659, 'grad_norm': 0.80891352891922, 'learning_rate': 1.7490311928905005e-05, 'epoch': 2.6363636363636362}\n{'loss': 0.6819, 'grad_norm': 0.9794456362724304, 'learning_rate': 1.7428068825955165e-05, 'epoch': 2.659090909090909}\n{'loss': 0.6206, 'grad_norm': 0.8734978437423706, 'learning_rate': 1.7365825723005325e-05, 'epoch': 2.6818181818181817}\n{'loss': 0.6784, 'grad_norm': 0.867698609828949, 'learning_rate': 1.7303582620055486e-05, 'epoch': 2.7045454545454546}\n{'loss': 0.6496, 'grad_norm': 0.8219112753868103, 'learning_rate': 1.7241339517105646e-05, 'epoch': 2.7272727272727275}\n{'loss': 0.6802, 'grad_norm': 1.0071712732315063, 'learning_rate': 1.7179096414155806e-05, 'epoch': 2.75}\n{'loss': 0.6819, 'grad_norm': 0.8535628318786621, 'learning_rate': 1.7116853311205966e-05, 'epoch': 2.7727272727272725}\n{'loss': 0.6774, 'grad_norm': 0.8286237120628357, 'learning_rate': 1.7054610208256127e-05, 'epoch': 2.7954545454545454}\n{'loss': 0.7124, 'grad_norm': 0.9166253209114075, 'learning_rate': 1.6992367105306287e-05, 'epoch': 2.8181818181818183}\n{'loss': 0.6514, 'grad_norm': 1.0527712106704712, 'learning_rate': 1.6930124002356447e-05, 'epoch': 2.840909090909091}\n{'loss': 0.6381, 'grad_norm': 0.9242057204246521, 'learning_rate': 1.6867880899406607e-05, 'epoch': 2.8636363636363638}\n{'loss': 0.5845, 'grad_norm': 1.0007216930389404, 'learning_rate': 1.6805637796456768e-05, 'epoch': 2.8863636363636362}\n{'loss': 0.6503, 'grad_norm': 1.1265491247177124, 'learning_rate': 1.6743394693506928e-05, 'epoch': 2.909090909090909}\n{'loss': 0.6462, 'grad_norm': 0.7582006454467773, 'learning_rate': 1.6681151590557085e-05, 'epoch': 2.9318181818181817}\n{'loss': 0.684, 'grad_norm': 0.829005777835846, 'learning_rate': 1.6618908487607248e-05, 'epoch': 2.9545454545454546}\n{'loss': 0.6382, 'grad_norm': 1.1024588346481323, 'learning_rate': 1.655666538465741e-05, 'epoch': 2.9772727272727275}\n{'loss': 0.5004, 'grad_norm': 2.966876268386841, 'learning_rate': 1.649442228170757e-05, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.6902    0.6213    0.6539      3142\n     PERCENT     1.0000    0.1667    0.2857        66\n        TYPE     0.8200    0.8957    0.8561     11415\n      VOLUME     0.0000    0.0000    0.0000        70\n\n   micro avg     0.7961    0.8294    0.8124     14693\n   macro avg     0.6275    0.4209    0.4489     14693\nweighted avg     0.7891    0.8294    0.8063     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.6036585569381714, 'eval_f1_macro': 0.4489471586144871, 'eval_precision': 0.7961196759864123, 'eval_recall': 0.8294425917103383, 'eval_f1': 0.8124395853471551, 'eval_accuracy': 0.8181521384263793, 'eval_runtime': 1.8203, 'eval_samples_per_second': 3027.571, 'eval_steps_per_second': 6.043, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.6114, 'grad_norm': 0.8921641111373901, 'learning_rate': 1.643217917875773e-05, 'epoch': 3.022727272727273}\n{'loss': 0.6316, 'grad_norm': 1.0252680778503418, 'learning_rate': 1.6369936075807886e-05, 'epoch': 3.0454545454545454}\n{'loss': 0.6379, 'grad_norm': 0.9718452095985413, 'learning_rate': 1.630769297285805e-05, 'epoch': 3.0681818181818183}\n{'loss': 0.5692, 'grad_norm': 0.9167243242263794, 'learning_rate': 1.6245449869908206e-05, 'epoch': 3.090909090909091}\n{'loss': 0.6608, 'grad_norm': 0.8015126585960388, 'learning_rate': 1.618320676695837e-05, 'epoch': 3.1136363636363638}\n{'loss': 0.5827, 'grad_norm': 0.7719815373420715, 'learning_rate': 1.6120963664008527e-05, 'epoch': 3.1363636363636362}\n{'loss': 0.6163, 'grad_norm': 1.2196294069290161, 'learning_rate': 1.6058720561058687e-05, 'epoch': 3.159090909090909}\n{'loss': 0.5834, 'grad_norm': 0.7536036372184753, 'learning_rate': 1.599647745810885e-05, 'epoch': 3.1818181818181817}\n{'loss': 0.6523, 'grad_norm': 0.8009840250015259, 'learning_rate': 1.5934234355159008e-05, 'epoch': 3.2045454545454546}\n{'loss': 0.5563, 'grad_norm': 0.7979511618614197, 'learning_rate': 1.5871991252209168e-05, 'epoch': 3.227272727272727}\n{'loss': 0.6426, 'grad_norm': 0.8014345765113831, 'learning_rate': 1.5809748149259328e-05, 'epoch': 3.25}\n{'loss': 0.6204, 'grad_norm': 0.9241330027580261, 'learning_rate': 1.574750504630949e-05, 'epoch': 3.2727272727272725}\n{'loss': 0.6203, 'grad_norm': 0.691598653793335, 'learning_rate': 1.568526194335965e-05, 'epoch': 3.2954545454545454}\n{'loss': 0.6392, 'grad_norm': 0.7895785570144653, 'learning_rate': 1.562301884040981e-05, 'epoch': 3.3181818181818183}\n{'loss': 0.5922, 'grad_norm': 0.7890068888664246, 'learning_rate': 1.556077573745997e-05, 'epoch': 3.340909090909091}\n{'loss': 0.5555, 'grad_norm': 0.7875480055809021, 'learning_rate': 1.549853263451013e-05, 'epoch': 3.3636363636363638}\n{'loss': 0.54, 'grad_norm': 0.7912775874137878, 'learning_rate': 1.543628953156029e-05, 'epoch': 3.3863636363636362}\n{'loss': 0.5712, 'grad_norm': 0.9222251772880554, 'learning_rate': 1.537404642861045e-05, 'epoch': 3.409090909090909}\n{'loss': 0.5937, 'grad_norm': 0.7951319217681885, 'learning_rate': 1.531180332566061e-05, 'epoch': 3.4318181818181817}\n{'loss': 0.6644, 'grad_norm': 0.9621911644935608, 'learning_rate': 1.5249560222710769e-05, 'epoch': 3.4545454545454546}\n{'loss': 0.5423, 'grad_norm': 0.7482816576957703, 'learning_rate': 1.518731711976093e-05, 'epoch': 3.4772727272727275}\n{'loss': 0.6028, 'grad_norm': 0.7406026124954224, 'learning_rate': 1.5125074016811089e-05, 'epoch': 3.5}\n{'loss': 0.6066, 'grad_norm': 0.6332428455352783, 'learning_rate': 1.5062830913861251e-05, 'epoch': 3.5227272727272725}\n{'loss': 0.5839, 'grad_norm': 0.9394952654838562, 'learning_rate': 1.5000587810911411e-05, 'epoch': 3.5454545454545454}\n{'loss': 0.5981, 'grad_norm': 0.8658363819122314, 'learning_rate': 1.493834470796157e-05, 'epoch': 3.5681818181818183}\n{'loss': 0.6252, 'grad_norm': 0.7251573801040649, 'learning_rate': 1.4876101605011732e-05, 'epoch': 3.590909090909091}\n{'loss': 0.6379, 'grad_norm': 0.9152493476867676, 'learning_rate': 1.481385850206189e-05, 'epoch': 3.6136363636363638}\n{'loss': 0.5536, 'grad_norm': 0.7131760716438293, 'learning_rate': 1.4751615399112052e-05, 'epoch': 3.6363636363636362}\n{'loss': 0.5298, 'grad_norm': 0.8131232261657715, 'learning_rate': 1.468937229616221e-05, 'epoch': 3.659090909090909}\n{'loss': 0.5258, 'grad_norm': 0.8375539183616638, 'learning_rate': 1.4627129193212371e-05, 'epoch': 3.6818181818181817}\n{'loss': 0.5665, 'grad_norm': 0.7833828330039978, 'learning_rate': 1.4564886090262533e-05, 'epoch': 3.7045454545454546}\n{'loss': 0.5159, 'grad_norm': 0.7365587949752808, 'learning_rate': 1.4502642987312692e-05, 'epoch': 3.7272727272727275}\n{'loss': 0.5583, 'grad_norm': 0.8771935105323792, 'learning_rate': 1.4440399884362852e-05, 'epoch': 3.75}\n{'loss': 0.5461, 'grad_norm': 0.8799415230751038, 'learning_rate': 1.4378156781413012e-05, 'epoch': 3.7727272727272725}\n{'loss': 0.5736, 'grad_norm': 0.8950327634811401, 'learning_rate': 1.4315913678463172e-05, 'epoch': 3.7954545454545454}\n{'loss': 0.5214, 'grad_norm': 0.7194807529449463, 'learning_rate': 1.4253670575513331e-05, 'epoch': 3.8181818181818183}\n{'loss': 0.5203, 'grad_norm': 0.8666473627090454, 'learning_rate': 1.4191427472563493e-05, 'epoch': 3.840909090909091}\n{'loss': 0.6674, 'grad_norm': 1.1809755563735962, 'learning_rate': 1.4129184369613651e-05, 'epoch': 3.8636363636363638}\n{'loss': 0.6013, 'grad_norm': 1.1895792484283447, 'learning_rate': 1.4066941266663812e-05, 'epoch': 3.8863636363636362}\n{'loss': 0.5451, 'grad_norm': 0.6464834809303284, 'learning_rate': 1.4004698163713974e-05, 'epoch': 3.909090909090909}\n{'loss': 0.5149, 'grad_norm': 0.7257050275802612, 'learning_rate': 1.3942455060764132e-05, 'epoch': 3.9318181818181817}\n{'loss': 0.5953, 'grad_norm': 1.0305075645446777, 'learning_rate': 1.3880211957814294e-05, 'epoch': 3.9545454545454546}\n{'loss': 0.5517, 'grad_norm': 0.6954528093338013, 'learning_rate': 1.3817968854864453e-05, 'epoch': 3.9772727272727275}\n{'loss': 0.718, 'grad_norm': 3.935774087905884, 'learning_rate': 1.3755725751914613e-05, 'epoch': 4.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.6998    0.6566    0.6775      3142\n     PERCENT     0.7347    0.5455    0.6261        66\n        TYPE     0.8598    0.9127    0.8854     11415\n      VOLUME     0.0000    0.0000    0.0000        70\n\n   micro avg     0.8282    0.8519    0.8399     14693\n   macro avg     0.5736    0.5287    0.5473     14693\nweighted avg     0.8209    0.8519    0.8356     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.525271475315094, 'eval_f1_macro': 0.5472559160064464, 'eval_precision': 0.828172555246791, 'eval_recall': 0.8519022663853536, 'eval_f1': 0.8398698292347435, 'eval_accuracy': 0.8383937316356513, 'eval_runtime': 1.4524, 'eval_samples_per_second': 3794.318, 'eval_steps_per_second': 7.573, 'epoch': 4.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.5727, 'grad_norm': 0.7224251627922058, 'learning_rate': 1.3693482648964771e-05, 'epoch': 4.0227272727272725}\n{'loss': 0.5796, 'grad_norm': 0.7594806551933289, 'learning_rate': 1.3631239546014933e-05, 'epoch': 4.045454545454546}\n{'loss': 0.5656, 'grad_norm': 1.1985188722610474, 'learning_rate': 1.3568996443065095e-05, 'epoch': 4.068181818181818}\n{'loss': 0.555, 'grad_norm': 0.70893794298172, 'learning_rate': 1.3506753340115254e-05, 'epoch': 4.090909090909091}\n{'loss': 0.4897, 'grad_norm': 1.7349549531936646, 'learning_rate': 1.3444510237165414e-05, 'epoch': 4.113636363636363}\n{'loss': 0.5303, 'grad_norm': 1.0548529624938965, 'learning_rate': 1.3382267134215573e-05, 'epoch': 4.136363636363637}\n{'loss': 0.5537, 'grad_norm': 0.9357603192329407, 'learning_rate': 1.3320024031265735e-05, 'epoch': 4.159090909090909}\n{'loss': 0.5615, 'grad_norm': 0.8099073767662048, 'learning_rate': 1.3257780928315893e-05, 'epoch': 4.181818181818182}\n{'loss': 0.5482, 'grad_norm': 1.0877375602722168, 'learning_rate': 1.3195537825366055e-05, 'epoch': 4.204545454545454}\n{'loss': 0.5375, 'grad_norm': 0.7594509720802307, 'learning_rate': 1.3133294722416214e-05, 'epoch': 4.2272727272727275}\n{'loss': 0.5928, 'grad_norm': 0.7119260430335999, 'learning_rate': 1.3071051619466374e-05, 'epoch': 4.25}\n{'loss': 0.5217, 'grad_norm': 0.6855470538139343, 'learning_rate': 1.3008808516516536e-05, 'epoch': 4.2727272727272725}\n{'loss': 0.5492, 'grad_norm': 0.887872576713562, 'learning_rate': 1.2946565413566694e-05, 'epoch': 4.295454545454546}\n{'loss': 0.5306, 'grad_norm': 0.9457188248634338, 'learning_rate': 1.2884322310616856e-05, 'epoch': 4.318181818181818}\n{'loss': 0.4971, 'grad_norm': 0.7360317707061768, 'learning_rate': 1.2822079207667015e-05, 'epoch': 4.340909090909091}\n{'loss': 0.5387, 'grad_norm': 1.1597877740859985, 'learning_rate': 1.2759836104717175e-05, 'epoch': 4.363636363636363}\n{'loss': 0.5436, 'grad_norm': 0.8956223726272583, 'learning_rate': 1.2697593001767334e-05, 'epoch': 4.386363636363637}\n{'loss': 0.4864, 'grad_norm': 1.0843268632888794, 'learning_rate': 1.2635349898817496e-05, 'epoch': 4.409090909090909}\n{'loss': 0.5017, 'grad_norm': 0.800764262676239, 'learning_rate': 1.2573106795867656e-05, 'epoch': 4.431818181818182}\n{'loss': 0.5496, 'grad_norm': 0.8140568733215332, 'learning_rate': 1.2510863692917816e-05, 'epoch': 4.454545454545454}\n{'loss': 0.604, 'grad_norm': 1.2022631168365479, 'learning_rate': 1.2448620589967976e-05, 'epoch': 4.4772727272727275}\n{'loss': 0.5553, 'grad_norm': 0.9342930316925049, 'learning_rate': 1.2386377487018135e-05, 'epoch': 4.5}\n{'loss': 0.5017, 'grad_norm': 0.8209661841392517, 'learning_rate': 1.2324134384068297e-05, 'epoch': 4.5227272727272725}\n{'loss': 0.4593, 'grad_norm': 0.8312841653823853, 'learning_rate': 1.2261891281118455e-05, 'epoch': 4.545454545454545}\n{'loss': 0.5104, 'grad_norm': 0.6786649823188782, 'learning_rate': 1.2199648178168616e-05, 'epoch': 4.568181818181818}\n{'loss': 0.4717, 'grad_norm': 0.7580417990684509, 'learning_rate': 1.2137405075218776e-05, 'epoch': 4.590909090909091}\n{'loss': 0.4935, 'grad_norm': 0.9174633622169495, 'learning_rate': 1.2075161972268936e-05, 'epoch': 4.613636363636363}\n{'loss': 0.526, 'grad_norm': 0.8393387198448181, 'learning_rate': 1.2012918869319098e-05, 'epoch': 4.636363636363637}\n{'loss': 0.5691, 'grad_norm': 1.1198755502700806, 'learning_rate': 1.1950675766369257e-05, 'epoch': 4.659090909090909}\n{'loss': 0.4702, 'grad_norm': 0.8245737552642822, 'learning_rate': 1.1888432663419417e-05, 'epoch': 4.681818181818182}\n{'loss': 0.474, 'grad_norm': 0.6405381560325623, 'learning_rate': 1.1826189560469577e-05, 'epoch': 4.704545454545455}\n{'loss': 0.5225, 'grad_norm': 0.6407800912857056, 'learning_rate': 1.1763946457519737e-05, 'epoch': 4.7272727272727275}\n{'loss': 0.4737, 'grad_norm': 0.7321564555168152, 'learning_rate': 1.1701703354569896e-05, 'epoch': 4.75}\n{'loss': 0.5318, 'grad_norm': 0.7302824854850769, 'learning_rate': 1.1639460251620058e-05, 'epoch': 4.7727272727272725}\n{'loss': 0.5378, 'grad_norm': 1.073749303817749, 'learning_rate': 1.1577217148670218e-05, 'epoch': 4.795454545454545}\n{'loss': 0.4845, 'grad_norm': 0.6169330477714539, 'learning_rate': 1.1514974045720377e-05, 'epoch': 4.818181818181818}\n{'loss': 0.4776, 'grad_norm': 0.8413194417953491, 'learning_rate': 1.1452730942770539e-05, 'epoch': 4.840909090909091}\n{'loss': 0.5095, 'grad_norm': 0.6147642731666565, 'learning_rate': 1.1390487839820697e-05, 'epoch': 4.863636363636363}\n{'loss': 0.5459, 'grad_norm': 0.7309996485710144, 'learning_rate': 1.1328244736870859e-05, 'epoch': 4.886363636363637}\n{'loss': 0.4474, 'grad_norm': 1.16354501247406, 'learning_rate': 1.1266001633921018e-05, 'epoch': 4.909090909090909}\n{'loss': 0.5359, 'grad_norm': 0.8324427604675293, 'learning_rate': 1.1203758530971178e-05, 'epoch': 4.931818181818182}\n{'loss': 0.4792, 'grad_norm': 0.877921462059021, 'learning_rate': 1.1141515428021336e-05, 'epoch': 4.954545454545455}\n{'loss': 0.498, 'grad_norm': 0.8819175958633423, 'learning_rate': 1.1079272325071498e-05, 'epoch': 4.9772727272727275}\n{'loss': 0.7465, 'grad_norm': 4.635499954223633, 'learning_rate': 1.1017029222121659e-05, 'epoch': 5.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.7400    0.6595    0.6974      3142\n     PERCENT     0.5714    0.7879    0.6624        66\n        TYPE     0.8723    0.9259    0.8983     11415\n      VOLUME     0.0000    0.0000    0.0000        70\n\n   micro avg     0.8458    0.8639    0.8547     14693\n   macro avg     0.5459    0.5933    0.5645     14693\nweighted avg     0.8385    0.8639    0.8500     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.48351427912712097, 'eval_f1_macro': 0.5645332566653487, 'eval_precision': 0.8458052908642634, 'eval_recall': 0.8638807595453617, 'eval_f1': 0.8547474747474748, 'eval_accuracy': 0.8504733920992491, 'eval_runtime': 1.4639, 'eval_samples_per_second': 3764.673, 'eval_steps_per_second': 7.514, 'epoch': 5.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.5456, 'grad_norm': 0.7451931238174438, 'learning_rate': 1.0954786119171819e-05, 'epoch': 5.0227272727272725}\n{'loss': 0.5135, 'grad_norm': 0.6813583970069885, 'learning_rate': 1.0892543016221979e-05, 'epoch': 5.045454545454546}\n{'loss': 0.5125, 'grad_norm': 0.9360639452934265, 'learning_rate': 1.0830299913272138e-05, 'epoch': 5.068181818181818}\n{'loss': 0.4595, 'grad_norm': 0.8500335216522217, 'learning_rate': 1.07680568103223e-05, 'epoch': 5.090909090909091}\n{'loss': 0.501, 'grad_norm': 0.6946488618850708, 'learning_rate': 1.070581370737246e-05, 'epoch': 5.113636363636363}\n{'loss': 0.495, 'grad_norm': 0.875795304775238, 'learning_rate': 1.064357060442262e-05, 'epoch': 5.136363636363637}\n{'loss': 0.4802, 'grad_norm': 0.6584014892578125, 'learning_rate': 1.0581327501472779e-05, 'epoch': 5.159090909090909}\n{'loss': 0.5063, 'grad_norm': 0.7059727311134338, 'learning_rate': 1.0519084398522939e-05, 'epoch': 5.181818181818182}\n{'loss': 0.4187, 'grad_norm': 1.0237047672271729, 'learning_rate': 1.0456841295573099e-05, 'epoch': 5.204545454545454}\n{'loss': 0.477, 'grad_norm': 0.7628598213195801, 'learning_rate': 1.039459819262326e-05, 'epoch': 5.2272727272727275}\n{'loss': 0.4536, 'grad_norm': 0.75995272397995, 'learning_rate': 1.033235508967342e-05, 'epoch': 5.25}\n{'loss': 0.497, 'grad_norm': 0.775027334690094, 'learning_rate': 1.027011198672358e-05, 'epoch': 5.2727272727272725}\n{'loss': 0.5295, 'grad_norm': 1.0569876432418823, 'learning_rate': 1.020786888377374e-05, 'epoch': 5.295454545454546}\n{'loss': 0.4856, 'grad_norm': 0.7820711135864258, 'learning_rate': 1.01456257808239e-05, 'epoch': 5.318181818181818}\n{'loss': 0.468, 'grad_norm': 0.6518409252166748, 'learning_rate': 1.008338267787406e-05, 'epoch': 5.340909090909091}\n{'loss': 0.4673, 'grad_norm': 0.6379355192184448, 'learning_rate': 1.002113957492422e-05, 'epoch': 5.363636363636363}\n{'loss': 0.5136, 'grad_norm': 0.7848720550537109, 'learning_rate': 9.958896471974381e-06, 'epoch': 5.386363636363637}\n{'loss': 0.5302, 'grad_norm': 1.1736708879470825, 'learning_rate': 9.89665336902454e-06, 'epoch': 5.409090909090909}\n{'loss': 0.5427, 'grad_norm': 1.0756607055664062, 'learning_rate': 9.8344102660747e-06, 'epoch': 5.431818181818182}\n{'loss': 0.4699, 'grad_norm': 0.734541118144989, 'learning_rate': 9.772167163124862e-06, 'epoch': 5.454545454545454}\n{'loss': 0.4747, 'grad_norm': 1.0207804441452026, 'learning_rate': 9.709924060175022e-06, 'epoch': 5.4772727272727275}\n{'loss': 0.4204, 'grad_norm': 0.9529977440834045, 'learning_rate': 9.64768095722518e-06, 'epoch': 5.5}\n{'loss': 0.4865, 'grad_norm': 0.8666548132896423, 'learning_rate': 9.58543785427534e-06, 'epoch': 5.5227272727272725}\n{'loss': 0.4244, 'grad_norm': 0.6768292784690857, 'learning_rate': 9.523194751325501e-06, 'epoch': 5.545454545454545}\n{'loss': 0.4916, 'grad_norm': 0.9349353909492493, 'learning_rate': 9.460951648375661e-06, 'epoch': 5.568181818181818}\n{'loss': 0.4607, 'grad_norm': 0.7550593018531799, 'learning_rate': 9.398708545425822e-06, 'epoch': 5.590909090909091}\n{'loss': 0.5374, 'grad_norm': 1.3027563095092773, 'learning_rate': 9.336465442475982e-06, 'epoch': 5.613636363636363}\n{'loss': 0.4214, 'grad_norm': 0.6807053089141846, 'learning_rate': 9.274222339526142e-06, 'epoch': 5.636363636363637}\n{'loss': 0.4789, 'grad_norm': 0.7953263521194458, 'learning_rate': 9.211979236576302e-06, 'epoch': 5.659090909090909}\n{'loss': 0.5294, 'grad_norm': 0.9406426548957825, 'learning_rate': 9.149736133626463e-06, 'epoch': 5.681818181818182}\n{'loss': 0.4743, 'grad_norm': 0.8980358242988586, 'learning_rate': 9.087493030676623e-06, 'epoch': 5.704545454545455}\n{'loss': 0.4549, 'grad_norm': 0.8315722346305847, 'learning_rate': 9.025249927726783e-06, 'epoch': 5.7272727272727275}\n{'loss': 0.464, 'grad_norm': 0.8136148452758789, 'learning_rate': 8.963006824776942e-06, 'epoch': 5.75}\n{'loss': 0.4225, 'grad_norm': 0.7647538781166077, 'learning_rate': 8.900763721827102e-06, 'epoch': 5.7727272727272725}\n{'loss': 0.5199, 'grad_norm': 0.9050672054290771, 'learning_rate': 8.838520618877262e-06, 'epoch': 5.795454545454545}\n{'loss': 0.506, 'grad_norm': 0.7021324038505554, 'learning_rate': 8.776277515927424e-06, 'epoch': 5.818181818181818}\n{'loss': 0.4738, 'grad_norm': 0.9247620105743408, 'learning_rate': 8.714034412977583e-06, 'epoch': 5.840909090909091}\n{'loss': 0.4393, 'grad_norm': 1.2459497451782227, 'learning_rate': 8.651791310027743e-06, 'epoch': 5.863636363636363}\n{'loss': 0.5071, 'grad_norm': 0.759280264377594, 'learning_rate': 8.589548207077903e-06, 'epoch': 5.886363636363637}\n{'loss': 0.4568, 'grad_norm': 0.974314272403717, 'learning_rate': 8.527305104128063e-06, 'epoch': 5.909090909090909}\n{'loss': 0.4919, 'grad_norm': 0.8910998106002808, 'learning_rate': 8.465062001178224e-06, 'epoch': 5.931818181818182}\n{'loss': 0.4698, 'grad_norm': 0.7823053598403931, 'learning_rate': 8.402818898228384e-06, 'epoch': 5.954545454545455}\n{'loss': 0.4123, 'grad_norm': 1.041474461555481, 'learning_rate': 8.340575795278542e-06, 'epoch': 5.9772727272727275}\n{'loss': 0.5641, 'grad_norm': 4.528250217437744, 'learning_rate': 8.278332692328704e-06, 'epoch': 6.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.7601    0.6715    0.7131      3142\n     PERCENT     0.5046    0.8333    0.6286        66\n        TYPE     0.8773    0.9367    0.9061     11415\n      VOLUME     0.0000    0.0000    0.0000        70\n\n   micro avg     0.8529    0.8751    0.8639     14693\n   macro avg     0.5355    0.6104    0.5619     14693\nweighted avg     0.8464    0.8751    0.8592     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.4571518301963806, 'eval_f1_macro': 0.561930358379411, 'eval_precision': 0.8529353233830845, 'eval_recall': 0.8751105968828694, 'eval_f1': 0.8638806772373018, 'eval_accuracy': 0.8586897377298944, 'eval_runtime': 1.5713, 'eval_samples_per_second': 3507.243, 'eval_steps_per_second': 7.0, 'epoch': 6.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.4566, 'grad_norm': 0.7891982793807983, 'learning_rate': 8.216089589378865e-06, 'epoch': 6.0227272727272725}\n{'loss': 0.4339, 'grad_norm': 1.1106501817703247, 'learning_rate': 8.153846486429025e-06, 'epoch': 6.045454545454546}\n{'loss': 0.4549, 'grad_norm': 0.7754213809967041, 'learning_rate': 8.091603383479185e-06, 'epoch': 6.068181818181818}\n{'loss': 0.5129, 'grad_norm': 1.0150065422058105, 'learning_rate': 8.029360280529344e-06, 'epoch': 6.090909090909091}\n{'loss': 0.4395, 'grad_norm': 0.8810274004936218, 'learning_rate': 7.967117177579504e-06, 'epoch': 6.113636363636363}\n{'loss': 0.4557, 'grad_norm': 0.6976893544197083, 'learning_rate': 7.904874074629664e-06, 'epoch': 6.136363636363637}\n{'loss': 0.4763, 'grad_norm': 0.8767915368080139, 'learning_rate': 7.842630971679824e-06, 'epoch': 6.159090909090909}\n{'loss': 0.4468, 'grad_norm': 0.7029081583023071, 'learning_rate': 7.780387868729985e-06, 'epoch': 6.181818181818182}\n{'loss': 0.4632, 'grad_norm': 0.947416365146637, 'learning_rate': 7.718144765780145e-06, 'epoch': 6.204545454545454}\n{'loss': 0.459, 'grad_norm': 0.8700524568557739, 'learning_rate': 7.655901662830305e-06, 'epoch': 6.2272727272727275}\n{'loss': 0.447, 'grad_norm': 0.664021909236908, 'learning_rate': 7.593658559880465e-06, 'epoch': 6.25}\n{'loss': 0.4951, 'grad_norm': 1.067812204360962, 'learning_rate': 7.5314154569306255e-06, 'epoch': 6.2727272727272725}\n{'loss': 0.441, 'grad_norm': 0.6543123126029968, 'learning_rate': 7.469172353980785e-06, 'epoch': 6.295454545454546}\n{'loss': 0.4742, 'grad_norm': 0.6891809105873108, 'learning_rate': 7.406929251030945e-06, 'epoch': 6.318181818181818}\n{'loss': 0.5375, 'grad_norm': 1.1424808502197266, 'learning_rate': 7.344686148081105e-06, 'epoch': 6.340909090909091}\n{'loss': 0.4813, 'grad_norm': 0.8731172680854797, 'learning_rate': 7.2824430451312665e-06, 'epoch': 6.363636363636363}\n{'loss': 0.4834, 'grad_norm': 1.1642053127288818, 'learning_rate': 7.220199942181426e-06, 'epoch': 6.386363636363637}\n{'loss': 0.4661, 'grad_norm': 0.8199577927589417, 'learning_rate': 7.157956839231586e-06, 'epoch': 6.409090909090909}\n{'loss': 0.433, 'grad_norm': 0.8039609789848328, 'learning_rate': 7.095713736281746e-06, 'epoch': 6.431818181818182}\n{'loss': 0.4342, 'grad_norm': 0.835901141166687, 'learning_rate': 7.033470633331906e-06, 'epoch': 6.454545454545454}\n{'loss': 0.4395, 'grad_norm': 0.7038763761520386, 'learning_rate': 6.971227530382066e-06, 'epoch': 6.4772727272727275}\n{'loss': 0.4129, 'grad_norm': 0.7299355864524841, 'learning_rate': 6.908984427432226e-06, 'epoch': 6.5}\n{'loss': 0.4342, 'grad_norm': 0.6461873054504395, 'learning_rate': 6.846741324482386e-06, 'epoch': 6.5227272727272725}\n{'loss': 0.4615, 'grad_norm': 0.9824042916297913, 'learning_rate': 6.784498221532548e-06, 'epoch': 6.545454545454545}\n{'loss': 0.4783, 'grad_norm': 0.7551277279853821, 'learning_rate': 6.722255118582707e-06, 'epoch': 6.568181818181818}\n{'loss': 0.4923, 'grad_norm': 0.8048993349075317, 'learning_rate': 6.660012015632867e-06, 'epoch': 6.590909090909091}\n{'loss': 0.5378, 'grad_norm': 0.7034103274345398, 'learning_rate': 6.5977689126830275e-06, 'epoch': 6.613636363636363}\n{'loss': 0.5068, 'grad_norm': 1.0223835706710815, 'learning_rate': 6.535525809733187e-06, 'epoch': 6.636363636363637}\n{'loss': 0.4155, 'grad_norm': 0.9623066782951355, 'learning_rate': 6.473282706783347e-06, 'epoch': 6.659090909090909}\n{'loss': 0.5083, 'grad_norm': 0.6348260641098022, 'learning_rate': 6.411039603833507e-06, 'epoch': 6.681818181818182}\n{'loss': 0.4375, 'grad_norm': 0.8517119288444519, 'learning_rate': 6.348796500883667e-06, 'epoch': 6.704545454545455}\n{'loss': 0.4548, 'grad_norm': 0.8964077234268188, 'learning_rate': 6.286553397933828e-06, 'epoch': 6.7272727272727275}\n{'loss': 0.4239, 'grad_norm': 0.8581390380859375, 'learning_rate': 6.224310294983988e-06, 'epoch': 6.75}\n{'loss': 0.3956, 'grad_norm': 0.9078724980354309, 'learning_rate': 6.162067192034148e-06, 'epoch': 6.7727272727272725}\n{'loss': 0.466, 'grad_norm': 1.188438892364502, 'learning_rate': 6.099824089084308e-06, 'epoch': 6.795454545454545}\n{'loss': 0.472, 'grad_norm': 0.9055268168449402, 'learning_rate': 6.037580986134468e-06, 'epoch': 6.818181818181818}\n{'loss': 0.444, 'grad_norm': 0.7637031078338623, 'learning_rate': 5.975337883184628e-06, 'epoch': 6.840909090909091}\n{'loss': 0.4735, 'grad_norm': 1.207022786140442, 'learning_rate': 5.9130947802347885e-06, 'epoch': 6.863636363636363}\n{'loss': 0.4697, 'grad_norm': 0.7461919784545898, 'learning_rate': 5.850851677284948e-06, 'epoch': 6.886363636363637}\n{'loss': 0.4033, 'grad_norm': 0.8493345379829407, 'learning_rate': 5.788608574335109e-06, 'epoch': 6.909090909090909}\n{'loss': 0.4296, 'grad_norm': 0.9255893230438232, 'learning_rate': 5.726365471385269e-06, 'epoch': 6.931818181818182}\n{'loss': 0.42, 'grad_norm': 0.742415726184845, 'learning_rate': 5.6641223684354295e-06, 'epoch': 6.954545454545455}\n{'loss': 0.435, 'grad_norm': 0.7626157402992249, 'learning_rate': 5.601879265485589e-06, 'epoch': 6.9772727272727275}\n{'loss': 0.2934, 'grad_norm': 1.8138357400894165, 'learning_rate': 5.539636162535749e-06, 'epoch': 7.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.7817    0.6735    0.7235      3142\n     PERCENT     0.4793    0.8788    0.6203        66\n        TYPE     0.8815    0.9421    0.9108     11415\n      VOLUME     0.0000    0.0000    0.0000        70\n\n   micro avg     0.8601    0.8799    0.8699     14693\n   macro avg     0.5356    0.6236    0.5637     14693\nweighted avg     0.8541    0.8799    0.8651     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.44347915053367615, 'eval_f1_macro': 0.5636600974990323, 'eval_precision': 0.860146373918829, 'eval_recall': 0.8798747702987817, 'eval_f1': 0.8698987316219762, 'eval_accuracy': 0.8642398519969529, 'eval_runtime': 1.4572, 'eval_samples_per_second': 3781.807, 'eval_steps_per_second': 7.549, 'epoch': 7.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.4975, 'grad_norm': 1.6022460460662842, 'learning_rate': 5.477393059585909e-06, 'epoch': 7.0227272727272725}\n{'loss': 0.4037, 'grad_norm': 0.6846306324005127, 'learning_rate': 5.415149956636069e-06, 'epoch': 7.045454545454546}\n{'loss': 0.4427, 'grad_norm': 0.7149554491043091, 'learning_rate': 5.35290685368623e-06, 'epoch': 7.068181818181818}\n{'loss': 0.4284, 'grad_norm': 0.7361020445823669, 'learning_rate': 5.290663750736389e-06, 'epoch': 7.090909090909091}\n{'loss': 0.4616, 'grad_norm': 0.6483013033866882, 'learning_rate': 5.2284206477865495e-06, 'epoch': 7.113636363636363}\n{'loss': 0.4066, 'grad_norm': 0.7214257717132568, 'learning_rate': 5.16617754483671e-06, 'epoch': 7.136363636363637}\n{'loss': 0.4119, 'grad_norm': 0.8630156517028809, 'learning_rate': 5.10393444188687e-06, 'epoch': 7.159090909090909}\n{'loss': 0.3815, 'grad_norm': 0.8798604011535645, 'learning_rate': 5.04169133893703e-06, 'epoch': 7.181818181818182}\n{'loss': 0.4349, 'grad_norm': 0.6295777559280396, 'learning_rate': 4.9794482359871905e-06, 'epoch': 7.204545454545454}\n{'loss': 0.4182, 'grad_norm': 0.5918470025062561, 'learning_rate': 4.91720513303735e-06, 'epoch': 7.2272727272727275}\n{'loss': 0.4616, 'grad_norm': 0.8621866703033447, 'learning_rate': 4.854962030087511e-06, 'epoch': 7.25}\n{'loss': 0.4474, 'grad_norm': 1.1610947847366333, 'learning_rate': 4.79271892713767e-06, 'epoch': 7.2727272727272725}\n{'loss': 0.4723, 'grad_norm': 0.9160723090171814, 'learning_rate': 4.730475824187831e-06, 'epoch': 7.295454545454546}\n{'loss': 0.4222, 'grad_norm': 0.8740696310997009, 'learning_rate': 4.668232721237991e-06, 'epoch': 7.318181818181818}\n{'loss': 0.502, 'grad_norm': 1.0499783754348755, 'learning_rate': 4.605989618288151e-06, 'epoch': 7.340909090909091}\n{'loss': 0.495, 'grad_norm': 0.8030893206596375, 'learning_rate': 4.543746515338311e-06, 'epoch': 7.363636363636363}\n{'loss': 0.4578, 'grad_norm': 1.1932905912399292, 'learning_rate': 4.481503412388471e-06, 'epoch': 7.386363636363637}\n{'loss': 0.4948, 'grad_norm': 0.8484957814216614, 'learning_rate': 4.419260309438631e-06, 'epoch': 7.409090909090909}\n{'loss': 0.4243, 'grad_norm': 1.1176882982254028, 'learning_rate': 4.357017206488791e-06, 'epoch': 7.431818181818182}\n{'loss': 0.3986, 'grad_norm': 0.7749545574188232, 'learning_rate': 4.2947741035389515e-06, 'epoch': 7.454545454545454}\n{'loss': 0.4238, 'grad_norm': 0.8343507647514343, 'learning_rate': 4.232531000589112e-06, 'epoch': 7.4772727272727275}\n{'loss': 0.442, 'grad_norm': 0.8969841003417969, 'learning_rate': 4.170287897639271e-06, 'epoch': 7.5}\n{'loss': 0.4334, 'grad_norm': 0.8460235595703125, 'learning_rate': 4.108044794689432e-06, 'epoch': 7.5227272727272725}\n{'loss': 0.4478, 'grad_norm': 0.777833878993988, 'learning_rate': 4.0458016917395925e-06, 'epoch': 7.545454545454545}\n{'loss': 0.4754, 'grad_norm': 1.3095948696136475, 'learning_rate': 3.983558588789752e-06, 'epoch': 7.568181818181818}\n{'loss': 0.4049, 'grad_norm': 0.6889205574989319, 'learning_rate': 3.921315485839912e-06, 'epoch': 7.590909090909091}\n{'loss': 0.4691, 'grad_norm': 0.9294597506523132, 'learning_rate': 3.859072382890072e-06, 'epoch': 7.613636363636363}\n{'loss': 0.4059, 'grad_norm': 1.2718756198883057, 'learning_rate': 3.7968292799402326e-06, 'epoch': 7.636363636363637}\n{'loss': 0.4669, 'grad_norm': 0.7996217012405396, 'learning_rate': 3.7345861769903925e-06, 'epoch': 7.659090909090909}\n{'loss': 0.4455, 'grad_norm': 1.325836181640625, 'learning_rate': 3.6723430740405527e-06, 'epoch': 7.681818181818182}\n{'loss': 0.3779, 'grad_norm': 0.8892723917961121, 'learning_rate': 3.610099971090713e-06, 'epoch': 7.704545454545455}\n{'loss': 0.395, 'grad_norm': 0.9041417837142944, 'learning_rate': 3.547856868140873e-06, 'epoch': 7.7272727272727275}\n{'loss': 0.4643, 'grad_norm': 1.0174144506454468, 'learning_rate': 3.485613765191033e-06, 'epoch': 7.75}\n{'loss': 0.462, 'grad_norm': 0.7944655418395996, 'learning_rate': 3.423370662241193e-06, 'epoch': 7.7727272727272725}\n{'loss': 0.4183, 'grad_norm': 0.7436453104019165, 'learning_rate': 3.3611275592913535e-06, 'epoch': 7.795454545454545}\n{'loss': 0.4522, 'grad_norm': 0.6850197911262512, 'learning_rate': 3.2988844563415138e-06, 'epoch': 7.818181818181818}\n{'loss': 0.4231, 'grad_norm': 1.0946176052093506, 'learning_rate': 3.2366413533916736e-06, 'epoch': 7.840909090909091}\n{'loss': 0.4667, 'grad_norm': 0.8258187770843506, 'learning_rate': 3.1743982504418334e-06, 'epoch': 7.863636363636363}\n{'loss': 0.4706, 'grad_norm': 0.9205062389373779, 'learning_rate': 3.112155147491994e-06, 'epoch': 7.886363636363637}\n{'loss': 0.4615, 'grad_norm': 0.9101837277412415, 'learning_rate': 3.049912044542154e-06, 'epoch': 7.909090909090909}\n{'loss': 0.4414, 'grad_norm': 1.0453884601593018, 'learning_rate': 2.987668941592314e-06, 'epoch': 7.931818181818182}\n{'loss': 0.4538, 'grad_norm': 0.70658278465271, 'learning_rate': 2.925425838642474e-06, 'epoch': 7.954545454545455}\n{'loss': 0.4667, 'grad_norm': 0.9097909331321716, 'learning_rate': 2.8631827356926346e-06, 'epoch': 7.9772727272727275}\n{'loss': 0.2947, 'grad_norm': 3.0917792320251465, 'learning_rate': 2.8009396327427945e-06, 'epoch': 8.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.7801    0.6910    0.7328      3142\n     PERCENT     0.4677    0.8788    0.6105        66\n        TYPE     0.8842    0.9444    0.9133     11415\n      VOLUME     0.0000    0.0000    0.0000        70\n\n   micro avg     0.8615    0.8854    0.8733     14693\n   macro avg     0.5330    0.6285    0.5642     14693\nweighted avg     0.8558    0.8854    0.8690     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.4331226944923401, 'eval_f1_macro': 0.5641604378515874, 'eval_precision': 0.8614661280709887, 'eval_recall': 0.8853875995371946, 'eval_f1': 0.8732630731019668, 'eval_accuracy': 0.8675590379801937, 'eval_runtime': 1.8632, 'eval_samples_per_second': 2957.769, 'eval_steps_per_second': 5.904, 'epoch': 8.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.4694, 'grad_norm': 0.7619121074676514, 'learning_rate': 2.7386965297929547e-06, 'epoch': 8.022727272727273}\n{'loss': 0.4427, 'grad_norm': 0.7950229048728943, 'learning_rate': 2.676453426843115e-06, 'epoch': 8.045454545454545}\n{'loss': 0.4263, 'grad_norm': 0.940202534198761, 'learning_rate': 2.6142103238932748e-06, 'epoch': 8.068181818181818}\n{'loss': 0.483, 'grad_norm': 0.8840975761413574, 'learning_rate': 2.551967220943435e-06, 'epoch': 8.090909090909092}\n{'loss': 0.4076, 'grad_norm': 0.7697790861129761, 'learning_rate': 2.4897241179935953e-06, 'epoch': 8.113636363636363}\n{'loss': 0.4247, 'grad_norm': 0.6500110626220703, 'learning_rate': 2.4274810150437555e-06, 'epoch': 8.136363636363637}\n{'loss': 0.4177, 'grad_norm': 0.8795241117477417, 'learning_rate': 2.3652379120939153e-06, 'epoch': 8.159090909090908}\n{'loss': 0.4581, 'grad_norm': 1.319646954536438, 'learning_rate': 2.3029948091440756e-06, 'epoch': 8.181818181818182}\n{'loss': 0.4618, 'grad_norm': 0.8419273495674133, 'learning_rate': 2.2407517061942354e-06, 'epoch': 8.204545454545455}\n{'loss': 0.404, 'grad_norm': 0.6994280219078064, 'learning_rate': 2.1785086032443956e-06, 'epoch': 8.227272727272727}\n{'loss': 0.423, 'grad_norm': 0.8251548409461975, 'learning_rate': 2.116265500294556e-06, 'epoch': 8.25}\n{'loss': 0.3841, 'grad_norm': 0.7087254524230957, 'learning_rate': 2.054022397344716e-06, 'epoch': 8.272727272727273}\n{'loss': 0.4746, 'grad_norm': 0.9218347072601318, 'learning_rate': 1.991779294394876e-06, 'epoch': 8.295454545454545}\n{'loss': 0.415, 'grad_norm': 0.718381941318512, 'learning_rate': 1.929536191445036e-06, 'epoch': 8.318181818181818}\n{'loss': 0.4018, 'grad_norm': 0.691997766494751, 'learning_rate': 1.8672930884951962e-06, 'epoch': 8.340909090909092}\n{'loss': 0.4556, 'grad_norm': 1.0826345682144165, 'learning_rate': 1.8050499855453565e-06, 'epoch': 8.363636363636363}\n{'loss': 0.352, 'grad_norm': 0.9652189612388611, 'learning_rate': 1.7428068825955165e-06, 'epoch': 8.386363636363637}\n{'loss': 0.414, 'grad_norm': 0.655371904373169, 'learning_rate': 1.6805637796456768e-06, 'epoch': 8.409090909090908}\n{'loss': 0.5658, 'grad_norm': 0.8327767848968506, 'learning_rate': 1.6183206766958368e-06, 'epoch': 8.431818181818182}\n{'loss': 0.3874, 'grad_norm': 1.2443289756774902, 'learning_rate': 1.556077573745997e-06, 'epoch': 8.454545454545455}\n{'loss': 0.4339, 'grad_norm': 0.7855163216590881, 'learning_rate': 1.493834470796157e-06, 'epoch': 8.477272727272727}\n{'loss': 0.4278, 'grad_norm': 0.7077744007110596, 'learning_rate': 1.4315913678463173e-06, 'epoch': 8.5}\n{'loss': 0.4467, 'grad_norm': 1.0720829963684082, 'learning_rate': 1.3693482648964773e-06, 'epoch': 8.522727272727273}\n{'loss': 0.4399, 'grad_norm': 0.8947713375091553, 'learning_rate': 1.3071051619466374e-06, 'epoch': 8.545454545454545}\n{'loss': 0.448, 'grad_norm': 0.7612889409065247, 'learning_rate': 1.2448620589967976e-06, 'epoch': 8.568181818181818}\n{'loss': 0.461, 'grad_norm': 0.903539776802063, 'learning_rate': 1.1826189560469577e-06, 'epoch': 8.590909090909092}\n{'loss': 0.4504, 'grad_norm': 0.90552818775177, 'learning_rate': 1.1203758530971177e-06, 'epoch': 8.613636363636363}\n{'loss': 0.416, 'grad_norm': 0.6550779342651367, 'learning_rate': 1.058132750147278e-06, 'epoch': 8.636363636363637}\n{'loss': 0.4185, 'grad_norm': 0.6910064816474915, 'learning_rate': 9.95889647197438e-07, 'epoch': 8.659090909090908}\n{'loss': 0.4793, 'grad_norm': 1.0576244592666626, 'learning_rate': 9.336465442475981e-07, 'epoch': 8.681818181818182}\n{'loss': 0.426, 'grad_norm': 0.8718156218528748, 'learning_rate': 8.714034412977583e-07, 'epoch': 8.704545454545455}\n{'loss': 0.4454, 'grad_norm': 0.6553836464881897, 'learning_rate': 8.091603383479184e-07, 'epoch': 8.727272727272727}\n{'loss': 0.4119, 'grad_norm': 1.1067389249801636, 'learning_rate': 7.469172353980785e-07, 'epoch': 8.75}\n{'loss': 0.4513, 'grad_norm': 0.7090360522270203, 'learning_rate': 6.846741324482387e-07, 'epoch': 8.772727272727273}\n{'loss': 0.4385, 'grad_norm': 0.7784644365310669, 'learning_rate': 6.224310294983988e-07, 'epoch': 8.795454545454545}\n{'loss': 0.3871, 'grad_norm': 0.7855235934257507, 'learning_rate': 5.601879265485588e-07, 'epoch': 8.818181818181818}\n{'loss': 0.441, 'grad_norm': 0.716600775718689, 'learning_rate': 4.97944823598719e-07, 'epoch': 8.840909090909092}\n{'loss': 0.4004, 'grad_norm': 0.7813600897789001, 'learning_rate': 4.3570172064887913e-07, 'epoch': 8.863636363636363}\n{'loss': 0.453, 'grad_norm': 0.7780451774597168, 'learning_rate': 3.7345861769903927e-07, 'epoch': 8.886363636363637}\n{'loss': 0.4293, 'grad_norm': 0.8342065215110779, 'learning_rate': 3.112155147491994e-07, 'epoch': 8.909090909090908}\n{'loss': 0.5089, 'grad_norm': 0.948259174823761, 'learning_rate': 2.489724117993595e-07, 'epoch': 8.931818181818182}\n{'loss': 0.4407, 'grad_norm': 0.6936707496643066, 'learning_rate': 1.8672930884951963e-07, 'epoch': 8.954545454545455}\n{'loss': 0.4146, 'grad_norm': 0.7654293179512024, 'learning_rate': 1.2448620589967975e-07, 'epoch': 8.977272727272727}\n{'loss': 0.5381, 'grad_norm': 3.458523750305176, 'learning_rate': 6.224310294983987e-08, 'epoch': 9.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.7827    0.6948    0.7361      3142\n     PERCENT     0.4603    0.8788    0.6042        66\n        TYPE     0.8858    0.9448    0.9144     11415\n      VOLUME     0.0000    0.0000    0.0000        70\n\n   micro avg     0.8631    0.8865    0.8747     14693\n   macro avg     0.5322    0.6296    0.5637     14693\nweighted avg     0.8576    0.8865    0.8705     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.4301195740699768, 'eval_f1_macro': 0.563667337363644, 'eval_precision': 0.8631062814736284, 'eval_recall': 0.886544613081059, 'eval_f1': 0.8746684572771529, 'eval_accuracy': 0.8688105343345304, 'eval_runtime': 1.4387, 'eval_samples_per_second': 3830.484, 'eval_steps_per_second': 7.646, 'epoch': 9.0}\n{'train_runtime': 56.1116, 'train_samples_per_second': 3535.261, 'train_steps_per_second': 7.057, 'train_loss': 0.7355787253590546, 'epoch': 9.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.7400    0.6595    0.6974      3142\n     PERCENT     0.5714    0.7879    0.6624        66\n        TYPE     0.8723    0.9259    0.8983     11415\n      VOLUME     0.0000    0.0000    0.0000        70\n\n   micro avg     0.8458    0.8639    0.8547     14693\n   macro avg     0.5459    0.5933    0.5645     14693\nweighted avg     0.8385    0.8639    0.8500     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.48351427912712097, 'eval_f1_macro': 0.5645332566653487, 'eval_precision': 0.8458052908642634, 'eval_recall': 0.8638807595453617, 'eval_f1': 0.8547474747474748, 'eval_accuracy': 0.8504733920992491, 'eval_runtime': 1.5208, 'eval_samples_per_second': 3623.705, 'eval_steps_per_second': 7.233, 'epoch': 9.0}\n","output_type":"stream"},{"name":"stderr","text":"The speedups for torchdynamo mostly come with GPU Ampere or higher and which is not detected here.\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/tmp/ipykernel_36/3364797558.py:66: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 2.2788, 'grad_norm': 7.534585952758789, 'learning_rate': 0.0, 'epoch': 0.022727272727272728}\n{'loss': 2.2942, 'grad_norm': 7.664220333099365, 'learning_rate': 5.539636162535749e-07, 'epoch': 0.045454545454545456}\n{'loss': 2.293, 'grad_norm': 7.74100923538208, 'learning_rate': 1.1079272325071499e-06, 'epoch': 0.06818181818181818}\n{'loss': 2.2876, 'grad_norm': 7.518697738647461, 'learning_rate': 1.6618908487607247e-06, 'epoch': 0.09090909090909091}\n{'loss': 2.2808, 'grad_norm': 7.489264011383057, 'learning_rate': 2.2158544650142997e-06, 'epoch': 0.11363636363636363}\n{'loss': 2.2971, 'grad_norm': 7.623854637145996, 'learning_rate': 2.7698180812678746e-06, 'epoch': 0.13636363636363635}\n{'loss': 2.2751, 'grad_norm': 7.4968581199646, 'learning_rate': 3.3237816975214494e-06, 'epoch': 0.1590909090909091}\n{'loss': 2.2643, 'grad_norm': 7.063910961151123, 'learning_rate': 3.877745313775024e-06, 'epoch': 0.18181818181818182}\n{'loss': 2.2503, 'grad_norm': 7.628448009490967, 'learning_rate': 4.4317089300285995e-06, 'epoch': 0.20454545454545456}\n{'loss': 2.2562, 'grad_norm': 7.26869535446167, 'learning_rate': 4.985672546282174e-06, 'epoch': 0.22727272727272727}\n{'loss': 2.2355, 'grad_norm': 7.558175563812256, 'learning_rate': 5.539636162535749e-06, 'epoch': 0.25}\n{'loss': 2.2264, 'grad_norm': 7.128407001495361, 'learning_rate': 6.093599778789325e-06, 'epoch': 0.2727272727272727}\n{'loss': 2.2002, 'grad_norm': 7.362556457519531, 'learning_rate': 6.647563395042899e-06, 'epoch': 0.29545454545454547}\n{'loss': 2.1912, 'grad_norm': 7.542572975158691, 'learning_rate': 7.2015270112964745e-06, 'epoch': 0.3181818181818182}\n{'loss': 2.1848, 'grad_norm': 7.000339984893799, 'learning_rate': 7.755490627550048e-06, 'epoch': 0.3409090909090909}\n{'loss': 2.1467, 'grad_norm': 7.141875267028809, 'learning_rate': 8.309454243803624e-06, 'epoch': 0.36363636363636365}\n{'loss': 2.1262, 'grad_norm': 7.193033695220947, 'learning_rate': 8.863417860057199e-06, 'epoch': 0.38636363636363635}\n{'loss': 2.1233, 'grad_norm': 7.086674690246582, 'learning_rate': 9.417381476310774e-06, 'epoch': 0.4090909090909091}\n{'loss': 2.0926, 'grad_norm': 7.137402534484863, 'learning_rate': 9.971345092564349e-06, 'epoch': 0.4318181818181818}\n{'loss': 2.0737, 'grad_norm': 6.869791507720947, 'learning_rate': 1.0525308708817923e-05, 'epoch': 0.45454545454545453}\n{'loss': 2.0448, 'grad_norm': 6.904433250427246, 'learning_rate': 1.1079272325071498e-05, 'epoch': 0.4772727272727273}\n{'loss': 2.0168, 'grad_norm': 7.0833940505981445, 'learning_rate': 1.1633235941325073e-05, 'epoch': 0.5}\n{'loss': 1.9825, 'grad_norm': 6.838171005249023, 'learning_rate': 1.218719955757865e-05, 'epoch': 0.5227272727272727}\n{'loss': 1.9895, 'grad_norm': 6.47838020324707, 'learning_rate': 1.2741163173832223e-05, 'epoch': 0.5454545454545454}\n{'loss': 1.9535, 'grad_norm': 6.559394836425781, 'learning_rate': 1.3295126790085798e-05, 'epoch': 0.5681818181818182}\n{'loss': 1.9239, 'grad_norm': 6.674401760101318, 'learning_rate': 1.3849090406339372e-05, 'epoch': 0.5909090909090909}\n{'loss': 1.9075, 'grad_norm': 6.289106845855713, 'learning_rate': 1.4403054022592949e-05, 'epoch': 0.6136363636363636}\n{'loss': 1.8565, 'grad_norm': 6.520740985870361, 'learning_rate': 1.4957017638846524e-05, 'epoch': 0.6363636363636364}\n{'loss': 1.8316, 'grad_norm': 6.163884162902832, 'learning_rate': 1.5510981255100095e-05, 'epoch': 0.6590909090909091}\n{'loss': 1.7949, 'grad_norm': 5.9026336669921875, 'learning_rate': 1.6064944871353672e-05, 'epoch': 0.6818181818181818}\n{'loss': 1.7809, 'grad_norm': 5.874733924865723, 'learning_rate': 1.6618908487607248e-05, 'epoch': 0.7045454545454546}\n{'loss': 1.7312, 'grad_norm': 5.751068592071533, 'learning_rate': 1.717287210386082e-05, 'epoch': 0.7272727272727273}\n{'loss': 1.6993, 'grad_norm': 5.6026201248168945, 'learning_rate': 1.7726835720114398e-05, 'epoch': 0.75}\n{'loss': 1.6795, 'grad_norm': 5.465810298919678, 'learning_rate': 1.828079933636797e-05, 'epoch': 0.7727272727272727}\n{'loss': 1.6179, 'grad_norm': 5.366769790649414, 'learning_rate': 1.8834762952621548e-05, 'epoch': 0.7954545454545454}\n{'loss': 1.5912, 'grad_norm': 5.258598327636719, 'learning_rate': 1.938872656887512e-05, 'epoch': 0.8181818181818182}\n{'loss': 1.5589, 'grad_norm': 4.881351947784424, 'learning_rate': 1.9942690185128697e-05, 'epoch': 0.8409090909090909}\n{'loss': 1.5434, 'grad_norm': 4.578836441040039, 'learning_rate': 2.0496653801382274e-05, 'epoch': 0.8636363636363636}\n{'loss': 1.4958, 'grad_norm': 4.544497966766357, 'learning_rate': 2.1050617417635847e-05, 'epoch': 0.8863636363636364}\n{'loss': 1.4552, 'grad_norm': 4.275197982788086, 'learning_rate': 2.160458103388942e-05, 'epoch': 0.9090909090909091}\n{'loss': 1.4661, 'grad_norm': 3.7524049282073975, 'learning_rate': 2.2158544650142997e-05, 'epoch': 0.9318181818181818}\n{'loss': 1.3738, 'grad_norm': 3.735830307006836, 'learning_rate': 2.2096301547193157e-05, 'epoch': 0.9545454545454546}\n{'loss': 1.3952, 'grad_norm': 3.2268996238708496, 'learning_rate': 2.2034058444243317e-05, 'epoch': 0.9772727272727273}\n{'loss': 1.2681, 'grad_norm': 4.010343551635742, 'learning_rate': 2.1971815341293477e-05, 'epoch': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.0000    0.0000    0.0000      3404\n     PERCENT     0.0000    0.0000    0.0000        71\n        TYPE     0.5639    0.9190    0.6989     11194\n      VOLUME     0.0000    0.0000    0.0000        56\n\n   micro avg     0.5639    0.6986    0.6241     14725\n   macro avg     0.1410    0.2297    0.1747     14725\nweighted avg     0.4287    0.6986    0.5313     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 1.2959386110305786, 'eval_f1_macro': 0.17472908244726024, 'eval_precision': 0.5638875185002467, 'eval_recall': 0.6986078098471986, 'eval_f1': 0.6240596942489687, 'eval_accuracy': 0.6129474318916844, 'eval_runtime': 1.4497, 'eval_samples_per_second': 3801.481, 'eval_steps_per_second': 7.588, 'epoch': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 1.2989, 'grad_norm': 3.025005340576172, 'learning_rate': 2.1909572238343638e-05, 'epoch': 1.0227272727272727}\n{'loss': 1.2474, 'grad_norm': 2.863396406173706, 'learning_rate': 2.1847329135393798e-05, 'epoch': 1.0454545454545454}\n{'loss': 1.2608, 'grad_norm': 2.433645725250244, 'learning_rate': 2.1785086032443958e-05, 'epoch': 1.0681818181818181}\n{'loss': 1.255, 'grad_norm': 2.2121734619140625, 'learning_rate': 2.172284292949412e-05, 'epoch': 1.0909090909090908}\n{'loss': 1.292, 'grad_norm': 2.0407330989837646, 'learning_rate': 2.1660599826544275e-05, 'epoch': 1.1136363636363635}\n{'loss': 1.2194, 'grad_norm': 2.0114898681640625, 'learning_rate': 2.159835672359444e-05, 'epoch': 1.1363636363636362}\n{'loss': 1.195, 'grad_norm': 1.8542108535766602, 'learning_rate': 2.15361136206446e-05, 'epoch': 1.1590909090909092}\n{'loss': 1.1752, 'grad_norm': 1.7086447477340698, 'learning_rate': 2.1473870517694756e-05, 'epoch': 1.1818181818181819}\n{'loss': 1.1657, 'grad_norm': 1.7339030504226685, 'learning_rate': 2.141162741474492e-05, 'epoch': 1.2045454545454546}\n{'loss': 1.1894, 'grad_norm': 1.8001230955123901, 'learning_rate': 2.1349384311795076e-05, 'epoch': 1.2272727272727273}\n{'loss': 1.1526, 'grad_norm': 1.6599037647247314, 'learning_rate': 2.128714120884524e-05, 'epoch': 1.25}\n{'loss': 1.1409, 'grad_norm': 1.5282560586929321, 'learning_rate': 2.1224898105895397e-05, 'epoch': 1.2727272727272727}\n{'loss': 1.1529, 'grad_norm': 1.7150592803955078, 'learning_rate': 2.1162655002945557e-05, 'epoch': 1.2954545454545454}\n{'loss': 1.1008, 'grad_norm': 1.3987977504730225, 'learning_rate': 2.110041189999572e-05, 'epoch': 1.3181818181818181}\n{'loss': 1.0637, 'grad_norm': 1.2670207023620605, 'learning_rate': 2.1038168797045878e-05, 'epoch': 1.3409090909090908}\n{'loss': 1.1092, 'grad_norm': 1.4044572114944458, 'learning_rate': 2.097592569409604e-05, 'epoch': 1.3636363636363638}\n{'loss': 1.0389, 'grad_norm': 1.3192975521087646, 'learning_rate': 2.0913682591146198e-05, 'epoch': 1.3863636363636362}\n{'loss': 1.0308, 'grad_norm': 1.2400847673416138, 'learning_rate': 2.085143948819636e-05, 'epoch': 1.4090909090909092}\n{'loss': 1.0237, 'grad_norm': 1.2305976152420044, 'learning_rate': 2.078919638524652e-05, 'epoch': 1.4318181818181819}\n{'loss': 1.0578, 'grad_norm': 1.2027240991592407, 'learning_rate': 2.072695328229668e-05, 'epoch': 1.4545454545454546}\n{'loss': 0.9842, 'grad_norm': 1.2983375787734985, 'learning_rate': 2.066471017934684e-05, 'epoch': 1.4772727272727273}\n{'loss': 1.0111, 'grad_norm': 1.1593914031982422, 'learning_rate': 2.0602467076397e-05, 'epoch': 1.5}\n{'loss': 1.0105, 'grad_norm': 1.1826796531677246, 'learning_rate': 2.054022397344716e-05, 'epoch': 1.5227272727272727}\n{'loss': 1.0197, 'grad_norm': 1.1613736152648926, 'learning_rate': 2.047798087049732e-05, 'epoch': 1.5454545454545454}\n{'loss': 0.9967, 'grad_norm': 1.1863638162612915, 'learning_rate': 2.041573776754748e-05, 'epoch': 1.5681818181818183}\n{'loss': 1.0022, 'grad_norm': 1.2312884330749512, 'learning_rate': 2.035349466459764e-05, 'epoch': 1.5909090909090908}\n{'loss': 0.9629, 'grad_norm': 1.1886489391326904, 'learning_rate': 2.02912515616478e-05, 'epoch': 1.6136363636363638}\n{'loss': 0.9584, 'grad_norm': 1.1247496604919434, 'learning_rate': 2.022900845869796e-05, 'epoch': 1.6363636363636362}\n{'loss': 0.9789, 'grad_norm': 1.0507317781448364, 'learning_rate': 2.016676535574812e-05, 'epoch': 1.6590909090909092}\n{'loss': 0.9204, 'grad_norm': 1.1201022863388062, 'learning_rate': 2.010452225279828e-05, 'epoch': 1.6818181818181817}\n{'loss': 0.9231, 'grad_norm': 1.090299367904663, 'learning_rate': 2.004227914984844e-05, 'epoch': 1.7045454545454546}\n{'loss': 0.9571, 'grad_norm': 1.0443087816238403, 'learning_rate': 1.9980036046898602e-05, 'epoch': 1.7272727272727273}\n{'loss': 0.8913, 'grad_norm': 1.0903536081314087, 'learning_rate': 1.9917792943948762e-05, 'epoch': 1.75}\n{'loss': 0.8636, 'grad_norm': 1.143568992614746, 'learning_rate': 1.9855549840998922e-05, 'epoch': 1.7727272727272727}\n{'loss': 0.9162, 'grad_norm': 1.1052355766296387, 'learning_rate': 1.979330673804908e-05, 'epoch': 1.7954545454545454}\n{'loss': 0.8862, 'grad_norm': 0.9875248074531555, 'learning_rate': 1.9731063635099243e-05, 'epoch': 1.8181818181818183}\n{'loss': 0.8975, 'grad_norm': 1.0065860748291016, 'learning_rate': 1.96688205321494e-05, 'epoch': 1.8409090909090908}\n{'loss': 0.8584, 'grad_norm': 0.9874272346496582, 'learning_rate': 1.960657742919956e-05, 'epoch': 1.8636363636363638}\n{'loss': 0.9257, 'grad_norm': 1.0408871173858643, 'learning_rate': 1.9544334326249724e-05, 'epoch': 1.8863636363636362}\n{'loss': 0.8634, 'grad_norm': 1.012386441230774, 'learning_rate': 1.948209122329988e-05, 'epoch': 1.9090909090909092}\n{'loss': 0.826, 'grad_norm': 1.0173149108886719, 'learning_rate': 1.9419848120350044e-05, 'epoch': 1.9318181818181817}\n{'loss': 0.7657, 'grad_norm': 1.2206183671951294, 'learning_rate': 1.93576050174002e-05, 'epoch': 1.9545454545454546}\n{'loss': 0.8436, 'grad_norm': 1.1114312410354614, 'learning_rate': 1.929536191445036e-05, 'epoch': 1.9772727272727273}\n{'loss': 0.7349, 'grad_norm': 2.178640127182007, 'learning_rate': 1.923311881150052e-05, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.6971    0.4509    0.5476      3404\n     PERCENT     0.0000    0.0000    0.0000        71\n        TYPE     0.6705    0.8979    0.7677     11194\n      VOLUME     0.0000    0.0000    0.0000        56\n\n   micro avg     0.6739    0.7868    0.7260     14725\n   macro avg     0.3419    0.3372    0.3288     14725\nweighted avg     0.6709    0.7868    0.7102     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.8004461526870728, 'eval_f1_macro': 0.3288370718532008, 'eval_precision': 0.6739181014425314, 'eval_recall': 0.7868251273344652, 'eval_f1': 0.7260080834664913, 'eval_accuracy': 0.7351312832319246, 'eval_runtime': 1.447, 'eval_samples_per_second': 3808.692, 'eval_steps_per_second': 7.602, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.7807, 'grad_norm': 1.137439250946045, 'learning_rate': 1.917087570855068e-05, 'epoch': 2.022727272727273}\n{'loss': 0.8204, 'grad_norm': 0.922264575958252, 'learning_rate': 1.9108632605600845e-05, 'epoch': 2.0454545454545454}\n{'loss': 0.81, 'grad_norm': 0.9682980179786682, 'learning_rate': 1.9046389502651002e-05, 'epoch': 2.0681818181818183}\n{'loss': 0.794, 'grad_norm': 0.8952094912528992, 'learning_rate': 1.8984146399701162e-05, 'epoch': 2.090909090909091}\n{'loss': 0.7832, 'grad_norm': 0.9152619242668152, 'learning_rate': 1.8921903296751323e-05, 'epoch': 2.1136363636363638}\n{'loss': 0.7491, 'grad_norm': 0.9985103011131287, 'learning_rate': 1.8859660193801483e-05, 'epoch': 2.1363636363636362}\n{'loss': 0.7967, 'grad_norm': 0.8657568097114563, 'learning_rate': 1.8797417090851643e-05, 'epoch': 2.159090909090909}\n{'loss': 0.8093, 'grad_norm': 0.8943018913269043, 'learning_rate': 1.8735173987901803e-05, 'epoch': 2.1818181818181817}\n{'loss': 0.7672, 'grad_norm': 0.8839656710624695, 'learning_rate': 1.8672930884951964e-05, 'epoch': 2.2045454545454546}\n{'loss': 0.74, 'grad_norm': 0.8912544250488281, 'learning_rate': 1.8610687782002124e-05, 'epoch': 2.227272727272727}\n{'loss': 0.8254, 'grad_norm': 1.1443853378295898, 'learning_rate': 1.8548444679052284e-05, 'epoch': 2.25}\n{'loss': 0.7156, 'grad_norm': 0.8939151763916016, 'learning_rate': 1.8486201576102444e-05, 'epoch': 2.2727272727272725}\n{'loss': 0.7034, 'grad_norm': 0.8899005651473999, 'learning_rate': 1.8423958473152605e-05, 'epoch': 2.2954545454545454}\n{'loss': 0.717, 'grad_norm': 0.8567277789115906, 'learning_rate': 1.8361715370202765e-05, 'epoch': 2.3181818181818183}\n{'loss': 0.7547, 'grad_norm': 0.8060570955276489, 'learning_rate': 1.8299472267252925e-05, 'epoch': 2.340909090909091}\n{'loss': 0.7412, 'grad_norm': 0.7599724531173706, 'learning_rate': 1.8237229164303082e-05, 'epoch': 2.3636363636363638}\n{'loss': 0.7036, 'grad_norm': 0.863327145576477, 'learning_rate': 1.8174986061353246e-05, 'epoch': 2.3863636363636362}\n{'loss': 0.6991, 'grad_norm': 0.909111738204956, 'learning_rate': 1.8112742958403406e-05, 'epoch': 2.409090909090909}\n{'loss': 0.6899, 'grad_norm': 0.9366584420204163, 'learning_rate': 1.8050499855453566e-05, 'epoch': 2.4318181818181817}\n{'loss': 0.6674, 'grad_norm': 0.8810292482376099, 'learning_rate': 1.7988256752503726e-05, 'epoch': 2.4545454545454546}\n{'loss': 0.6835, 'grad_norm': 0.83709716796875, 'learning_rate': 1.7926013649553883e-05, 'epoch': 2.4772727272727275}\n{'loss': 0.6353, 'grad_norm': 0.8767431378364563, 'learning_rate': 1.7863770546604047e-05, 'epoch': 2.5}\n{'loss': 0.6901, 'grad_norm': 0.9555135369300842, 'learning_rate': 1.7801527443654204e-05, 'epoch': 2.5227272727272725}\n{'loss': 0.6637, 'grad_norm': 0.8448939323425293, 'learning_rate': 1.7739284340704364e-05, 'epoch': 2.5454545454545454}\n{'loss': 0.7163, 'grad_norm': 0.8783878684043884, 'learning_rate': 1.7677041237754524e-05, 'epoch': 2.5681818181818183}\n{'loss': 0.6866, 'grad_norm': 0.8503130674362183, 'learning_rate': 1.7614798134804684e-05, 'epoch': 2.590909090909091}\n{'loss': 0.6809, 'grad_norm': 0.7661965489387512, 'learning_rate': 1.7552555031854848e-05, 'epoch': 2.6136363636363638}\n{'loss': 0.6175, 'grad_norm': 0.8243811726570129, 'learning_rate': 1.7490311928905005e-05, 'epoch': 2.6363636363636362}\n{'loss': 0.6177, 'grad_norm': 0.8053113222122192, 'learning_rate': 1.7428068825955165e-05, 'epoch': 2.659090909090909}\n{'loss': 0.655, 'grad_norm': 0.8854163289070129, 'learning_rate': 1.7365825723005325e-05, 'epoch': 2.6818181818181817}\n{'loss': 0.6741, 'grad_norm': 0.753476083278656, 'learning_rate': 1.7303582620055486e-05, 'epoch': 2.7045454545454546}\n{'loss': 0.6064, 'grad_norm': 0.7889732718467712, 'learning_rate': 1.7241339517105646e-05, 'epoch': 2.7272727272727275}\n{'loss': 0.6644, 'grad_norm': 0.778719425201416, 'learning_rate': 1.7179096414155806e-05, 'epoch': 2.75}\n{'loss': 0.7074, 'grad_norm': 0.9426801800727844, 'learning_rate': 1.7116853311205966e-05, 'epoch': 2.7727272727272725}\n{'loss': 0.6152, 'grad_norm': 0.7577324509620667, 'learning_rate': 1.7054610208256127e-05, 'epoch': 2.7954545454545454}\n{'loss': 0.6309, 'grad_norm': 0.9473270177841187, 'learning_rate': 1.6992367105306287e-05, 'epoch': 2.8181818181818183}\n{'loss': 0.6405, 'grad_norm': 0.9192147850990295, 'learning_rate': 1.6930124002356447e-05, 'epoch': 2.840909090909091}\n{'loss': 0.7364, 'grad_norm': 0.9093042016029358, 'learning_rate': 1.6867880899406607e-05, 'epoch': 2.8636363636363638}\n{'loss': 0.6474, 'grad_norm': 0.7957682013511658, 'learning_rate': 1.6805637796456768e-05, 'epoch': 2.8863636363636362}\n{'loss': 0.6186, 'grad_norm': 0.8352212905883789, 'learning_rate': 1.6743394693506928e-05, 'epoch': 2.909090909090909}\n{'loss': 0.5848, 'grad_norm': 1.0574079751968384, 'learning_rate': 1.6681151590557085e-05, 'epoch': 2.9318181818181817}\n{'loss': 0.6531, 'grad_norm': 0.7292535901069641, 'learning_rate': 1.6618908487607248e-05, 'epoch': 2.9545454545454546}\n{'loss': 0.5917, 'grad_norm': 1.0760341882705688, 'learning_rate': 1.655666538465741e-05, 'epoch': 2.9772727272727275}\n{'loss': 0.4746, 'grad_norm': 2.169631004333496, 'learning_rate': 1.649442228170757e-05, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.6985    0.6228    0.6585      3404\n     PERCENT     1.0000    0.0423    0.0811        71\n        TYPE     0.8153    0.9058    0.8582     11194\n      VOLUME     0.0000    0.0000    0.0000        56\n\n   micro avg     0.7924    0.8328    0.8121     14725\n   macro avg     0.6285    0.3927    0.3994     14725\nweighted avg     0.7861    0.8328    0.8050     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.6120673418045044, 'eval_f1_macro': 0.39944079200702964, 'eval_precision': 0.7924394184168013, 'eval_recall': 0.8328013582342955, 'eval_f1': 0.8121192052980133, 'eval_accuracy': 0.818396097133147, 'eval_runtime': 1.4397, 'eval_samples_per_second': 3827.918, 'eval_steps_per_second': 7.641, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.6246, 'grad_norm': 1.0133628845214844, 'learning_rate': 1.643217917875773e-05, 'epoch': 3.022727272727273}\n{'loss': 0.6172, 'grad_norm': 0.7980997562408447, 'learning_rate': 1.6369936075807886e-05, 'epoch': 3.0454545454545454}\n{'loss': 0.6397, 'grad_norm': 0.6928749084472656, 'learning_rate': 1.630769297285805e-05, 'epoch': 3.0681818181818183}\n{'loss': 0.546, 'grad_norm': 1.1430339813232422, 'learning_rate': 1.6245449869908206e-05, 'epoch': 3.090909090909091}\n{'loss': 0.5255, 'grad_norm': 0.7540177702903748, 'learning_rate': 1.618320676695837e-05, 'epoch': 3.1136363636363638}\n{'loss': 0.647, 'grad_norm': 0.8193257451057434, 'learning_rate': 1.6120963664008527e-05, 'epoch': 3.1363636363636362}\n{'loss': 0.6376, 'grad_norm': 1.1197946071624756, 'learning_rate': 1.6058720561058687e-05, 'epoch': 3.159090909090909}\n{'loss': 0.6183, 'grad_norm': 1.1695042848587036, 'learning_rate': 1.599647745810885e-05, 'epoch': 3.1818181818181817}\n{'loss': 0.5631, 'grad_norm': 0.7627506256103516, 'learning_rate': 1.5934234355159008e-05, 'epoch': 3.2045454545454546}\n{'loss': 0.6088, 'grad_norm': 1.2896106243133545, 'learning_rate': 1.5871991252209168e-05, 'epoch': 3.227272727272727}\n{'loss': 0.5268, 'grad_norm': 0.8141274452209473, 'learning_rate': 1.5809748149259328e-05, 'epoch': 3.25}\n{'loss': 0.5379, 'grad_norm': 0.8245667219161987, 'learning_rate': 1.574750504630949e-05, 'epoch': 3.2727272727272725}\n{'loss': 0.6317, 'grad_norm': 1.051306962966919, 'learning_rate': 1.568526194335965e-05, 'epoch': 3.2954545454545454}\n{'loss': 0.5828, 'grad_norm': 0.7238636016845703, 'learning_rate': 1.562301884040981e-05, 'epoch': 3.3181818181818183}\n{'loss': 0.5436, 'grad_norm': 0.8309585452079773, 'learning_rate': 1.556077573745997e-05, 'epoch': 3.340909090909091}\n{'loss': 0.5693, 'grad_norm': 0.859013020992279, 'learning_rate': 1.549853263451013e-05, 'epoch': 3.3636363636363638}\n{'loss': 0.6407, 'grad_norm': 0.7185022234916687, 'learning_rate': 1.543628953156029e-05, 'epoch': 3.3863636363636362}\n{'loss': 0.5869, 'grad_norm': 0.8155952095985413, 'learning_rate': 1.537404642861045e-05, 'epoch': 3.409090909090909}\n{'loss': 0.5813, 'grad_norm': 0.850943386554718, 'learning_rate': 1.531180332566061e-05, 'epoch': 3.4318181818181817}\n{'loss': 0.5672, 'grad_norm': 0.7442553043365479, 'learning_rate': 1.5249560222710769e-05, 'epoch': 3.4545454545454546}\n{'loss': 0.6172, 'grad_norm': 0.8633655905723572, 'learning_rate': 1.518731711976093e-05, 'epoch': 3.4772727272727275}\n{'loss': 0.4905, 'grad_norm': 1.2746349573135376, 'learning_rate': 1.5125074016811089e-05, 'epoch': 3.5}\n{'loss': 0.5875, 'grad_norm': 0.7857603430747986, 'learning_rate': 1.5062830913861251e-05, 'epoch': 3.5227272727272725}\n{'loss': 0.5649, 'grad_norm': 0.7750363349914551, 'learning_rate': 1.5000587810911411e-05, 'epoch': 3.5454545454545454}\n{'loss': 0.5912, 'grad_norm': 0.8969106078147888, 'learning_rate': 1.493834470796157e-05, 'epoch': 3.5681818181818183}\n{'loss': 0.574, 'grad_norm': 0.6676210761070251, 'learning_rate': 1.4876101605011732e-05, 'epoch': 3.590909090909091}\n{'loss': 0.5449, 'grad_norm': 0.7116690278053284, 'learning_rate': 1.481385850206189e-05, 'epoch': 3.6136363636363638}\n{'loss': 0.5597, 'grad_norm': 0.6344314217567444, 'learning_rate': 1.4751615399112052e-05, 'epoch': 3.6363636363636362}\n{'loss': 0.5027, 'grad_norm': 0.6420184373855591, 'learning_rate': 1.468937229616221e-05, 'epoch': 3.659090909090909}\n{'loss': 0.5484, 'grad_norm': 1.195981502532959, 'learning_rate': 1.4627129193212371e-05, 'epoch': 3.6818181818181817}\n{'loss': 0.6266, 'grad_norm': 1.1794137954711914, 'learning_rate': 1.4564886090262533e-05, 'epoch': 3.7045454545454546}\n{'loss': 0.5518, 'grad_norm': 0.6753103733062744, 'learning_rate': 1.4502642987312692e-05, 'epoch': 3.7272727272727275}\n{'loss': 0.5069, 'grad_norm': 0.7445498704910278, 'learning_rate': 1.4440399884362852e-05, 'epoch': 3.75}\n{'loss': 0.4943, 'grad_norm': 1.026357650756836, 'learning_rate': 1.4378156781413012e-05, 'epoch': 3.7727272727272725}\n{'loss': 0.513, 'grad_norm': 0.8618201613426208, 'learning_rate': 1.4315913678463172e-05, 'epoch': 3.7954545454545454}\n{'loss': 0.5274, 'grad_norm': 0.7237940430641174, 'learning_rate': 1.4253670575513331e-05, 'epoch': 3.8181818181818183}\n{'loss': 0.5505, 'grad_norm': 0.919182300567627, 'learning_rate': 1.4191427472563493e-05, 'epoch': 3.840909090909091}\n{'loss': 0.5671, 'grad_norm': 0.7521100044250488, 'learning_rate': 1.4129184369613651e-05, 'epoch': 3.8636363636363638}\n{'loss': 0.5634, 'grad_norm': 0.7559042572975159, 'learning_rate': 1.4066941266663812e-05, 'epoch': 3.8863636363636362}\n{'loss': 0.5376, 'grad_norm': 0.7419888377189636, 'learning_rate': 1.4004698163713974e-05, 'epoch': 3.909090909090909}\n{'loss': 0.4941, 'grad_norm': 0.9714173674583435, 'learning_rate': 1.3942455060764132e-05, 'epoch': 3.9318181818181817}\n{'loss': 0.5334, 'grad_norm': 0.6917082667350769, 'learning_rate': 1.3880211957814294e-05, 'epoch': 3.9545454545454546}\n{'loss': 0.5065, 'grad_norm': 1.0255485773086548, 'learning_rate': 1.3817968854864453e-05, 'epoch': 3.9772727272727275}\n{'loss': 0.5885, 'grad_norm': 2.4554762840270996, 'learning_rate': 1.3755725751914613e-05, 'epoch': 4.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.7443    0.6457    0.6915      3404\n     PERCENT     1.0000    0.3099    0.4731        71\n        TYPE     0.8499    0.9273    0.8869     11194\n      VOLUME     0.0000    0.0000    0.0000        56\n\n   micro avg     0.8296    0.8557    0.8424     14725\n   macro avg     0.6486    0.4707    0.5129     14725\nweighted avg     0.8230    0.8557    0.8364     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.5325341820716858, 'eval_f1_macro': 0.5128884020808034, 'eval_precision': 0.829602317619173, 'eval_recall': 0.8556876061120543, 'eval_f1': 0.8424430849463445, 'eval_accuracy': 0.8423504905991339, 'eval_runtime': 1.4529, 'eval_samples_per_second': 3793.005, 'eval_steps_per_second': 7.571, 'epoch': 4.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.5857, 'grad_norm': 0.7979321479797363, 'learning_rate': 1.3693482648964771e-05, 'epoch': 4.0227272727272725}\n{'loss': 0.5574, 'grad_norm': 0.7332891821861267, 'learning_rate': 1.3631239546014933e-05, 'epoch': 4.045454545454546}\n{'loss': 0.5391, 'grad_norm': 0.7311596870422363, 'learning_rate': 1.3568996443065095e-05, 'epoch': 4.068181818181818}\n{'loss': 0.4998, 'grad_norm': 0.7617766857147217, 'learning_rate': 1.3506753340115254e-05, 'epoch': 4.090909090909091}\n{'loss': 0.4772, 'grad_norm': 0.8938035368919373, 'learning_rate': 1.3444510237165414e-05, 'epoch': 4.113636363636363}\n{'loss': 0.5109, 'grad_norm': 0.9074878692626953, 'learning_rate': 1.3382267134215573e-05, 'epoch': 4.136363636363637}\n{'loss': 0.5836, 'grad_norm': 0.9566583633422852, 'learning_rate': 1.3320024031265735e-05, 'epoch': 4.159090909090909}\n{'loss': 0.527, 'grad_norm': 0.8404104113578796, 'learning_rate': 1.3257780928315893e-05, 'epoch': 4.181818181818182}\n{'loss': 0.5378, 'grad_norm': 0.7896676063537598, 'learning_rate': 1.3195537825366055e-05, 'epoch': 4.204545454545454}\n{'loss': 0.5587, 'grad_norm': 0.75262850522995, 'learning_rate': 1.3133294722416214e-05, 'epoch': 4.2272727272727275}\n{'loss': 0.549, 'grad_norm': 0.8863765597343445, 'learning_rate': 1.3071051619466374e-05, 'epoch': 4.25}\n{'loss': 0.4776, 'grad_norm': 0.8833293318748474, 'learning_rate': 1.3008808516516536e-05, 'epoch': 4.2727272727272725}\n{'loss': 0.4839, 'grad_norm': 1.1872526407241821, 'learning_rate': 1.2946565413566694e-05, 'epoch': 4.295454545454546}\n{'loss': 0.5044, 'grad_norm': 0.6716181039810181, 'learning_rate': 1.2884322310616856e-05, 'epoch': 4.318181818181818}\n{'loss': 0.4528, 'grad_norm': 0.874366819858551, 'learning_rate': 1.2822079207667015e-05, 'epoch': 4.340909090909091}\n{'loss': 0.5479, 'grad_norm': 0.9056442379951477, 'learning_rate': 1.2759836104717175e-05, 'epoch': 4.363636363636363}\n{'loss': 0.5194, 'grad_norm': 1.0897375345230103, 'learning_rate': 1.2697593001767334e-05, 'epoch': 4.386363636363637}\n{'loss': 0.5203, 'grad_norm': 0.9199755191802979, 'learning_rate': 1.2635349898817496e-05, 'epoch': 4.409090909090909}\n{'loss': 0.4741, 'grad_norm': 0.7067906856536865, 'learning_rate': 1.2573106795867656e-05, 'epoch': 4.431818181818182}\n{'loss': 0.4668, 'grad_norm': 0.8230502009391785, 'learning_rate': 1.2510863692917816e-05, 'epoch': 4.454545454545454}\n{'loss': 0.4943, 'grad_norm': 0.7267978191375732, 'learning_rate': 1.2448620589967976e-05, 'epoch': 4.4772727272727275}\n{'loss': 0.4619, 'grad_norm': 0.9008375406265259, 'learning_rate': 1.2386377487018135e-05, 'epoch': 4.5}\n{'loss': 0.4963, 'grad_norm': 0.9763143062591553, 'learning_rate': 1.2324134384068297e-05, 'epoch': 4.5227272727272725}\n{'loss': 0.5178, 'grad_norm': 0.7724621891975403, 'learning_rate': 1.2261891281118455e-05, 'epoch': 4.545454545454545}\n{'loss': 0.506, 'grad_norm': 0.9773206114768982, 'learning_rate': 1.2199648178168616e-05, 'epoch': 4.568181818181818}\n{'loss': 0.5102, 'grad_norm': 0.7921117544174194, 'learning_rate': 1.2137405075218776e-05, 'epoch': 4.590909090909091}\n{'loss': 0.5135, 'grad_norm': 0.7327098846435547, 'learning_rate': 1.2075161972268936e-05, 'epoch': 4.613636363636363}\n{'loss': 0.4541, 'grad_norm': 0.7359855771064758, 'learning_rate': 1.2012918869319098e-05, 'epoch': 4.636363636363637}\n{'loss': 0.4359, 'grad_norm': 0.9385747909545898, 'learning_rate': 1.1950675766369257e-05, 'epoch': 4.659090909090909}\n{'loss': 0.5734, 'grad_norm': 1.075441837310791, 'learning_rate': 1.1888432663419417e-05, 'epoch': 4.681818181818182}\n{'loss': 0.4738, 'grad_norm': 0.9512227773666382, 'learning_rate': 1.1826189560469577e-05, 'epoch': 4.704545454545455}\n{'loss': 0.4734, 'grad_norm': 0.9359812140464783, 'learning_rate': 1.1763946457519737e-05, 'epoch': 4.7272727272727275}\n{'loss': 0.565, 'grad_norm': 0.8302521109580994, 'learning_rate': 1.1701703354569896e-05, 'epoch': 4.75}\n{'loss': 0.4779, 'grad_norm': 0.8443398475646973, 'learning_rate': 1.1639460251620058e-05, 'epoch': 4.7727272727272725}\n{'loss': 0.4324, 'grad_norm': 0.8416856527328491, 'learning_rate': 1.1577217148670218e-05, 'epoch': 4.795454545454545}\n{'loss': 0.5492, 'grad_norm': 1.120753526687622, 'learning_rate': 1.1514974045720377e-05, 'epoch': 4.818181818181818}\n{'loss': 0.4816, 'grad_norm': 0.807553768157959, 'learning_rate': 1.1452730942770539e-05, 'epoch': 4.840909090909091}\n{'loss': 0.4418, 'grad_norm': 0.749564528465271, 'learning_rate': 1.1390487839820697e-05, 'epoch': 4.863636363636363}\n{'loss': 0.4599, 'grad_norm': 0.9669403433799744, 'learning_rate': 1.1328244736870859e-05, 'epoch': 4.886363636363637}\n{'loss': 0.4483, 'grad_norm': 0.838720977306366, 'learning_rate': 1.1266001633921018e-05, 'epoch': 4.909090909090909}\n{'loss': 0.5084, 'grad_norm': 0.9205508828163147, 'learning_rate': 1.1203758530971178e-05, 'epoch': 4.931818181818182}\n{'loss': 0.4478, 'grad_norm': 0.7732610106468201, 'learning_rate': 1.1141515428021336e-05, 'epoch': 4.954545454545455}\n{'loss': 0.492, 'grad_norm': 0.705941379070282, 'learning_rate': 1.1079272325071498e-05, 'epoch': 4.9772727272727275}\n{'loss': 0.3357, 'grad_norm': 4.516063213348389, 'learning_rate': 1.1017029222121659e-05, 'epoch': 5.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.7683    0.6672    0.7142      3404\n     PERCENT     0.8889    0.6761    0.7680        71\n        TYPE     0.8622    0.9410    0.8999     11194\n      VOLUME     0.0000    0.0000    0.0000        56\n\n   micro avg     0.8441    0.8729    0.8582     14725\n   macro avg     0.6298    0.5711    0.5955     14725\nweighted avg     0.8374    0.8729    0.8529     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.4862658381462097, 'eval_f1_macro': 0.5955174462416071, 'eval_precision': 0.8440927300190452, 'eval_recall': 0.8728692699490662, 'eval_f1': 0.8582398504273504, 'eval_accuracy': 0.8561640081127008, 'eval_runtime': 1.5034, 'eval_samples_per_second': 3665.668, 'eval_steps_per_second': 7.317, 'epoch': 5.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.4976, 'grad_norm': 0.7928628921508789, 'learning_rate': 1.0954786119171819e-05, 'epoch': 5.0227272727272725}\n{'loss': 0.5528, 'grad_norm': 1.065647006034851, 'learning_rate': 1.0892543016221979e-05, 'epoch': 5.045454545454546}\n{'loss': 0.468, 'grad_norm': 0.6603977084159851, 'learning_rate': 1.0830299913272138e-05, 'epoch': 5.068181818181818}\n{'loss': 0.5377, 'grad_norm': 1.0255112648010254, 'learning_rate': 1.07680568103223e-05, 'epoch': 5.090909090909091}\n{'loss': 0.4888, 'grad_norm': 0.6907525658607483, 'learning_rate': 1.070581370737246e-05, 'epoch': 5.113636363636363}\n{'loss': 0.4597, 'grad_norm': 0.8080713152885437, 'learning_rate': 1.064357060442262e-05, 'epoch': 5.136363636363637}\n{'loss': 0.4965, 'grad_norm': 0.7083708643913269, 'learning_rate': 1.0581327501472779e-05, 'epoch': 5.159090909090909}\n{'loss': 0.4296, 'grad_norm': 1.0131077766418457, 'learning_rate': 1.0519084398522939e-05, 'epoch': 5.181818181818182}\n{'loss': 0.4995, 'grad_norm': 0.7545952200889587, 'learning_rate': 1.0456841295573099e-05, 'epoch': 5.204545454545454}\n{'loss': 0.4438, 'grad_norm': 0.7307749390602112, 'learning_rate': 1.039459819262326e-05, 'epoch': 5.2272727272727275}\n{'loss': 0.4627, 'grad_norm': 0.9016637802124023, 'learning_rate': 1.033235508967342e-05, 'epoch': 5.25}\n{'loss': 0.4801, 'grad_norm': 0.9117031097412109, 'learning_rate': 1.027011198672358e-05, 'epoch': 5.2727272727272725}\n{'loss': 0.5034, 'grad_norm': 0.994254469871521, 'learning_rate': 1.020786888377374e-05, 'epoch': 5.295454545454546}\n{'loss': 0.4574, 'grad_norm': 0.8236084580421448, 'learning_rate': 1.01456257808239e-05, 'epoch': 5.318181818181818}\n{'loss': 0.5027, 'grad_norm': 1.1438803672790527, 'learning_rate': 1.008338267787406e-05, 'epoch': 5.340909090909091}\n{'loss': 0.4579, 'grad_norm': 0.828983724117279, 'learning_rate': 1.002113957492422e-05, 'epoch': 5.363636363636363}\n{'loss': 0.4857, 'grad_norm': 0.9770885705947876, 'learning_rate': 9.958896471974381e-06, 'epoch': 5.386363636363637}\n{'loss': 0.4303, 'grad_norm': 0.7144597768783569, 'learning_rate': 9.89665336902454e-06, 'epoch': 5.409090909090909}\n{'loss': 0.486, 'grad_norm': 0.7315683364868164, 'learning_rate': 9.8344102660747e-06, 'epoch': 5.431818181818182}\n{'loss': 0.4685, 'grad_norm': 0.8697793483734131, 'learning_rate': 9.772167163124862e-06, 'epoch': 5.454545454545454}\n{'loss': 0.5203, 'grad_norm': 1.0166490077972412, 'learning_rate': 9.709924060175022e-06, 'epoch': 5.4772727272727275}\n{'loss': 0.4173, 'grad_norm': 0.9114461541175842, 'learning_rate': 9.64768095722518e-06, 'epoch': 5.5}\n{'loss': 0.4564, 'grad_norm': 0.9902899265289307, 'learning_rate': 9.58543785427534e-06, 'epoch': 5.5227272727272725}\n{'loss': 0.4561, 'grad_norm': 0.9570764899253845, 'learning_rate': 9.523194751325501e-06, 'epoch': 5.545454545454545}\n{'loss': 0.4172, 'grad_norm': 0.8686565160751343, 'learning_rate': 9.460951648375661e-06, 'epoch': 5.568181818181818}\n{'loss': 0.4717, 'grad_norm': 0.7696423530578613, 'learning_rate': 9.398708545425822e-06, 'epoch': 5.590909090909091}\n{'loss': 0.4611, 'grad_norm': 1.0197420120239258, 'learning_rate': 9.336465442475982e-06, 'epoch': 5.613636363636363}\n{'loss': 0.4605, 'grad_norm': 0.7808334231376648, 'learning_rate': 9.274222339526142e-06, 'epoch': 5.636363636363637}\n{'loss': 0.4573, 'grad_norm': 0.7574976682662964, 'learning_rate': 9.211979236576302e-06, 'epoch': 5.659090909090909}\n{'loss': 0.4654, 'grad_norm': 1.015461802482605, 'learning_rate': 9.149736133626463e-06, 'epoch': 5.681818181818182}\n{'loss': 0.4297, 'grad_norm': 0.8359457850456238, 'learning_rate': 9.087493030676623e-06, 'epoch': 5.704545454545455}\n{'loss': 0.4617, 'grad_norm': 1.4142998456954956, 'learning_rate': 9.025249927726783e-06, 'epoch': 5.7272727272727275}\n{'loss': 0.4504, 'grad_norm': 1.2781461477279663, 'learning_rate': 8.963006824776942e-06, 'epoch': 5.75}\n{'loss': 0.4232, 'grad_norm': 0.8747126460075378, 'learning_rate': 8.900763721827102e-06, 'epoch': 5.7727272727272725}\n{'loss': 0.4321, 'grad_norm': 0.7684568166732788, 'learning_rate': 8.838520618877262e-06, 'epoch': 5.795454545454545}\n{'loss': 0.3849, 'grad_norm': 1.091905117034912, 'learning_rate': 8.776277515927424e-06, 'epoch': 5.818181818181818}\n{'loss': 0.4478, 'grad_norm': 0.7638680934906006, 'learning_rate': 8.714034412977583e-06, 'epoch': 5.840909090909091}\n{'loss': 0.4376, 'grad_norm': 0.7036575078964233, 'learning_rate': 8.651791310027743e-06, 'epoch': 5.863636363636363}\n{'loss': 0.4353, 'grad_norm': 0.7498524785041809, 'learning_rate': 8.589548207077903e-06, 'epoch': 5.886363636363637}\n{'loss': 0.4616, 'grad_norm': 0.8624171614646912, 'learning_rate': 8.527305104128063e-06, 'epoch': 5.909090909090909}\n{'loss': 0.4433, 'grad_norm': 0.6817734241485596, 'learning_rate': 8.465062001178224e-06, 'epoch': 5.931818181818182}\n{'loss': 0.4673, 'grad_norm': 1.6431254148483276, 'learning_rate': 8.402818898228384e-06, 'epoch': 5.954545454545455}\n{'loss': 0.4815, 'grad_norm': 1.016007900238037, 'learning_rate': 8.340575795278542e-06, 'epoch': 5.9772727272727275}\n{'loss': 0.1846, 'grad_norm': 2.440319299697876, 'learning_rate': 8.278332692328704e-06, 'epoch': 6.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8097    0.6575    0.7257      3404\n     PERCENT     0.7867    0.8310    0.8082        71\n        TYPE     0.8645    0.9488    0.9047     11194\n      VOLUME     0.0000    0.0000    0.0000        56\n\n   micro avg     0.8540    0.8773    0.8655     14725\n   macro avg     0.6152    0.6093    0.6096     14725\nweighted avg     0.8482    0.8773    0.8594     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.4675753116607666, 'eval_f1_macro': 0.609646237523617, 'eval_precision': 0.8540261800872669, 'eval_recall': 0.877283531409168, 'eval_f1': 0.8654986432615323, 'eval_accuracy': 0.862412980321219, 'eval_runtime': 1.4466, 'eval_samples_per_second': 3809.522, 'eval_steps_per_second': 7.604, 'epoch': 6.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.4134, 'grad_norm': 0.8800238370895386, 'learning_rate': 8.216089589378865e-06, 'epoch': 6.0227272727272725}\n{'loss': 0.4959, 'grad_norm': 1.1398118734359741, 'learning_rate': 8.153846486429025e-06, 'epoch': 6.045454545454546}\n{'loss': 0.4309, 'grad_norm': 0.8521592617034912, 'learning_rate': 8.091603383479185e-06, 'epoch': 6.068181818181818}\n{'loss': 0.3911, 'grad_norm': 0.7493093013763428, 'learning_rate': 8.029360280529344e-06, 'epoch': 6.090909090909091}\n{'loss': 0.4592, 'grad_norm': 0.8469979763031006, 'learning_rate': 7.967117177579504e-06, 'epoch': 6.113636363636363}\n{'loss': 0.3706, 'grad_norm': 1.267510175704956, 'learning_rate': 7.904874074629664e-06, 'epoch': 6.136363636363637}\n{'loss': 0.4093, 'grad_norm': 0.7703683972358704, 'learning_rate': 7.842630971679824e-06, 'epoch': 6.159090909090909}\n{'loss': 0.4779, 'grad_norm': 0.8533309698104858, 'learning_rate': 7.780387868729985e-06, 'epoch': 6.181818181818182}\n{'loss': 0.4724, 'grad_norm': 0.7915840148925781, 'learning_rate': 7.718144765780145e-06, 'epoch': 6.204545454545454}\n{'loss': 0.3841, 'grad_norm': 0.9489777684211731, 'learning_rate': 7.655901662830305e-06, 'epoch': 6.2272727272727275}\n{'loss': 0.4305, 'grad_norm': 0.6114952564239502, 'learning_rate': 7.593658559880465e-06, 'epoch': 6.25}\n{'loss': 0.4445, 'grad_norm': 0.8530318737030029, 'learning_rate': 7.5314154569306255e-06, 'epoch': 6.2727272727272725}\n{'loss': 0.4578, 'grad_norm': 0.7953078746795654, 'learning_rate': 7.469172353980785e-06, 'epoch': 6.295454545454546}\n{'loss': 0.4257, 'grad_norm': 0.8024438619613647, 'learning_rate': 7.406929251030945e-06, 'epoch': 6.318181818181818}\n{'loss': 0.4569, 'grad_norm': 0.7990128397941589, 'learning_rate': 7.344686148081105e-06, 'epoch': 6.340909090909091}\n{'loss': 0.4092, 'grad_norm': 0.7762005925178528, 'learning_rate': 7.2824430451312665e-06, 'epoch': 6.363636363636363}\n{'loss': 0.4317, 'grad_norm': 0.7470712065696716, 'learning_rate': 7.220199942181426e-06, 'epoch': 6.386363636363637}\n{'loss': 0.4235, 'grad_norm': 1.2971704006195068, 'learning_rate': 7.157956839231586e-06, 'epoch': 6.409090909090909}\n{'loss': 0.4321, 'grad_norm': 0.8418457508087158, 'learning_rate': 7.095713736281746e-06, 'epoch': 6.431818181818182}\n{'loss': 0.4656, 'grad_norm': 0.9029961824417114, 'learning_rate': 7.033470633331906e-06, 'epoch': 6.454545454545454}\n{'loss': 0.4792, 'grad_norm': 1.0352579355239868, 'learning_rate': 6.971227530382066e-06, 'epoch': 6.4772727272727275}\n{'loss': 0.4529, 'grad_norm': 0.9165645837783813, 'learning_rate': 6.908984427432226e-06, 'epoch': 6.5}\n{'loss': 0.3964, 'grad_norm': 0.7091516852378845, 'learning_rate': 6.846741324482386e-06, 'epoch': 6.5227272727272725}\n{'loss': 0.4436, 'grad_norm': 0.8542049527168274, 'learning_rate': 6.784498221532548e-06, 'epoch': 6.545454545454545}\n{'loss': 0.4129, 'grad_norm': 0.7475599646568298, 'learning_rate': 6.722255118582707e-06, 'epoch': 6.568181818181818}\n{'loss': 0.4891, 'grad_norm': 0.8646552562713623, 'learning_rate': 6.660012015632867e-06, 'epoch': 6.590909090909091}\n{'loss': 0.4322, 'grad_norm': 0.8102277517318726, 'learning_rate': 6.5977689126830275e-06, 'epoch': 6.613636363636363}\n{'loss': 0.5291, 'grad_norm': 0.9146604537963867, 'learning_rate': 6.535525809733187e-06, 'epoch': 6.636363636363637}\n{'loss': 0.4978, 'grad_norm': 1.037007451057434, 'learning_rate': 6.473282706783347e-06, 'epoch': 6.659090909090909}\n{'loss': 0.3998, 'grad_norm': 0.8929008841514587, 'learning_rate': 6.411039603833507e-06, 'epoch': 6.681818181818182}\n{'loss': 0.4186, 'grad_norm': 0.810775101184845, 'learning_rate': 6.348796500883667e-06, 'epoch': 6.704545454545455}\n{'loss': 0.4315, 'grad_norm': 0.7514236569404602, 'learning_rate': 6.286553397933828e-06, 'epoch': 6.7272727272727275}\n{'loss': 0.4593, 'grad_norm': 0.7077969908714294, 'learning_rate': 6.224310294983988e-06, 'epoch': 6.75}\n{'loss': 0.4317, 'grad_norm': 0.7934516668319702, 'learning_rate': 6.162067192034148e-06, 'epoch': 6.7727272727272725}\n{'loss': 0.452, 'grad_norm': 0.7247561812400818, 'learning_rate': 6.099824089084308e-06, 'epoch': 6.795454545454545}\n{'loss': 0.4108, 'grad_norm': 0.8208372592926025, 'learning_rate': 6.037580986134468e-06, 'epoch': 6.818181818181818}\n{'loss': 0.4195, 'grad_norm': 0.6758221983909607, 'learning_rate': 5.975337883184628e-06, 'epoch': 6.840909090909091}\n{'loss': 0.4701, 'grad_norm': 0.7913186550140381, 'learning_rate': 5.9130947802347885e-06, 'epoch': 6.863636363636363}\n{'loss': 0.4799, 'grad_norm': 0.766400158405304, 'learning_rate': 5.850851677284948e-06, 'epoch': 6.886363636363637}\n{'loss': 0.4264, 'grad_norm': 0.8822098970413208, 'learning_rate': 5.788608574335109e-06, 'epoch': 6.909090909090909}\n{'loss': 0.4541, 'grad_norm': 0.7683905959129333, 'learning_rate': 5.726365471385269e-06, 'epoch': 6.931818181818182}\n{'loss': 0.379, 'grad_norm': 0.8027663230895996, 'learning_rate': 5.6641223684354295e-06, 'epoch': 6.954545454545455}\n{'loss': 0.4681, 'grad_norm': 0.8620842099189758, 'learning_rate': 5.601879265485589e-06, 'epoch': 6.9772727272727275}\n{'loss': 0.235, 'grad_norm': 2.943464994430542, 'learning_rate': 5.539636162535749e-06, 'epoch': 7.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.7931    0.6924    0.7393      3404\n     PERCENT     0.6809    0.9014    0.7758        71\n        TYPE     0.8725    0.9496    0.9094     11194\n      VOLUME     0.3333    0.0357    0.0645        56\n\n   micro avg     0.8556    0.8865    0.8708     14725\n   macro avg     0.6699    0.6448    0.6223     14725\nweighted avg     0.8511    0.8865    0.8662     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.4435797333717346, 'eval_f1_macro': 0.6222526782363678, 'eval_precision': 0.8555977975878343, 'eval_recall': 0.8864516129032258, 'eval_f1': 0.8707514759347588, 'eval_accuracy': 0.8680589815271611, 'eval_runtime': 1.4667, 'eval_samples_per_second': 3757.405, 'eval_steps_per_second': 7.5, 'epoch': 7.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.4623, 'grad_norm': 0.9080736637115479, 'learning_rate': 5.477393059585909e-06, 'epoch': 7.0227272727272725}\n{'loss': 0.4864, 'grad_norm': 0.8805024027824402, 'learning_rate': 5.415149956636069e-06, 'epoch': 7.045454545454546}\n{'loss': 0.4175, 'grad_norm': 0.8089312314987183, 'learning_rate': 5.35290685368623e-06, 'epoch': 7.068181818181818}\n{'loss': 0.4118, 'grad_norm': 0.9344029426574707, 'learning_rate': 5.290663750736389e-06, 'epoch': 7.090909090909091}\n{'loss': 0.4415, 'grad_norm': 0.9441349506378174, 'learning_rate': 5.2284206477865495e-06, 'epoch': 7.113636363636363}\n{'loss': 0.404, 'grad_norm': 0.9193956255912781, 'learning_rate': 5.16617754483671e-06, 'epoch': 7.136363636363637}\n{'loss': 0.4352, 'grad_norm': 0.885764479637146, 'learning_rate': 5.10393444188687e-06, 'epoch': 7.159090909090909}\n{'loss': 0.4535, 'grad_norm': 0.790829598903656, 'learning_rate': 5.04169133893703e-06, 'epoch': 7.181818181818182}\n{'loss': 0.3895, 'grad_norm': 0.7628477811813354, 'learning_rate': 4.9794482359871905e-06, 'epoch': 7.204545454545454}\n{'loss': 0.409, 'grad_norm': 0.9488314390182495, 'learning_rate': 4.91720513303735e-06, 'epoch': 7.2272727272727275}\n{'loss': 0.3906, 'grad_norm': 0.7937328219413757, 'learning_rate': 4.854962030087511e-06, 'epoch': 7.25}\n{'loss': 0.4111, 'grad_norm': 0.7353665828704834, 'learning_rate': 4.79271892713767e-06, 'epoch': 7.2727272727272725}\n{'loss': 0.4136, 'grad_norm': 0.7610895037651062, 'learning_rate': 4.730475824187831e-06, 'epoch': 7.295454545454546}\n{'loss': 0.4121, 'grad_norm': 0.7196134328842163, 'learning_rate': 4.668232721237991e-06, 'epoch': 7.318181818181818}\n{'loss': 0.4499, 'grad_norm': 0.86573326587677, 'learning_rate': 4.605989618288151e-06, 'epoch': 7.340909090909091}\n{'loss': 0.4595, 'grad_norm': 0.7921833395957947, 'learning_rate': 4.543746515338311e-06, 'epoch': 7.363636363636363}\n{'loss': 0.4302, 'grad_norm': 0.9152093529701233, 'learning_rate': 4.481503412388471e-06, 'epoch': 7.386363636363637}\n{'loss': 0.3995, 'grad_norm': 0.7827302813529968, 'learning_rate': 4.419260309438631e-06, 'epoch': 7.409090909090909}\n{'loss': 0.4227, 'grad_norm': 0.8212273716926575, 'learning_rate': 4.357017206488791e-06, 'epoch': 7.431818181818182}\n{'loss': 0.3973, 'grad_norm': 0.954505443572998, 'learning_rate': 4.2947741035389515e-06, 'epoch': 7.454545454545454}\n{'loss': 0.4439, 'grad_norm': 0.7324309349060059, 'learning_rate': 4.232531000589112e-06, 'epoch': 7.4772727272727275}\n{'loss': 0.3854, 'grad_norm': 0.6459276676177979, 'learning_rate': 4.170287897639271e-06, 'epoch': 7.5}\n{'loss': 0.4171, 'grad_norm': 0.8036094307899475, 'learning_rate': 4.108044794689432e-06, 'epoch': 7.5227272727272725}\n{'loss': 0.4156, 'grad_norm': 0.6834613680839539, 'learning_rate': 4.0458016917395925e-06, 'epoch': 7.545454545454545}\n{'loss': 0.4583, 'grad_norm': 0.7642590403556824, 'learning_rate': 3.983558588789752e-06, 'epoch': 7.568181818181818}\n{'loss': 0.4151, 'grad_norm': 0.6478370428085327, 'learning_rate': 3.921315485839912e-06, 'epoch': 7.590909090909091}\n{'loss': 0.4596, 'grad_norm': 0.8304810523986816, 'learning_rate': 3.859072382890072e-06, 'epoch': 7.613636363636363}\n{'loss': 0.4401, 'grad_norm': 0.8200846314430237, 'learning_rate': 3.7968292799402326e-06, 'epoch': 7.636363636363637}\n{'loss': 0.4457, 'grad_norm': 0.9180681109428406, 'learning_rate': 3.7345861769903925e-06, 'epoch': 7.659090909090909}\n{'loss': 0.4583, 'grad_norm': 0.7889301180839539, 'learning_rate': 3.6723430740405527e-06, 'epoch': 7.681818181818182}\n{'loss': 0.4704, 'grad_norm': 0.8108986020088196, 'learning_rate': 3.610099971090713e-06, 'epoch': 7.704545454545455}\n{'loss': 0.4178, 'grad_norm': 1.4311013221740723, 'learning_rate': 3.547856868140873e-06, 'epoch': 7.7272727272727275}\n{'loss': 0.4149, 'grad_norm': 0.7471058368682861, 'learning_rate': 3.485613765191033e-06, 'epoch': 7.75}\n{'loss': 0.4399, 'grad_norm': 0.8892781138420105, 'learning_rate': 3.423370662241193e-06, 'epoch': 7.7727272727272725}\n{'loss': 0.4554, 'grad_norm': 1.0275694131851196, 'learning_rate': 3.3611275592913535e-06, 'epoch': 7.795454545454545}\n{'loss': 0.418, 'grad_norm': 0.7424034476280212, 'learning_rate': 3.2988844563415138e-06, 'epoch': 7.818181818181818}\n{'loss': 0.4253, 'grad_norm': 1.1447739601135254, 'learning_rate': 3.2366413533916736e-06, 'epoch': 7.840909090909091}\n{'loss': 0.4175, 'grad_norm': 0.9149842858314514, 'learning_rate': 3.1743982504418334e-06, 'epoch': 7.863636363636363}\n{'loss': 0.4446, 'grad_norm': 0.7601420283317566, 'learning_rate': 3.112155147491994e-06, 'epoch': 7.886363636363637}\n{'loss': 0.4103, 'grad_norm': 0.8277906775474548, 'learning_rate': 3.049912044542154e-06, 'epoch': 7.909090909090909}\n{'loss': 0.4408, 'grad_norm': 0.8383362293243408, 'learning_rate': 2.987668941592314e-06, 'epoch': 7.931818181818182}\n{'loss': 0.4304, 'grad_norm': 0.9175698161125183, 'learning_rate': 2.925425838642474e-06, 'epoch': 7.954545454545455}\n{'loss': 0.3957, 'grad_norm': 0.7416733503341675, 'learning_rate': 2.8631827356926346e-06, 'epoch': 7.9772727272727275}\n{'loss': 0.5307, 'grad_norm': 4.082716941833496, 'learning_rate': 2.8009396327427945e-06, 'epoch': 8.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.7907    0.7024    0.7439      3404\n     PERCENT     0.6346    0.9296    0.7543        71\n        TYPE     0.8776    0.9504    0.9126     11194\n      VOLUME     0.2857    0.0357    0.0635        56\n\n   micro avg     0.8584    0.8895    0.8737     14725\n   macro avg     0.6471    0.6545    0.6186     14725\nweighted avg     0.8541    0.8895    0.8696     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.4341758191585541, 'eval_f1_macro': 0.6185659111673012, 'eval_precision': 0.8584349193865514, 'eval_recall': 0.8895076400679117, 'eval_f1': 0.8736950938865357, 'eval_accuracy': 0.870580496628844, 'eval_runtime': 1.8262, 'eval_samples_per_second': 3017.823, 'eval_steps_per_second': 6.024, 'epoch': 8.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.4289, 'grad_norm': 1.1427428722381592, 'learning_rate': 2.7386965297929547e-06, 'epoch': 8.022727272727273}\n{'loss': 0.5049, 'grad_norm': 0.877004086971283, 'learning_rate': 2.676453426843115e-06, 'epoch': 8.045454545454545}\n{'loss': 0.4108, 'grad_norm': 0.8562635779380798, 'learning_rate': 2.6142103238932748e-06, 'epoch': 8.068181818181818}\n{'loss': 0.4163, 'grad_norm': 0.9551539421081543, 'learning_rate': 2.551967220943435e-06, 'epoch': 8.090909090909092}\n{'loss': 0.4537, 'grad_norm': 0.6904939413070679, 'learning_rate': 2.4897241179935953e-06, 'epoch': 8.113636363636363}\n{'loss': 0.4051, 'grad_norm': 0.9489415884017944, 'learning_rate': 2.4274810150437555e-06, 'epoch': 8.136363636363637}\n{'loss': 0.4561, 'grad_norm': 0.9606552124023438, 'learning_rate': 2.3652379120939153e-06, 'epoch': 8.159090909090908}\n{'loss': 0.4157, 'grad_norm': 0.8477575182914734, 'learning_rate': 2.3029948091440756e-06, 'epoch': 8.181818181818182}\n{'loss': 0.4307, 'grad_norm': 0.8267502188682556, 'learning_rate': 2.2407517061942354e-06, 'epoch': 8.204545454545455}\n{'loss': 0.376, 'grad_norm': 0.7590528130531311, 'learning_rate': 2.1785086032443956e-06, 'epoch': 8.227272727272727}\n{'loss': 0.4415, 'grad_norm': 0.8891371488571167, 'learning_rate': 2.116265500294556e-06, 'epoch': 8.25}\n{'loss': 0.3997, 'grad_norm': 1.0273468494415283, 'learning_rate': 2.054022397344716e-06, 'epoch': 8.272727272727273}\n{'loss': 0.4834, 'grad_norm': 0.7766890525817871, 'learning_rate': 1.991779294394876e-06, 'epoch': 8.295454545454545}\n{'loss': 0.4352, 'grad_norm': 0.7326125502586365, 'learning_rate': 1.929536191445036e-06, 'epoch': 8.318181818181818}\n{'loss': 0.4086, 'grad_norm': 0.7995496392250061, 'learning_rate': 1.8672930884951962e-06, 'epoch': 8.340909090909092}\n{'loss': 0.3657, 'grad_norm': 0.7302499413490295, 'learning_rate': 1.8050499855453565e-06, 'epoch': 8.363636363636363}\n{'loss': 0.3866, 'grad_norm': 0.842021644115448, 'learning_rate': 1.7428068825955165e-06, 'epoch': 8.386363636363637}\n{'loss': 0.4641, 'grad_norm': 1.0949726104736328, 'learning_rate': 1.6805637796456768e-06, 'epoch': 8.409090909090908}\n{'loss': 0.3949, 'grad_norm': 0.7454227209091187, 'learning_rate': 1.6183206766958368e-06, 'epoch': 8.431818181818182}\n{'loss': 0.3681, 'grad_norm': 0.6662403345108032, 'learning_rate': 1.556077573745997e-06, 'epoch': 8.454545454545455}\n{'loss': 0.4411, 'grad_norm': 0.717991054058075, 'learning_rate': 1.493834470796157e-06, 'epoch': 8.477272727272727}\n{'loss': 0.4144, 'grad_norm': 0.8728306293487549, 'learning_rate': 1.4315913678463173e-06, 'epoch': 8.5}\n{'loss': 0.3785, 'grad_norm': 0.6419079303741455, 'learning_rate': 1.3693482648964773e-06, 'epoch': 8.522727272727273}\n{'loss': 0.3528, 'grad_norm': 0.6637849807739258, 'learning_rate': 1.3071051619466374e-06, 'epoch': 8.545454545454545}\n{'loss': 0.4315, 'grad_norm': 0.7154181599617004, 'learning_rate': 1.2448620589967976e-06, 'epoch': 8.568181818181818}\n{'loss': 0.4046, 'grad_norm': 0.7257773280143738, 'learning_rate': 1.1826189560469577e-06, 'epoch': 8.590909090909092}\n{'loss': 0.3981, 'grad_norm': 0.82354736328125, 'learning_rate': 1.1203758530971177e-06, 'epoch': 8.613636363636363}\n{'loss': 0.4307, 'grad_norm': 0.7496294379234314, 'learning_rate': 1.058132750147278e-06, 'epoch': 8.636363636363637}\n{'loss': 0.3657, 'grad_norm': 0.9694535136222839, 'learning_rate': 9.95889647197438e-07, 'epoch': 8.659090909090908}\n{'loss': 0.3989, 'grad_norm': 0.7693846225738525, 'learning_rate': 9.336465442475981e-07, 'epoch': 8.681818181818182}\n{'loss': 0.4647, 'grad_norm': 0.8712347745895386, 'learning_rate': 8.714034412977583e-07, 'epoch': 8.704545454545455}\n{'loss': 0.4009, 'grad_norm': 0.7036758065223694, 'learning_rate': 8.091603383479184e-07, 'epoch': 8.727272727272727}\n{'loss': 0.4031, 'grad_norm': 0.7983571887016296, 'learning_rate': 7.469172353980785e-07, 'epoch': 8.75}\n{'loss': 0.4138, 'grad_norm': 0.8605173230171204, 'learning_rate': 6.846741324482387e-07, 'epoch': 8.772727272727273}\n{'loss': 0.4166, 'grad_norm': 0.822417140007019, 'learning_rate': 6.224310294983988e-07, 'epoch': 8.795454545454545}\n{'loss': 0.4233, 'grad_norm': 0.9599282145500183, 'learning_rate': 5.601879265485588e-07, 'epoch': 8.818181818181818}\n{'loss': 0.3853, 'grad_norm': 0.9025004506111145, 'learning_rate': 4.97944823598719e-07, 'epoch': 8.840909090909092}\n{'loss': 0.4233, 'grad_norm': 0.7927359938621521, 'learning_rate': 4.3570172064887913e-07, 'epoch': 8.863636363636363}\n{'loss': 0.4394, 'grad_norm': 0.8379464149475098, 'learning_rate': 3.7345861769903927e-07, 'epoch': 8.886363636363637}\n{'loss': 0.4588, 'grad_norm': 0.9488542675971985, 'learning_rate': 3.112155147491994e-07, 'epoch': 8.909090909090908}\n{'loss': 0.4357, 'grad_norm': 0.9869693517684937, 'learning_rate': 2.489724117993595e-07, 'epoch': 8.931818181818182}\n{'loss': 0.4446, 'grad_norm': 0.6877236366271973, 'learning_rate': 1.8672930884951963e-07, 'epoch': 8.954545454545455}\n{'loss': 0.4347, 'grad_norm': 0.7732093930244446, 'learning_rate': 1.2448620589967975e-07, 'epoch': 8.977272727272727}\n{'loss': 0.4512, 'grad_norm': 2.9310202598571777, 'learning_rate': 6.224310294983987e-08, 'epoch': 9.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.7945    0.7021    0.7455      3404\n     PERCENT     0.6111    0.9296    0.7374        71\n        TYPE     0.8770    0.9514    0.9127     11194\n      VOLUME     0.2500    0.0357    0.0625        56\n\n   micro avg     0.8586    0.8902    0.8741     14725\n   macro avg     0.6332    0.6547    0.6145     14725\nweighted avg     0.8543    0.8902    0.8700     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.43268951773643494, 'eval_f1_macro': 0.6145302785517861, 'eval_precision': 0.8585838737145477, 'eval_recall': 0.8901867572156197, 'eval_f1': 0.8740997599359829, 'eval_accuracy': 0.8711834676314203, 'eval_runtime': 1.4461, 'eval_samples_per_second': 3810.872, 'eval_steps_per_second': 7.607, 'epoch': 9.0}\n{'train_runtime': 55.7466, 'train_samples_per_second': 3558.403, 'train_steps_per_second': 7.104, 'train_loss': 0.7187272560491105, 'epoch': 9.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nThe speedups for torchdynamo mostly come with GPU Ampere or higher and which is not detected here.\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.7931    0.6924    0.7393      3404\n     PERCENT     0.6809    0.9014    0.7758        71\n        TYPE     0.8725    0.9496    0.9094     11194\n      VOLUME     0.3333    0.0357    0.0645        56\n\n   micro avg     0.8556    0.8865    0.8708     14725\n   macro avg     0.6699    0.6448    0.6223     14725\nweighted avg     0.8511    0.8865    0.8662     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.4435797333717346, 'eval_f1_macro': 0.6222526782363678, 'eval_precision': 0.8555977975878343, 'eval_recall': 0.8864516129032258, 'eval_f1': 0.8707514759347588, 'eval_accuracy': 0.8680589815271611, 'eval_runtime': 1.5401, 'eval_samples_per_second': 3578.406, 'eval_steps_per_second': 7.143, 'epoch': 9.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/3364797558.py:66: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 2.176, 'grad_norm': 7.275242328643799, 'learning_rate': 0.0, 'epoch': 0.022727272727272728}\n{'loss': 2.1793, 'grad_norm': 7.353992462158203, 'learning_rate': 5.539636162535749e-07, 'epoch': 0.045454545454545456}\n{'loss': 2.1862, 'grad_norm': 7.396754741668701, 'learning_rate': 1.1079272325071499e-06, 'epoch': 0.06818181818181818}\n{'loss': 2.1884, 'grad_norm': 7.28239631652832, 'learning_rate': 1.6618908487607247e-06, 'epoch': 0.09090909090909091}\n{'loss': 2.1759, 'grad_norm': 7.252343654632568, 'learning_rate': 2.2158544650142997e-06, 'epoch': 0.11363636363636363}\n{'loss': 2.1758, 'grad_norm': 7.725768089294434, 'learning_rate': 2.7698180812678746e-06, 'epoch': 0.13636363636363635}\n{'loss': 2.1594, 'grad_norm': 6.947290420532227, 'learning_rate': 3.3237816975214494e-06, 'epoch': 0.1590909090909091}\n{'loss': 2.1556, 'grad_norm': 7.315883636474609, 'learning_rate': 3.877745313775024e-06, 'epoch': 0.18181818181818182}\n{'loss': 2.1402, 'grad_norm': 7.206728935241699, 'learning_rate': 4.4317089300285995e-06, 'epoch': 0.20454545454545456}\n{'loss': 2.1398, 'grad_norm': 7.269426345825195, 'learning_rate': 4.985672546282174e-06, 'epoch': 0.22727272727272727}\n{'loss': 2.1331, 'grad_norm': 6.941597938537598, 'learning_rate': 5.539636162535749e-06, 'epoch': 0.25}\n{'loss': 2.1136, 'grad_norm': 6.951212406158447, 'learning_rate': 6.093599778789325e-06, 'epoch': 0.2727272727272727}\n{'loss': 2.106, 'grad_norm': 7.126608371734619, 'learning_rate': 6.647563395042899e-06, 'epoch': 0.29545454545454547}\n{'loss': 2.0838, 'grad_norm': 6.897426128387451, 'learning_rate': 7.2015270112964745e-06, 'epoch': 0.3181818181818182}\n{'loss': 2.0866, 'grad_norm': 6.745570182800293, 'learning_rate': 7.755490627550048e-06, 'epoch': 0.3409090909090909}\n{'loss': 2.0499, 'grad_norm': 7.096632957458496, 'learning_rate': 8.309454243803624e-06, 'epoch': 0.36363636363636365}\n{'loss': 2.0339, 'grad_norm': 6.858035564422607, 'learning_rate': 8.863417860057199e-06, 'epoch': 0.38636363636363635}\n{'loss': 2.0099, 'grad_norm': 6.9902143478393555, 'learning_rate': 9.417381476310774e-06, 'epoch': 0.4090909090909091}\n{'loss': 1.9853, 'grad_norm': 7.0003461837768555, 'learning_rate': 9.971345092564349e-06, 'epoch': 0.4318181818181818}\n{'loss': 1.9736, 'grad_norm': 6.663019180297852, 'learning_rate': 1.0525308708817923e-05, 'epoch': 0.45454545454545453}\n{'loss': 1.9403, 'grad_norm': 6.928691387176514, 'learning_rate': 1.1079272325071498e-05, 'epoch': 0.4772727272727273}\n{'loss': 1.9176, 'grad_norm': 6.678692817687988, 'learning_rate': 1.1633235941325073e-05, 'epoch': 0.5}\n{'loss': 1.8985, 'grad_norm': 6.56572961807251, 'learning_rate': 1.218719955757865e-05, 'epoch': 0.5227272727272727}\n{'loss': 1.8492, 'grad_norm': 6.704495429992676, 'learning_rate': 1.2741163173832223e-05, 'epoch': 0.5454545454545454}\n{'loss': 1.8524, 'grad_norm': 6.426218032836914, 'learning_rate': 1.3295126790085798e-05, 'epoch': 0.5681818181818182}\n{'loss': 1.8295, 'grad_norm': 6.207386016845703, 'learning_rate': 1.3849090406339372e-05, 'epoch': 0.5909090909090909}\n{'loss': 1.8109, 'grad_norm': 5.93108606338501, 'learning_rate': 1.4403054022592949e-05, 'epoch': 0.6136363636363636}\n{'loss': 1.7728, 'grad_norm': 6.066915035247803, 'learning_rate': 1.4957017638846524e-05, 'epoch': 0.6363636363636364}\n{'loss': 1.7551, 'grad_norm': 6.157548427581787, 'learning_rate': 1.5510981255100095e-05, 'epoch': 0.6590909090909091}\n{'loss': 1.6946, 'grad_norm': 5.942066669464111, 'learning_rate': 1.6064944871353672e-05, 'epoch': 0.6818181818181818}\n{'loss': 1.6867, 'grad_norm': 5.684919357299805, 'learning_rate': 1.6618908487607248e-05, 'epoch': 0.7045454545454546}\n{'loss': 1.6997, 'grad_norm': 5.072123050689697, 'learning_rate': 1.717287210386082e-05, 'epoch': 0.7272727272727273}\n{'loss': 1.6379, 'grad_norm': 5.54621696472168, 'learning_rate': 1.7726835720114398e-05, 'epoch': 0.75}\n{'loss': 1.5969, 'grad_norm': 5.231238842010498, 'learning_rate': 1.828079933636797e-05, 'epoch': 0.7727272727272727}\n{'loss': 1.619, 'grad_norm': 4.710127353668213, 'learning_rate': 1.8834762952621548e-05, 'epoch': 0.7954545454545454}\n{'loss': 1.5496, 'grad_norm': 4.880743026733398, 'learning_rate': 1.938872656887512e-05, 'epoch': 0.8181818181818182}\n{'loss': 1.5083, 'grad_norm': 4.932693958282471, 'learning_rate': 1.9942690185128697e-05, 'epoch': 0.8409090909090909}\n{'loss': 1.4977, 'grad_norm': 4.5998334884643555, 'learning_rate': 2.0496653801382274e-05, 'epoch': 0.8636363636363636}\n{'loss': 1.4614, 'grad_norm': 4.253036975860596, 'learning_rate': 2.1050617417635847e-05, 'epoch': 0.8863636363636364}\n{'loss': 1.4116, 'grad_norm': 4.382587909698486, 'learning_rate': 2.160458103388942e-05, 'epoch': 0.9090909090909091}\n{'loss': 1.4032, 'grad_norm': 3.864220380783081, 'learning_rate': 2.2158544650142997e-05, 'epoch': 0.9318181818181818}\n{'loss': 1.3878, 'grad_norm': 3.7296037673950195, 'learning_rate': 2.2096301547193157e-05, 'epoch': 0.9545454545454546}\n{'loss': 1.3756, 'grad_norm': 3.4109787940979004, 'learning_rate': 2.2034058444243317e-05, 'epoch': 0.9772727272727273}\n{'loss': 1.423, 'grad_norm': 3.706374406814575, 'learning_rate': 2.1971815341293477e-05, 'epoch': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.0000    0.0000    0.0000      3311\n     PERCENT     0.0000    0.0000    0.0000        86\n        TYPE     0.5694    0.9221    0.7041     11299\n      VOLUME     0.0000    0.0000    0.0000        42\n\n   micro avg     0.5693    0.7069    0.6307     14738\n   macro avg     0.1424    0.2305    0.1760     14738\nweighted avg     0.4365    0.7069    0.5398     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 1.2859307527542114, 'eval_f1_macro': 0.17601446092509376, 'eval_precision': 0.5693131522867603, 'eval_recall': 0.7069480255122812, 'eval_f1': 0.6307091618995732, 'eval_accuracy': 0.6164690454073548, 'eval_runtime': 1.5732, 'eval_samples_per_second': 3502.498, 'eval_steps_per_second': 6.992, 'epoch': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 1.3387, 'grad_norm': 2.9915804862976074, 'learning_rate': 2.1909572238343638e-05, 'epoch': 1.0227272727272727}\n{'loss': 1.2809, 'grad_norm': 2.816002368927002, 'learning_rate': 2.1847329135393798e-05, 'epoch': 1.0454545454545454}\n{'loss': 1.2759, 'grad_norm': 2.6817538738250732, 'learning_rate': 2.1785086032443958e-05, 'epoch': 1.0681818181818181}\n{'loss': 1.2327, 'grad_norm': 2.588886260986328, 'learning_rate': 2.172284292949412e-05, 'epoch': 1.0909090909090908}\n{'loss': 1.2276, 'grad_norm': 2.395082712173462, 'learning_rate': 2.1660599826544275e-05, 'epoch': 1.1136363636363635}\n{'loss': 1.2562, 'grad_norm': 2.0885019302368164, 'learning_rate': 2.159835672359444e-05, 'epoch': 1.1363636363636362}\n{'loss': 1.1408, 'grad_norm': 2.3035967350006104, 'learning_rate': 2.15361136206446e-05, 'epoch': 1.1590909090909092}\n{'loss': 1.227, 'grad_norm': 1.91878342628479, 'learning_rate': 2.1473870517694756e-05, 'epoch': 1.1818181818181819}\n{'loss': 1.1988, 'grad_norm': 1.841726303100586, 'learning_rate': 2.141162741474492e-05, 'epoch': 1.2045454545454546}\n{'loss': 1.186, 'grad_norm': 1.68092679977417, 'learning_rate': 2.1349384311795076e-05, 'epoch': 1.2272727272727273}\n{'loss': 1.1625, 'grad_norm': 1.6901594400405884, 'learning_rate': 2.128714120884524e-05, 'epoch': 1.25}\n{'loss': 1.1381, 'grad_norm': 1.8215789794921875, 'learning_rate': 2.1224898105895397e-05, 'epoch': 1.2727272727272727}\n{'loss': 1.1178, 'grad_norm': 1.5330673456192017, 'learning_rate': 2.1162655002945557e-05, 'epoch': 1.2954545454545454}\n{'loss': 1.1331, 'grad_norm': 1.498002529144287, 'learning_rate': 2.110041189999572e-05, 'epoch': 1.3181818181818181}\n{'loss': 1.077, 'grad_norm': 1.5550576448440552, 'learning_rate': 2.1038168797045878e-05, 'epoch': 1.3409090909090908}\n{'loss': 1.0971, 'grad_norm': 1.4406296014785767, 'learning_rate': 2.097592569409604e-05, 'epoch': 1.3636363636363638}\n{'loss': 1.1222, 'grad_norm': 1.4072147607803345, 'learning_rate': 2.0913682591146198e-05, 'epoch': 1.3863636363636362}\n{'loss': 1.0394, 'grad_norm': 1.4785511493682861, 'learning_rate': 2.085143948819636e-05, 'epoch': 1.4090909090909092}\n{'loss': 1.0764, 'grad_norm': 1.3714386224746704, 'learning_rate': 2.078919638524652e-05, 'epoch': 1.4318181818181819}\n{'loss': 1.0017, 'grad_norm': 1.5134925842285156, 'learning_rate': 2.072695328229668e-05, 'epoch': 1.4545454545454546}\n{'loss': 1.04, 'grad_norm': 1.4104225635528564, 'learning_rate': 2.066471017934684e-05, 'epoch': 1.4772727272727273}\n{'loss': 1.0016, 'grad_norm': 1.4702153205871582, 'learning_rate': 2.0602467076397e-05, 'epoch': 1.5}\n{'loss': 1.0029, 'grad_norm': 1.352258324623108, 'learning_rate': 2.054022397344716e-05, 'epoch': 1.5227272727272727}\n{'loss': 0.9975, 'grad_norm': 1.326314926147461, 'learning_rate': 2.047798087049732e-05, 'epoch': 1.5454545454545454}\n{'loss': 1.0085, 'grad_norm': 1.2114437818527222, 'learning_rate': 2.041573776754748e-05, 'epoch': 1.5681818181818183}\n{'loss': 0.9744, 'grad_norm': 1.3950350284576416, 'learning_rate': 2.035349466459764e-05, 'epoch': 1.5909090909090908}\n{'loss': 0.9895, 'grad_norm': 1.2184839248657227, 'learning_rate': 2.02912515616478e-05, 'epoch': 1.6136363636363638}\n{'loss': 0.9864, 'grad_norm': 1.1872460842132568, 'learning_rate': 2.022900845869796e-05, 'epoch': 1.6363636363636362}\n{'loss': 0.934, 'grad_norm': 1.2763680219650269, 'learning_rate': 2.016676535574812e-05, 'epoch': 1.6590909090909092}\n{'loss': 0.9735, 'grad_norm': 1.1456081867218018, 'learning_rate': 2.010452225279828e-05, 'epoch': 1.6818181818181817}\n{'loss': 0.9482, 'grad_norm': 1.2243478298187256, 'learning_rate': 2.004227914984844e-05, 'epoch': 1.7045454545454546}\n{'loss': 0.9401, 'grad_norm': 1.1815050840377808, 'learning_rate': 1.9980036046898602e-05, 'epoch': 1.7272727272727273}\n{'loss': 0.9346, 'grad_norm': 1.1394566297531128, 'learning_rate': 1.9917792943948762e-05, 'epoch': 1.75}\n{'loss': 0.9383, 'grad_norm': 1.1567884683609009, 'learning_rate': 1.9855549840998922e-05, 'epoch': 1.7727272727272727}\n{'loss': 0.8377, 'grad_norm': 1.338904857635498, 'learning_rate': 1.979330673804908e-05, 'epoch': 1.7954545454545454}\n{'loss': 0.8886, 'grad_norm': 1.115572452545166, 'learning_rate': 1.9731063635099243e-05, 'epoch': 1.8181818181818183}\n{'loss': 0.8283, 'grad_norm': 1.254064679145813, 'learning_rate': 1.96688205321494e-05, 'epoch': 1.8409090909090908}\n{'loss': 0.8583, 'grad_norm': 1.1522947549819946, 'learning_rate': 1.960657742919956e-05, 'epoch': 1.8636363636363638}\n{'loss': 0.8535, 'grad_norm': 1.1073997020721436, 'learning_rate': 1.9544334326249724e-05, 'epoch': 1.8863636363636362}\n{'loss': 0.9118, 'grad_norm': 1.2434933185577393, 'learning_rate': 1.948209122329988e-05, 'epoch': 1.9090909090909092}\n{'loss': 0.8811, 'grad_norm': 0.9864493608474731, 'learning_rate': 1.9419848120350044e-05, 'epoch': 1.9318181818181817}\n{'loss': 0.8509, 'grad_norm': 1.0249003171920776, 'learning_rate': 1.93576050174002e-05, 'epoch': 1.9545454545454546}\n{'loss': 0.7692, 'grad_norm': 1.04869544506073, 'learning_rate': 1.929536191445036e-05, 'epoch': 1.9772727272727273}\n{'loss': 0.6946, 'grad_norm': 2.624079942703247, 'learning_rate': 1.923311881150052e-05, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.7086    0.4098    0.5193      3311\n     PERCENT     0.0000    0.0000    0.0000        86\n        TYPE     0.6775    0.8968    0.7719     11299\n      VOLUME     0.0000    0.0000    0.0000        42\n\n   micro avg     0.6811    0.7796    0.7270     14738\n   macro avg     0.3465    0.3267    0.3228     14738\nweighted avg     0.6786    0.7796    0.7084     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.7706565856933594, 'eval_f1_macro': 0.32280437826687725, 'eval_precision': 0.6810503230395353, 'eval_recall': 0.7796173157823314, 'eval_f1': 0.7270081305957163, 'eval_accuracy': 0.7378831757827441, 'eval_runtime': 1.5725, 'eval_samples_per_second': 3503.988, 'eval_steps_per_second': 6.995, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.8015, 'grad_norm': 1.0975773334503174, 'learning_rate': 1.917087570855068e-05, 'epoch': 2.022727272727273}\n{'loss': 0.8064, 'grad_norm': 1.1986967325210571, 'learning_rate': 1.9108632605600845e-05, 'epoch': 2.0454545454545454}\n{'loss': 0.7941, 'grad_norm': 1.003039002418518, 'learning_rate': 1.9046389502651002e-05, 'epoch': 2.0681818181818183}\n{'loss': 0.8408, 'grad_norm': 1.0556162595748901, 'learning_rate': 1.8984146399701162e-05, 'epoch': 2.090909090909091}\n{'loss': 0.7894, 'grad_norm': 1.009177803993225, 'learning_rate': 1.8921903296751323e-05, 'epoch': 2.1136363636363638}\n{'loss': 0.7017, 'grad_norm': 1.2084109783172607, 'learning_rate': 1.8859660193801483e-05, 'epoch': 2.1363636363636362}\n{'loss': 0.7553, 'grad_norm': 1.0228431224822998, 'learning_rate': 1.8797417090851643e-05, 'epoch': 2.159090909090909}\n{'loss': 0.7779, 'grad_norm': 0.9348264932632446, 'learning_rate': 1.8735173987901803e-05, 'epoch': 2.1818181818181817}\n{'loss': 0.749, 'grad_norm': 0.9461250901222229, 'learning_rate': 1.8672930884951964e-05, 'epoch': 2.2045454545454546}\n{'loss': 0.7822, 'grad_norm': 0.9316099882125854, 'learning_rate': 1.8610687782002124e-05, 'epoch': 2.227272727272727}\n{'loss': 0.7795, 'grad_norm': 0.933300256729126, 'learning_rate': 1.8548444679052284e-05, 'epoch': 2.25}\n{'loss': 0.7527, 'grad_norm': 1.0122802257537842, 'learning_rate': 1.8486201576102444e-05, 'epoch': 2.2727272727272725}\n{'loss': 0.7994, 'grad_norm': 0.9181544780731201, 'learning_rate': 1.8423958473152605e-05, 'epoch': 2.2954545454545454}\n{'loss': 0.7291, 'grad_norm': 0.906730592250824, 'learning_rate': 1.8361715370202765e-05, 'epoch': 2.3181818181818183}\n{'loss': 0.6556, 'grad_norm': 1.202183723449707, 'learning_rate': 1.8299472267252925e-05, 'epoch': 2.340909090909091}\n{'loss': 0.7242, 'grad_norm': 1.0616739988327026, 'learning_rate': 1.8237229164303082e-05, 'epoch': 2.3636363636363638}\n{'loss': 0.6671, 'grad_norm': 0.9707639217376709, 'learning_rate': 1.8174986061353246e-05, 'epoch': 2.3863636363636362}\n{'loss': 0.718, 'grad_norm': 0.9507846832275391, 'learning_rate': 1.8112742958403406e-05, 'epoch': 2.409090909090909}\n{'loss': 0.7132, 'grad_norm': 0.8748204112052917, 'learning_rate': 1.8050499855453566e-05, 'epoch': 2.4318181818181817}\n{'loss': 0.6127, 'grad_norm': 1.2081129550933838, 'learning_rate': 1.7988256752503726e-05, 'epoch': 2.4545454545454546}\n{'loss': 0.6833, 'grad_norm': 0.8631817102432251, 'learning_rate': 1.7926013649553883e-05, 'epoch': 2.4772727272727275}\n{'loss': 0.7231, 'grad_norm': 0.9365000128746033, 'learning_rate': 1.7863770546604047e-05, 'epoch': 2.5}\n{'loss': 0.6767, 'grad_norm': 0.8629806637763977, 'learning_rate': 1.7801527443654204e-05, 'epoch': 2.5227272727272725}\n{'loss': 0.7014, 'grad_norm': 1.0326390266418457, 'learning_rate': 1.7739284340704364e-05, 'epoch': 2.5454545454545454}\n{'loss': 0.6829, 'grad_norm': 1.0943453311920166, 'learning_rate': 1.7677041237754524e-05, 'epoch': 2.5681818181818183}\n{'loss': 0.6301, 'grad_norm': 0.9714370369911194, 'learning_rate': 1.7614798134804684e-05, 'epoch': 2.590909090909091}\n{'loss': 0.6733, 'grad_norm': 0.8209537863731384, 'learning_rate': 1.7552555031854848e-05, 'epoch': 2.6136363636363638}\n{'loss': 0.6631, 'grad_norm': 0.7411694526672363, 'learning_rate': 1.7490311928905005e-05, 'epoch': 2.6363636363636362}\n{'loss': 0.6528, 'grad_norm': 0.8758139610290527, 'learning_rate': 1.7428068825955165e-05, 'epoch': 2.659090909090909}\n{'loss': 0.5606, 'grad_norm': 0.9600470066070557, 'learning_rate': 1.7365825723005325e-05, 'epoch': 2.6818181818181817}\n{'loss': 0.5959, 'grad_norm': 0.8017988204956055, 'learning_rate': 1.7303582620055486e-05, 'epoch': 2.7045454545454546}\n{'loss': 0.6861, 'grad_norm': 0.7890251874923706, 'learning_rate': 1.7241339517105646e-05, 'epoch': 2.7272727272727275}\n{'loss': 0.624, 'grad_norm': 0.891353189945221, 'learning_rate': 1.7179096414155806e-05, 'epoch': 2.75}\n{'loss': 0.6899, 'grad_norm': 0.9797232747077942, 'learning_rate': 1.7116853311205966e-05, 'epoch': 2.7727272727272725}\n{'loss': 0.5998, 'grad_norm': 1.1431289911270142, 'learning_rate': 1.7054610208256127e-05, 'epoch': 2.7954545454545454}\n{'loss': 0.6863, 'grad_norm': 1.0008955001831055, 'learning_rate': 1.6992367105306287e-05, 'epoch': 2.8181818181818183}\n{'loss': 0.6481, 'grad_norm': 0.9238794445991516, 'learning_rate': 1.6930124002356447e-05, 'epoch': 2.840909090909091}\n{'loss': 0.6009, 'grad_norm': 0.8182241320610046, 'learning_rate': 1.6867880899406607e-05, 'epoch': 2.8636363636363638}\n{'loss': 0.6313, 'grad_norm': 1.008683204650879, 'learning_rate': 1.6805637796456768e-05, 'epoch': 2.8863636363636362}\n{'loss': 0.5923, 'grad_norm': 0.7617092132568359, 'learning_rate': 1.6743394693506928e-05, 'epoch': 2.909090909090909}\n{'loss': 0.6173, 'grad_norm': 0.7376192808151245, 'learning_rate': 1.6681151590557085e-05, 'epoch': 2.9318181818181817}\n{'loss': 0.6476, 'grad_norm': 0.7769070267677307, 'learning_rate': 1.6618908487607248e-05, 'epoch': 2.9545454545454546}\n{'loss': 0.6225, 'grad_norm': 0.9110736846923828, 'learning_rate': 1.655666538465741e-05, 'epoch': 2.9772727272727275}\n{'loss': 0.5158, 'grad_norm': 3.076300859451294, 'learning_rate': 1.649442228170757e-05, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.7034    0.6367    0.6684      3311\n     PERCENT     0.9545    0.2442    0.3889        86\n        TYPE     0.8489    0.9067    0.8768     11299\n      VOLUME     0.0000    0.0000    0.0000        42\n\n   micro avg     0.8201    0.8396    0.8297     14738\n   macro avg     0.6267    0.4469    0.4835     14738\nweighted avg     0.8144    0.8396    0.8247     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.5693608522415161, 'eval_f1_macro': 0.4835216633062579, 'eval_precision': 0.8201219512195121, 'eval_recall': 0.8395983172750713, 'eval_f1': 0.8297458593173741, 'eval_accuracy': 0.8301185727555871, 'eval_runtime': 1.4989, 'eval_samples_per_second': 3676.067, 'eval_steps_per_second': 7.339, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.5974, 'grad_norm': 0.7910370826721191, 'learning_rate': 1.643217917875773e-05, 'epoch': 3.022727272727273}\n{'loss': 0.5747, 'grad_norm': 0.8510962724685669, 'learning_rate': 1.6369936075807886e-05, 'epoch': 3.0454545454545454}\n{'loss': 0.6063, 'grad_norm': 0.7966465950012207, 'learning_rate': 1.630769297285805e-05, 'epoch': 3.0681818181818183}\n{'loss': 0.6085, 'grad_norm': 0.7459991574287415, 'learning_rate': 1.6245449869908206e-05, 'epoch': 3.090909090909091}\n{'loss': 0.6076, 'grad_norm': 0.7835690379142761, 'learning_rate': 1.618320676695837e-05, 'epoch': 3.1136363636363638}\n{'loss': 0.5816, 'grad_norm': 0.7638066411018372, 'learning_rate': 1.6120963664008527e-05, 'epoch': 3.1363636363636362}\n{'loss': 0.636, 'grad_norm': 0.7350690960884094, 'learning_rate': 1.6058720561058687e-05, 'epoch': 3.159090909090909}\n{'loss': 0.5328, 'grad_norm': 0.8715030550956726, 'learning_rate': 1.599647745810885e-05, 'epoch': 3.1818181818181817}\n{'loss': 0.6236, 'grad_norm': 0.720085084438324, 'learning_rate': 1.5934234355159008e-05, 'epoch': 3.2045454545454546}\n{'loss': 0.6016, 'grad_norm': 0.7454034090042114, 'learning_rate': 1.5871991252209168e-05, 'epoch': 3.227272727272727}\n{'loss': 0.5603, 'grad_norm': 0.6660863757133484, 'learning_rate': 1.5809748149259328e-05, 'epoch': 3.25}\n{'loss': 0.5465, 'grad_norm': 0.7587954998016357, 'learning_rate': 1.574750504630949e-05, 'epoch': 3.2727272727272725}\n{'loss': 0.537, 'grad_norm': 0.7636995911598206, 'learning_rate': 1.568526194335965e-05, 'epoch': 3.2954545454545454}\n{'loss': 0.647, 'grad_norm': 0.7524527311325073, 'learning_rate': 1.562301884040981e-05, 'epoch': 3.3181818181818183}\n{'loss': 0.567, 'grad_norm': 0.8465794920921326, 'learning_rate': 1.556077573745997e-05, 'epoch': 3.340909090909091}\n{'loss': 0.5408, 'grad_norm': 0.686164379119873, 'learning_rate': 1.549853263451013e-05, 'epoch': 3.3636363636363638}\n{'loss': 0.5471, 'grad_norm': 1.0764391422271729, 'learning_rate': 1.543628953156029e-05, 'epoch': 3.3863636363636362}\n{'loss': 0.611, 'grad_norm': 0.7085055708885193, 'learning_rate': 1.537404642861045e-05, 'epoch': 3.409090909090909}\n{'loss': 0.5329, 'grad_norm': 0.8706771731376648, 'learning_rate': 1.531180332566061e-05, 'epoch': 3.4318181818181817}\n{'loss': 0.6015, 'grad_norm': 0.9215527772903442, 'learning_rate': 1.5249560222710769e-05, 'epoch': 3.4545454545454546}\n{'loss': 0.5238, 'grad_norm': 0.8839816451072693, 'learning_rate': 1.518731711976093e-05, 'epoch': 3.4772727272727275}\n{'loss': 0.5445, 'grad_norm': 0.860989511013031, 'learning_rate': 1.5125074016811089e-05, 'epoch': 3.5}\n{'loss': 0.6317, 'grad_norm': 0.9996260404586792, 'learning_rate': 1.5062830913861251e-05, 'epoch': 3.5227272727272725}\n{'loss': 0.5819, 'grad_norm': 0.7195078730583191, 'learning_rate': 1.5000587810911411e-05, 'epoch': 3.5454545454545454}\n{'loss': 0.5226, 'grad_norm': 0.798022985458374, 'learning_rate': 1.493834470796157e-05, 'epoch': 3.5681818181818183}\n{'loss': 0.5662, 'grad_norm': 0.6999961137771606, 'learning_rate': 1.4876101605011732e-05, 'epoch': 3.590909090909091}\n{'loss': 0.5502, 'grad_norm': 0.7480311989784241, 'learning_rate': 1.481385850206189e-05, 'epoch': 3.6136363636363638}\n{'loss': 0.5166, 'grad_norm': 0.6873355507850647, 'learning_rate': 1.4751615399112052e-05, 'epoch': 3.6363636363636362}\n{'loss': 0.5069, 'grad_norm': 0.6874735355377197, 'learning_rate': 1.468937229616221e-05, 'epoch': 3.659090909090909}\n{'loss': 0.5055, 'grad_norm': 0.7654288411140442, 'learning_rate': 1.4627129193212371e-05, 'epoch': 3.6818181818181817}\n{'loss': 0.5452, 'grad_norm': 1.0525407791137695, 'learning_rate': 1.4564886090262533e-05, 'epoch': 3.7045454545454546}\n{'loss': 0.534, 'grad_norm': 0.6860256195068359, 'learning_rate': 1.4502642987312692e-05, 'epoch': 3.7272727272727275}\n{'loss': 0.6207, 'grad_norm': 0.7821052670478821, 'learning_rate': 1.4440399884362852e-05, 'epoch': 3.75}\n{'loss': 0.4932, 'grad_norm': 1.0268453359603882, 'learning_rate': 1.4378156781413012e-05, 'epoch': 3.7727272727272725}\n{'loss': 0.5724, 'grad_norm': 0.7518611550331116, 'learning_rate': 1.4315913678463172e-05, 'epoch': 3.7954545454545454}\n{'loss': 0.5341, 'grad_norm': 0.7246431112289429, 'learning_rate': 1.4253670575513331e-05, 'epoch': 3.8181818181818183}\n{'loss': 0.5465, 'grad_norm': 1.04875648021698, 'learning_rate': 1.4191427472563493e-05, 'epoch': 3.840909090909091}\n{'loss': 0.5574, 'grad_norm': 0.6825386881828308, 'learning_rate': 1.4129184369613651e-05, 'epoch': 3.8636363636363638}\n{'loss': 0.4618, 'grad_norm': 0.8473690748214722, 'learning_rate': 1.4066941266663812e-05, 'epoch': 3.8863636363636362}\n{'loss': 0.5343, 'grad_norm': 0.6433570384979248, 'learning_rate': 1.4004698163713974e-05, 'epoch': 3.909090909090909}\n{'loss': 0.5424, 'grad_norm': 0.746891438961029, 'learning_rate': 1.3942455060764132e-05, 'epoch': 3.9318181818181817}\n{'loss': 0.5552, 'grad_norm': 0.9932224750518799, 'learning_rate': 1.3880211957814294e-05, 'epoch': 3.9545454545454546}\n{'loss': 0.5254, 'grad_norm': 0.961604118347168, 'learning_rate': 1.3817968854864453e-05, 'epoch': 3.9772727272727275}\n{'loss': 0.1855, 'grad_norm': 2.5011496543884277, 'learning_rate': 1.3755725751914613e-05, 'epoch': 4.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.7785    0.6391    0.7019      3311\n     PERCENT     0.8158    0.7209    0.7654        86\n        TYPE     0.8703    0.9287    0.8985     11299\n      VOLUME     0.0000    0.0000    0.0000        42\n\n   micro avg     0.8530    0.8598    0.8564     14738\n   macro avg     0.6161    0.5722    0.5915     14738\nweighted avg     0.8469    0.8598    0.8510     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.4985288679599762, 'eval_f1_macro': 0.5914749660398912, 'eval_precision': 0.8530362192002154, 'eval_recall': 0.8597503053331524, 'eval_f1': 0.8563801027304678, 'eval_accuracy': 0.8512103163761543, 'eval_runtime': 1.5362, 'eval_samples_per_second': 3586.77, 'eval_steps_per_second': 7.161, 'epoch': 4.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.5266, 'grad_norm': 0.7992786169052124, 'learning_rate': 1.3693482648964771e-05, 'epoch': 4.0227272727272725}\n{'loss': 0.4445, 'grad_norm': 0.8459336161613464, 'learning_rate': 1.3631239546014933e-05, 'epoch': 4.045454545454546}\n{'loss': 0.5575, 'grad_norm': 0.7165088653564453, 'learning_rate': 1.3568996443065095e-05, 'epoch': 4.068181818181818}\n{'loss': 0.5299, 'grad_norm': 0.8377694487571716, 'learning_rate': 1.3506753340115254e-05, 'epoch': 4.090909090909091}\n{'loss': 0.5896, 'grad_norm': 1.1664613485336304, 'learning_rate': 1.3444510237165414e-05, 'epoch': 4.113636363636363}\n{'loss': 0.4969, 'grad_norm': 0.8183268308639526, 'learning_rate': 1.3382267134215573e-05, 'epoch': 4.136363636363637}\n{'loss': 0.464, 'grad_norm': 0.8361140489578247, 'learning_rate': 1.3320024031265735e-05, 'epoch': 4.159090909090909}\n{'loss': 0.5246, 'grad_norm': 0.7960447669029236, 'learning_rate': 1.3257780928315893e-05, 'epoch': 4.181818181818182}\n{'loss': 0.4641, 'grad_norm': 0.7425571084022522, 'learning_rate': 1.3195537825366055e-05, 'epoch': 4.204545454545454}\n{'loss': 0.5655, 'grad_norm': 0.7955331206321716, 'learning_rate': 1.3133294722416214e-05, 'epoch': 4.2272727272727275}\n{'loss': 0.5254, 'grad_norm': 0.6897484660148621, 'learning_rate': 1.3071051619466374e-05, 'epoch': 4.25}\n{'loss': 0.5524, 'grad_norm': 0.8275352120399475, 'learning_rate': 1.3008808516516536e-05, 'epoch': 4.2727272727272725}\n{'loss': 0.4974, 'grad_norm': 0.971813440322876, 'learning_rate': 1.2946565413566694e-05, 'epoch': 4.295454545454546}\n{'loss': 0.519, 'grad_norm': 0.8719830513000488, 'learning_rate': 1.2884322310616856e-05, 'epoch': 4.318181818181818}\n{'loss': 0.5547, 'grad_norm': 0.9287128448486328, 'learning_rate': 1.2822079207667015e-05, 'epoch': 4.340909090909091}\n{'loss': 0.4992, 'grad_norm': 0.876663088798523, 'learning_rate': 1.2759836104717175e-05, 'epoch': 4.363636363636363}\n{'loss': 0.4834, 'grad_norm': 0.7564231753349304, 'learning_rate': 1.2697593001767334e-05, 'epoch': 4.386363636363637}\n{'loss': 0.5585, 'grad_norm': 0.8197281956672668, 'learning_rate': 1.2635349898817496e-05, 'epoch': 4.409090909090909}\n{'loss': 0.511, 'grad_norm': 0.7402580976486206, 'learning_rate': 1.2573106795867656e-05, 'epoch': 4.431818181818182}\n{'loss': 0.4743, 'grad_norm': 0.8900059461593628, 'learning_rate': 1.2510863692917816e-05, 'epoch': 4.454545454545454}\n{'loss': 0.5166, 'grad_norm': 0.6902375221252441, 'learning_rate': 1.2448620589967976e-05, 'epoch': 4.4772727272727275}\n{'loss': 0.545, 'grad_norm': 0.7500302195549011, 'learning_rate': 1.2386377487018135e-05, 'epoch': 4.5}\n{'loss': 0.5116, 'grad_norm': 0.7320728302001953, 'learning_rate': 1.2324134384068297e-05, 'epoch': 4.5227272727272725}\n{'loss': 0.5284, 'grad_norm': 1.636474370956421, 'learning_rate': 1.2261891281118455e-05, 'epoch': 4.545454545454545}\n{'loss': 0.4768, 'grad_norm': 0.772903561592102, 'learning_rate': 1.2199648178168616e-05, 'epoch': 4.568181818181818}\n{'loss': 0.4641, 'grad_norm': 0.9297249913215637, 'learning_rate': 1.2137405075218776e-05, 'epoch': 4.590909090909091}\n{'loss': 0.5012, 'grad_norm': 0.7293835878372192, 'learning_rate': 1.2075161972268936e-05, 'epoch': 4.613636363636363}\n{'loss': 0.5004, 'grad_norm': 0.6943044662475586, 'learning_rate': 1.2012918869319098e-05, 'epoch': 4.636363636363637}\n{'loss': 0.5162, 'grad_norm': 0.6988005042076111, 'learning_rate': 1.1950675766369257e-05, 'epoch': 4.659090909090909}\n{'loss': 0.4377, 'grad_norm': 0.7001980543136597, 'learning_rate': 1.1888432663419417e-05, 'epoch': 4.681818181818182}\n{'loss': 0.4741, 'grad_norm': 0.8418354392051697, 'learning_rate': 1.1826189560469577e-05, 'epoch': 4.704545454545455}\n{'loss': 0.4413, 'grad_norm': 0.8192055821418762, 'learning_rate': 1.1763946457519737e-05, 'epoch': 4.7272727272727275}\n{'loss': 0.5098, 'grad_norm': 1.0306446552276611, 'learning_rate': 1.1701703354569896e-05, 'epoch': 4.75}\n{'loss': 0.4611, 'grad_norm': 0.9330729842185974, 'learning_rate': 1.1639460251620058e-05, 'epoch': 4.7727272727272725}\n{'loss': 0.511, 'grad_norm': 1.8526310920715332, 'learning_rate': 1.1577217148670218e-05, 'epoch': 4.795454545454545}\n{'loss': 0.5159, 'grad_norm': 1.1699151992797852, 'learning_rate': 1.1514974045720377e-05, 'epoch': 4.818181818181818}\n{'loss': 0.4618, 'grad_norm': 1.0717389583587646, 'learning_rate': 1.1452730942770539e-05, 'epoch': 4.840909090909091}\n{'loss': 0.5135, 'grad_norm': 0.7817230224609375, 'learning_rate': 1.1390487839820697e-05, 'epoch': 4.863636363636363}\n{'loss': 0.5469, 'grad_norm': 0.862514853477478, 'learning_rate': 1.1328244736870859e-05, 'epoch': 4.886363636363637}\n{'loss': 0.4878, 'grad_norm': 1.2585973739624023, 'learning_rate': 1.1266001633921018e-05, 'epoch': 4.909090909090909}\n{'loss': 0.4501, 'grad_norm': 1.1268106698989868, 'learning_rate': 1.1203758530971178e-05, 'epoch': 4.931818181818182}\n{'loss': 0.477, 'grad_norm': 0.7358377575874329, 'learning_rate': 1.1141515428021336e-05, 'epoch': 4.954545454545455}\n{'loss': 0.4536, 'grad_norm': 1.1316852569580078, 'learning_rate': 1.1079272325071498e-05, 'epoch': 4.9772727272727275}\n{'loss': 0.5343, 'grad_norm': 3.7734904289245605, 'learning_rate': 1.1017029222121659e-05, 'epoch': 5.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.7708    0.6895    0.7279      3311\n     PERCENT     0.6847    0.8837    0.7716        86\n        TYPE     0.8841    0.9365    0.9096     11299\n      VOLUME     0.2308    0.0714    0.1091        42\n\n   micro avg     0.8598    0.8783    0.8689     14738\n   macro avg     0.6426    0.6453    0.6295     14738\nweighted avg     0.8556    0.8783    0.8657     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.462125688791275, 'eval_f1_macro': 0.6295303230281041, 'eval_precision': 0.8597808037196945, 'eval_recall': 0.8782738499117927, 'eval_f1': 0.8689289430403115, 'eval_accuracy': 0.8640511447461887, 'eval_runtime': 1.5211, 'eval_samples_per_second': 3622.384, 'eval_steps_per_second': 7.232, 'epoch': 5.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.4425, 'grad_norm': 0.9451390504837036, 'learning_rate': 1.0954786119171819e-05, 'epoch': 5.0227272727272725}\n{'loss': 0.5086, 'grad_norm': 0.953234076499939, 'learning_rate': 1.0892543016221979e-05, 'epoch': 5.045454545454546}\n{'loss': 0.4398, 'grad_norm': 0.9214457273483276, 'learning_rate': 1.0830299913272138e-05, 'epoch': 5.068181818181818}\n{'loss': 0.4874, 'grad_norm': 0.7365114092826843, 'learning_rate': 1.07680568103223e-05, 'epoch': 5.090909090909091}\n{'loss': 0.505, 'grad_norm': 0.7339307069778442, 'learning_rate': 1.070581370737246e-05, 'epoch': 5.113636363636363}\n{'loss': 0.5143, 'grad_norm': 0.7295292019844055, 'learning_rate': 1.064357060442262e-05, 'epoch': 5.136363636363637}\n{'loss': 0.4882, 'grad_norm': 0.9671683311462402, 'learning_rate': 1.0581327501472779e-05, 'epoch': 5.159090909090909}\n{'loss': 0.4344, 'grad_norm': 0.7734841704368591, 'learning_rate': 1.0519084398522939e-05, 'epoch': 5.181818181818182}\n{'loss': 0.5534, 'grad_norm': 0.8147000670433044, 'learning_rate': 1.0456841295573099e-05, 'epoch': 5.204545454545454}\n{'loss': 0.5238, 'grad_norm': 0.8823366761207581, 'learning_rate': 1.039459819262326e-05, 'epoch': 5.2272727272727275}\n{'loss': 0.4608, 'grad_norm': 0.7190056443214417, 'learning_rate': 1.033235508967342e-05, 'epoch': 5.25}\n{'loss': 0.5378, 'grad_norm': 0.920965850353241, 'learning_rate': 1.027011198672358e-05, 'epoch': 5.2727272727272725}\n{'loss': 0.4518, 'grad_norm': 1.0088831186294556, 'learning_rate': 1.020786888377374e-05, 'epoch': 5.295454545454546}\n{'loss': 0.4988, 'grad_norm': 0.9514790177345276, 'learning_rate': 1.01456257808239e-05, 'epoch': 5.318181818181818}\n{'loss': 0.4433, 'grad_norm': 0.7799137830734253, 'learning_rate': 1.008338267787406e-05, 'epoch': 5.340909090909091}\n{'loss': 0.4029, 'grad_norm': 1.0402395725250244, 'learning_rate': 1.002113957492422e-05, 'epoch': 5.363636363636363}\n{'loss': 0.4645, 'grad_norm': 0.67557692527771, 'learning_rate': 9.958896471974381e-06, 'epoch': 5.386363636363637}\n{'loss': 0.4142, 'grad_norm': 0.8527584075927734, 'learning_rate': 9.89665336902454e-06, 'epoch': 5.409090909090909}\n{'loss': 0.4307, 'grad_norm': 0.9536605477333069, 'learning_rate': 9.8344102660747e-06, 'epoch': 5.431818181818182}\n{'loss': 0.468, 'grad_norm': 1.0649935007095337, 'learning_rate': 9.772167163124862e-06, 'epoch': 5.454545454545454}\n{'loss': 0.5233, 'grad_norm': 0.6906070113182068, 'learning_rate': 9.709924060175022e-06, 'epoch': 5.4772727272727275}\n{'loss': 0.4939, 'grad_norm': 0.9654585719108582, 'learning_rate': 9.64768095722518e-06, 'epoch': 5.5}\n{'loss': 0.4401, 'grad_norm': 0.7820335626602173, 'learning_rate': 9.58543785427534e-06, 'epoch': 5.5227272727272725}\n{'loss': 0.5125, 'grad_norm': 1.0067477226257324, 'learning_rate': 9.523194751325501e-06, 'epoch': 5.545454545454545}\n{'loss': 0.4617, 'grad_norm': 0.7620985507965088, 'learning_rate': 9.460951648375661e-06, 'epoch': 5.568181818181818}\n{'loss': 0.3987, 'grad_norm': 0.901508629322052, 'learning_rate': 9.398708545425822e-06, 'epoch': 5.590909090909091}\n{'loss': 0.5788, 'grad_norm': 1.244451642036438, 'learning_rate': 9.336465442475982e-06, 'epoch': 5.613636363636363}\n{'loss': 0.4461, 'grad_norm': 0.9911916851997375, 'learning_rate': 9.274222339526142e-06, 'epoch': 5.636363636363637}\n{'loss': 0.4784, 'grad_norm': 0.994968831539154, 'learning_rate': 9.211979236576302e-06, 'epoch': 5.659090909090909}\n{'loss': 0.4499, 'grad_norm': 1.012031078338623, 'learning_rate': 9.149736133626463e-06, 'epoch': 5.681818181818182}\n{'loss': 0.4487, 'grad_norm': 0.7335257530212402, 'learning_rate': 9.087493030676623e-06, 'epoch': 5.704545454545455}\n{'loss': 0.5088, 'grad_norm': 1.1085091829299927, 'learning_rate': 9.025249927726783e-06, 'epoch': 5.7272727272727275}\n{'loss': 0.492, 'grad_norm': 1.2239123582839966, 'learning_rate': 8.963006824776942e-06, 'epoch': 5.75}\n{'loss': 0.439, 'grad_norm': 0.6944370865821838, 'learning_rate': 8.900763721827102e-06, 'epoch': 5.7727272727272725}\n{'loss': 0.4539, 'grad_norm': 0.8523861169815063, 'learning_rate': 8.838520618877262e-06, 'epoch': 5.795454545454545}\n{'loss': 0.4435, 'grad_norm': 0.9025305509567261, 'learning_rate': 8.776277515927424e-06, 'epoch': 5.818181818181818}\n{'loss': 0.4648, 'grad_norm': 0.8623844385147095, 'learning_rate': 8.714034412977583e-06, 'epoch': 5.840909090909091}\n{'loss': 0.4599, 'grad_norm': 0.9289374947547913, 'learning_rate': 8.651791310027743e-06, 'epoch': 5.863636363636363}\n{'loss': 0.408, 'grad_norm': 0.8076367974281311, 'learning_rate': 8.589548207077903e-06, 'epoch': 5.886363636363637}\n{'loss': 0.4667, 'grad_norm': 0.726581871509552, 'learning_rate': 8.527305104128063e-06, 'epoch': 5.909090909090909}\n{'loss': 0.4606, 'grad_norm': 0.7375680804252625, 'learning_rate': 8.465062001178224e-06, 'epoch': 5.931818181818182}\n{'loss': 0.4146, 'grad_norm': 0.7319016456604004, 'learning_rate': 8.402818898228384e-06, 'epoch': 5.954545454545455}\n{'loss': 0.4326, 'grad_norm': 0.9738734364509583, 'learning_rate': 8.340575795278542e-06, 'epoch': 5.9772727272727275}\n{'loss': 0.615, 'grad_norm': 3.388883113861084, 'learning_rate': 8.278332692328704e-06, 'epoch': 6.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.7851    0.6974    0.7386      3311\n     PERCENT     0.6475    0.9186    0.7596        86\n        TYPE     0.8877    0.9405    0.9133     11299\n      VOLUME     0.2083    0.1190    0.1515        42\n\n   micro avg     0.8646    0.8834    0.8739     14738\n   macro avg     0.6322    0.6689    0.6408     14738\nweighted avg     0.8613    0.8834    0.8710     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.4423520267009735, 'eval_f1_macro': 0.6407749421379748, 'eval_precision': 0.8645992429776214, 'eval_recall': 0.8834305875966888, 'eval_f1': 0.873913481222942, 'eval_accuracy': 0.8689142669799464, 'eval_runtime': 1.4315, 'eval_samples_per_second': 3849.115, 'eval_steps_per_second': 7.684, 'epoch': 6.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.4416, 'grad_norm': 1.1104689836502075, 'learning_rate': 8.216089589378865e-06, 'epoch': 6.0227272727272725}\n{'loss': 0.4111, 'grad_norm': 0.8638057708740234, 'learning_rate': 8.153846486429025e-06, 'epoch': 6.045454545454546}\n{'loss': 0.4512, 'grad_norm': 0.7640923857688904, 'learning_rate': 8.091603383479185e-06, 'epoch': 6.068181818181818}\n{'loss': 0.4178, 'grad_norm': 1.0868982076644897, 'learning_rate': 8.029360280529344e-06, 'epoch': 6.090909090909091}\n{'loss': 0.467, 'grad_norm': 1.008718490600586, 'learning_rate': 7.967117177579504e-06, 'epoch': 6.113636363636363}\n{'loss': 0.5047, 'grad_norm': 1.2062184810638428, 'learning_rate': 7.904874074629664e-06, 'epoch': 6.136363636363637}\n{'loss': 0.4734, 'grad_norm': 0.7880787253379822, 'learning_rate': 7.842630971679824e-06, 'epoch': 6.159090909090909}\n{'loss': 0.3994, 'grad_norm': 0.8272903561592102, 'learning_rate': 7.780387868729985e-06, 'epoch': 6.181818181818182}\n{'loss': 0.4401, 'grad_norm': 0.778494119644165, 'learning_rate': 7.718144765780145e-06, 'epoch': 6.204545454545454}\n{'loss': 0.4787, 'grad_norm': 1.0598100423812866, 'learning_rate': 7.655901662830305e-06, 'epoch': 6.2272727272727275}\n{'loss': 0.4297, 'grad_norm': 0.9357891082763672, 'learning_rate': 7.593658559880465e-06, 'epoch': 6.25}\n{'loss': 0.4025, 'grad_norm': 0.8225591778755188, 'learning_rate': 7.5314154569306255e-06, 'epoch': 6.2727272727272725}\n{'loss': 0.4786, 'grad_norm': 0.762573778629303, 'learning_rate': 7.469172353980785e-06, 'epoch': 6.295454545454546}\n{'loss': 0.4958, 'grad_norm': 0.8636038899421692, 'learning_rate': 7.406929251030945e-06, 'epoch': 6.318181818181818}\n{'loss': 0.428, 'grad_norm': 0.7561801075935364, 'learning_rate': 7.344686148081105e-06, 'epoch': 6.340909090909091}\n{'loss': 0.4584, 'grad_norm': 0.9401914477348328, 'learning_rate': 7.2824430451312665e-06, 'epoch': 6.363636363636363}\n{'loss': 0.4952, 'grad_norm': 0.7881731986999512, 'learning_rate': 7.220199942181426e-06, 'epoch': 6.386363636363637}\n{'loss': 0.4285, 'grad_norm': 0.9957228899002075, 'learning_rate': 7.157956839231586e-06, 'epoch': 6.409090909090909}\n{'loss': 0.4775, 'grad_norm': 0.9141228795051575, 'learning_rate': 7.095713736281746e-06, 'epoch': 6.431818181818182}\n{'loss': 0.3935, 'grad_norm': 0.7458661794662476, 'learning_rate': 7.033470633331906e-06, 'epoch': 6.454545454545454}\n{'loss': 0.475, 'grad_norm': 0.8918846845626831, 'learning_rate': 6.971227530382066e-06, 'epoch': 6.4772727272727275}\n{'loss': 0.5412, 'grad_norm': 0.818328857421875, 'learning_rate': 6.908984427432226e-06, 'epoch': 6.5}\n{'loss': 0.4672, 'grad_norm': 0.7102353572845459, 'learning_rate': 6.846741324482386e-06, 'epoch': 6.5227272727272725}\n{'loss': 0.4654, 'grad_norm': 0.7303412556648254, 'learning_rate': 6.784498221532548e-06, 'epoch': 6.545454545454545}\n{'loss': 0.446, 'grad_norm': 0.9299303293228149, 'learning_rate': 6.722255118582707e-06, 'epoch': 6.568181818181818}\n{'loss': 0.4377, 'grad_norm': 0.9797187447547913, 'learning_rate': 6.660012015632867e-06, 'epoch': 6.590909090909091}\n{'loss': 0.3718, 'grad_norm': 0.7726216912269592, 'learning_rate': 6.5977689126830275e-06, 'epoch': 6.613636363636363}\n{'loss': 0.3794, 'grad_norm': 1.0192705392837524, 'learning_rate': 6.535525809733187e-06, 'epoch': 6.636363636363637}\n{'loss': 0.4513, 'grad_norm': 0.8163581490516663, 'learning_rate': 6.473282706783347e-06, 'epoch': 6.659090909090909}\n{'loss': 0.484, 'grad_norm': 1.1740611791610718, 'learning_rate': 6.411039603833507e-06, 'epoch': 6.681818181818182}\n{'loss': 0.4118, 'grad_norm': 0.9533903002738953, 'learning_rate': 6.348796500883667e-06, 'epoch': 6.704545454545455}\n{'loss': 0.4233, 'grad_norm': 0.9211399555206299, 'learning_rate': 6.286553397933828e-06, 'epoch': 6.7272727272727275}\n{'loss': 0.4959, 'grad_norm': 1.1481492519378662, 'learning_rate': 6.224310294983988e-06, 'epoch': 6.75}\n{'loss': 0.4994, 'grad_norm': 0.9081276655197144, 'learning_rate': 6.162067192034148e-06, 'epoch': 6.7727272727272725}\n{'loss': 0.4616, 'grad_norm': 0.743995726108551, 'learning_rate': 6.099824089084308e-06, 'epoch': 6.795454545454545}\n{'loss': 0.3839, 'grad_norm': 0.7125176787376404, 'learning_rate': 6.037580986134468e-06, 'epoch': 6.818181818181818}\n{'loss': 0.449, 'grad_norm': 0.8012779355049133, 'learning_rate': 5.975337883184628e-06, 'epoch': 6.840909090909091}\n{'loss': 0.4227, 'grad_norm': 1.1797046661376953, 'learning_rate': 5.9130947802347885e-06, 'epoch': 6.863636363636363}\n{'loss': 0.3923, 'grad_norm': 0.8031394481658936, 'learning_rate': 5.850851677284948e-06, 'epoch': 6.886363636363637}\n{'loss': 0.4822, 'grad_norm': 0.7505722045898438, 'learning_rate': 5.788608574335109e-06, 'epoch': 6.909090909090909}\n{'loss': 0.4557, 'grad_norm': 1.3835773468017578, 'learning_rate': 5.726365471385269e-06, 'epoch': 6.931818181818182}\n{'loss': 0.4744, 'grad_norm': 0.8261498212814331, 'learning_rate': 5.6641223684354295e-06, 'epoch': 6.954545454545455}\n{'loss': 0.4741, 'grad_norm': 0.8420316576957703, 'learning_rate': 5.601879265485589e-06, 'epoch': 6.9772727272727275}\n{'loss': 0.4897, 'grad_norm': 3.5937867164611816, 'learning_rate': 5.539636162535749e-06, 'epoch': 7.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.7855    0.7155    0.7489      3311\n     PERCENT     0.6504    0.9302    0.7656        86\n        TYPE     0.8922    0.9411    0.9160     11299\n      VOLUME     0.3226    0.2381    0.2740        42\n\n   micro avg     0.8677    0.8883    0.8779     14738\n   macro avg     0.6627    0.7062    0.6761     14738\nweighted avg     0.8652    0.8883    0.8757     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.42939746379852295, 'eval_f1_macro': 0.6760859269381494, 'eval_precision': 0.8677094379639448, 'eval_recall': 0.8883159180350115, 'eval_f1': 0.8778917722792194, 'eval_accuracy': 0.8730670455166384, 'eval_runtime': 1.4354, 'eval_samples_per_second': 3838.638, 'eval_steps_per_second': 7.663, 'epoch': 7.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.486, 'grad_norm': 0.7487975358963013, 'learning_rate': 5.477393059585909e-06, 'epoch': 7.0227272727272725}\n{'loss': 0.4761, 'grad_norm': 0.8721815943717957, 'learning_rate': 5.415149956636069e-06, 'epoch': 7.045454545454546}\n{'loss': 0.4687, 'grad_norm': 0.7766255140304565, 'learning_rate': 5.35290685368623e-06, 'epoch': 7.068181818181818}\n{'loss': 0.4144, 'grad_norm': 0.7740992903709412, 'learning_rate': 5.290663750736389e-06, 'epoch': 7.090909090909091}\n{'loss': 0.4592, 'grad_norm': 0.9316577315330505, 'learning_rate': 5.2284206477865495e-06, 'epoch': 7.113636363636363}\n{'loss': 0.449, 'grad_norm': 0.696234941482544, 'learning_rate': 5.16617754483671e-06, 'epoch': 7.136363636363637}\n{'loss': 0.4545, 'grad_norm': 0.7468797564506531, 'learning_rate': 5.10393444188687e-06, 'epoch': 7.159090909090909}\n{'loss': 0.4197, 'grad_norm': 0.7591421008110046, 'learning_rate': 5.04169133893703e-06, 'epoch': 7.181818181818182}\n{'loss': 0.456, 'grad_norm': 0.8540006875991821, 'learning_rate': 4.9794482359871905e-06, 'epoch': 7.204545454545454}\n{'loss': 0.4458, 'grad_norm': 0.781035840511322, 'learning_rate': 4.91720513303735e-06, 'epoch': 7.2272727272727275}\n{'loss': 0.4077, 'grad_norm': 0.9089345335960388, 'learning_rate': 4.854962030087511e-06, 'epoch': 7.25}\n{'loss': 0.431, 'grad_norm': 0.8057780861854553, 'learning_rate': 4.79271892713767e-06, 'epoch': 7.2727272727272725}\n{'loss': 0.4554, 'grad_norm': 0.7314890027046204, 'learning_rate': 4.730475824187831e-06, 'epoch': 7.295454545454546}\n{'loss': 0.4569, 'grad_norm': 1.1257514953613281, 'learning_rate': 4.668232721237991e-06, 'epoch': 7.318181818181818}\n{'loss': 0.4728, 'grad_norm': 0.906049907207489, 'learning_rate': 4.605989618288151e-06, 'epoch': 7.340909090909091}\n{'loss': 0.4517, 'grad_norm': 0.9754122495651245, 'learning_rate': 4.543746515338311e-06, 'epoch': 7.363636363636363}\n{'loss': 0.4202, 'grad_norm': 0.8028268814086914, 'learning_rate': 4.481503412388471e-06, 'epoch': 7.386363636363637}\n{'loss': 0.4655, 'grad_norm': 0.7240729331970215, 'learning_rate': 4.419260309438631e-06, 'epoch': 7.409090909090909}\n{'loss': 0.3789, 'grad_norm': 1.131700873374939, 'learning_rate': 4.357017206488791e-06, 'epoch': 7.431818181818182}\n{'loss': 0.4188, 'grad_norm': 0.8437978029251099, 'learning_rate': 4.2947741035389515e-06, 'epoch': 7.454545454545454}\n{'loss': 0.4218, 'grad_norm': 0.7500682473182678, 'learning_rate': 4.232531000589112e-06, 'epoch': 7.4772727272727275}\n{'loss': 0.4184, 'grad_norm': 0.9640881419181824, 'learning_rate': 4.170287897639271e-06, 'epoch': 7.5}\n{'loss': 0.4765, 'grad_norm': 0.8777405023574829, 'learning_rate': 4.108044794689432e-06, 'epoch': 7.5227272727272725}\n{'loss': 0.4244, 'grad_norm': 0.9335067272186279, 'learning_rate': 4.0458016917395925e-06, 'epoch': 7.545454545454545}\n{'loss': 0.3942, 'grad_norm': 0.8277714848518372, 'learning_rate': 3.983558588789752e-06, 'epoch': 7.568181818181818}\n{'loss': 0.426, 'grad_norm': 0.7185879349708557, 'learning_rate': 3.921315485839912e-06, 'epoch': 7.590909090909091}\n{'loss': 0.5015, 'grad_norm': 0.9962426424026489, 'learning_rate': 3.859072382890072e-06, 'epoch': 7.613636363636363}\n{'loss': 0.4699, 'grad_norm': 0.9728714823722839, 'learning_rate': 3.7968292799402326e-06, 'epoch': 7.636363636363637}\n{'loss': 0.4288, 'grad_norm': 0.6945823431015015, 'learning_rate': 3.7345861769903925e-06, 'epoch': 7.659090909090909}\n{'loss': 0.4521, 'grad_norm': 0.7854250073432922, 'learning_rate': 3.6723430740405527e-06, 'epoch': 7.681818181818182}\n{'loss': 0.4839, 'grad_norm': 0.8288958072662354, 'learning_rate': 3.610099971090713e-06, 'epoch': 7.704545454545455}\n{'loss': 0.4838, 'grad_norm': 0.7822319865226746, 'learning_rate': 3.547856868140873e-06, 'epoch': 7.7272727272727275}\n{'loss': 0.4048, 'grad_norm': 0.7826443910598755, 'learning_rate': 3.485613765191033e-06, 'epoch': 7.75}\n{'loss': 0.3849, 'grad_norm': 0.6988320350646973, 'learning_rate': 3.423370662241193e-06, 'epoch': 7.7727272727272725}\n{'loss': 0.4053, 'grad_norm': 0.8459277749061584, 'learning_rate': 3.3611275592913535e-06, 'epoch': 7.795454545454545}\n{'loss': 0.3665, 'grad_norm': 0.8047062158584595, 'learning_rate': 3.2988844563415138e-06, 'epoch': 7.818181818181818}\n{'loss': 0.4324, 'grad_norm': 0.7567710280418396, 'learning_rate': 3.2366413533916736e-06, 'epoch': 7.840909090909091}\n{'loss': 0.4109, 'grad_norm': 1.1906287670135498, 'learning_rate': 3.1743982504418334e-06, 'epoch': 7.863636363636363}\n{'loss': 0.4422, 'grad_norm': 0.8152698278427124, 'learning_rate': 3.112155147491994e-06, 'epoch': 7.886363636363637}\n{'loss': 0.4437, 'grad_norm': 0.9318056702613831, 'learning_rate': 3.049912044542154e-06, 'epoch': 7.909090909090909}\n{'loss': 0.4497, 'grad_norm': 0.8096404671669006, 'learning_rate': 2.987668941592314e-06, 'epoch': 7.931818181818182}\n{'loss': 0.4427, 'grad_norm': 0.9875428676605225, 'learning_rate': 2.925425838642474e-06, 'epoch': 7.954545454545455}\n{'loss': 0.408, 'grad_norm': 0.8807462453842163, 'learning_rate': 2.8631827356926346e-06, 'epoch': 7.9772727272727275}\n{'loss': 0.454, 'grad_norm': 5.307551383972168, 'learning_rate': 2.8009396327427945e-06, 'epoch': 8.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.7913    0.7170    0.7523      3311\n     PERCENT     0.6400    0.9302    0.7583        86\n        TYPE     0.8924    0.9428    0.9169     11299\n      VOLUME     0.3235    0.2619    0.2895        42\n\n   micro avg     0.8689    0.8901    0.8794     14738\n   macro avg     0.6618    0.7130    0.6793     14738\nweighted avg     0.8666    0.8901    0.8772     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.42237621545791626, 'eval_f1_macro': 0.6792511210058958, 'eval_precision': 0.8689143538451348, 'eval_recall': 0.8900800651377392, 'eval_f1': 0.8793698676051617, 'eval_accuracy': 0.8746516583793236, 'eval_runtime': 1.8157, 'eval_samples_per_second': 3034.637, 'eval_steps_per_second': 6.058, 'epoch': 8.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.4289, 'grad_norm': 0.8662462830543518, 'learning_rate': 2.7386965297929547e-06, 'epoch': 8.022727272727273}\n{'loss': 0.4738, 'grad_norm': 0.6741620302200317, 'learning_rate': 2.676453426843115e-06, 'epoch': 8.045454545454545}\n{'loss': 0.445, 'grad_norm': 1.0083742141723633, 'learning_rate': 2.6142103238932748e-06, 'epoch': 8.068181818181818}\n{'loss': 0.4227, 'grad_norm': 0.7946215867996216, 'learning_rate': 2.551967220943435e-06, 'epoch': 8.090909090909092}\n{'loss': 0.3611, 'grad_norm': 0.874499499797821, 'learning_rate': 2.4897241179935953e-06, 'epoch': 8.113636363636363}\n{'loss': 0.4691, 'grad_norm': 1.0417171716690063, 'learning_rate': 2.4274810150437555e-06, 'epoch': 8.136363636363637}\n{'loss': 0.4445, 'grad_norm': 0.7890623211860657, 'learning_rate': 2.3652379120939153e-06, 'epoch': 8.159090909090908}\n{'loss': 0.4936, 'grad_norm': 1.0631932020187378, 'learning_rate': 2.3029948091440756e-06, 'epoch': 8.181818181818182}\n{'loss': 0.3933, 'grad_norm': 1.100574254989624, 'learning_rate': 2.2407517061942354e-06, 'epoch': 8.204545454545455}\n{'loss': 0.4083, 'grad_norm': 0.9501357078552246, 'learning_rate': 2.1785086032443956e-06, 'epoch': 8.227272727272727}\n{'loss': 0.431, 'grad_norm': 0.7439053058624268, 'learning_rate': 2.116265500294556e-06, 'epoch': 8.25}\n{'loss': 0.4664, 'grad_norm': 0.8282196521759033, 'learning_rate': 2.054022397344716e-06, 'epoch': 8.272727272727273}\n{'loss': 0.4623, 'grad_norm': 0.9565415978431702, 'learning_rate': 1.991779294394876e-06, 'epoch': 8.295454545454545}\n{'loss': 0.4216, 'grad_norm': 0.8470660448074341, 'learning_rate': 1.929536191445036e-06, 'epoch': 8.318181818181818}\n{'loss': 0.4141, 'grad_norm': 1.0592344999313354, 'learning_rate': 1.8672930884951962e-06, 'epoch': 8.340909090909092}\n{'loss': 0.5069, 'grad_norm': 0.9846398234367371, 'learning_rate': 1.8050499855453565e-06, 'epoch': 8.363636363636363}\n{'loss': 0.4501, 'grad_norm': 0.8219089508056641, 'learning_rate': 1.7428068825955165e-06, 'epoch': 8.386363636363637}\n{'loss': 0.4234, 'grad_norm': 0.7459638714790344, 'learning_rate': 1.6805637796456768e-06, 'epoch': 8.409090909090908}\n{'loss': 0.4667, 'grad_norm': 0.8227691650390625, 'learning_rate': 1.6183206766958368e-06, 'epoch': 8.431818181818182}\n{'loss': 0.3964, 'grad_norm': 0.8432027697563171, 'learning_rate': 1.556077573745997e-06, 'epoch': 8.454545454545455}\n{'loss': 0.386, 'grad_norm': 0.7491468787193298, 'learning_rate': 1.493834470796157e-06, 'epoch': 8.477272727272727}\n{'loss': 0.4495, 'grad_norm': 0.8935680389404297, 'learning_rate': 1.4315913678463173e-06, 'epoch': 8.5}\n{'loss': 0.4116, 'grad_norm': 0.7423348426818848, 'learning_rate': 1.3693482648964773e-06, 'epoch': 8.522727272727273}\n{'loss': 0.4736, 'grad_norm': 0.8684942126274109, 'learning_rate': 1.3071051619466374e-06, 'epoch': 8.545454545454545}\n{'loss': 0.415, 'grad_norm': 0.7873053550720215, 'learning_rate': 1.2448620589967976e-06, 'epoch': 8.568181818181818}\n{'loss': 0.4531, 'grad_norm': 0.839145839214325, 'learning_rate': 1.1826189560469577e-06, 'epoch': 8.590909090909092}\n{'loss': 0.5417, 'grad_norm': 1.0169845819473267, 'learning_rate': 1.1203758530971177e-06, 'epoch': 8.613636363636363}\n{'loss': 0.3603, 'grad_norm': 1.13913094997406, 'learning_rate': 1.058132750147278e-06, 'epoch': 8.636363636363637}\n{'loss': 0.4367, 'grad_norm': 0.7638950943946838, 'learning_rate': 9.95889647197438e-07, 'epoch': 8.659090909090908}\n{'loss': 0.4285, 'grad_norm': 0.7820109128952026, 'learning_rate': 9.336465442475981e-07, 'epoch': 8.681818181818182}\n{'loss': 0.4749, 'grad_norm': 0.7861039042472839, 'learning_rate': 8.714034412977583e-07, 'epoch': 8.704545454545455}\n{'loss': 0.4403, 'grad_norm': 0.8589596748352051, 'learning_rate': 8.091603383479184e-07, 'epoch': 8.727272727272727}\n{'loss': 0.3588, 'grad_norm': 0.8419771790504456, 'learning_rate': 7.469172353980785e-07, 'epoch': 8.75}\n{'loss': 0.4012, 'grad_norm': 1.0080887079238892, 'learning_rate': 6.846741324482387e-07, 'epoch': 8.772727272727273}\n{'loss': 0.4182, 'grad_norm': 0.7737226486206055, 'learning_rate': 6.224310294983988e-07, 'epoch': 8.795454545454545}\n{'loss': 0.3564, 'grad_norm': 1.1792089939117432, 'learning_rate': 5.601879265485588e-07, 'epoch': 8.818181818181818}\n{'loss': 0.4819, 'grad_norm': 0.7938697934150696, 'learning_rate': 4.97944823598719e-07, 'epoch': 8.840909090909092}\n{'loss': 0.4334, 'grad_norm': 0.8925006985664368, 'learning_rate': 4.3570172064887913e-07, 'epoch': 8.863636363636363}\n{'loss': 0.4545, 'grad_norm': 0.7553014755249023, 'learning_rate': 3.7345861769903927e-07, 'epoch': 8.886363636363637}\n{'loss': 0.3955, 'grad_norm': 0.7538952827453613, 'learning_rate': 3.112155147491994e-07, 'epoch': 8.909090909090908}\n{'loss': 0.3935, 'grad_norm': 0.7812740802764893, 'learning_rate': 2.489724117993595e-07, 'epoch': 8.931818181818182}\n{'loss': 0.3862, 'grad_norm': 0.9235272407531738, 'learning_rate': 1.8672930884951963e-07, 'epoch': 8.954545454545455}\n{'loss': 0.4545, 'grad_norm': 0.98835289478302, 'learning_rate': 1.2448620589967975e-07, 'epoch': 8.977272727272727}\n{'loss': 0.4035, 'grad_norm': 3.512951612472534, 'learning_rate': 6.224310294983987e-08, 'epoch': 9.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.7906    0.7197    0.7535      3311\n     PERCENT     0.6452    0.9302    0.7619        86\n        TYPE     0.8929    0.9426    0.9171     11299\n      VOLUME     0.3235    0.2619    0.2895        42\n\n   micro avg     0.8692    0.8905    0.8797     14738\n   macro avg     0.6631    0.7136    0.6805     14738\nweighted avg     0.8669    0.8905    0.8776     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.42021608352661133, 'eval_f1_macro': 0.6804929809281153, 'eval_precision': 0.8691966355387774, 'eval_recall': 0.8904871760075994, 'eval_f1': 0.8797131078861816, 'eval_accuracy': 0.875034151139282, 'eval_runtime': 1.4359, 'eval_samples_per_second': 3837.397, 'eval_steps_per_second': 7.661, 'epoch': 9.0}\n{'train_runtime': 56.557, 'train_samples_per_second': 3507.577, 'train_steps_per_second': 7.002, 'train_loss': 0.7144588807767088, 'epoch': 9.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n[I 2025-09-26 18:08:59,006] Trial 4 pruned. \n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.7906    0.7197    0.7535      3311\n     PERCENT     0.6452    0.9302    0.7619        86\n        TYPE     0.8929    0.9426    0.9171     11299\n      VOLUME     0.3235    0.2619    0.2895        42\n\n   micro avg     0.8692    0.8905    0.8797     14738\n   macro avg     0.6631    0.7136    0.6805     14738\nweighted avg     0.8669    0.8905    0.8776     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.42021608352661133, 'eval_f1_macro': 0.6804929809281153, 'eval_precision': 0.8691966355387774, 'eval_recall': 0.8904871760075994, 'eval_f1': 0.8797131078861816, 'eval_accuracy': 0.875034151139282, 'eval_runtime': 1.503, 'eval_samples_per_second': 3665.947, 'eval_steps_per_second': 7.319, 'epoch': 9.0}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/27552 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2735c368f014ed683eb3e0b104358f7"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nThe speedups for torchdynamo mostly come with GPU Ampere or higher and which is not detected here.\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/tmp/ipykernel_36/3364797558.py:66: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 2.2015, 'grad_norm': 6.750888347625732, 'learning_rate': 0.0, 'epoch': 0.022727272727272728}\n{'loss': 2.197, 'grad_norm': 6.8518571853637695, 'learning_rate': 4.100356769732791e-06, 'epoch': 0.045454545454545456}\n{'loss': 2.198, 'grad_norm': 6.754837512969971, 'learning_rate': 8.200713539465582e-06, 'epoch': 0.06818181818181818}\n{'loss': 2.1838, 'grad_norm': 7.03603982925415, 'learning_rate': 1.2301070309198374e-05, 'epoch': 0.09090909090909091}\n{'loss': 2.1453, 'grad_norm': 6.601052284240723, 'learning_rate': 1.6401427078931164e-05, 'epoch': 0.11363636363636363}\n{'loss': 2.1276, 'grad_norm': 6.649338245391846, 'learning_rate': 2.0501783848663958e-05, 'epoch': 0.13636363636363635}\n{'loss': 2.0787, 'grad_norm': 6.3609700202941895, 'learning_rate': 2.4602140618396748e-05, 'epoch': 0.1590909090909091}\n{'loss': 2.0174, 'grad_norm': 6.590847015380859, 'learning_rate': 2.8702497388129538e-05, 'epoch': 0.18181818181818182}\n{'loss': 1.9874, 'grad_norm': 6.203037261962891, 'learning_rate': 3.280285415786233e-05, 'epoch': 0.20454545454545456}\n{'loss': 1.9242, 'grad_norm': 6.305242538452148, 'learning_rate': 3.690321092759512e-05, 'epoch': 0.22727272727272727}\n{'loss': 1.8317, 'grad_norm': 6.298113822937012, 'learning_rate': 4.1003567697327915e-05, 'epoch': 0.25}\n{'loss': 1.7682, 'grad_norm': 6.041388511657715, 'learning_rate': 4.510392446706071e-05, 'epoch': 0.2727272727272727}\n{'loss': 1.6963, 'grad_norm': 5.669629096984863, 'learning_rate': 4.9204281236793496e-05, 'epoch': 0.29545454545454547}\n{'loss': 1.6334, 'grad_norm': 5.186293125152588, 'learning_rate': 5.330463800652629e-05, 'epoch': 0.3181818181818182}\n{'loss': 1.5569, 'grad_norm': 4.783466815948486, 'learning_rate': 5.7404994776259076e-05, 'epoch': 0.3409090909090909}\n{'loss': 1.4911, 'grad_norm': 4.185986042022705, 'learning_rate': 6.150535154599188e-05, 'epoch': 0.36363636363636365}\n{'loss': 1.4118, 'grad_norm': 3.8252713680267334, 'learning_rate': 6.560570831572466e-05, 'epoch': 0.38636363636363635}\n{'loss': 1.3188, 'grad_norm': 3.2821502685546875, 'learning_rate': 6.970606508545745e-05, 'epoch': 0.4090909090909091}\n{'loss': 1.2935, 'grad_norm': 2.5860438346862793, 'learning_rate': 7.380642185519024e-05, 'epoch': 0.4318181818181818}\n{'loss': 1.2098, 'grad_norm': 2.2018752098083496, 'learning_rate': 7.790677862492304e-05, 'epoch': 0.45454545454545453}\n{'loss': 1.2132, 'grad_norm': 1.84651780128479, 'learning_rate': 8.200713539465583e-05, 'epoch': 0.4772727272727273}\n{'loss': 1.1479, 'grad_norm': 1.857578158378601, 'learning_rate': 8.610749216438861e-05, 'epoch': 0.5}\n{'loss': 1.1799, 'grad_norm': 2.148660182952881, 'learning_rate': 9.020784893412142e-05, 'epoch': 0.5227272727272727}\n{'loss': 1.0944, 'grad_norm': 1.747178077697754, 'learning_rate': 9.43082057038542e-05, 'epoch': 0.5454545454545454}\n{'loss': 1.1146, 'grad_norm': 1.8341710567474365, 'learning_rate': 9.840856247358699e-05, 'epoch': 0.5681818181818182}\n{'loss': 1.0564, 'grad_norm': 1.620929479598999, 'learning_rate': 0.00010250891924331977, 'epoch': 0.5909090909090909}\n{'loss': 0.9883, 'grad_norm': 1.2329572439193726, 'learning_rate': 0.00010660927601305258, 'epoch': 0.6136363636363636}\n{'loss': 0.9753, 'grad_norm': 1.3261159658432007, 'learning_rate': 0.00011070963278278537, 'epoch': 0.6363636363636364}\n{'loss': 0.9535, 'grad_norm': 1.57249915599823, 'learning_rate': 0.00011480998955251815, 'epoch': 0.6590909090909091}\n{'loss': 0.9066, 'grad_norm': 1.900818109512329, 'learning_rate': 0.00011891034632225095, 'epoch': 0.6818181818181818}\n{'loss': 0.8732, 'grad_norm': 1.8091121912002563, 'learning_rate': 0.00012301070309198375, 'epoch': 0.7045454545454546}\n{'loss': 0.8221, 'grad_norm': 1.44058096408844, 'learning_rate': 0.00012711105986171653, 'epoch': 0.7272727272727273}\n{'loss': 0.8398, 'grad_norm': 1.1409177780151367, 'learning_rate': 0.00012665217516907496, 'epoch': 0.75}\n{'loss': 0.8523, 'grad_norm': 1.2460057735443115, 'learning_rate': 0.00012619329047643338, 'epoch': 0.7727272727272727}\n{'loss': 0.7411, 'grad_norm': 0.8913649320602417, 'learning_rate': 0.0001257344057837918, 'epoch': 0.7954545454545454}\n{'loss': 0.7123, 'grad_norm': 0.872954785823822, 'learning_rate': 0.00012527552109115023, 'epoch': 0.8181818181818182}\n{'loss': 0.7214, 'grad_norm': 0.826974093914032, 'learning_rate': 0.00012481663639850866, 'epoch': 0.8409090909090909}\n{'loss': 0.6681, 'grad_norm': 0.7975564002990723, 'learning_rate': 0.00012435775170586708, 'epoch': 0.8636363636363636}\n{'loss': 0.6651, 'grad_norm': 0.9812060594558716, 'learning_rate': 0.0001238988670132255, 'epoch': 0.8863636363636364}\n{'loss': 0.6553, 'grad_norm': 1.1626768112182617, 'learning_rate': 0.00012343998232058393, 'epoch': 0.9090909090909091}\n{'loss': 0.7129, 'grad_norm': 0.9970523715019226, 'learning_rate': 0.00012298109762794236, 'epoch': 0.9318181818181818}\n{'loss': 0.6225, 'grad_norm': 0.7593608498573303, 'learning_rate': 0.00012252221293530078, 'epoch': 0.9545454545454546}\n{'loss': 0.6231, 'grad_norm': 1.1053996086120605, 'learning_rate': 0.0001220633282426592, 'epoch': 0.9772727272727273}\n{'loss': 0.5911, 'grad_norm': 2.9629528522491455, 'learning_rate': 0.00012160444355001763, 'epoch': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.6874    0.6515    0.6690      3142\n     PERCENT     1.0000    0.0455    0.0870        66\n        TYPE     0.8312    0.9005    0.8645     11415\n      VOLUME     0.0000    0.0000    0.0000        70\n\n   micro avg     0.8033    0.8391    0.8208     14693\n   macro avg     0.6297    0.3994    0.4051     14693\nweighted avg     0.7973    0.8391    0.8151     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.5787249803543091, 'eval_f1_macro': 0.4050956017815738, 'eval_precision': 0.8033491887665342, 'eval_recall': 0.8391070577826176, 'eval_f1': 0.8208388814913449, 'eval_accuracy': 0.8245184459680053, 'eval_runtime': 1.441, 'eval_samples_per_second': 3824.383, 'eval_steps_per_second': 7.633, 'epoch': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.5581, 'grad_norm': 1.3813971281051636, 'learning_rate': 0.00012114555885737605, 'epoch': 1.0227272727272727}\n{'loss': 0.5741, 'grad_norm': 0.9424018859863281, 'learning_rate': 0.00012068667416473446, 'epoch': 1.0454545454545454}\n{'loss': 0.573, 'grad_norm': 0.9257428050041199, 'learning_rate': 0.0001202277894720929, 'epoch': 1.0681818181818181}\n{'loss': 0.5266, 'grad_norm': 0.7516172528266907, 'learning_rate': 0.00011976890477945131, 'epoch': 1.0909090909090908}\n{'loss': 0.5204, 'grad_norm': 1.060097336769104, 'learning_rate': 0.00011931002008680975, 'epoch': 1.1136363636363635}\n{'loss': 0.5577, 'grad_norm': 0.7266805768013, 'learning_rate': 0.00011885113539416816, 'epoch': 1.1363636363636362}\n{'loss': 0.5469, 'grad_norm': 0.9445772171020508, 'learning_rate': 0.0001183922507015266, 'epoch': 1.1590909090909092}\n{'loss': 0.5419, 'grad_norm': 1.4390318393707275, 'learning_rate': 0.00011793336600888501, 'epoch': 1.1818181818181819}\n{'loss': 0.5084, 'grad_norm': 1.1490479707717896, 'learning_rate': 0.00011747448131624345, 'epoch': 1.2045454545454546}\n{'loss': 0.4972, 'grad_norm': 0.6870197057723999, 'learning_rate': 0.00011701559662360186, 'epoch': 1.2272727272727273}\n{'loss': 0.5162, 'grad_norm': 1.093956708908081, 'learning_rate': 0.0001165567119309603, 'epoch': 1.25}\n{'loss': 0.5039, 'grad_norm': 0.7288813591003418, 'learning_rate': 0.00011609782723831871, 'epoch': 1.2727272727272727}\n{'loss': 0.4574, 'grad_norm': 0.8194051384925842, 'learning_rate': 0.00011563894254567713, 'epoch': 1.2954545454545454}\n{'loss': 0.4437, 'grad_norm': 1.2321134805679321, 'learning_rate': 0.00011518005785303556, 'epoch': 1.3181818181818181}\n{'loss': 0.4885, 'grad_norm': 1.0289678573608398, 'learning_rate': 0.00011472117316039397, 'epoch': 1.3409090909090908}\n{'loss': 0.4892, 'grad_norm': 0.7727891206741333, 'learning_rate': 0.00011426228846775241, 'epoch': 1.3636363636363638}\n{'loss': 0.4986, 'grad_norm': 0.7406771183013916, 'learning_rate': 0.00011380340377511082, 'epoch': 1.3863636363636362}\n{'loss': 0.4001, 'grad_norm': 0.9031446576118469, 'learning_rate': 0.00011334451908246926, 'epoch': 1.4090909090909092}\n{'loss': 0.4685, 'grad_norm': 0.8651310801506042, 'learning_rate': 0.00011288563438982767, 'epoch': 1.4318181818181819}\n{'loss': 0.486, 'grad_norm': 0.8514887690544128, 'learning_rate': 0.00011242674969718611, 'epoch': 1.4545454545454546}\n{'loss': 0.423, 'grad_norm': 0.8475826978683472, 'learning_rate': 0.00011196786500454452, 'epoch': 1.4772727272727273}\n{'loss': 0.4493, 'grad_norm': 0.9265095591545105, 'learning_rate': 0.00011150898031190296, 'epoch': 1.5}\n{'loss': 0.4667, 'grad_norm': 0.7291719913482666, 'learning_rate': 0.00011105009561926137, 'epoch': 1.5227272727272727}\n{'loss': 0.379, 'grad_norm': 0.7923359870910645, 'learning_rate': 0.00011059121092661981, 'epoch': 1.5454545454545454}\n{'loss': 0.3836, 'grad_norm': 1.3122533559799194, 'learning_rate': 0.00011013232623397822, 'epoch': 1.5681818181818183}\n{'loss': 0.3973, 'grad_norm': 1.2759194374084473, 'learning_rate': 0.00010967344154133665, 'epoch': 1.5909090909090908}\n{'loss': 0.4203, 'grad_norm': 0.921123206615448, 'learning_rate': 0.00010921455684869507, 'epoch': 1.6136363636363638}\n{'loss': 0.4438, 'grad_norm': 0.8435301184654236, 'learning_rate': 0.00010875567215605348, 'epoch': 1.6363636363636362}\n{'loss': 0.3549, 'grad_norm': 1.4692708253860474, 'learning_rate': 0.00010829678746341192, 'epoch': 1.6590909090909092}\n{'loss': 0.4309, 'grad_norm': 1.0635048151016235, 'learning_rate': 0.00010783790277077033, 'epoch': 1.6818181818181817}\n{'loss': 0.3667, 'grad_norm': 1.5976901054382324, 'learning_rate': 0.00010737901807812877, 'epoch': 1.7045454545454546}\n{'loss': 0.3606, 'grad_norm': 0.980758786201477, 'learning_rate': 0.00010692013338548718, 'epoch': 1.7272727272727273}\n{'loss': 0.4572, 'grad_norm': 1.0806456804275513, 'learning_rate': 0.00010646124869284562, 'epoch': 1.75}\n{'loss': 0.419, 'grad_norm': 0.8903639316558838, 'learning_rate': 0.00010600236400020403, 'epoch': 1.7727272727272727}\n{'loss': 0.3806, 'grad_norm': 1.1721476316452026, 'learning_rate': 0.00010554347930756247, 'epoch': 1.7954545454545454}\n{'loss': 0.3988, 'grad_norm': 1.4132553339004517, 'learning_rate': 0.00010508459461492089, 'epoch': 1.8181818181818183}\n{'loss': 0.3955, 'grad_norm': 1.3809834718704224, 'learning_rate': 0.00010462570992227931, 'epoch': 1.8409090909090908}\n{'loss': 0.4263, 'grad_norm': 2.1073153018951416, 'learning_rate': 0.00010416682522963774, 'epoch': 1.8636363636363638}\n{'loss': 0.3601, 'grad_norm': 1.7477264404296875, 'learning_rate': 0.00010370794053699616, 'epoch': 1.8863636363636362}\n{'loss': 0.4535, 'grad_norm': 0.9331072568893433, 'learning_rate': 0.00010324905584435459, 'epoch': 1.9090909090909092}\n{'loss': 0.3263, 'grad_norm': 0.8862389922142029, 'learning_rate': 0.000102790171151713, 'epoch': 1.9318181818181817}\n{'loss': 0.4138, 'grad_norm': 1.1083749532699585, 'learning_rate': 0.00010233128645907144, 'epoch': 1.9545454545454546}\n{'loss': 0.3455, 'grad_norm': 1.212327003479004, 'learning_rate': 0.00010187240176642985, 'epoch': 1.9772727272727273}\n{'loss': 0.4711, 'grad_norm': 3.5170905590057373, 'learning_rate': 0.00010141351707378829, 'epoch': 2.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8483    0.7228    0.7805      3142\n     PERCENT     0.4200    0.9545    0.5833        66\n        TYPE     0.9037    0.9576    0.9299     11415\n      VOLUME     0.1613    0.0714    0.0990        70\n\n   micro avg     0.8874    0.9032    0.8952     14693\n   macro avg     0.5833    0.6766    0.5982     14693\nweighted avg     0.8861    0.9032    0.8924     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.36755645275115967, 'eval_f1_macro': 0.5981880843680201, 'eval_precision': 0.8873879898354955, 'eval_recall': 0.9031511604165249, 'eval_f1': 0.8952001888892637, 'eval_accuracy': 0.8886712373490042, 'eval_runtime': 1.4363, 'eval_samples_per_second': 3836.92, 'eval_steps_per_second': 7.659, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.3538, 'grad_norm': 2.12419056892395, 'learning_rate': 0.0001009546323811467, 'epoch': 2.022727272727273}\n{'loss': 0.3691, 'grad_norm': 1.2350350618362427, 'learning_rate': 0.00010049574768850514, 'epoch': 2.0454545454545454}\n{'loss': 0.3861, 'grad_norm': 1.0453873872756958, 'learning_rate': 0.00010003686299586355, 'epoch': 2.0681818181818183}\n{'loss': 0.3004, 'grad_norm': 1.005638599395752, 'learning_rate': 9.957797830322199e-05, 'epoch': 2.090909090909091}\n{'loss': 0.3019, 'grad_norm': 1.0054020881652832, 'learning_rate': 9.91190936105804e-05, 'epoch': 2.1136363636363638}\n{'loss': 0.2638, 'grad_norm': 0.7846150994300842, 'learning_rate': 9.866020891793882e-05, 'epoch': 2.1363636363636362}\n{'loss': 0.2998, 'grad_norm': 0.7582922577857971, 'learning_rate': 9.820132422529725e-05, 'epoch': 2.159090909090909}\n{'loss': 0.3293, 'grad_norm': 1.1260026693344116, 'learning_rate': 9.774243953265567e-05, 'epoch': 2.1818181818181817}\n{'loss': 0.3454, 'grad_norm': 1.0010014772415161, 'learning_rate': 9.72835548400141e-05, 'epoch': 2.2045454545454546}\n{'loss': 0.3374, 'grad_norm': 1.0580157041549683, 'learning_rate': 9.682467014737251e-05, 'epoch': 2.227272727272727}\n{'loss': 0.3236, 'grad_norm': 1.2366340160369873, 'learning_rate': 9.636578545473095e-05, 'epoch': 2.25}\n{'loss': 0.2723, 'grad_norm': 1.2863646745681763, 'learning_rate': 9.590690076208936e-05, 'epoch': 2.2727272727272725}\n{'loss': 0.3355, 'grad_norm': 1.271290898323059, 'learning_rate': 9.54480160694478e-05, 'epoch': 2.2954545454545454}\n{'loss': 0.3178, 'grad_norm': 0.9203542470932007, 'learning_rate': 9.498913137680621e-05, 'epoch': 2.3181818181818183}\n{'loss': 0.2771, 'grad_norm': 1.1178011894226074, 'learning_rate': 9.453024668416465e-05, 'epoch': 2.340909090909091}\n{'loss': 0.3116, 'grad_norm': 1.0747607946395874, 'learning_rate': 9.407136199152306e-05, 'epoch': 2.3636363636363638}\n{'loss': 0.3007, 'grad_norm': 0.730513334274292, 'learning_rate': 9.361247729888149e-05, 'epoch': 2.3863636363636362}\n{'loss': 0.3429, 'grad_norm': 0.7345240712165833, 'learning_rate': 9.315359260623991e-05, 'epoch': 2.409090909090909}\n{'loss': 0.3007, 'grad_norm': 0.8251691460609436, 'learning_rate': 9.269470791359834e-05, 'epoch': 2.4318181818181817}\n{'loss': 0.3211, 'grad_norm': 1.123751163482666, 'learning_rate': 9.223582322095676e-05, 'epoch': 2.4545454545454546}\n{'loss': 0.3274, 'grad_norm': 0.8354085683822632, 'learning_rate': 9.177693852831519e-05, 'epoch': 2.4772727272727275}\n{'loss': 0.2791, 'grad_norm': 0.9432068467140198, 'learning_rate': 9.131805383567361e-05, 'epoch': 2.5}\n{'loss': 0.339, 'grad_norm': 1.1489744186401367, 'learning_rate': 9.085916914303204e-05, 'epoch': 2.5227272727272725}\n{'loss': 0.3174, 'grad_norm': 0.95381098985672, 'learning_rate': 9.040028445039046e-05, 'epoch': 2.5454545454545454}\n{'loss': 0.334, 'grad_norm': 1.2397596836090088, 'learning_rate': 8.994139975774887e-05, 'epoch': 2.5681818181818183}\n{'loss': 0.3746, 'grad_norm': 1.395876169204712, 'learning_rate': 8.948251506510731e-05, 'epoch': 2.590909090909091}\n{'loss': 0.256, 'grad_norm': 0.8665085434913635, 'learning_rate': 8.902363037246572e-05, 'epoch': 2.6136363636363638}\n{'loss': 0.2857, 'grad_norm': 1.5489070415496826, 'learning_rate': 8.856474567982416e-05, 'epoch': 2.6363636363636362}\n{'loss': 0.2846, 'grad_norm': 1.4444911479949951, 'learning_rate': 8.810586098718257e-05, 'epoch': 2.659090909090909}\n{'loss': 0.2566, 'grad_norm': 1.1536924839019775, 'learning_rate': 8.7646976294541e-05, 'epoch': 2.6818181818181817}\n{'loss': 0.3055, 'grad_norm': 0.9422070384025574, 'learning_rate': 8.718809160189943e-05, 'epoch': 2.7045454545454546}\n{'loss': 0.275, 'grad_norm': 1.0417841672897339, 'learning_rate': 8.672920690925785e-05, 'epoch': 2.7272727272727275}\n{'loss': 0.2814, 'grad_norm': 0.9454742670059204, 'learning_rate': 8.627032221661628e-05, 'epoch': 2.75}\n{'loss': 0.2469, 'grad_norm': 0.6483280062675476, 'learning_rate': 8.58114375239747e-05, 'epoch': 2.7727272727272725}\n{'loss': 0.3199, 'grad_norm': 1.1220810413360596, 'learning_rate': 8.535255283133313e-05, 'epoch': 2.7954545454545454}\n{'loss': 0.3123, 'grad_norm': 1.0137560367584229, 'learning_rate': 8.489366813869155e-05, 'epoch': 2.8181818181818183}\n{'loss': 0.3116, 'grad_norm': 0.9582814574241638, 'learning_rate': 8.443478344604998e-05, 'epoch': 2.840909090909091}\n{'loss': 0.2627, 'grad_norm': 1.2005587816238403, 'learning_rate': 8.397589875340839e-05, 'epoch': 2.8636363636363638}\n{'loss': 0.254, 'grad_norm': 1.2020463943481445, 'learning_rate': 8.351701406076683e-05, 'epoch': 2.8863636363636362}\n{'loss': 0.3163, 'grad_norm': 2.076902151107788, 'learning_rate': 8.305812936812524e-05, 'epoch': 2.909090909090909}\n{'loss': 0.2796, 'grad_norm': 0.8447690606117249, 'learning_rate': 8.259924467548366e-05, 'epoch': 2.9318181818181817}\n{'loss': 0.3267, 'grad_norm': 1.6703543663024902, 'learning_rate': 8.214035998284209e-05, 'epoch': 2.9545454545454546}\n{'loss': 0.3002, 'grad_norm': 0.8801170587539673, 'learning_rate': 8.168147529020051e-05, 'epoch': 2.9772727272727275}\n{'loss': 0.1388, 'grad_norm': 4.76458215713501, 'learning_rate': 8.122259059755894e-05, 'epoch': 3.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8643    0.8109    0.8368      3142\n     PERCENT     0.6019    0.9394    0.7337        66\n        TYPE     0.9233    0.9687    0.9455     11415\n      VOLUME     0.1304    0.1286    0.1295        70\n\n   micro avg     0.9059    0.9309    0.9182     14693\n   macro avg     0.6300    0.7119    0.6614     14693\nweighted avg     0.9054    0.9309    0.9174     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.2991989254951477, 'eval_f1_macro': 0.6613643147781242, 'eval_precision': 0.9059415777969133, 'eval_recall': 0.9308514258490438, 'eval_f1': 0.9182275931520645, 'eval_accuracy': 0.9110893459571227, 'eval_runtime': 1.819, 'eval_samples_per_second': 3029.617, 'eval_steps_per_second': 6.047, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.2496, 'grad_norm': 1.600311517715454, 'learning_rate': 8.076370590491736e-05, 'epoch': 3.022727272727273}\n{'loss': 0.2638, 'grad_norm': 1.161098837852478, 'learning_rate': 8.030482121227579e-05, 'epoch': 3.0454545454545454}\n{'loss': 0.2374, 'grad_norm': 0.7431811690330505, 'learning_rate': 7.984593651963421e-05, 'epoch': 3.0681818181818183}\n{'loss': 0.2301, 'grad_norm': 0.7931379079818726, 'learning_rate': 7.938705182699264e-05, 'epoch': 3.090909090909091}\n{'loss': 0.2773, 'grad_norm': 1.1111198663711548, 'learning_rate': 7.892816713435106e-05, 'epoch': 3.1136363636363638}\n{'loss': 0.2358, 'grad_norm': 1.1692107915878296, 'learning_rate': 7.846928244170949e-05, 'epoch': 3.1363636363636362}\n{'loss': 0.2261, 'grad_norm': 0.8415764570236206, 'learning_rate': 7.80103977490679e-05, 'epoch': 3.159090909090909}\n{'loss': 0.2317, 'grad_norm': 0.8487828373908997, 'learning_rate': 7.755151305642634e-05, 'epoch': 3.1818181818181817}\n{'loss': 0.2973, 'grad_norm': 1.3244085311889648, 'learning_rate': 7.709262836378475e-05, 'epoch': 3.2045454545454546}\n{'loss': 0.2095, 'grad_norm': 1.1444833278656006, 'learning_rate': 7.663374367114318e-05, 'epoch': 3.227272727272727}\n{'loss': 0.2712, 'grad_norm': 0.9609181880950928, 'learning_rate': 7.61748589785016e-05, 'epoch': 3.25}\n{'loss': 0.2434, 'grad_norm': 0.8927958011627197, 'learning_rate': 7.571597428586003e-05, 'epoch': 3.2727272727272725}\n{'loss': 0.2602, 'grad_norm': 1.1190531253814697, 'learning_rate': 7.525708959321845e-05, 'epoch': 3.2954545454545454}\n{'loss': 0.2571, 'grad_norm': 1.4461252689361572, 'learning_rate': 7.479820490057688e-05, 'epoch': 3.3181818181818183}\n{'loss': 0.2563, 'grad_norm': 1.1617379188537598, 'learning_rate': 7.43393202079353e-05, 'epoch': 3.340909090909091}\n{'loss': 0.2372, 'grad_norm': 0.8959304094314575, 'learning_rate': 7.388043551529373e-05, 'epoch': 3.3636363636363638}\n{'loss': 0.1975, 'grad_norm': 0.7463535666465759, 'learning_rate': 7.342155082265215e-05, 'epoch': 3.3863636363636362}\n{'loss': 0.2137, 'grad_norm': 1.1363521814346313, 'learning_rate': 7.296266613001058e-05, 'epoch': 3.409090909090909}\n{'loss': 0.2161, 'grad_norm': 0.8813228011131287, 'learning_rate': 7.2503781437369e-05, 'epoch': 3.4318181818181817}\n{'loss': 0.3076, 'grad_norm': 1.5466573238372803, 'learning_rate': 7.204489674472741e-05, 'epoch': 3.4545454545454546}\n{'loss': 0.2285, 'grad_norm': 1.457146167755127, 'learning_rate': 7.158601205208584e-05, 'epoch': 3.4772727272727275}\n{'loss': 0.2538, 'grad_norm': 0.8157095313072205, 'learning_rate': 7.112712735944426e-05, 'epoch': 3.5}\n{'loss': 0.2671, 'grad_norm': 0.9247927665710449, 'learning_rate': 7.066824266680269e-05, 'epoch': 3.5227272727272725}\n{'loss': 0.2049, 'grad_norm': 0.9161482453346252, 'learning_rate': 7.020935797416112e-05, 'epoch': 3.5454545454545454}\n{'loss': 0.2262, 'grad_norm': 0.9991779923439026, 'learning_rate': 6.975047328151954e-05, 'epoch': 3.5681818181818183}\n{'loss': 0.2642, 'grad_norm': 1.1054532527923584, 'learning_rate': 6.929158858887797e-05, 'epoch': 3.590909090909091}\n{'loss': 0.2795, 'grad_norm': 0.9748549461364746, 'learning_rate': 6.883270389623639e-05, 'epoch': 3.6136363636363638}\n{'loss': 0.2143, 'grad_norm': 0.7949565649032593, 'learning_rate': 6.837381920359482e-05, 'epoch': 3.6363636363636362}\n{'loss': 0.2091, 'grad_norm': 1.0178550481796265, 'learning_rate': 6.791493451095324e-05, 'epoch': 3.659090909090909}\n{'loss': 0.2172, 'grad_norm': 1.149766206741333, 'learning_rate': 6.745604981831167e-05, 'epoch': 3.6818181818181817}\n{'loss': 0.1969, 'grad_norm': 0.7704799771308899, 'learning_rate': 6.699716512567009e-05, 'epoch': 3.7045454545454546}\n{'loss': 0.2122, 'grad_norm': 0.774827241897583, 'learning_rate': 6.653828043302852e-05, 'epoch': 3.7272727272727275}\n{'loss': 0.1857, 'grad_norm': 1.3253629207611084, 'learning_rate': 6.607939574038693e-05, 'epoch': 3.75}\n{'loss': 0.2537, 'grad_norm': 0.8129653334617615, 'learning_rate': 6.562051104774535e-05, 'epoch': 3.7727272727272725}\n{'loss': 0.2415, 'grad_norm': 1.3826202154159546, 'learning_rate': 6.516162635510378e-05, 'epoch': 3.7954545454545454}\n{'loss': 0.219, 'grad_norm': 0.857438325881958, 'learning_rate': 6.47027416624622e-05, 'epoch': 3.8181818181818183}\n{'loss': 0.2111, 'grad_norm': 1.5499825477600098, 'learning_rate': 6.424385696982063e-05, 'epoch': 3.840909090909091}\n{'loss': 0.3262, 'grad_norm': 1.3207716941833496, 'learning_rate': 6.378497227717905e-05, 'epoch': 3.8636363636363638}\n{'loss': 0.2536, 'grad_norm': 0.9747309684753418, 'learning_rate': 6.332608758453748e-05, 'epoch': 3.8863636363636362}\n{'loss': 0.2086, 'grad_norm': 1.0451865196228027, 'learning_rate': 6.28672028918959e-05, 'epoch': 3.909090909090909}\n{'loss': 0.22, 'grad_norm': 1.989266037940979, 'learning_rate': 6.240831819925433e-05, 'epoch': 3.9318181818181817}\n{'loss': 0.2614, 'grad_norm': 1.067506194114685, 'learning_rate': 6.194943350661275e-05, 'epoch': 3.9545454545454546}\n{'loss': 0.2639, 'grad_norm': 2.0760772228240967, 'learning_rate': 6.149054881397118e-05, 'epoch': 3.9772727272727275}\n{'loss': 0.385, 'grad_norm': 5.1206889152526855, 'learning_rate': 6.10316641213296e-05, 'epoch': 4.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8608    0.8682    0.8645      3142\n     PERCENT     0.6632    0.9545    0.7826        66\n        TYPE     0.9404    0.9650    0.9525     11415\n      VOLUME     0.4348    0.4286    0.4317        70\n\n   micro avg     0.9196    0.9417    0.9305     14693\n   macro avg     0.7248    0.8041    0.7578     14693\nweighted avg     0.9197    0.9417    0.9305     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.26272353529930115, 'eval_f1_macro': 0.7578276781137002, 'eval_precision': 0.9195799548052639, 'eval_recall': 0.9416729054651876, 'eval_f1': 0.9304953091899526, 'eval_accuracy': 0.9228969419958646, 'eval_runtime': 1.4982, 'eval_samples_per_second': 3678.38, 'eval_steps_per_second': 7.342, 'epoch': 4.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.2115, 'grad_norm': 0.8010435700416565, 'learning_rate': 6.057277942868802e-05, 'epoch': 4.0227272727272725}\n{'loss': 0.2124, 'grad_norm': 1.07585871219635, 'learning_rate': 6.011389473604645e-05, 'epoch': 4.045454545454546}\n{'loss': 0.2394, 'grad_norm': 0.8391266465187073, 'learning_rate': 5.965501004340487e-05, 'epoch': 4.068181818181818}\n{'loss': 0.2213, 'grad_norm': 1.0327601432800293, 'learning_rate': 5.91961253507633e-05, 'epoch': 4.090909090909091}\n{'loss': 0.1935, 'grad_norm': 1.0174543857574463, 'learning_rate': 5.8737240658121723e-05, 'epoch': 4.113636363636363}\n{'loss': 0.2027, 'grad_norm': 1.1028902530670166, 'learning_rate': 5.827835596548015e-05, 'epoch': 4.136363636363637}\n{'loss': 0.181, 'grad_norm': 1.4708741903305054, 'learning_rate': 5.781947127283857e-05, 'epoch': 4.159090909090909}\n{'loss': 0.2041, 'grad_norm': 0.9891551733016968, 'learning_rate': 5.7360586580196985e-05, 'epoch': 4.181818181818182}\n{'loss': 0.2141, 'grad_norm': 1.41028892993927, 'learning_rate': 5.690170188755541e-05, 'epoch': 4.204545454545454}\n{'loss': 0.205, 'grad_norm': 1.2855674028396606, 'learning_rate': 5.6442817194913836e-05, 'epoch': 4.2272727272727275}\n{'loss': 0.2501, 'grad_norm': 2.8912951946258545, 'learning_rate': 5.598393250227226e-05, 'epoch': 4.25}\n{'loss': 0.2208, 'grad_norm': 2.453706979751587, 'learning_rate': 5.5525047809630686e-05, 'epoch': 4.2727272727272725}\n{'loss': 0.2094, 'grad_norm': 2.974945306777954, 'learning_rate': 5.506616311698911e-05, 'epoch': 4.295454545454546}\n{'loss': 0.2238, 'grad_norm': 1.6076774597167969, 'learning_rate': 5.4607278424347536e-05, 'epoch': 4.318181818181818}\n{'loss': 0.2152, 'grad_norm': 1.514938473701477, 'learning_rate': 5.414839373170596e-05, 'epoch': 4.340909090909091}\n{'loss': 0.2152, 'grad_norm': 1.2209904193878174, 'learning_rate': 5.3689509039064386e-05, 'epoch': 4.363636363636363}\n{'loss': 0.2233, 'grad_norm': 1.116628885269165, 'learning_rate': 5.323062434642281e-05, 'epoch': 4.386363636363637}\n{'loss': 0.1862, 'grad_norm': 1.0687304735183716, 'learning_rate': 5.277173965378124e-05, 'epoch': 4.409090909090909}\n{'loss': 0.1873, 'grad_norm': 1.5012038946151733, 'learning_rate': 5.2312854961139655e-05, 'epoch': 4.431818181818182}\n{'loss': 0.2007, 'grad_norm': 1.803466558456421, 'learning_rate': 5.185397026849808e-05, 'epoch': 4.454545454545454}\n{'loss': 0.2602, 'grad_norm': 3.03718900680542, 'learning_rate': 5.13950855758565e-05, 'epoch': 4.4772727272727275}\n{'loss': 0.2176, 'grad_norm': 2.1356985569000244, 'learning_rate': 5.0936200883214924e-05, 'epoch': 4.5}\n{'loss': 0.2325, 'grad_norm': 1.8382242918014526, 'learning_rate': 5.047731619057335e-05, 'epoch': 4.5227272727272725}\n{'loss': 0.1642, 'grad_norm': 0.9308739304542542, 'learning_rate': 5.0018431497931774e-05, 'epoch': 4.545454545454545}\n{'loss': 0.2252, 'grad_norm': 1.172976016998291, 'learning_rate': 4.95595468052902e-05, 'epoch': 4.568181818181818}\n{'loss': 0.2078, 'grad_norm': 1.658398151397705, 'learning_rate': 4.9100662112648624e-05, 'epoch': 4.590909090909091}\n{'loss': 0.1801, 'grad_norm': 0.8560682535171509, 'learning_rate': 4.864177742000705e-05, 'epoch': 4.613636363636363}\n{'loss': 0.2333, 'grad_norm': 2.1631107330322266, 'learning_rate': 4.8182892727365475e-05, 'epoch': 4.636363636363637}\n{'loss': 0.2117, 'grad_norm': 1.0304014682769775, 'learning_rate': 4.77240080347239e-05, 'epoch': 4.659090909090909}\n{'loss': 0.1681, 'grad_norm': 1.748324990272522, 'learning_rate': 4.7265123342082325e-05, 'epoch': 4.681818181818182}\n{'loss': 0.1794, 'grad_norm': 1.3857253789901733, 'learning_rate': 4.680623864944074e-05, 'epoch': 4.704545454545455}\n{'loss': 0.2104, 'grad_norm': 1.3082146644592285, 'learning_rate': 4.634735395679917e-05, 'epoch': 4.7272727272727275}\n{'loss': 0.1489, 'grad_norm': 0.6431570053100586, 'learning_rate': 4.5888469264157594e-05, 'epoch': 4.75}\n{'loss': 0.2031, 'grad_norm': 1.1464669704437256, 'learning_rate': 4.542958457151602e-05, 'epoch': 4.7727272727272725}\n{'loss': 0.2201, 'grad_norm': 1.8407375812530518, 'learning_rate': 4.497069987887444e-05, 'epoch': 4.795454545454545}\n{'loss': 0.1797, 'grad_norm': 1.17695951461792, 'learning_rate': 4.451181518623286e-05, 'epoch': 4.818181818181818}\n{'loss': 0.2172, 'grad_norm': 1.9854010343551636, 'learning_rate': 4.405293049359129e-05, 'epoch': 4.840909090909091}\n{'loss': 0.223, 'grad_norm': 1.0268300771713257, 'learning_rate': 4.359404580094971e-05, 'epoch': 4.863636363636363}\n{'loss': 0.2296, 'grad_norm': 0.8814860582351685, 'learning_rate': 4.313516110830814e-05, 'epoch': 4.886363636363637}\n{'loss': 0.1567, 'grad_norm': 1.2367949485778809, 'learning_rate': 4.267627641566656e-05, 'epoch': 4.909090909090909}\n{'loss': 0.1997, 'grad_norm': 0.9733037948608398, 'learning_rate': 4.221739172302499e-05, 'epoch': 4.931818181818182}\n{'loss': 0.1777, 'grad_norm': 0.9796882271766663, 'learning_rate': 4.175850703038341e-05, 'epoch': 4.954545454545455}\n{'loss': 0.1876, 'grad_norm': 0.9701217412948608, 'learning_rate': 4.129962233774183e-05, 'epoch': 4.9772727272727275}\n{'loss': 0.3659, 'grad_norm': 9.295666694641113, 'learning_rate': 4.084073764510026e-05, 'epoch': 5.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8564    0.8791    0.8676      3142\n     PERCENT     0.7778    0.9545    0.8571        66\n        TYPE     0.9444    0.9615    0.9529     11415\n      VOLUME     0.7286    0.7286    0.7286        70\n\n   micro avg     0.9236    0.9428    0.9331     14693\n   macro avg     0.8268    0.8809    0.8516     14693\nweighted avg     0.9238    0.9428    0.9332     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.25936126708984375, 'eval_f1_macro': 0.8515536735759293, 'eval_precision': 0.92358981197493, 'eval_recall': 0.9427618593888246, 'eval_f1': 0.933077363510828, 'eval_accuracy': 0.9242028512351725, 'eval_runtime': 1.5167, 'eval_samples_per_second': 3633.615, 'eval_steps_per_second': 7.253, 'epoch': 5.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.2638, 'grad_norm': 1.0807442665100098, 'learning_rate': 4.038185295245868e-05, 'epoch': 5.0227272727272725}\n{'loss': 0.1966, 'grad_norm': 0.9004493951797485, 'learning_rate': 3.992296825981711e-05, 'epoch': 5.045454545454546}\n{'loss': 0.2185, 'grad_norm': 0.88396155834198, 'learning_rate': 3.946408356717553e-05, 'epoch': 5.068181818181818}\n{'loss': 0.1614, 'grad_norm': 1.6754729747772217, 'learning_rate': 3.900519887453395e-05, 'epoch': 5.090909090909091}\n{'loss': 0.2028, 'grad_norm': 1.6221758127212524, 'learning_rate': 3.8546314181892376e-05, 'epoch': 5.113636363636363}\n{'loss': 0.2077, 'grad_norm': 1.7239925861358643, 'learning_rate': 3.80874294892508e-05, 'epoch': 5.136363636363637}\n{'loss': 0.1807, 'grad_norm': 1.0403460264205933, 'learning_rate': 3.7628544796609226e-05, 'epoch': 5.159090909090909}\n{'loss': 0.2065, 'grad_norm': 1.2606335878372192, 'learning_rate': 3.716966010396765e-05, 'epoch': 5.181818181818182}\n{'loss': 0.1383, 'grad_norm': 0.7349395751953125, 'learning_rate': 3.6710775411326076e-05, 'epoch': 5.204545454545454}\n{'loss': 0.1624, 'grad_norm': 1.352743148803711, 'learning_rate': 3.62518907186845e-05, 'epoch': 5.2272727272727275}\n{'loss': 0.1561, 'grad_norm': 1.039768099784851, 'learning_rate': 3.579300602604292e-05, 'epoch': 5.25}\n{'loss': 0.2065, 'grad_norm': 0.7102944850921631, 'learning_rate': 3.5334121333401345e-05, 'epoch': 5.2727272727272725}\n{'loss': 0.2087, 'grad_norm': 1.1306509971618652, 'learning_rate': 3.487523664075977e-05, 'epoch': 5.295454545454546}\n{'loss': 0.1758, 'grad_norm': 1.5173059701919556, 'learning_rate': 3.4416351948118195e-05, 'epoch': 5.318181818181818}\n{'loss': 0.1956, 'grad_norm': 0.8100531697273254, 'learning_rate': 3.395746725547662e-05, 'epoch': 5.340909090909091}\n{'loss': 0.168, 'grad_norm': 1.164556622505188, 'learning_rate': 3.3498582562835046e-05, 'epoch': 5.363636363636363}\n{'loss': 0.2102, 'grad_norm': 1.1981987953186035, 'learning_rate': 3.3039697870193464e-05, 'epoch': 5.386363636363637}\n{'loss': 0.2103, 'grad_norm': 0.9685429930686951, 'learning_rate': 3.258081317755189e-05, 'epoch': 5.409090909090909}\n{'loss': 0.191, 'grad_norm': 0.8550200462341309, 'learning_rate': 3.2121928484910314e-05, 'epoch': 5.431818181818182}\n{'loss': 0.1925, 'grad_norm': 0.9669480919837952, 'learning_rate': 3.166304379226874e-05, 'epoch': 5.454545454545454}\n{'loss': 0.1933, 'grad_norm': 1.3135119676589966, 'learning_rate': 3.1204159099627165e-05, 'epoch': 5.4772727272727275}\n{'loss': 0.1367, 'grad_norm': 0.9891908764839172, 'learning_rate': 3.074527440698559e-05, 'epoch': 5.5}\n{'loss': 0.2069, 'grad_norm': 0.9302765130996704, 'learning_rate': 3.028638971434401e-05, 'epoch': 5.5227272727272725}\n{'loss': 0.1273, 'grad_norm': 1.251634120941162, 'learning_rate': 2.9827505021702437e-05, 'epoch': 5.545454545454545}\n{'loss': 0.1833, 'grad_norm': 1.0995287895202637, 'learning_rate': 2.9368620329060862e-05, 'epoch': 5.568181818181818}\n{'loss': 0.1743, 'grad_norm': 0.8833702206611633, 'learning_rate': 2.8909735636419283e-05, 'epoch': 5.590909090909091}\n{'loss': 0.2039, 'grad_norm': 2.034982681274414, 'learning_rate': 2.8450850943777705e-05, 'epoch': 5.613636363636363}\n{'loss': 0.1793, 'grad_norm': 0.879329264163971, 'learning_rate': 2.799196625113613e-05, 'epoch': 5.636363636363637}\n{'loss': 0.1651, 'grad_norm': 0.9717053771018982, 'learning_rate': 2.7533081558494556e-05, 'epoch': 5.659090909090909}\n{'loss': 0.1899, 'grad_norm': 1.108250379562378, 'learning_rate': 2.707419686585298e-05, 'epoch': 5.681818181818182}\n{'loss': 0.1566, 'grad_norm': 0.889971911907196, 'learning_rate': 2.6615312173211406e-05, 'epoch': 5.704545454545455}\n{'loss': 0.1647, 'grad_norm': 0.7972729206085205, 'learning_rate': 2.6156427480569828e-05, 'epoch': 5.7272727272727275}\n{'loss': 0.157, 'grad_norm': 0.8283090591430664, 'learning_rate': 2.569754278792825e-05, 'epoch': 5.75}\n{'loss': 0.1592, 'grad_norm': 0.8651530146598816, 'learning_rate': 2.5238658095286674e-05, 'epoch': 5.7727272727272725}\n{'loss': 0.184, 'grad_norm': 1.7992619276046753, 'learning_rate': 2.47797734026451e-05, 'epoch': 5.795454545454545}\n{'loss': 0.1851, 'grad_norm': 1.1496753692626953, 'learning_rate': 2.4320888710003525e-05, 'epoch': 5.818181818181818}\n{'loss': 0.1686, 'grad_norm': 1.4486360549926758, 'learning_rate': 2.386200401736195e-05, 'epoch': 5.840909090909091}\n{'loss': 0.1921, 'grad_norm': 0.9574927091598511, 'learning_rate': 2.340311932472037e-05, 'epoch': 5.863636363636363}\n{'loss': 0.1784, 'grad_norm': 1.2597216367721558, 'learning_rate': 2.2944234632078797e-05, 'epoch': 5.886363636363637}\n{'loss': 0.1883, 'grad_norm': 1.0680615901947021, 'learning_rate': 2.248534993943722e-05, 'epoch': 5.909090909090909}\n{'loss': 0.1683, 'grad_norm': 1.5303205251693726, 'learning_rate': 2.2026465246795644e-05, 'epoch': 5.931818181818182}\n{'loss': 0.1853, 'grad_norm': 1.0795786380767822, 'learning_rate': 2.156758055415407e-05, 'epoch': 5.954545454545455}\n{'loss': 0.1575, 'grad_norm': 1.1549885272979736, 'learning_rate': 2.1108695861512494e-05, 'epoch': 5.9772727272727275}\n{'loss': 0.1269, 'grad_norm': 3.8196487426757812, 'learning_rate': 2.0649811168870916e-05, 'epoch': 6.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8676    0.8778    0.8726      3142\n     PERCENT     0.7778    0.9545    0.8571        66\n        TYPE     0.9437    0.9654    0.9544     11415\n      VOLUME     0.7647    0.7429    0.7536        70\n\n   micro avg     0.9259    0.9456    0.9356     14693\n   macro avg     0.8384    0.8851    0.8595     14693\nweighted avg     0.9258    0.9456    0.9356     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.2554018497467041, 'eval_f1_macro': 0.8594639689592538, 'eval_precision': 0.9258913695434855, 'eval_recall': 0.9455523038181447, 'eval_f1': 0.9356185601724022, 'eval_accuracy': 0.9269779083687018, 'eval_runtime': 1.4338, 'eval_samples_per_second': 3843.648, 'eval_steps_per_second': 7.672, 'epoch': 6.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.1588, 'grad_norm': 0.9043072462081909, 'learning_rate': 2.019092647622934e-05, 'epoch': 6.0227272727272725}\n{'loss': 0.1561, 'grad_norm': 0.9797122478485107, 'learning_rate': 1.9732041783587766e-05, 'epoch': 6.045454545454546}\n{'loss': 0.1762, 'grad_norm': 0.9506365656852722, 'learning_rate': 1.9273157090946188e-05, 'epoch': 6.068181818181818}\n{'loss': 0.1821, 'grad_norm': 0.9962958693504333, 'learning_rate': 1.8814272398304613e-05, 'epoch': 6.090909090909091}\n{'loss': 0.164, 'grad_norm': 0.8853088021278381, 'learning_rate': 1.8355387705663038e-05, 'epoch': 6.113636363636363}\n{'loss': 0.1828, 'grad_norm': 1.0473802089691162, 'learning_rate': 1.789650301302146e-05, 'epoch': 6.136363636363637}\n{'loss': 0.1595, 'grad_norm': 0.8947424292564392, 'learning_rate': 1.7437618320379885e-05, 'epoch': 6.159090909090909}\n{'loss': 0.1455, 'grad_norm': 0.788487434387207, 'learning_rate': 1.697873362773831e-05, 'epoch': 6.181818181818182}\n{'loss': 0.1644, 'grad_norm': 0.9377343654632568, 'learning_rate': 1.6519848935096732e-05, 'epoch': 6.204545454545454}\n{'loss': 0.1341, 'grad_norm': 1.0949292182922363, 'learning_rate': 1.6060964242455157e-05, 'epoch': 6.2272727272727275}\n{'loss': 0.142, 'grad_norm': 1.1142692565917969, 'learning_rate': 1.5602079549813582e-05, 'epoch': 6.25}\n{'loss': 0.2126, 'grad_norm': 1.295009970664978, 'learning_rate': 1.5143194857172006e-05, 'epoch': 6.2727272727272725}\n{'loss': 0.1611, 'grad_norm': 0.7630001306533813, 'learning_rate': 1.4684310164530431e-05, 'epoch': 6.295454545454546}\n{'loss': 0.1916, 'grad_norm': 1.2907874584197998, 'learning_rate': 1.4225425471888853e-05, 'epoch': 6.318181818181818}\n{'loss': 0.188, 'grad_norm': 1.1820467710494995, 'learning_rate': 1.3766540779247278e-05, 'epoch': 6.340909090909091}\n{'loss': 0.1955, 'grad_norm': 1.053495168685913, 'learning_rate': 1.3307656086605703e-05, 'epoch': 6.363636363636363}\n{'loss': 0.1868, 'grad_norm': 1.2879939079284668, 'learning_rate': 1.2848771393964125e-05, 'epoch': 6.386363636363637}\n{'loss': 0.1565, 'grad_norm': 0.8944472074508667, 'learning_rate': 1.238988670132255e-05, 'epoch': 6.409090909090909}\n{'loss': 0.1566, 'grad_norm': 0.9489755630493164, 'learning_rate': 1.1931002008680975e-05, 'epoch': 6.431818181818182}\n{'loss': 0.1535, 'grad_norm': 0.911263644695282, 'learning_rate': 1.1472117316039398e-05, 'epoch': 6.454545454545454}\n{'loss': 0.1675, 'grad_norm': 0.7649736404418945, 'learning_rate': 1.1013232623397822e-05, 'epoch': 6.4772727272727275}\n{'loss': 0.1295, 'grad_norm': 0.8201974630355835, 'learning_rate': 1.0554347930756247e-05, 'epoch': 6.5}\n{'loss': 0.1834, 'grad_norm': 1.0402461290359497, 'learning_rate': 1.009546323811467e-05, 'epoch': 6.5227272727272725}\n{'loss': 0.1604, 'grad_norm': 0.9299957752227783, 'learning_rate': 9.636578545473094e-06, 'epoch': 6.545454545454545}\n{'loss': 0.1917, 'grad_norm': 1.077307105064392, 'learning_rate': 9.177693852831519e-06, 'epoch': 6.568181818181818}\n{'loss': 0.1788, 'grad_norm': 0.8293865919113159, 'learning_rate': 8.718809160189943e-06, 'epoch': 6.590909090909091}\n{'loss': 0.1887, 'grad_norm': 1.2190934419631958, 'learning_rate': 8.259924467548366e-06, 'epoch': 6.613636363636363}\n{'loss': 0.1975, 'grad_norm': 1.0939289331436157, 'learning_rate': 7.801039774906791e-06, 'epoch': 6.636363636363637}\n{'loss': 0.138, 'grad_norm': 0.8820200562477112, 'learning_rate': 7.3421550822652154e-06, 'epoch': 6.659090909090909}\n{'loss': 0.1886, 'grad_norm': 1.0467051267623901, 'learning_rate': 6.883270389623639e-06, 'epoch': 6.681818181818182}\n{'loss': 0.1556, 'grad_norm': 0.7656412720680237, 'learning_rate': 6.424385696982062e-06, 'epoch': 6.704545454545455}\n{'loss': 0.1676, 'grad_norm': 0.9962536096572876, 'learning_rate': 5.9655010043404875e-06, 'epoch': 6.7272727272727275}\n{'loss': 0.1748, 'grad_norm': 0.9862726926803589, 'learning_rate': 5.506616311698911e-06, 'epoch': 6.75}\n{'loss': 0.1593, 'grad_norm': 0.7110270857810974, 'learning_rate': 5.047731619057335e-06, 'epoch': 6.7727272727272725}\n{'loss': 0.1925, 'grad_norm': 1.8215036392211914, 'learning_rate': 4.5888469264157595e-06, 'epoch': 6.795454545454545}\n{'loss': 0.1794, 'grad_norm': 0.8427959084510803, 'learning_rate': 4.129962233774183e-06, 'epoch': 6.818181818181818}\n{'loss': 0.1756, 'grad_norm': 0.924065887928009, 'learning_rate': 3.6710775411326077e-06, 'epoch': 6.840909090909091}\n{'loss': 0.1731, 'grad_norm': 1.1839617490768433, 'learning_rate': 3.212192848491031e-06, 'epoch': 6.863636363636363}\n{'loss': 0.1767, 'grad_norm': 1.1212462186813354, 'learning_rate': 2.7533081558494555e-06, 'epoch': 6.886363636363637}\n{'loss': 0.1276, 'grad_norm': 0.8665251135826111, 'learning_rate': 2.2944234632078798e-06, 'epoch': 6.909090909090909}\n{'loss': 0.1612, 'grad_norm': 1.198987364768982, 'learning_rate': 1.8355387705663039e-06, 'epoch': 6.931818181818182}\n{'loss': 0.1611, 'grad_norm': 0.9200978875160217, 'learning_rate': 1.3766540779247277e-06, 'epoch': 6.954545454545455}\n{'loss': 0.1727, 'grad_norm': 0.8578947186470032, 'learning_rate': 9.177693852831519e-07, 'epoch': 6.9772727272727275}\n{'loss': 0.1378, 'grad_norm': 3.757749557495117, 'learning_rate': 4.5888469264157596e-07, 'epoch': 7.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8709    0.8740    0.8724      3142\n     PERCENT     0.7778    0.9545    0.8571        66\n        TYPE     0.9436    0.9660    0.9547     11415\n      VOLUME     0.7761    0.7429    0.7591        70\n\n   micro avg     0.9267    0.9452    0.9358     14693\n   macro avg     0.8421    0.8843    0.8608     14693\nweighted avg     0.9265    0.9452    0.9357     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.25581657886505127, 'eval_f1_macro': 0.8608456686601349, 'eval_precision': 0.9266697804764129, 'eval_recall': 0.9452120057170081, 'eval_f1': 0.9358490566037736, 'eval_accuracy': 0.9273043856785287, 'eval_runtime': 1.4493, 'eval_samples_per_second': 3802.483, 'eval_steps_per_second': 7.59, 'epoch': 7.0}\n{'train_runtime': 43.0221, 'train_samples_per_second': 3586.227, 'train_steps_per_second': 7.159, 'train_loss': 0.4077571622253238, 'epoch': 7.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nThe speedups for torchdynamo mostly come with GPU Ampere or higher and which is not detected here.\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8709    0.8740    0.8724      3142\n     PERCENT     0.7778    0.9545    0.8571        66\n        TYPE     0.9436    0.9660    0.9547     11415\n      VOLUME     0.7761    0.7429    0.7591        70\n\n   micro avg     0.9267    0.9452    0.9358     14693\n   macro avg     0.8421    0.8843    0.8608     14693\nweighted avg     0.9265    0.9452    0.9357     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.25581657886505127, 'eval_f1_macro': 0.8608456686601349, 'eval_precision': 0.9266697804764129, 'eval_recall': 0.9452120057170081, 'eval_f1': 0.9358490566037736, 'eval_accuracy': 0.9273043856785287, 'eval_runtime': 1.5411, 'eval_samples_per_second': 3575.947, 'eval_steps_per_second': 7.138, 'epoch': 7.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/3364797558.py:66: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 2.1776, 'grad_norm': 7.06667423248291, 'learning_rate': 0.0, 'epoch': 0.022727272727272728}\n{'loss': 2.1675, 'grad_norm': 7.197719573974609, 'learning_rate': 4.100356769732791e-06, 'epoch': 0.045454545454545456}\n{'loss': 2.1613, 'grad_norm': 7.228405475616455, 'learning_rate': 8.200713539465582e-06, 'epoch': 0.06818181818181818}\n{'loss': 2.1183, 'grad_norm': 6.9398369789123535, 'learning_rate': 1.2301070309198374e-05, 'epoch': 0.09090909090909091}\n{'loss': 2.1116, 'grad_norm': 6.8962297439575195, 'learning_rate': 1.6401427078931164e-05, 'epoch': 0.11363636363636363}\n{'loss': 2.0645, 'grad_norm': 6.974628448486328, 'learning_rate': 2.0501783848663958e-05, 'epoch': 0.13636363636363635}\n{'loss': 2.0378, 'grad_norm': 6.777029037475586, 'learning_rate': 2.4602140618396748e-05, 'epoch': 0.1590909090909091}\n{'loss': 1.9962, 'grad_norm': 6.223751068115234, 'learning_rate': 2.8702497388129538e-05, 'epoch': 0.18181818181818182}\n{'loss': 1.9194, 'grad_norm': 6.691356182098389, 'learning_rate': 3.280285415786233e-05, 'epoch': 0.20454545454545456}\n{'loss': 1.8606, 'grad_norm': 6.114904880523682, 'learning_rate': 3.690321092759512e-05, 'epoch': 0.22727272727272727}\n{'loss': 1.782, 'grad_norm': 6.25588846206665, 'learning_rate': 4.1003567697327915e-05, 'epoch': 0.25}\n{'loss': 1.7386, 'grad_norm': 5.572015762329102, 'learning_rate': 4.510392446706071e-05, 'epoch': 0.2727272727272727}\n{'loss': 1.627, 'grad_norm': 5.458523750305176, 'learning_rate': 4.9204281236793496e-05, 'epoch': 0.29545454545454547}\n{'loss': 1.5281, 'grad_norm': 5.3553972244262695, 'learning_rate': 5.330463800652629e-05, 'epoch': 0.3181818181818182}\n{'loss': 1.5139, 'grad_norm': 4.394402027130127, 'learning_rate': 5.7404994776259076e-05, 'epoch': 0.3409090909090909}\n{'loss': 1.4192, 'grad_norm': 4.131046295166016, 'learning_rate': 6.150535154599188e-05, 'epoch': 0.36363636363636365}\n{'loss': 1.3278, 'grad_norm': 3.524972438812256, 'learning_rate': 6.560570831572466e-05, 'epoch': 0.38636363636363635}\n{'loss': 1.2811, 'grad_norm': 2.8223512172698975, 'learning_rate': 6.970606508545745e-05, 'epoch': 0.4090909090909091}\n{'loss': 1.2129, 'grad_norm': 2.289980411529541, 'learning_rate': 7.380642185519024e-05, 'epoch': 0.4318181818181818}\n{'loss': 1.1973, 'grad_norm': 1.9755579233169556, 'learning_rate': 7.790677862492304e-05, 'epoch': 0.45454545454545453}\n{'loss': 1.1556, 'grad_norm': 1.9372344017028809, 'learning_rate': 8.200713539465583e-05, 'epoch': 0.4772727272727273}\n{'loss': 1.0635, 'grad_norm': 1.767504096031189, 'learning_rate': 8.610749216438861e-05, 'epoch': 0.5}\n{'loss': 1.0581, 'grad_norm': 1.6323469877243042, 'learning_rate': 9.020784893412142e-05, 'epoch': 0.5227272727272727}\n{'loss': 1.0998, 'grad_norm': 1.7514582872390747, 'learning_rate': 9.43082057038542e-05, 'epoch': 0.5454545454545454}\n{'loss': 1.0277, 'grad_norm': 1.4260458946228027, 'learning_rate': 9.840856247358699e-05, 'epoch': 0.5681818181818182}\n{'loss': 0.9422, 'grad_norm': 1.222481369972229, 'learning_rate': 0.00010250891924331977, 'epoch': 0.5909090909090909}\n{'loss': 1.0021, 'grad_norm': 1.2530626058578491, 'learning_rate': 0.00010660927601305258, 'epoch': 0.6136363636363636}\n{'loss': 0.9079, 'grad_norm': 1.7382830381393433, 'learning_rate': 0.00011070963278278537, 'epoch': 0.6363636363636364}\n{'loss': 0.9377, 'grad_norm': 1.4363141059875488, 'learning_rate': 0.00011480998955251815, 'epoch': 0.6590909090909091}\n{'loss': 0.8965, 'grad_norm': 1.498421311378479, 'learning_rate': 0.00011891034632225095, 'epoch': 0.6818181818181818}\n{'loss': 0.8434, 'grad_norm': 1.1536556482315063, 'learning_rate': 0.00012301070309198375, 'epoch': 0.7045454545454546}\n{'loss': 0.8338, 'grad_norm': 0.8764169216156006, 'learning_rate': 0.00012711105986171653, 'epoch': 0.7272727272727273}\n{'loss': 0.7718, 'grad_norm': 0.9387682676315308, 'learning_rate': 0.00012665217516907496, 'epoch': 0.75}\n{'loss': 0.7655, 'grad_norm': 1.0101820230484009, 'learning_rate': 0.00012619329047643338, 'epoch': 0.7727272727272727}\n{'loss': 0.7093, 'grad_norm': 1.0924046039581299, 'learning_rate': 0.0001257344057837918, 'epoch': 0.7954545454545454}\n{'loss': 0.6828, 'grad_norm': 0.8257341980934143, 'learning_rate': 0.00012527552109115023, 'epoch': 0.8181818181818182}\n{'loss': 0.6503, 'grad_norm': 1.1824966669082642, 'learning_rate': 0.00012481663639850866, 'epoch': 0.8409090909090909}\n{'loss': 0.6834, 'grad_norm': 0.9595913887023926, 'learning_rate': 0.00012435775170586708, 'epoch': 0.8636363636363636}\n{'loss': 0.6153, 'grad_norm': 1.3217732906341553, 'learning_rate': 0.0001238988670132255, 'epoch': 0.8863636363636364}\n{'loss': 0.6372, 'grad_norm': 1.4318660497665405, 'learning_rate': 0.00012343998232058393, 'epoch': 0.9090909090909091}\n{'loss': 0.6987, 'grad_norm': 1.2096443176269531, 'learning_rate': 0.00012298109762794236, 'epoch': 0.9318181818181818}\n{'loss': 0.6352, 'grad_norm': 0.7919872999191284, 'learning_rate': 0.00012252221293530078, 'epoch': 0.9545454545454546}\n{'loss': 0.6418, 'grad_norm': 1.5247629880905151, 'learning_rate': 0.0001220633282426592, 'epoch': 0.9772727272727273}\n{'loss': 0.4892, 'grad_norm': 2.494171380996704, 'learning_rate': 0.00012160444355001763, 'epoch': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.7274    0.6090    0.6629      3404\n     PERCENT     1.0000    0.2535    0.4045        71\n        TYPE     0.8238    0.9047    0.8623     11194\n      VOLUME     0.0000    0.0000    0.0000        56\n\n   micro avg     0.8059    0.8297    0.8176     14725\n   macro avg     0.6378    0.4418    0.4824     14725\nweighted avg     0.7992    0.8297    0.8108     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.5951205492019653, 'eval_f1_macro': 0.482444871981746, 'eval_precision': 0.8058835169184091, 'eval_recall': 0.8297453310696095, 'eval_f1': 0.8176403667268954, 'eval_accuracy': 0.8217398454201612, 'eval_runtime': 1.4384, 'eval_samples_per_second': 3831.311, 'eval_steps_per_second': 7.647, 'epoch': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.5385, 'grad_norm': 0.8227225542068481, 'learning_rate': 0.00012114555885737605, 'epoch': 1.0227272727272727}\n{'loss': 0.5195, 'grad_norm': 0.7739861011505127, 'learning_rate': 0.00012068667416473446, 'epoch': 1.0454545454545454}\n{'loss': 0.5351, 'grad_norm': 0.8821610808372498, 'learning_rate': 0.0001202277894720929, 'epoch': 1.0681818181818181}\n{'loss': 0.4959, 'grad_norm': 0.6783893704414368, 'learning_rate': 0.00011976890477945131, 'epoch': 1.0909090909090908}\n{'loss': 0.5514, 'grad_norm': 1.114359736442566, 'learning_rate': 0.00011931002008680975, 'epoch': 1.1136363636363635}\n{'loss': 0.4505, 'grad_norm': 1.2475855350494385, 'learning_rate': 0.00011885113539416816, 'epoch': 1.1363636363636362}\n{'loss': 0.477, 'grad_norm': 0.9701132774353027, 'learning_rate': 0.0001183922507015266, 'epoch': 1.1590909090909092}\n{'loss': 0.5278, 'grad_norm': 0.7999686002731323, 'learning_rate': 0.00011793336600888501, 'epoch': 1.1818181818181819}\n{'loss': 0.5061, 'grad_norm': 0.9081205129623413, 'learning_rate': 0.00011747448131624345, 'epoch': 1.2045454545454546}\n{'loss': 0.5629, 'grad_norm': 1.669460415840149, 'learning_rate': 0.00011701559662360186, 'epoch': 1.2272727272727273}\n{'loss': 0.5036, 'grad_norm': 1.0764460563659668, 'learning_rate': 0.0001165567119309603, 'epoch': 1.25}\n{'loss': 0.472, 'grad_norm': 0.7203815579414368, 'learning_rate': 0.00011609782723831871, 'epoch': 1.2727272727272727}\n{'loss': 0.4137, 'grad_norm': 0.9040585160255432, 'learning_rate': 0.00011563894254567713, 'epoch': 1.2954545454545454}\n{'loss': 0.4607, 'grad_norm': 1.1966079473495483, 'learning_rate': 0.00011518005785303556, 'epoch': 1.3181818181818181}\n{'loss': 0.4906, 'grad_norm': 1.6290783882141113, 'learning_rate': 0.00011472117316039397, 'epoch': 1.3409090909090908}\n{'loss': 0.5005, 'grad_norm': 0.6535853147506714, 'learning_rate': 0.00011426228846775241, 'epoch': 1.3636363636363638}\n{'loss': 0.4127, 'grad_norm': 0.7324514389038086, 'learning_rate': 0.00011380340377511082, 'epoch': 1.3863636363636362}\n{'loss': 0.4204, 'grad_norm': 0.7989382743835449, 'learning_rate': 0.00011334451908246926, 'epoch': 1.4090909090909092}\n{'loss': 0.4692, 'grad_norm': 1.2019932270050049, 'learning_rate': 0.00011288563438982767, 'epoch': 1.4318181818181819}\n{'loss': 0.4542, 'grad_norm': 1.0939502716064453, 'learning_rate': 0.00011242674969718611, 'epoch': 1.4545454545454546}\n{'loss': 0.4329, 'grad_norm': 0.960276186466217, 'learning_rate': 0.00011196786500454452, 'epoch': 1.4772727272727273}\n{'loss': 0.4419, 'grad_norm': 1.423994779586792, 'learning_rate': 0.00011150898031190296, 'epoch': 1.5}\n{'loss': 0.45, 'grad_norm': 0.760201632976532, 'learning_rate': 0.00011105009561926137, 'epoch': 1.5227272727272727}\n{'loss': 0.3698, 'grad_norm': 0.6635482311248779, 'learning_rate': 0.00011059121092661981, 'epoch': 1.5454545454545454}\n{'loss': 0.4389, 'grad_norm': 1.2054678201675415, 'learning_rate': 0.00011013232623397822, 'epoch': 1.5681818181818183}\n{'loss': 0.3889, 'grad_norm': 0.7345433235168457, 'learning_rate': 0.00010967344154133665, 'epoch': 1.5909090909090908}\n{'loss': 0.3805, 'grad_norm': 0.7216848134994507, 'learning_rate': 0.00010921455684869507, 'epoch': 1.6136363636363638}\n{'loss': 0.4128, 'grad_norm': 0.7661169171333313, 'learning_rate': 0.00010875567215605348, 'epoch': 1.6363636363636362}\n{'loss': 0.4149, 'grad_norm': 0.7488476634025574, 'learning_rate': 0.00010829678746341192, 'epoch': 1.6590909090909092}\n{'loss': 0.4109, 'grad_norm': 0.8573226928710938, 'learning_rate': 0.00010783790277077033, 'epoch': 1.6818181818181817}\n{'loss': 0.3805, 'grad_norm': 0.6653963923454285, 'learning_rate': 0.00010737901807812877, 'epoch': 1.7045454545454546}\n{'loss': 0.4416, 'grad_norm': 0.7270403504371643, 'learning_rate': 0.00010692013338548718, 'epoch': 1.7272727272727273}\n{'loss': 0.3609, 'grad_norm': 0.714401364326477, 'learning_rate': 0.00010646124869284562, 'epoch': 1.75}\n{'loss': 0.3123, 'grad_norm': 0.9109159708023071, 'learning_rate': 0.00010600236400020403, 'epoch': 1.7727272727272727}\n{'loss': 0.4249, 'grad_norm': 0.8914217352867126, 'learning_rate': 0.00010554347930756247, 'epoch': 1.7954545454545454}\n{'loss': 0.4048, 'grad_norm': 0.7298586964607239, 'learning_rate': 0.00010508459461492089, 'epoch': 1.8181818181818183}\n{'loss': 0.4199, 'grad_norm': 0.8735408782958984, 'learning_rate': 0.00010462570992227931, 'epoch': 1.8409090909090908}\n{'loss': 0.3737, 'grad_norm': 0.6734716892242432, 'learning_rate': 0.00010416682522963774, 'epoch': 1.8636363636363638}\n{'loss': 0.4059, 'grad_norm': 0.7363862991333008, 'learning_rate': 0.00010370794053699616, 'epoch': 1.8863636363636362}\n{'loss': 0.4032, 'grad_norm': 1.6070424318313599, 'learning_rate': 0.00010324905584435459, 'epoch': 1.9090909090909092}\n{'loss': 0.3371, 'grad_norm': 0.6838908195495605, 'learning_rate': 0.000102790171151713, 'epoch': 1.9318181818181817}\n{'loss': 0.3569, 'grad_norm': 0.8409461975097656, 'learning_rate': 0.00010233128645907144, 'epoch': 1.9545454545454546}\n{'loss': 0.3591, 'grad_norm': 0.9801751971244812, 'learning_rate': 0.00010187240176642985, 'epoch': 1.9772727272727273}\n{'loss': 0.1969, 'grad_norm': 2.6529898643493652, 'learning_rate': 0.00010141351707378829, 'epoch': 2.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.7955    0.7929    0.7942      3404\n     PERCENT     0.6087    0.9859    0.7527        71\n        TYPE     0.9074    0.9591    0.9325     11194\n      VOLUME     0.6957    0.2857    0.4051        56\n\n   micro avg     0.8801    0.9182    0.8988     14725\n   macro avg     0.7518    0.7559    0.7211     14725\nweighted avg     0.8793    0.9182    0.8977     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.35845911502838135, 'eval_f1_macro': 0.7211091094674885, 'eval_precision': 0.8801015426674478, 'eval_recall': 0.9182342954159592, 'eval_f1': 0.898763626695028, 'eval_accuracy': 0.8944800745491421, 'eval_runtime': 1.4942, 'eval_samples_per_second': 3688.161, 'eval_steps_per_second': 7.362, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.3056, 'grad_norm': 1.6784952878952026, 'learning_rate': 0.0001009546323811467, 'epoch': 2.022727272727273}\n{'loss': 0.3459, 'grad_norm': 0.8712109923362732, 'learning_rate': 0.00010049574768850514, 'epoch': 2.0454545454545454}\n{'loss': 0.3053, 'grad_norm': 1.190008521080017, 'learning_rate': 0.00010003686299586355, 'epoch': 2.0681818181818183}\n{'loss': 0.3415, 'grad_norm': 0.903595507144928, 'learning_rate': 9.957797830322199e-05, 'epoch': 2.090909090909091}\n{'loss': 0.3454, 'grad_norm': 0.8405331373214722, 'learning_rate': 9.91190936105804e-05, 'epoch': 2.1136363636363638}\n{'loss': 0.313, 'grad_norm': 0.9736238121986389, 'learning_rate': 9.866020891793882e-05, 'epoch': 2.1363636363636362}\n{'loss': 0.3251, 'grad_norm': 1.52708101272583, 'learning_rate': 9.820132422529725e-05, 'epoch': 2.159090909090909}\n{'loss': 0.3689, 'grad_norm': 0.9602945446968079, 'learning_rate': 9.774243953265567e-05, 'epoch': 2.1818181818181817}\n{'loss': 0.3492, 'grad_norm': 1.2304534912109375, 'learning_rate': 9.72835548400141e-05, 'epoch': 2.2045454545454546}\n{'loss': 0.3005, 'grad_norm': 1.1382440328598022, 'learning_rate': 9.682467014737251e-05, 'epoch': 2.227272727272727}\n{'loss': 0.3576, 'grad_norm': 1.1342010498046875, 'learning_rate': 9.636578545473095e-05, 'epoch': 2.25}\n{'loss': 0.319, 'grad_norm': 1.149577021598816, 'learning_rate': 9.590690076208936e-05, 'epoch': 2.2727272727272725}\n{'loss': 0.2896, 'grad_norm': 1.8001500368118286, 'learning_rate': 9.54480160694478e-05, 'epoch': 2.2954545454545454}\n{'loss': 0.2938, 'grad_norm': 1.3033422231674194, 'learning_rate': 9.498913137680621e-05, 'epoch': 2.3181818181818183}\n{'loss': 0.2957, 'grad_norm': 1.1342796087265015, 'learning_rate': 9.453024668416465e-05, 'epoch': 2.340909090909091}\n{'loss': 0.3655, 'grad_norm': 1.911183476448059, 'learning_rate': 9.407136199152306e-05, 'epoch': 2.3636363636363638}\n{'loss': 0.3123, 'grad_norm': 0.7617093324661255, 'learning_rate': 9.361247729888149e-05, 'epoch': 2.3863636363636362}\n{'loss': 0.3123, 'grad_norm': 0.9066677689552307, 'learning_rate': 9.315359260623991e-05, 'epoch': 2.409090909090909}\n{'loss': 0.2843, 'grad_norm': 0.9548920392990112, 'learning_rate': 9.269470791359834e-05, 'epoch': 2.4318181818181817}\n{'loss': 0.2928, 'grad_norm': 1.0010912418365479, 'learning_rate': 9.223582322095676e-05, 'epoch': 2.4545454545454546}\n{'loss': 0.2817, 'grad_norm': 0.8331610560417175, 'learning_rate': 9.177693852831519e-05, 'epoch': 2.4772727272727275}\n{'loss': 0.2563, 'grad_norm': 0.701841413974762, 'learning_rate': 9.131805383567361e-05, 'epoch': 2.5}\n{'loss': 0.3031, 'grad_norm': 0.7932159900665283, 'learning_rate': 9.085916914303204e-05, 'epoch': 2.5227272727272725}\n{'loss': 0.2708, 'grad_norm': 0.942272424697876, 'learning_rate': 9.040028445039046e-05, 'epoch': 2.5454545454545454}\n{'loss': 0.3306, 'grad_norm': 1.1644566059112549, 'learning_rate': 8.994139975774887e-05, 'epoch': 2.5681818181818183}\n{'loss': 0.2958, 'grad_norm': 1.2356247901916504, 'learning_rate': 8.948251506510731e-05, 'epoch': 2.590909090909091}\n{'loss': 0.3184, 'grad_norm': 0.8556642532348633, 'learning_rate': 8.902363037246572e-05, 'epoch': 2.6136363636363638}\n{'loss': 0.2422, 'grad_norm': 1.1414768695831299, 'learning_rate': 8.856474567982416e-05, 'epoch': 2.6363636363636362}\n{'loss': 0.2598, 'grad_norm': 0.8450719714164734, 'learning_rate': 8.810586098718257e-05, 'epoch': 2.659090909090909}\n{'loss': 0.2663, 'grad_norm': 0.7289586663246155, 'learning_rate': 8.7646976294541e-05, 'epoch': 2.6818181818181817}\n{'loss': 0.2992, 'grad_norm': 0.7325660586357117, 'learning_rate': 8.718809160189943e-05, 'epoch': 2.7045454545454546}\n{'loss': 0.2468, 'grad_norm': 0.8204008340835571, 'learning_rate': 8.672920690925785e-05, 'epoch': 2.7272727272727275}\n{'loss': 0.2959, 'grad_norm': 0.8406568765640259, 'learning_rate': 8.627032221661628e-05, 'epoch': 2.75}\n{'loss': 0.3057, 'grad_norm': 0.8784226775169373, 'learning_rate': 8.58114375239747e-05, 'epoch': 2.7727272727272725}\n{'loss': 0.2745, 'grad_norm': 0.881945788860321, 'learning_rate': 8.535255283133313e-05, 'epoch': 2.7954545454545454}\n{'loss': 0.2877, 'grad_norm': 0.799531102180481, 'learning_rate': 8.489366813869155e-05, 'epoch': 2.8181818181818183}\n{'loss': 0.2925, 'grad_norm': 1.1864018440246582, 'learning_rate': 8.443478344604998e-05, 'epoch': 2.840909090909091}\n{'loss': 0.3278, 'grad_norm': 1.1383503675460815, 'learning_rate': 8.397589875340839e-05, 'epoch': 2.8636363636363638}\n{'loss': 0.311, 'grad_norm': 0.9069918990135193, 'learning_rate': 8.351701406076683e-05, 'epoch': 2.8863636363636362}\n{'loss': 0.2374, 'grad_norm': 0.9678889513015747, 'learning_rate': 8.305812936812524e-05, 'epoch': 2.909090909090909}\n{'loss': 0.2412, 'grad_norm': 1.5771883726119995, 'learning_rate': 8.259924467548366e-05, 'epoch': 2.9318181818181817}\n{'loss': 0.2793, 'grad_norm': 1.397409200668335, 'learning_rate': 8.214035998284209e-05, 'epoch': 2.9545454545454546}\n{'loss': 0.2535, 'grad_norm': 1.041911005973816, 'learning_rate': 8.168147529020051e-05, 'epoch': 2.9772727272727275}\n{'loss': 0.1655, 'grad_norm': 2.665797233581543, 'learning_rate': 8.122259059755894e-05, 'epoch': 3.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8670    0.7835    0.8231      3404\n     PERCENT     0.8462    0.9296    0.8859        71\n        TYPE     0.9122    0.9682    0.9394     11194\n      VOLUME     0.6400    0.5714    0.6038        56\n\n   micro avg     0.9018    0.9238    0.9126     14725\n   macro avg     0.8164    0.8132    0.8130     14725\nweighted avg     0.9004    0.9238    0.9110     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.3232764005661011, 'eval_f1_macro': 0.8130498469058642, 'eval_precision': 0.9017567119655286, 'eval_recall': 0.9238030560271647, 'eval_f1': 0.9126467628312647, 'eval_accuracy': 0.90648467905498, 'eval_runtime': 1.5327, 'eval_samples_per_second': 3595.679, 'eval_steps_per_second': 7.177, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.251, 'grad_norm': 1.6488295793533325, 'learning_rate': 8.076370590491736e-05, 'epoch': 3.022727272727273}\n{'loss': 0.2836, 'grad_norm': 1.6013761758804321, 'learning_rate': 8.030482121227579e-05, 'epoch': 3.0454545454545454}\n{'loss': 0.2671, 'grad_norm': 1.0458635091781616, 'learning_rate': 7.984593651963421e-05, 'epoch': 3.0681818181818183}\n{'loss': 0.2227, 'grad_norm': 1.1774989366531372, 'learning_rate': 7.938705182699264e-05, 'epoch': 3.090909090909091}\n{'loss': 0.2013, 'grad_norm': 0.8068820238113403, 'learning_rate': 7.892816713435106e-05, 'epoch': 3.1136363636363638}\n{'loss': 0.2884, 'grad_norm': 1.0582140684127808, 'learning_rate': 7.846928244170949e-05, 'epoch': 3.1363636363636362}\n{'loss': 0.2801, 'grad_norm': 0.8165023922920227, 'learning_rate': 7.80103977490679e-05, 'epoch': 3.159090909090909}\n{'loss': 0.2384, 'grad_norm': 1.5318853855133057, 'learning_rate': 7.755151305642634e-05, 'epoch': 3.1818181818181817}\n{'loss': 0.2493, 'grad_norm': 0.7903140783309937, 'learning_rate': 7.709262836378475e-05, 'epoch': 3.2045454545454546}\n{'loss': 0.2494, 'grad_norm': 1.0819330215454102, 'learning_rate': 7.663374367114318e-05, 'epoch': 3.227272727272727}\n{'loss': 0.1882, 'grad_norm': 1.5957525968551636, 'learning_rate': 7.61748589785016e-05, 'epoch': 3.25}\n{'loss': 0.2316, 'grad_norm': 0.852026104927063, 'learning_rate': 7.571597428586003e-05, 'epoch': 3.2727272727272725}\n{'loss': 0.2595, 'grad_norm': 1.3027350902557373, 'learning_rate': 7.525708959321845e-05, 'epoch': 3.2954545454545454}\n{'loss': 0.2677, 'grad_norm': 1.4765559434890747, 'learning_rate': 7.479820490057688e-05, 'epoch': 3.3181818181818183}\n{'loss': 0.2086, 'grad_norm': 0.8278147578239441, 'learning_rate': 7.43393202079353e-05, 'epoch': 3.340909090909091}\n{'loss': 0.2357, 'grad_norm': 1.3878450393676758, 'learning_rate': 7.388043551529373e-05, 'epoch': 3.3636363636363638}\n{'loss': 0.2872, 'grad_norm': 2.016519784927368, 'learning_rate': 7.342155082265215e-05, 'epoch': 3.3863636363636362}\n{'loss': 0.2252, 'grad_norm': 1.0395110845565796, 'learning_rate': 7.296266613001058e-05, 'epoch': 3.409090909090909}\n{'loss': 0.238, 'grad_norm': 0.834790050983429, 'learning_rate': 7.2503781437369e-05, 'epoch': 3.4318181818181817}\n{'loss': 0.222, 'grad_norm': 1.8941376209259033, 'learning_rate': 7.204489674472741e-05, 'epoch': 3.4545454545454546}\n{'loss': 0.264, 'grad_norm': 0.9791531562805176, 'learning_rate': 7.158601205208584e-05, 'epoch': 3.4772727272727275}\n{'loss': 0.1881, 'grad_norm': 1.77976655960083, 'learning_rate': 7.112712735944426e-05, 'epoch': 3.5}\n{'loss': 0.2416, 'grad_norm': 1.1276715993881226, 'learning_rate': 7.066824266680269e-05, 'epoch': 3.5227272727272725}\n{'loss': 0.2525, 'grad_norm': 1.036089301109314, 'learning_rate': 7.020935797416112e-05, 'epoch': 3.5454545454545454}\n{'loss': 0.2726, 'grad_norm': 1.0917761325836182, 'learning_rate': 6.975047328151954e-05, 'epoch': 3.5681818181818183}\n{'loss': 0.253, 'grad_norm': 1.0265591144561768, 'learning_rate': 6.929158858887797e-05, 'epoch': 3.590909090909091}\n{'loss': 0.2167, 'grad_norm': 0.9079312682151794, 'learning_rate': 6.883270389623639e-05, 'epoch': 3.6136363636363638}\n{'loss': 0.2216, 'grad_norm': 0.9081659317016602, 'learning_rate': 6.837381920359482e-05, 'epoch': 3.6363636363636362}\n{'loss': 0.1994, 'grad_norm': 0.6393508315086365, 'learning_rate': 6.791493451095324e-05, 'epoch': 3.659090909090909}\n{'loss': 0.2222, 'grad_norm': 1.3179935216903687, 'learning_rate': 6.745604981831167e-05, 'epoch': 3.6818181818181817}\n{'loss': 0.2448, 'grad_norm': 1.0612586736679077, 'learning_rate': 6.699716512567009e-05, 'epoch': 3.7045454545454546}\n{'loss': 0.237, 'grad_norm': 1.1072745323181152, 'learning_rate': 6.653828043302852e-05, 'epoch': 3.7272727272727275}\n{'loss': 0.2181, 'grad_norm': 1.9287118911743164, 'learning_rate': 6.607939574038693e-05, 'epoch': 3.75}\n{'loss': 0.226, 'grad_norm': 1.512047529220581, 'learning_rate': 6.562051104774535e-05, 'epoch': 3.7727272727272725}\n{'loss': 0.1999, 'grad_norm': 0.9607634544372559, 'learning_rate': 6.516162635510378e-05, 'epoch': 3.7954545454545454}\n{'loss': 0.2146, 'grad_norm': 0.9144341349601746, 'learning_rate': 6.47027416624622e-05, 'epoch': 3.8181818181818183}\n{'loss': 0.2182, 'grad_norm': 2.2467331886291504, 'learning_rate': 6.424385696982063e-05, 'epoch': 3.840909090909091}\n{'loss': 0.2491, 'grad_norm': 1.4106578826904297, 'learning_rate': 6.378497227717905e-05, 'epoch': 3.8636363636363638}\n{'loss': 0.223, 'grad_norm': 1.056965947151184, 'learning_rate': 6.332608758453748e-05, 'epoch': 3.8863636363636362}\n{'loss': 0.2103, 'grad_norm': 0.720948338508606, 'learning_rate': 6.28672028918959e-05, 'epoch': 3.909090909090909}\n{'loss': 0.1992, 'grad_norm': 0.8450778722763062, 'learning_rate': 6.240831819925433e-05, 'epoch': 3.9318181818181817}\n{'loss': 0.2248, 'grad_norm': 0.7455568909645081, 'learning_rate': 6.194943350661275e-05, 'epoch': 3.9545454545454546}\n{'loss': 0.2055, 'grad_norm': 0.8563239574432373, 'learning_rate': 6.149054881397118e-05, 'epoch': 3.9772727272727275}\n{'loss': 0.2153, 'grad_norm': 4.175712585449219, 'learning_rate': 6.10316641213296e-05, 'epoch': 4.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8603    0.8590    0.8596      3404\n     PERCENT     0.9306    0.9437    0.9371        71\n        TYPE     0.9308    0.9668    0.9485     11194\n      VOLUME     0.7347    0.6429    0.6857        56\n\n   micro avg     0.9144    0.9405    0.9273     14725\n   macro avg     0.8641    0.8531    0.8577     14725\nweighted avg     0.9138    0.9405    0.9269     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.2791900038719177, 'eval_f1_macro': 0.8577160589987392, 'eval_precision': 0.9143668295259475, 'eval_recall': 0.940509337860781, 'eval_f1': 0.9272538582571725, 'eval_accuracy': 0.9202981965685468, 'eval_runtime': 1.4409, 'eval_samples_per_second': 3824.7, 'eval_steps_per_second': 7.634, 'epoch': 4.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.2435, 'grad_norm': 1.0966100692749023, 'learning_rate': 6.057277942868802e-05, 'epoch': 4.0227272727272725}\n{'loss': 0.2302, 'grad_norm': 1.1779345273971558, 'learning_rate': 6.011389473604645e-05, 'epoch': 4.045454545454546}\n{'loss': 0.2221, 'grad_norm': 0.8939775228500366, 'learning_rate': 5.965501004340487e-05, 'epoch': 4.068181818181818}\n{'loss': 0.2074, 'grad_norm': 1.2718982696533203, 'learning_rate': 5.91961253507633e-05, 'epoch': 4.090909090909091}\n{'loss': 0.1709, 'grad_norm': 1.0592632293701172, 'learning_rate': 5.8737240658121723e-05, 'epoch': 4.113636363636363}\n{'loss': 0.1916, 'grad_norm': 0.9217939972877502, 'learning_rate': 5.827835596548015e-05, 'epoch': 4.136363636363637}\n{'loss': 0.2249, 'grad_norm': 0.9666634202003479, 'learning_rate': 5.781947127283857e-05, 'epoch': 4.159090909090909}\n{'loss': 0.208, 'grad_norm': 0.9519042372703552, 'learning_rate': 5.7360586580196985e-05, 'epoch': 4.181818181818182}\n{'loss': 0.2004, 'grad_norm': 0.8500098586082458, 'learning_rate': 5.690170188755541e-05, 'epoch': 4.204545454545454}\n{'loss': 0.2373, 'grad_norm': 1.4414042234420776, 'learning_rate': 5.6442817194913836e-05, 'epoch': 4.2272727272727275}\n{'loss': 0.1918, 'grad_norm': 0.8171781897544861, 'learning_rate': 5.598393250227226e-05, 'epoch': 4.25}\n{'loss': 0.1531, 'grad_norm': 1.249695897102356, 'learning_rate': 5.5525047809630686e-05, 'epoch': 4.2727272727272725}\n{'loss': 0.1774, 'grad_norm': 1.2587119340896606, 'learning_rate': 5.506616311698911e-05, 'epoch': 4.295454545454546}\n{'loss': 0.1874, 'grad_norm': 0.794600248336792, 'learning_rate': 5.4607278424347536e-05, 'epoch': 4.318181818181818}\n{'loss': 0.163, 'grad_norm': 0.7839913964271545, 'learning_rate': 5.414839373170596e-05, 'epoch': 4.340909090909091}\n{'loss': 0.2312, 'grad_norm': 2.0228607654571533, 'learning_rate': 5.3689509039064386e-05, 'epoch': 4.363636363636363}\n{'loss': 0.1906, 'grad_norm': 0.9283410310745239, 'learning_rate': 5.323062434642281e-05, 'epoch': 4.386363636363637}\n{'loss': 0.1744, 'grad_norm': 0.92421555519104, 'learning_rate': 5.277173965378124e-05, 'epoch': 4.409090909090909}\n{'loss': 0.1732, 'grad_norm': 0.8405672311782837, 'learning_rate': 5.2312854961139655e-05, 'epoch': 4.431818181818182}\n{'loss': 0.1931, 'grad_norm': 0.8661573529243469, 'learning_rate': 5.185397026849808e-05, 'epoch': 4.454545454545454}\n{'loss': 0.1836, 'grad_norm': 0.7157054543495178, 'learning_rate': 5.13950855758565e-05, 'epoch': 4.4772727272727275}\n{'loss': 0.1832, 'grad_norm': 1.3246885538101196, 'learning_rate': 5.0936200883214924e-05, 'epoch': 4.5}\n{'loss': 0.1857, 'grad_norm': 0.892628014087677, 'learning_rate': 5.047731619057335e-05, 'epoch': 4.5227272727272725}\n{'loss': 0.235, 'grad_norm': 0.9904842972755432, 'learning_rate': 5.0018431497931774e-05, 'epoch': 4.545454545454545}\n{'loss': 0.1978, 'grad_norm': 0.8659442067146301, 'learning_rate': 4.95595468052902e-05, 'epoch': 4.568181818181818}\n{'loss': 0.2364, 'grad_norm': 1.0653133392333984, 'learning_rate': 4.9100662112648624e-05, 'epoch': 4.590909090909091}\n{'loss': 0.1779, 'grad_norm': 0.7731285691261292, 'learning_rate': 4.864177742000705e-05, 'epoch': 4.613636363636363}\n{'loss': 0.1988, 'grad_norm': 0.9257105588912964, 'learning_rate': 4.8182892727365475e-05, 'epoch': 4.636363636363637}\n{'loss': 0.1647, 'grad_norm': 0.9348530769348145, 'learning_rate': 4.77240080347239e-05, 'epoch': 4.659090909090909}\n{'loss': 0.2536, 'grad_norm': 2.2932991981506348, 'learning_rate': 4.7265123342082325e-05, 'epoch': 4.681818181818182}\n{'loss': 0.1731, 'grad_norm': 1.1329091787338257, 'learning_rate': 4.680623864944074e-05, 'epoch': 4.704545454545455}\n{'loss': 0.2033, 'grad_norm': 0.8376997709274292, 'learning_rate': 4.634735395679917e-05, 'epoch': 4.7272727272727275}\n{'loss': 0.2303, 'grad_norm': 1.4676024913787842, 'learning_rate': 4.5888469264157594e-05, 'epoch': 4.75}\n{'loss': 0.1869, 'grad_norm': 1.1765780448913574, 'learning_rate': 4.542958457151602e-05, 'epoch': 4.7727272727272725}\n{'loss': 0.1733, 'grad_norm': 1.0996387004852295, 'learning_rate': 4.497069987887444e-05, 'epoch': 4.795454545454545}\n{'loss': 0.2507, 'grad_norm': 0.9975095391273499, 'learning_rate': 4.451181518623286e-05, 'epoch': 4.818181818181818}\n{'loss': 0.2035, 'grad_norm': 1.2773971557617188, 'learning_rate': 4.405293049359129e-05, 'epoch': 4.840909090909091}\n{'loss': 0.1809, 'grad_norm': 1.018910527229309, 'learning_rate': 4.359404580094971e-05, 'epoch': 4.863636363636363}\n{'loss': 0.1675, 'grad_norm': 1.1275193691253662, 'learning_rate': 4.313516110830814e-05, 'epoch': 4.886363636363637}\n{'loss': 0.186, 'grad_norm': 0.8229063749313354, 'learning_rate': 4.267627641566656e-05, 'epoch': 4.909090909090909}\n{'loss': 0.1691, 'grad_norm': 1.4875587224960327, 'learning_rate': 4.221739172302499e-05, 'epoch': 4.931818181818182}\n{'loss': 0.1634, 'grad_norm': 0.8663178086280823, 'learning_rate': 4.175850703038341e-05, 'epoch': 4.954545454545455}\n{'loss': 0.2216, 'grad_norm': 1.1259565353393555, 'learning_rate': 4.129962233774183e-05, 'epoch': 4.9772727272727275}\n{'loss': 0.1236, 'grad_norm': 4.624413967132568, 'learning_rate': 4.084073764510026e-05, 'epoch': 5.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8847    0.8455    0.8647      3404\n     PERCENT     0.9565    0.9296    0.9429        71\n        TYPE     0.9251    0.9751    0.9494     11194\n      VOLUME     0.7451    0.6786    0.7103        56\n\n   micro avg     0.9160    0.9438    0.9297     14725\n   macro avg     0.8779    0.8572    0.8668     14725\nweighted avg     0.9152    0.9438    0.9289     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.2839409410953522, 'eval_f1_macro': 0.8668026632832425, 'eval_precision': 0.9159636171895598, 'eval_recall': 0.9437691001697793, 'eval_f1': 0.9296584941632939, 'eval_accuracy': 0.9226004494874747, 'eval_runtime': 1.4546, 'eval_samples_per_second': 3788.784, 'eval_steps_per_second': 7.562, 'epoch': 5.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.2035, 'grad_norm': 1.4258142709732056, 'learning_rate': 4.038185295245868e-05, 'epoch': 5.0227272727272725}\n{'loss': 0.2265, 'grad_norm': 1.8401542901992798, 'learning_rate': 3.992296825981711e-05, 'epoch': 5.045454545454546}\n{'loss': 0.1774, 'grad_norm': 1.3362313508987427, 'learning_rate': 3.946408356717553e-05, 'epoch': 5.068181818181818}\n{'loss': 0.2279, 'grad_norm': 1.7319897413253784, 'learning_rate': 3.900519887453395e-05, 'epoch': 5.090909090909091}\n{'loss': 0.1973, 'grad_norm': 0.8347750306129456, 'learning_rate': 3.8546314181892376e-05, 'epoch': 5.113636363636363}\n{'loss': 0.2054, 'grad_norm': 1.2351610660552979, 'learning_rate': 3.80874294892508e-05, 'epoch': 5.136363636363637}\n{'loss': 0.2069, 'grad_norm': 1.040825605392456, 'learning_rate': 3.7628544796609226e-05, 'epoch': 5.159090909090909}\n{'loss': 0.176, 'grad_norm': 1.806316614151001, 'learning_rate': 3.716966010396765e-05, 'epoch': 5.181818181818182}\n{'loss': 0.1951, 'grad_norm': 0.837510347366333, 'learning_rate': 3.6710775411326076e-05, 'epoch': 5.204545454545454}\n{'loss': 0.1545, 'grad_norm': 2.2677793502807617, 'learning_rate': 3.62518907186845e-05, 'epoch': 5.2272727272727275}\n{'loss': 0.1793, 'grad_norm': 2.2346882820129395, 'learning_rate': 3.579300602604292e-05, 'epoch': 5.25}\n{'loss': 0.1828, 'grad_norm': 1.3521862030029297, 'learning_rate': 3.5334121333401345e-05, 'epoch': 5.2727272727272725}\n{'loss': 0.1827, 'grad_norm': 1.0471196174621582, 'learning_rate': 3.487523664075977e-05, 'epoch': 5.295454545454546}\n{'loss': 0.1564, 'grad_norm': 1.3073800802230835, 'learning_rate': 3.4416351948118195e-05, 'epoch': 5.318181818181818}\n{'loss': 0.2036, 'grad_norm': 0.8027551770210266, 'learning_rate': 3.395746725547662e-05, 'epoch': 5.340909090909091}\n{'loss': 0.1453, 'grad_norm': 0.8208193778991699, 'learning_rate': 3.3498582562835046e-05, 'epoch': 5.363636363636363}\n{'loss': 0.1784, 'grad_norm': 1.03177809715271, 'learning_rate': 3.3039697870193464e-05, 'epoch': 5.386363636363637}\n{'loss': 0.1581, 'grad_norm': 1.6641643047332764, 'learning_rate': 3.258081317755189e-05, 'epoch': 5.409090909090909}\n{'loss': 0.2193, 'grad_norm': 1.3608982563018799, 'learning_rate': 3.2121928484910314e-05, 'epoch': 5.431818181818182}\n{'loss': 0.1548, 'grad_norm': 0.9643439054489136, 'learning_rate': 3.166304379226874e-05, 'epoch': 5.454545454545454}\n{'loss': 0.1955, 'grad_norm': 1.1514548063278198, 'learning_rate': 3.1204159099627165e-05, 'epoch': 5.4772727272727275}\n{'loss': 0.1379, 'grad_norm': 0.791383683681488, 'learning_rate': 3.074527440698559e-05, 'epoch': 5.5}\n{'loss': 0.1667, 'grad_norm': 0.8466902375221252, 'learning_rate': 3.028638971434401e-05, 'epoch': 5.5227272727272725}\n{'loss': 0.1747, 'grad_norm': 0.9056419730186462, 'learning_rate': 2.9827505021702437e-05, 'epoch': 5.545454545454545}\n{'loss': 0.162, 'grad_norm': 0.9244315028190613, 'learning_rate': 2.9368620329060862e-05, 'epoch': 5.568181818181818}\n{'loss': 0.1652, 'grad_norm': 1.2708746194839478, 'learning_rate': 2.8909735636419283e-05, 'epoch': 5.590909090909091}\n{'loss': 0.1654, 'grad_norm': 0.8023860454559326, 'learning_rate': 2.8450850943777705e-05, 'epoch': 5.613636363636363}\n{'loss': 0.1878, 'grad_norm': 1.6950773000717163, 'learning_rate': 2.799196625113613e-05, 'epoch': 5.636363636363637}\n{'loss': 0.1485, 'grad_norm': 0.997592031955719, 'learning_rate': 2.7533081558494556e-05, 'epoch': 5.659090909090909}\n{'loss': 0.1893, 'grad_norm': 0.806831955909729, 'learning_rate': 2.707419686585298e-05, 'epoch': 5.681818181818182}\n{'loss': 0.1563, 'grad_norm': 1.3274389505386353, 'learning_rate': 2.6615312173211406e-05, 'epoch': 5.704545454545455}\n{'loss': 0.1819, 'grad_norm': 1.029489278793335, 'learning_rate': 2.6156427480569828e-05, 'epoch': 5.7272727272727275}\n{'loss': 0.1444, 'grad_norm': 1.248544692993164, 'learning_rate': 2.569754278792825e-05, 'epoch': 5.75}\n{'loss': 0.1392, 'grad_norm': 0.846272885799408, 'learning_rate': 2.5238658095286674e-05, 'epoch': 5.7727272727272725}\n{'loss': 0.1566, 'grad_norm': 0.8450718522071838, 'learning_rate': 2.47797734026451e-05, 'epoch': 5.795454545454545}\n{'loss': 0.1337, 'grad_norm': 0.7774035334587097, 'learning_rate': 2.4320888710003525e-05, 'epoch': 5.818181818181818}\n{'loss': 0.1548, 'grad_norm': 0.753779411315918, 'learning_rate': 2.386200401736195e-05, 'epoch': 5.840909090909091}\n{'loss': 0.1681, 'grad_norm': 0.933479368686676, 'learning_rate': 2.340311932472037e-05, 'epoch': 5.863636363636363}\n{'loss': 0.1842, 'grad_norm': 0.8205536007881165, 'learning_rate': 2.2944234632078797e-05, 'epoch': 5.886363636363637}\n{'loss': 0.1682, 'grad_norm': 0.9836185574531555, 'learning_rate': 2.248534993943722e-05, 'epoch': 5.909090909090909}\n{'loss': 0.202, 'grad_norm': 1.3156516551971436, 'learning_rate': 2.2026465246795644e-05, 'epoch': 5.931818181818182}\n{'loss': 0.1741, 'grad_norm': 1.4537285566329956, 'learning_rate': 2.156758055415407e-05, 'epoch': 5.954545454545455}\n{'loss': 0.1759, 'grad_norm': 1.2756414413452148, 'learning_rate': 2.1108695861512494e-05, 'epoch': 5.9772727272727275}\n{'loss': 0.0337, 'grad_norm': 2.4004743099212646, 'learning_rate': 2.0649811168870916e-05, 'epoch': 6.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8924    0.8552    0.8734      3404\n     PERCENT     0.9306    0.9437    0.9371        71\n        TYPE     0.9314    0.9729    0.9517     11194\n      VOLUME     0.7500    0.6429    0.6923        56\n\n   micro avg     0.9224    0.9443    0.9332     14725\n   macro avg     0.8761    0.8537    0.8636     14725\nweighted avg     0.9217    0.9443    0.9326     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.2682323455810547, 'eval_f1_macro': 0.8636193211867846, 'eval_precision': 0.9223880597014925, 'eval_recall': 0.9443123938879456, 'eval_f1': 0.933221476510067, 'eval_accuracy': 0.9260538288658664, 'eval_runtime': 1.441, 'eval_samples_per_second': 3824.5, 'eval_steps_per_second': 7.634, 'epoch': 6.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.156, 'grad_norm': 1.0212057828903198, 'learning_rate': 2.019092647622934e-05, 'epoch': 6.0227272727272725}\n{'loss': 0.2086, 'grad_norm': 1.9470494985580444, 'learning_rate': 1.9732041783587766e-05, 'epoch': 6.045454545454546}\n{'loss': 0.1587, 'grad_norm': 1.0558949708938599, 'learning_rate': 1.9273157090946188e-05, 'epoch': 6.068181818181818}\n{'loss': 0.149, 'grad_norm': 1.552207112312317, 'learning_rate': 1.8814272398304613e-05, 'epoch': 6.090909090909091}\n{'loss': 0.1666, 'grad_norm': 0.948195219039917, 'learning_rate': 1.8355387705663038e-05, 'epoch': 6.113636363636363}\n{'loss': 0.1365, 'grad_norm': 2.4103784561157227, 'learning_rate': 1.789650301302146e-05, 'epoch': 6.136363636363637}\n{'loss': 0.1689, 'grad_norm': 0.8588116765022278, 'learning_rate': 1.7437618320379885e-05, 'epoch': 6.159090909090909}\n{'loss': 0.1855, 'grad_norm': 0.8577831983566284, 'learning_rate': 1.697873362773831e-05, 'epoch': 6.181818181818182}\n{'loss': 0.1666, 'grad_norm': 1.4004777669906616, 'learning_rate': 1.6519848935096732e-05, 'epoch': 6.204545454545454}\n{'loss': 0.1337, 'grad_norm': 0.8400872349739075, 'learning_rate': 1.6060964242455157e-05, 'epoch': 6.2272727272727275}\n{'loss': 0.1811, 'grad_norm': 0.9055111408233643, 'learning_rate': 1.5602079549813582e-05, 'epoch': 6.25}\n{'loss': 0.1208, 'grad_norm': 0.7343105673789978, 'learning_rate': 1.5143194857172006e-05, 'epoch': 6.2727272727272725}\n{'loss': 0.1586, 'grad_norm': 0.8827933073043823, 'learning_rate': 1.4684310164530431e-05, 'epoch': 6.295454545454546}\n{'loss': 0.1419, 'grad_norm': 1.0287951231002808, 'learning_rate': 1.4225425471888853e-05, 'epoch': 6.318181818181818}\n{'loss': 0.174, 'grad_norm': 1.0510131120681763, 'learning_rate': 1.3766540779247278e-05, 'epoch': 6.340909090909091}\n{'loss': 0.1355, 'grad_norm': 0.6997187733650208, 'learning_rate': 1.3307656086605703e-05, 'epoch': 6.363636363636363}\n{'loss': 0.1675, 'grad_norm': 1.0508769750595093, 'learning_rate': 1.2848771393964125e-05, 'epoch': 6.386363636363637}\n{'loss': 0.1529, 'grad_norm': 0.8040804862976074, 'learning_rate': 1.238988670132255e-05, 'epoch': 6.409090909090909}\n{'loss': 0.1638, 'grad_norm': 1.2282794713974, 'learning_rate': 1.1931002008680975e-05, 'epoch': 6.431818181818182}\n{'loss': 0.1797, 'grad_norm': 1.0493078231811523, 'learning_rate': 1.1472117316039398e-05, 'epoch': 6.454545454545454}\n{'loss': 0.1654, 'grad_norm': 1.552747368812561, 'learning_rate': 1.1013232623397822e-05, 'epoch': 6.4772727272727275}\n{'loss': 0.1557, 'grad_norm': 1.199730634689331, 'learning_rate': 1.0554347930756247e-05, 'epoch': 6.5}\n{'loss': 0.1303, 'grad_norm': 0.6349314451217651, 'learning_rate': 1.009546323811467e-05, 'epoch': 6.5227272727272725}\n{'loss': 0.1595, 'grad_norm': 0.890609085559845, 'learning_rate': 9.636578545473094e-06, 'epoch': 6.545454545454545}\n{'loss': 0.1329, 'grad_norm': 0.8131702542304993, 'learning_rate': 9.177693852831519e-06, 'epoch': 6.568181818181818}\n{'loss': 0.1717, 'grad_norm': 1.1094801425933838, 'learning_rate': 8.718809160189943e-06, 'epoch': 6.590909090909091}\n{'loss': 0.1526, 'grad_norm': 0.8611587285995483, 'learning_rate': 8.259924467548366e-06, 'epoch': 6.613636363636363}\n{'loss': 0.1947, 'grad_norm': 1.0228921175003052, 'learning_rate': 7.801039774906791e-06, 'epoch': 6.636363636363637}\n{'loss': 0.2007, 'grad_norm': 1.0871639251708984, 'learning_rate': 7.3421550822652154e-06, 'epoch': 6.659090909090909}\n{'loss': 0.1287, 'grad_norm': 1.1033381223678589, 'learning_rate': 6.883270389623639e-06, 'epoch': 6.681818181818182}\n{'loss': 0.1562, 'grad_norm': 1.1435519456863403, 'learning_rate': 6.424385696982062e-06, 'epoch': 6.704545454545455}\n{'loss': 0.1566, 'grad_norm': 0.7749409675598145, 'learning_rate': 5.9655010043404875e-06, 'epoch': 6.7272727272727275}\n{'loss': 0.1935, 'grad_norm': 0.8138976693153381, 'learning_rate': 5.506616311698911e-06, 'epoch': 6.75}\n{'loss': 0.1442, 'grad_norm': 0.8279205560684204, 'learning_rate': 5.047731619057335e-06, 'epoch': 6.7727272727272725}\n{'loss': 0.1651, 'grad_norm': 0.982661783695221, 'learning_rate': 4.5888469264157595e-06, 'epoch': 6.795454545454545}\n{'loss': 0.1383, 'grad_norm': 0.7624577283859253, 'learning_rate': 4.129962233774183e-06, 'epoch': 6.818181818181818}\n{'loss': 0.19, 'grad_norm': 0.9087877869606018, 'learning_rate': 3.6710775411326077e-06, 'epoch': 6.840909090909091}\n{'loss': 0.1597, 'grad_norm': 1.070534586906433, 'learning_rate': 3.212192848491031e-06, 'epoch': 6.863636363636363}\n{'loss': 0.1891, 'grad_norm': 0.8049632906913757, 'learning_rate': 2.7533081558494555e-06, 'epoch': 6.886363636363637}\n{'loss': 0.1659, 'grad_norm': 1.0051970481872559, 'learning_rate': 2.2944234632078798e-06, 'epoch': 6.909090909090909}\n{'loss': 0.1625, 'grad_norm': 1.4801782369613647, 'learning_rate': 1.8355387705663039e-06, 'epoch': 6.931818181818182}\n{'loss': 0.115, 'grad_norm': 0.8496116399765015, 'learning_rate': 1.3766540779247277e-06, 'epoch': 6.954545454545455}\n{'loss': 0.1658, 'grad_norm': 0.7852962017059326, 'learning_rate': 9.177693852831519e-07, 'epoch': 6.9772727272727275}\n{'loss': 0.0796, 'grad_norm': 4.662920951843262, 'learning_rate': 4.5888469264157596e-07, 'epoch': 7.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8840    0.8684    0.8761      3404\n     PERCENT     0.9306    0.9437    0.9371        71\n        TYPE     0.9339    0.9721    0.9526     11194\n      VOLUME     0.7755    0.6786    0.7238        56\n\n   micro avg     0.9223    0.9469    0.9345     14725\n   macro avg     0.8810    0.8657    0.8724     14725\nweighted avg     0.9218    0.9469    0.9340     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.2660529911518097, 'eval_f1_macro': 0.8724058282531161, 'eval_precision': 0.9223390884434742, 'eval_recall': 0.9468930390492359, 'eval_f1': 0.9344547952550096, 'eval_accuracy': 0.9273145864167078, 'eval_runtime': 1.433, 'eval_samples_per_second': 3845.866, 'eval_steps_per_second': 7.676, 'epoch': 7.0}\n{'train_runtime': 42.8729, 'train_samples_per_second': 3598.705, 'train_steps_per_second': 7.184, 'train_loss': 0.39172113606972353, 'epoch': 7.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nThe speedups for torchdynamo mostly come with GPU Ampere or higher and which is not detected here.\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8840    0.8684    0.8761      3404\n     PERCENT     0.9306    0.9437    0.9371        71\n        TYPE     0.9339    0.9721    0.9526     11194\n      VOLUME     0.7755    0.6786    0.7238        56\n\n   micro avg     0.9223    0.9469    0.9345     14725\n   macro avg     0.8810    0.8657    0.8724     14725\nweighted avg     0.9218    0.9469    0.9340     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.2660529911518097, 'eval_f1_macro': 0.8724058282531161, 'eval_precision': 0.9223390884434742, 'eval_recall': 0.9468930390492359, 'eval_f1': 0.9344547952550096, 'eval_accuracy': 0.9273145864167078, 'eval_runtime': 1.5025, 'eval_samples_per_second': 3667.792, 'eval_steps_per_second': 7.321, 'epoch': 7.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/3364797558.py:66: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 2.2537, 'grad_norm': 7.209476947784424, 'learning_rate': 0.0, 'epoch': 0.022727272727272728}\n{'loss': 2.2455, 'grad_norm': 7.228684902191162, 'learning_rate': 4.100356769732791e-06, 'epoch': 0.045454545454545456}\n{'loss': 2.2381, 'grad_norm': 7.246882438659668, 'learning_rate': 8.200713539465582e-06, 'epoch': 0.06818181818181818}\n{'loss': 2.2221, 'grad_norm': 7.004398822784424, 'learning_rate': 1.2301070309198374e-05, 'epoch': 0.09090909090909091}\n{'loss': 2.195, 'grad_norm': 7.060441970825195, 'learning_rate': 1.6401427078931164e-05, 'epoch': 0.11363636363636363}\n{'loss': 2.1453, 'grad_norm': 7.361405372619629, 'learning_rate': 2.0501783848663958e-05, 'epoch': 0.13636363636363635}\n{'loss': 2.1309, 'grad_norm': 6.646059989929199, 'learning_rate': 2.4602140618396748e-05, 'epoch': 0.1590909090909091}\n{'loss': 2.0666, 'grad_norm': 6.8168110847473145, 'learning_rate': 2.8702497388129538e-05, 'epoch': 0.18181818181818182}\n{'loss': 1.9916, 'grad_norm': 6.649717807769775, 'learning_rate': 3.280285415786233e-05, 'epoch': 0.20454545454545456}\n{'loss': 1.9401, 'grad_norm': 6.5116190910339355, 'learning_rate': 3.690321092759512e-05, 'epoch': 0.22727272727272727}\n{'loss': 1.8859, 'grad_norm': 6.019749641418457, 'learning_rate': 4.1003567697327915e-05, 'epoch': 0.25}\n{'loss': 1.7809, 'grad_norm': 5.845913410186768, 'learning_rate': 4.510392446706071e-05, 'epoch': 0.2727272727272727}\n{'loss': 1.6998, 'grad_norm': 5.694859027862549, 'learning_rate': 4.9204281236793496e-05, 'epoch': 0.29545454545454547}\n{'loss': 1.6419, 'grad_norm': 5.2347283363342285, 'learning_rate': 5.330463800652629e-05, 'epoch': 0.3181818181818182}\n{'loss': 1.577, 'grad_norm': 4.694947719573975, 'learning_rate': 5.7404994776259076e-05, 'epoch': 0.3409090909090909}\n{'loss': 1.4644, 'grad_norm': 4.52936315536499, 'learning_rate': 6.150535154599188e-05, 'epoch': 0.36363636363636365}\n{'loss': 1.4148, 'grad_norm': 3.9304990768432617, 'learning_rate': 6.560570831572466e-05, 'epoch': 0.38636363636363635}\n{'loss': 1.3198, 'grad_norm': 3.5027387142181396, 'learning_rate': 6.970606508545745e-05, 'epoch': 0.4090909090909091}\n{'loss': 1.2646, 'grad_norm': 2.90653395652771, 'learning_rate': 7.380642185519024e-05, 'epoch': 0.4318181818181818}\n{'loss': 1.2463, 'grad_norm': 2.1216914653778076, 'learning_rate': 7.790677862492304e-05, 'epoch': 0.45454545454545453}\n{'loss': 1.1615, 'grad_norm': 1.806475043296814, 'learning_rate': 8.200713539465583e-05, 'epoch': 0.4772727272727273}\n{'loss': 1.1368, 'grad_norm': 1.6871249675750732, 'learning_rate': 8.610749216438861e-05, 'epoch': 0.5}\n{'loss': 1.1077, 'grad_norm': 1.6620054244995117, 'learning_rate': 9.020784893412142e-05, 'epoch': 0.5227272727272727}\n{'loss': 1.0314, 'grad_norm': 1.3230921030044556, 'learning_rate': 9.43082057038542e-05, 'epoch': 0.5454545454545454}\n{'loss': 1.0713, 'grad_norm': 1.5481373071670532, 'learning_rate': 9.840856247358699e-05, 'epoch': 0.5681818181818182}\n{'loss': 1.048, 'grad_norm': 1.1666104793548584, 'learning_rate': 0.00010250891924331977, 'epoch': 0.5909090909090909}\n{'loss': 1.0447, 'grad_norm': 1.2260291576385498, 'learning_rate': 0.00010660927601305258, 'epoch': 0.6136363636363636}\n{'loss': 0.9811, 'grad_norm': 1.3506604433059692, 'learning_rate': 0.00011070963278278537, 'epoch': 0.6363636363636364}\n{'loss': 0.9426, 'grad_norm': 1.843135118484497, 'learning_rate': 0.00011480998955251815, 'epoch': 0.6590909090909091}\n{'loss': 0.9042, 'grad_norm': 1.2997664213180542, 'learning_rate': 0.00011891034632225095, 'epoch': 0.6818181818181818}\n{'loss': 0.8879, 'grad_norm': 1.1251477003097534, 'learning_rate': 0.00012301070309198375, 'epoch': 0.7045454545454546}\n{'loss': 0.9377, 'grad_norm': 1.3568413257598877, 'learning_rate': 0.00012711105986171653, 'epoch': 0.7272727272727273}\n{'loss': 0.8146, 'grad_norm': 0.8575782775878906, 'learning_rate': 0.00012665217516907496, 'epoch': 0.75}\n{'loss': 0.7936, 'grad_norm': 0.948834240436554, 'learning_rate': 0.00012619329047643338, 'epoch': 0.7727272727272727}\n{'loss': 0.8672, 'grad_norm': 0.848668098449707, 'learning_rate': 0.0001257344057837918, 'epoch': 0.7954545454545454}\n{'loss': 0.7692, 'grad_norm': 0.9520955681800842, 'learning_rate': 0.00012527552109115023, 'epoch': 0.8181818181818182}\n{'loss': 0.6653, 'grad_norm': 1.1058294773101807, 'learning_rate': 0.00012481663639850866, 'epoch': 0.8409090909090909}\n{'loss': 0.6986, 'grad_norm': 0.8139292597770691, 'learning_rate': 0.00012435775170586708, 'epoch': 0.8636363636363636}\n{'loss': 0.69, 'grad_norm': 0.9273372292518616, 'learning_rate': 0.0001238988670132255, 'epoch': 0.8863636363636364}\n{'loss': 0.6507, 'grad_norm': 0.8629874587059021, 'learning_rate': 0.00012343998232058393, 'epoch': 0.9090909090909091}\n{'loss': 0.6186, 'grad_norm': 1.1733092069625854, 'learning_rate': 0.00012298109762794236, 'epoch': 0.9318181818181818}\n{'loss': 0.6121, 'grad_norm': 0.7776479125022888, 'learning_rate': 0.00012252221293530078, 'epoch': 0.9545454545454546}\n{'loss': 0.6444, 'grad_norm': 0.7486988306045532, 'learning_rate': 0.0001220633282426592, 'epoch': 0.9772727272727273}\n{'loss': 0.7111, 'grad_norm': 2.995100736618042, 'learning_rate': 0.00012160444355001763, 'epoch': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.6670    0.7022    0.6841      3311\n     PERCENT     0.0000    0.0000    0.0000        86\n        TYPE     0.8539    0.8972    0.8750     11299\n      VOLUME     0.0000    0.0000    0.0000        42\n\n   micro avg     0.8114    0.8456    0.8281     14738\n   macro avg     0.3802    0.3998    0.3898     14738\nweighted avg     0.8045    0.8456    0.8245     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.5640626549720764, 'eval_f1_macro': 0.3897745940117435, 'eval_precision': 0.8114337804401615, 'eval_recall': 0.8455692766996878, 'eval_f1': 0.8281499202551834, 'eval_accuracy': 0.8301732145784383, 'eval_runtime': 1.4372, 'eval_samples_per_second': 3833.94, 'eval_steps_per_second': 7.654, 'epoch': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.5599, 'grad_norm': 1.4754680395126343, 'learning_rate': 0.00012114555885737605, 'epoch': 1.0227272727272727}\n{'loss': 0.5867, 'grad_norm': 1.4942543506622314, 'learning_rate': 0.00012068667416473446, 'epoch': 1.0454545454545454}\n{'loss': 0.5853, 'grad_norm': 0.8639747500419617, 'learning_rate': 0.0001202277894720929, 'epoch': 1.0681818181818181}\n{'loss': 0.5474, 'grad_norm': 0.6610881090164185, 'learning_rate': 0.00011976890477945131, 'epoch': 1.0909090909090908}\n{'loss': 0.5219, 'grad_norm': 0.9012497663497925, 'learning_rate': 0.00011931002008680975, 'epoch': 1.1136363636363635}\n{'loss': 0.5979, 'grad_norm': 1.1551281213760376, 'learning_rate': 0.00011885113539416816, 'epoch': 1.1363636363636362}\n{'loss': 0.4527, 'grad_norm': 0.8796942830085754, 'learning_rate': 0.0001183922507015266, 'epoch': 1.1590909090909092}\n{'loss': 0.5245, 'grad_norm': 0.9099571108818054, 'learning_rate': 0.00011793336600888501, 'epoch': 1.1818181818181819}\n{'loss': 0.5029, 'grad_norm': 0.7194554805755615, 'learning_rate': 0.00011747448131624345, 'epoch': 1.2045454545454546}\n{'loss': 0.5621, 'grad_norm': 0.9113385081291199, 'learning_rate': 0.00011701559662360186, 'epoch': 1.2272727272727273}\n{'loss': 0.5286, 'grad_norm': 0.7709293961524963, 'learning_rate': 0.0001165567119309603, 'epoch': 1.25}\n{'loss': 0.4793, 'grad_norm': 0.7862682938575745, 'learning_rate': 0.00011609782723831871, 'epoch': 1.2727272727272727}\n{'loss': 0.4675, 'grad_norm': 1.1875810623168945, 'learning_rate': 0.00011563894254567713, 'epoch': 1.2954545454545454}\n{'loss': 0.5322, 'grad_norm': 1.0410943031311035, 'learning_rate': 0.00011518005785303556, 'epoch': 1.3181818181818181}\n{'loss': 0.4225, 'grad_norm': 0.9943481683731079, 'learning_rate': 0.00011472117316039397, 'epoch': 1.3409090909090908}\n{'loss': 0.4845, 'grad_norm': 1.1881413459777832, 'learning_rate': 0.00011426228846775241, 'epoch': 1.3636363636363638}\n{'loss': 0.486, 'grad_norm': 1.763633131980896, 'learning_rate': 0.00011380340377511082, 'epoch': 1.3863636363636362}\n{'loss': 0.4606, 'grad_norm': 1.024949073791504, 'learning_rate': 0.00011334451908246926, 'epoch': 1.4090909090909092}\n{'loss': 0.4402, 'grad_norm': 0.8444370627403259, 'learning_rate': 0.00011288563438982767, 'epoch': 1.4318181818181819}\n{'loss': 0.4368, 'grad_norm': 1.3856502771377563, 'learning_rate': 0.00011242674969718611, 'epoch': 1.4545454545454546}\n{'loss': 0.4082, 'grad_norm': 0.7314215302467346, 'learning_rate': 0.00011196786500454452, 'epoch': 1.4772727272727273}\n{'loss': 0.4418, 'grad_norm': 1.087971806526184, 'learning_rate': 0.00011150898031190296, 'epoch': 1.5}\n{'loss': 0.4128, 'grad_norm': 0.8644031882286072, 'learning_rate': 0.00011105009561926137, 'epoch': 1.5227272727272727}\n{'loss': 0.3902, 'grad_norm': 1.0535595417022705, 'learning_rate': 0.00011059121092661981, 'epoch': 1.5454545454545454}\n{'loss': 0.4158, 'grad_norm': 1.2372848987579346, 'learning_rate': 0.00011013232623397822, 'epoch': 1.5681818181818183}\n{'loss': 0.387, 'grad_norm': 0.57101970911026, 'learning_rate': 0.00010967344154133665, 'epoch': 1.5909090909090908}\n{'loss': 0.4213, 'grad_norm': 1.4282615184783936, 'learning_rate': 0.00010921455684869507, 'epoch': 1.6136363636363638}\n{'loss': 0.4236, 'grad_norm': 1.3272603750228882, 'learning_rate': 0.00010875567215605348, 'epoch': 1.6363636363636362}\n{'loss': 0.3945, 'grad_norm': 1.3347208499908447, 'learning_rate': 0.00010829678746341192, 'epoch': 1.6590909090909092}\n{'loss': 0.4241, 'grad_norm': 0.9252329468727112, 'learning_rate': 0.00010783790277077033, 'epoch': 1.6818181818181817}\n{'loss': 0.389, 'grad_norm': 0.9237862229347229, 'learning_rate': 0.00010737901807812877, 'epoch': 1.7045454545454546}\n{'loss': 0.4008, 'grad_norm': 0.788098931312561, 'learning_rate': 0.00010692013338548718, 'epoch': 1.7272727272727273}\n{'loss': 0.4161, 'grad_norm': 1.7811729907989502, 'learning_rate': 0.00010646124869284562, 'epoch': 1.75}\n{'loss': 0.3956, 'grad_norm': 1.124848484992981, 'learning_rate': 0.00010600236400020403, 'epoch': 1.7727272727272727}\n{'loss': 0.3559, 'grad_norm': 1.4459137916564941, 'learning_rate': 0.00010554347930756247, 'epoch': 1.7954545454545454}\n{'loss': 0.3919, 'grad_norm': 0.9992513656616211, 'learning_rate': 0.00010508459461492089, 'epoch': 1.8181818181818183}\n{'loss': 0.3135, 'grad_norm': 0.933051586151123, 'learning_rate': 0.00010462570992227931, 'epoch': 1.8409090909090908}\n{'loss': 0.4009, 'grad_norm': 1.0094308853149414, 'learning_rate': 0.00010416682522963774, 'epoch': 1.8636363636363638}\n{'loss': 0.3954, 'grad_norm': 0.8999991416931152, 'learning_rate': 0.00010370794053699616, 'epoch': 1.8863636363636362}\n{'loss': 0.42, 'grad_norm': 0.9818360805511475, 'learning_rate': 0.00010324905584435459, 'epoch': 1.9090909090909092}\n{'loss': 0.429, 'grad_norm': 1.1183022260665894, 'learning_rate': 0.000102790171151713, 'epoch': 1.9318181818181817}\n{'loss': 0.4032, 'grad_norm': 0.7616375088691711, 'learning_rate': 0.00010233128645907144, 'epoch': 1.9545454545454546}\n{'loss': 0.3329, 'grad_norm': 1.1311702728271484, 'learning_rate': 0.00010187240176642985, 'epoch': 1.9772727272727273}\n{'loss': 0.2769, 'grad_norm': 3.2173471450805664, 'learning_rate': 0.00010141351707378829, 'epoch': 2.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8231    0.7813    0.8017      3311\n     PERCENT     0.6136    0.9419    0.7431        86\n        TYPE     0.9068    0.9607    0.9330     11299\n      VOLUME     0.7143    0.3571    0.4762        42\n\n   micro avg     0.8867    0.9186    0.9024     14738\n   macro avg     0.7644    0.7603    0.7385     14738\nweighted avg     0.8857    0.9186    0.9011     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.34924808144569397, 'eval_f1_macro': 0.7384860042373391, 'eval_precision': 0.8867491976157725, 'eval_recall': 0.9185778260279549, 'eval_f1': 0.9023829361773037, 'eval_accuracy': 0.8972733730397247, 'eval_runtime': 1.4314, 'eval_samples_per_second': 3849.307, 'eval_steps_per_second': 7.685, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.3257, 'grad_norm': 1.1137104034423828, 'learning_rate': 0.0001009546323811467, 'epoch': 2.022727272727273}\n{'loss': 0.3346, 'grad_norm': 1.1818287372589111, 'learning_rate': 0.00010049574768850514, 'epoch': 2.0454545454545454}\n{'loss': 0.3778, 'grad_norm': 0.8719515204429626, 'learning_rate': 0.00010003686299586355, 'epoch': 2.0681818181818183}\n{'loss': 0.3775, 'grad_norm': 1.5040611028671265, 'learning_rate': 9.957797830322199e-05, 'epoch': 2.090909090909091}\n{'loss': 0.329, 'grad_norm': 0.8175734877586365, 'learning_rate': 9.91190936105804e-05, 'epoch': 2.1136363636363638}\n{'loss': 0.2611, 'grad_norm': 1.4619410037994385, 'learning_rate': 9.866020891793882e-05, 'epoch': 2.1363636363636362}\n{'loss': 0.3281, 'grad_norm': 0.7298605442047119, 'learning_rate': 9.820132422529725e-05, 'epoch': 2.159090909090909}\n{'loss': 0.3529, 'grad_norm': 0.9200974702835083, 'learning_rate': 9.774243953265567e-05, 'epoch': 2.1818181818181817}\n{'loss': 0.3494, 'grad_norm': 0.7883594632148743, 'learning_rate': 9.72835548400141e-05, 'epoch': 2.2045454545454546}\n{'loss': 0.3786, 'grad_norm': 0.9708265066146851, 'learning_rate': 9.682467014737251e-05, 'epoch': 2.227272727272727}\n{'loss': 0.3525, 'grad_norm': 0.7870956063270569, 'learning_rate': 9.636578545473095e-05, 'epoch': 2.25}\n{'loss': 0.2949, 'grad_norm': 0.9222887754440308, 'learning_rate': 9.590690076208936e-05, 'epoch': 2.2727272727272725}\n{'loss': 0.388, 'grad_norm': 0.9895247220993042, 'learning_rate': 9.54480160694478e-05, 'epoch': 2.2954545454545454}\n{'loss': 0.3371, 'grad_norm': 1.0482850074768066, 'learning_rate': 9.498913137680621e-05, 'epoch': 2.3181818181818183}\n{'loss': 0.2425, 'grad_norm': 1.099799394607544, 'learning_rate': 9.453024668416465e-05, 'epoch': 2.340909090909091}\n{'loss': 0.3077, 'grad_norm': 1.0414669513702393, 'learning_rate': 9.407136199152306e-05, 'epoch': 2.3636363636363638}\n{'loss': 0.2816, 'grad_norm': 0.8325647115707397, 'learning_rate': 9.361247729888149e-05, 'epoch': 2.3863636363636362}\n{'loss': 0.3632, 'grad_norm': 0.874363124370575, 'learning_rate': 9.315359260623991e-05, 'epoch': 2.409090909090909}\n{'loss': 0.3129, 'grad_norm': 0.7180702686309814, 'learning_rate': 9.269470791359834e-05, 'epoch': 2.4318181818181817}\n{'loss': 0.2539, 'grad_norm': 1.5781663656234741, 'learning_rate': 9.223582322095676e-05, 'epoch': 2.4545454545454546}\n{'loss': 0.2993, 'grad_norm': 0.8859938383102417, 'learning_rate': 9.177693852831519e-05, 'epoch': 2.4772727272727275}\n{'loss': 0.2882, 'grad_norm': 1.0587520599365234, 'learning_rate': 9.131805383567361e-05, 'epoch': 2.5}\n{'loss': 0.2915, 'grad_norm': 1.0322245359420776, 'learning_rate': 9.085916914303204e-05, 'epoch': 2.5227272727272725}\n{'loss': 0.3034, 'grad_norm': 0.8135605454444885, 'learning_rate': 9.040028445039046e-05, 'epoch': 2.5454545454545454}\n{'loss': 0.286, 'grad_norm': 1.131787657737732, 'learning_rate': 8.994139975774887e-05, 'epoch': 2.5681818181818183}\n{'loss': 0.295, 'grad_norm': 1.4275805950164795, 'learning_rate': 8.948251506510731e-05, 'epoch': 2.590909090909091}\n{'loss': 0.3096, 'grad_norm': 1.0112965106964111, 'learning_rate': 8.902363037246572e-05, 'epoch': 2.6136363636363638}\n{'loss': 0.3195, 'grad_norm': 0.7992662191390991, 'learning_rate': 8.856474567982416e-05, 'epoch': 2.6363636363636362}\n{'loss': 0.2777, 'grad_norm': 0.8774783611297607, 'learning_rate': 8.810586098718257e-05, 'epoch': 2.659090909090909}\n{'loss': 0.2394, 'grad_norm': 1.110810399055481, 'learning_rate': 8.7646976294541e-05, 'epoch': 2.6818181818181817}\n{'loss': 0.2661, 'grad_norm': 0.9883676171302795, 'learning_rate': 8.718809160189943e-05, 'epoch': 2.7045454545454546}\n{'loss': 0.354, 'grad_norm': 1.106087565422058, 'learning_rate': 8.672920690925785e-05, 'epoch': 2.7272727272727275}\n{'loss': 0.2918, 'grad_norm': 0.9752638936042786, 'learning_rate': 8.627032221661628e-05, 'epoch': 2.75}\n{'loss': 0.3168, 'grad_norm': 2.009366035461426, 'learning_rate': 8.58114375239747e-05, 'epoch': 2.7727272727272725}\n{'loss': 0.2899, 'grad_norm': 1.4583755731582642, 'learning_rate': 8.535255283133313e-05, 'epoch': 2.7954545454545454}\n{'loss': 0.3195, 'grad_norm': 1.0857182741165161, 'learning_rate': 8.489366813869155e-05, 'epoch': 2.8181818181818183}\n{'loss': 0.3334, 'grad_norm': 1.4026871919631958, 'learning_rate': 8.443478344604998e-05, 'epoch': 2.840909090909091}\n{'loss': 0.2816, 'grad_norm': 1.3772810697555542, 'learning_rate': 8.397589875340839e-05, 'epoch': 2.8636363636363638}\n{'loss': 0.2863, 'grad_norm': 1.3258693218231201, 'learning_rate': 8.351701406076683e-05, 'epoch': 2.8863636363636362}\n{'loss': 0.274, 'grad_norm': 0.7660472393035889, 'learning_rate': 8.305812936812524e-05, 'epoch': 2.909090909090909}\n{'loss': 0.3222, 'grad_norm': 1.4478996992111206, 'learning_rate': 8.259924467548366e-05, 'epoch': 2.9318181818181817}\n{'loss': 0.3067, 'grad_norm': 0.8783790469169617, 'learning_rate': 8.214035998284209e-05, 'epoch': 2.9545454545454546}\n{'loss': 0.3183, 'grad_norm': 0.8479524850845337, 'learning_rate': 8.168147529020051e-05, 'epoch': 2.9772727272727275}\n{'loss': 0.1824, 'grad_norm': 3.107375383377075, 'learning_rate': 8.122259059755894e-05, 'epoch': 3.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8267    0.8541    0.8402      3311\n     PERCENT     0.8105    0.8953    0.8508        86\n        TYPE     0.9297    0.9633    0.9462     11299\n      VOLUME     0.4600    0.5476    0.5000        42\n\n   micro avg     0.9043    0.9372    0.9205     14738\n   macro avg     0.7567    0.8151    0.7843     14738\nweighted avg     0.9045    0.9372    0.9205     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.29563790559768677, 'eval_f1_macro': 0.7842957624573037, 'eval_precision': 0.904340993910823, 'eval_recall': 0.9371692224182385, 'eval_f1': 0.9204624970844024, 'eval_accuracy': 0.9143216217693022, 'eval_runtime': 1.8199, 'eval_samples_per_second': 3027.592, 'eval_steps_per_second': 6.044, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.2354, 'grad_norm': 0.9587237238883972, 'learning_rate': 8.076370590491736e-05, 'epoch': 3.022727272727273}\n{'loss': 0.2759, 'grad_norm': 1.1790590286254883, 'learning_rate': 8.030482121227579e-05, 'epoch': 3.0454545454545454}\n{'loss': 0.2694, 'grad_norm': 1.2219557762145996, 'learning_rate': 7.984593651963421e-05, 'epoch': 3.0681818181818183}\n{'loss': 0.2634, 'grad_norm': 0.9560607075691223, 'learning_rate': 7.938705182699264e-05, 'epoch': 3.090909090909091}\n{'loss': 0.2585, 'grad_norm': 0.9478045701980591, 'learning_rate': 7.892816713435106e-05, 'epoch': 3.1136363636363638}\n{'loss': 0.2286, 'grad_norm': 1.2668052911758423, 'learning_rate': 7.846928244170949e-05, 'epoch': 3.1363636363636362}\n{'loss': 0.306, 'grad_norm': 1.5271493196487427, 'learning_rate': 7.80103977490679e-05, 'epoch': 3.159090909090909}\n{'loss': 0.2123, 'grad_norm': 0.7561611533164978, 'learning_rate': 7.755151305642634e-05, 'epoch': 3.1818181818181817}\n{'loss': 0.2683, 'grad_norm': 0.9020891785621643, 'learning_rate': 7.709262836378475e-05, 'epoch': 3.2045454545454546}\n{'loss': 0.2801, 'grad_norm': 0.8716951012611389, 'learning_rate': 7.663374367114318e-05, 'epoch': 3.227272727272727}\n{'loss': 0.2731, 'grad_norm': 1.0047838687896729, 'learning_rate': 7.61748589785016e-05, 'epoch': 3.25}\n{'loss': 0.2431, 'grad_norm': 1.1566907167434692, 'learning_rate': 7.571597428586003e-05, 'epoch': 3.2727272727272725}\n{'loss': 0.2233, 'grad_norm': 0.9322670698165894, 'learning_rate': 7.525708959321845e-05, 'epoch': 3.2954545454545454}\n{'loss': 0.2938, 'grad_norm': 0.784858226776123, 'learning_rate': 7.479820490057688e-05, 'epoch': 3.3181818181818183}\n{'loss': 0.2295, 'grad_norm': 0.8375238180160522, 'learning_rate': 7.43393202079353e-05, 'epoch': 3.340909090909091}\n{'loss': 0.2189, 'grad_norm': 1.2219465970993042, 'learning_rate': 7.388043551529373e-05, 'epoch': 3.3636363636363638}\n{'loss': 0.2097, 'grad_norm': 0.9706080555915833, 'learning_rate': 7.342155082265215e-05, 'epoch': 3.3863636363636362}\n{'loss': 0.2395, 'grad_norm': 1.1843082904815674, 'learning_rate': 7.296266613001058e-05, 'epoch': 3.409090909090909}\n{'loss': 0.2238, 'grad_norm': 1.1470067501068115, 'learning_rate': 7.2503781437369e-05, 'epoch': 3.4318181818181817}\n{'loss': 0.2583, 'grad_norm': 0.7652418613433838, 'learning_rate': 7.204489674472741e-05, 'epoch': 3.4545454545454546}\n{'loss': 0.2173, 'grad_norm': 1.2707812786102295, 'learning_rate': 7.158601205208584e-05, 'epoch': 3.4772727272727275}\n{'loss': 0.24, 'grad_norm': 1.3270940780639648, 'learning_rate': 7.112712735944426e-05, 'epoch': 3.5}\n{'loss': 0.2532, 'grad_norm': 1.0779812335968018, 'learning_rate': 7.066824266680269e-05, 'epoch': 3.5227272727272725}\n{'loss': 0.2325, 'grad_norm': 1.2251564264297485, 'learning_rate': 7.020935797416112e-05, 'epoch': 3.5454545454545454}\n{'loss': 0.2437, 'grad_norm': 0.8422737121582031, 'learning_rate': 6.975047328151954e-05, 'epoch': 3.5681818181818183}\n{'loss': 0.2736, 'grad_norm': 2.0696802139282227, 'learning_rate': 6.929158858887797e-05, 'epoch': 3.590909090909091}\n{'loss': 0.2631, 'grad_norm': 1.5410434007644653, 'learning_rate': 6.883270389623639e-05, 'epoch': 3.6136363636363638}\n{'loss': 0.2414, 'grad_norm': 1.9664160013198853, 'learning_rate': 6.837381920359482e-05, 'epoch': 3.6363636363636362}\n{'loss': 0.2409, 'grad_norm': 1.08390474319458, 'learning_rate': 6.791493451095324e-05, 'epoch': 3.659090909090909}\n{'loss': 0.2304, 'grad_norm': 0.826204776763916, 'learning_rate': 6.745604981831167e-05, 'epoch': 3.6818181818181817}\n{'loss': 0.2334, 'grad_norm': 1.0739350318908691, 'learning_rate': 6.699716512567009e-05, 'epoch': 3.7045454545454546}\n{'loss': 0.2518, 'grad_norm': 0.884426474571228, 'learning_rate': 6.653828043302852e-05, 'epoch': 3.7272727272727275}\n{'loss': 0.2958, 'grad_norm': 1.2142771482467651, 'learning_rate': 6.607939574038693e-05, 'epoch': 3.75}\n{'loss': 0.2353, 'grad_norm': 1.988505482673645, 'learning_rate': 6.562051104774535e-05, 'epoch': 3.7727272727272725}\n{'loss': 0.2705, 'grad_norm': 1.7043079137802124, 'learning_rate': 6.516162635510378e-05, 'epoch': 3.7954545454545454}\n{'loss': 0.2355, 'grad_norm': 1.1926852464675903, 'learning_rate': 6.47027416624622e-05, 'epoch': 3.8181818181818183}\n{'loss': 0.3069, 'grad_norm': 1.1280444860458374, 'learning_rate': 6.424385696982063e-05, 'epoch': 3.840909090909091}\n{'loss': 0.2818, 'grad_norm': 2.1455914974212646, 'learning_rate': 6.378497227717905e-05, 'epoch': 3.8636363636363638}\n{'loss': 0.19, 'grad_norm': 1.4404311180114746, 'learning_rate': 6.332608758453748e-05, 'epoch': 3.8863636363636362}\n{'loss': 0.2497, 'grad_norm': 1.3368875980377197, 'learning_rate': 6.28672028918959e-05, 'epoch': 3.909090909090909}\n{'loss': 0.2793, 'grad_norm': 1.1566786766052246, 'learning_rate': 6.240831819925433e-05, 'epoch': 3.9318181818181817}\n{'loss': 0.2569, 'grad_norm': 0.9848448634147644, 'learning_rate': 6.194943350661275e-05, 'epoch': 3.9545454545454546}\n{'loss': 0.237, 'grad_norm': 0.7410618662834167, 'learning_rate': 6.149054881397118e-05, 'epoch': 3.9772727272727275}\n{'loss': 0.0328, 'grad_norm': 1.062883973121643, 'learning_rate': 6.10316641213296e-05, 'epoch': 4.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8440    0.8659    0.8548      3311\n     PERCENT     0.8211    0.9070    0.8619        86\n        TYPE     0.9366    0.9627    0.9495     11299\n      VOLUME     0.8000    0.7619    0.7805        42\n\n   micro avg     0.9148    0.9400    0.9272     14738\n   macro avg     0.8504    0.8744    0.8617     14738\nweighted avg     0.9147    0.9400    0.9272     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.27133437991142273, 'eval_f1_macro': 0.8616563238301433, 'eval_precision': 0.9147573456586332, 'eval_recall': 0.9400189985072601, 'eval_f1': 0.9272161429575344, 'eval_accuracy': 0.9196765204087208, 'eval_runtime': 1.5402, 'eval_samples_per_second': 3577.465, 'eval_steps_per_second': 7.142, 'epoch': 4.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.2102, 'grad_norm': 0.9233428239822388, 'learning_rate': 6.057277942868802e-05, 'epoch': 4.0227272727272725}\n{'loss': 0.1607, 'grad_norm': 0.8882285952568054, 'learning_rate': 6.011389473604645e-05, 'epoch': 4.045454545454546}\n{'loss': 0.252, 'grad_norm': 0.7749335765838623, 'learning_rate': 5.965501004340487e-05, 'epoch': 4.068181818181818}\n{'loss': 0.209, 'grad_norm': 0.8044480681419373, 'learning_rate': 5.91961253507633e-05, 'epoch': 4.090909090909091}\n{'loss': 0.2445, 'grad_norm': 1.565788745880127, 'learning_rate': 5.8737240658121723e-05, 'epoch': 4.113636363636363}\n{'loss': 0.1938, 'grad_norm': 1.0450197458267212, 'learning_rate': 5.827835596548015e-05, 'epoch': 4.136363636363637}\n{'loss': 0.2069, 'grad_norm': 1.357498049736023, 'learning_rate': 5.781947127283857e-05, 'epoch': 4.159090909090909}\n{'loss': 0.2094, 'grad_norm': 0.7536768317222595, 'learning_rate': 5.7360586580196985e-05, 'epoch': 4.181818181818182}\n{'loss': 0.1645, 'grad_norm': 0.8144906759262085, 'learning_rate': 5.690170188755541e-05, 'epoch': 4.204545454545454}\n{'loss': 0.2456, 'grad_norm': 1.2410675287246704, 'learning_rate': 5.6442817194913836e-05, 'epoch': 4.2272727272727275}\n{'loss': 0.2171, 'grad_norm': 1.1061805486679077, 'learning_rate': 5.598393250227226e-05, 'epoch': 4.25}\n{'loss': 0.2436, 'grad_norm': 1.1392093896865845, 'learning_rate': 5.5525047809630686e-05, 'epoch': 4.2727272727272725}\n{'loss': 0.2398, 'grad_norm': 0.9699946641921997, 'learning_rate': 5.506616311698911e-05, 'epoch': 4.295454545454546}\n{'loss': 0.2339, 'grad_norm': 1.2448008060455322, 'learning_rate': 5.4607278424347536e-05, 'epoch': 4.318181818181818}\n{'loss': 0.2237, 'grad_norm': 0.8547831773757935, 'learning_rate': 5.414839373170596e-05, 'epoch': 4.340909090909091}\n{'loss': 0.1844, 'grad_norm': 1.0109986066818237, 'learning_rate': 5.3689509039064386e-05, 'epoch': 4.363636363636363}\n{'loss': 0.2166, 'grad_norm': 1.0337330102920532, 'learning_rate': 5.323062434642281e-05, 'epoch': 4.386363636363637}\n{'loss': 0.2593, 'grad_norm': 0.8631852865219116, 'learning_rate': 5.277173965378124e-05, 'epoch': 4.409090909090909}\n{'loss': 0.2049, 'grad_norm': 1.0043776035308838, 'learning_rate': 5.2312854961139655e-05, 'epoch': 4.431818181818182}\n{'loss': 0.2068, 'grad_norm': 1.3723798990249634, 'learning_rate': 5.185397026849808e-05, 'epoch': 4.454545454545454}\n{'loss': 0.2237, 'grad_norm': 0.7061530947685242, 'learning_rate': 5.13950855758565e-05, 'epoch': 4.4772727272727275}\n{'loss': 0.2511, 'grad_norm': 1.066794753074646, 'learning_rate': 5.0936200883214924e-05, 'epoch': 4.5}\n{'loss': 0.1873, 'grad_norm': 0.9146205186843872, 'learning_rate': 5.047731619057335e-05, 'epoch': 4.5227272727272725}\n{'loss': 0.2245, 'grad_norm': 1.5103511810302734, 'learning_rate': 5.0018431497931774e-05, 'epoch': 4.545454545454545}\n{'loss': 0.2061, 'grad_norm': 1.3138231039047241, 'learning_rate': 4.95595468052902e-05, 'epoch': 4.568181818181818}\n{'loss': 0.2055, 'grad_norm': 0.739811360836029, 'learning_rate': 4.9100662112648624e-05, 'epoch': 4.590909090909091}\n{'loss': 0.2269, 'grad_norm': 1.0418040752410889, 'learning_rate': 4.864177742000705e-05, 'epoch': 4.613636363636363}\n{'loss': 0.2238, 'grad_norm': 0.8857842683792114, 'learning_rate': 4.8182892727365475e-05, 'epoch': 4.636363636363637}\n{'loss': 0.2626, 'grad_norm': 1.1482932567596436, 'learning_rate': 4.77240080347239e-05, 'epoch': 4.659090909090909}\n{'loss': 0.183, 'grad_norm': 0.8348869681358337, 'learning_rate': 4.7265123342082325e-05, 'epoch': 4.681818181818182}\n{'loss': 0.2255, 'grad_norm': 1.3688545227050781, 'learning_rate': 4.680623864944074e-05, 'epoch': 4.704545454545455}\n{'loss': 0.1824, 'grad_norm': 1.043925166130066, 'learning_rate': 4.634735395679917e-05, 'epoch': 4.7272727272727275}\n{'loss': 0.2153, 'grad_norm': 1.1634801626205444, 'learning_rate': 4.5888469264157594e-05, 'epoch': 4.75}\n{'loss': 0.1953, 'grad_norm': 1.687029480934143, 'learning_rate': 4.542958457151602e-05, 'epoch': 4.7727272727272725}\n{'loss': 0.2154, 'grad_norm': 2.2253246307373047, 'learning_rate': 4.497069987887444e-05, 'epoch': 4.795454545454545}\n{'loss': 0.2275, 'grad_norm': 1.4526243209838867, 'learning_rate': 4.451181518623286e-05, 'epoch': 4.818181818181818}\n{'loss': 0.1509, 'grad_norm': 1.0003465414047241, 'learning_rate': 4.405293049359129e-05, 'epoch': 4.840909090909091}\n{'loss': 0.2003, 'grad_norm': 1.0855509042739868, 'learning_rate': 4.359404580094971e-05, 'epoch': 4.863636363636363}\n{'loss': 0.245, 'grad_norm': 1.1945551633834839, 'learning_rate': 4.313516110830814e-05, 'epoch': 4.886363636363637}\n{'loss': 0.2056, 'grad_norm': 1.7179944515228271, 'learning_rate': 4.267627641566656e-05, 'epoch': 4.909090909090909}\n{'loss': 0.1929, 'grad_norm': 2.34598970413208, 'learning_rate': 4.221739172302499e-05, 'epoch': 4.931818181818182}\n{'loss': 0.1962, 'grad_norm': 1.4188084602355957, 'learning_rate': 4.175850703038341e-05, 'epoch': 4.954545454545455}\n{'loss': 0.1694, 'grad_norm': 1.0860556364059448, 'learning_rate': 4.129962233774183e-05, 'epoch': 4.9772727272727275}\n{'loss': 0.1977, 'grad_norm': 7.301517963409424, 'learning_rate': 4.084073764510026e-05, 'epoch': 5.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8578    0.8674    0.8626      3311\n     PERCENT     0.8211    0.9070    0.8619        86\n        TYPE     0.9402    0.9638    0.9518     11299\n      VOLUME     0.8000    0.7619    0.7805        42\n\n   micro avg     0.9207    0.9412    0.9309     14738\n   macro avg     0.8548    0.8750    0.8642     14738\nweighted avg     0.9206    0.9412    0.9308     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.26288139820098877, 'eval_f1_macro': 0.8641995282078734, 'eval_precision': 0.9207487056949423, 'eval_recall': 0.9412403311168408, 'eval_f1': 0.9308817608374715, 'eval_accuracy': 0.9224086115512814, 'eval_runtime': 1.4533, 'eval_samples_per_second': 3791.414, 'eval_steps_per_second': 7.569, 'epoch': 5.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.1871, 'grad_norm': 0.9757246375083923, 'learning_rate': 4.038185295245868e-05, 'epoch': 5.0227272727272725}\n{'loss': 0.2104, 'grad_norm': 0.8550188541412354, 'learning_rate': 3.992296825981711e-05, 'epoch': 5.045454545454546}\n{'loss': 0.1788, 'grad_norm': 0.8510244488716125, 'learning_rate': 3.946408356717553e-05, 'epoch': 5.068181818181818}\n{'loss': 0.2249, 'grad_norm': 1.371591329574585, 'learning_rate': 3.900519887453395e-05, 'epoch': 5.090909090909091}\n{'loss': 0.2025, 'grad_norm': 0.8518195152282715, 'learning_rate': 3.8546314181892376e-05, 'epoch': 5.113636363636363}\n{'loss': 0.212, 'grad_norm': 1.8716858625411987, 'learning_rate': 3.80874294892508e-05, 'epoch': 5.136363636363637}\n{'loss': 0.1976, 'grad_norm': 1.1193442344665527, 'learning_rate': 3.7628544796609226e-05, 'epoch': 5.159090909090909}\n{'loss': 0.1634, 'grad_norm': 1.2643001079559326, 'learning_rate': 3.716966010396765e-05, 'epoch': 5.181818181818182}\n{'loss': 0.2243, 'grad_norm': 1.0211951732635498, 'learning_rate': 3.6710775411326076e-05, 'epoch': 5.204545454545454}\n{'loss': 0.1961, 'grad_norm': 0.8030474781990051, 'learning_rate': 3.62518907186845e-05, 'epoch': 5.2272727272727275}\n{'loss': 0.1586, 'grad_norm': 1.0382028818130493, 'learning_rate': 3.579300602604292e-05, 'epoch': 5.25}\n{'loss': 0.1985, 'grad_norm': 1.1518480777740479, 'learning_rate': 3.5334121333401345e-05, 'epoch': 5.2727272727272725}\n{'loss': 0.1913, 'grad_norm': 1.7855147123336792, 'learning_rate': 3.487523664075977e-05, 'epoch': 5.295454545454546}\n{'loss': 0.2208, 'grad_norm': 1.167686939239502, 'learning_rate': 3.4416351948118195e-05, 'epoch': 5.318181818181818}\n{'loss': 0.1691, 'grad_norm': 1.005113124847412, 'learning_rate': 3.395746725547662e-05, 'epoch': 5.340909090909091}\n{'loss': 0.151, 'grad_norm': 0.6938492059707642, 'learning_rate': 3.3498582562835046e-05, 'epoch': 5.363636363636363}\n{'loss': 0.1909, 'grad_norm': 1.2304009199142456, 'learning_rate': 3.3039697870193464e-05, 'epoch': 5.386363636363637}\n{'loss': 0.1539, 'grad_norm': 0.7591865062713623, 'learning_rate': 3.258081317755189e-05, 'epoch': 5.409090909090909}\n{'loss': 0.1542, 'grad_norm': 1.0706396102905273, 'learning_rate': 3.2121928484910314e-05, 'epoch': 5.431818181818182}\n{'loss': 0.2119, 'grad_norm': 0.9740626811981201, 'learning_rate': 3.166304379226874e-05, 'epoch': 5.454545454545454}\n{'loss': 0.2464, 'grad_norm': 1.5001028776168823, 'learning_rate': 3.1204159099627165e-05, 'epoch': 5.4772727272727275}\n{'loss': 0.2225, 'grad_norm': 1.7857612371444702, 'learning_rate': 3.074527440698559e-05, 'epoch': 5.5}\n{'loss': 0.1679, 'grad_norm': 1.092161774635315, 'learning_rate': 3.028638971434401e-05, 'epoch': 5.5227272727272725}\n{'loss': 0.2099, 'grad_norm': 1.1991603374481201, 'learning_rate': 2.9827505021702437e-05, 'epoch': 5.545454545454545}\n{'loss': 0.1924, 'grad_norm': 0.9860206842422485, 'learning_rate': 2.9368620329060862e-05, 'epoch': 5.568181818181818}\n{'loss': 0.1616, 'grad_norm': 1.2821978330612183, 'learning_rate': 2.8909735636419283e-05, 'epoch': 5.590909090909091}\n{'loss': 0.2506, 'grad_norm': 1.4596613645553589, 'learning_rate': 2.8450850943777705e-05, 'epoch': 5.613636363636363}\n{'loss': 0.1719, 'grad_norm': 0.8930200338363647, 'learning_rate': 2.799196625113613e-05, 'epoch': 5.636363636363637}\n{'loss': 0.2129, 'grad_norm': 1.1901777982711792, 'learning_rate': 2.7533081558494556e-05, 'epoch': 5.659090909090909}\n{'loss': 0.1785, 'grad_norm': 1.1952974796295166, 'learning_rate': 2.707419686585298e-05, 'epoch': 5.681818181818182}\n{'loss': 0.1671, 'grad_norm': 1.9882549047470093, 'learning_rate': 2.6615312173211406e-05, 'epoch': 5.704545454545455}\n{'loss': 0.1966, 'grad_norm': 0.9933282136917114, 'learning_rate': 2.6156427480569828e-05, 'epoch': 5.7272727272727275}\n{'loss': 0.1728, 'grad_norm': 0.848191499710083, 'learning_rate': 2.569754278792825e-05, 'epoch': 5.75}\n{'loss': 0.1731, 'grad_norm': 0.7979171276092529, 'learning_rate': 2.5238658095286674e-05, 'epoch': 5.7727272727272725}\n{'loss': 0.1649, 'grad_norm': 0.9425323605537415, 'learning_rate': 2.47797734026451e-05, 'epoch': 5.795454545454545}\n{'loss': 0.2054, 'grad_norm': 1.187717318534851, 'learning_rate': 2.4320888710003525e-05, 'epoch': 5.818181818181818}\n{'loss': 0.1968, 'grad_norm': 1.2747288942337036, 'learning_rate': 2.386200401736195e-05, 'epoch': 5.840909090909091}\n{'loss': 0.1649, 'grad_norm': 0.8057012557983398, 'learning_rate': 2.340311932472037e-05, 'epoch': 5.863636363636363}\n{'loss': 0.1326, 'grad_norm': 0.8972280025482178, 'learning_rate': 2.2944234632078797e-05, 'epoch': 5.886363636363637}\n{'loss': 0.1856, 'grad_norm': 1.4437757730484009, 'learning_rate': 2.248534993943722e-05, 'epoch': 5.909090909090909}\n{'loss': 0.1987, 'grad_norm': 0.9236131906509399, 'learning_rate': 2.2026465246795644e-05, 'epoch': 5.931818181818182}\n{'loss': 0.1732, 'grad_norm': 1.096975564956665, 'learning_rate': 2.156758055415407e-05, 'epoch': 5.954545454545455}\n{'loss': 0.1981, 'grad_norm': 0.8325340151786804, 'learning_rate': 2.1108695861512494e-05, 'epoch': 5.9772727272727275}\n{'loss': 0.2704, 'grad_norm': 3.002802610397339, 'learning_rate': 2.0649811168870916e-05, 'epoch': 6.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8696    0.8719    0.8708      3311\n     PERCENT     0.8298    0.9070    0.8667        86\n        TYPE     0.9390    0.9676    0.9531     11299\n      VOLUME     0.7750    0.7381    0.7561        42\n\n   micro avg     0.9226    0.9451    0.9337     14738\n   macro avg     0.8533    0.8712    0.8617     14738\nweighted avg     0.9223    0.9451    0.9335     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.26133066415786743, 'eval_f1_macro': 0.8616554763619745, 'eval_precision': 0.9226336358216864, 'eval_recall': 0.945107884380513, 'eval_f1': 0.9337355455002514, 'eval_accuracy': 0.9250314190481395, 'eval_runtime': 1.5025, 'eval_samples_per_second': 3667.276, 'eval_steps_per_second': 7.321, 'epoch': 6.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.2073, 'grad_norm': 1.1305888891220093, 'learning_rate': 2.019092647622934e-05, 'epoch': 6.0227272727272725}\n{'loss': 0.1497, 'grad_norm': 0.82735675573349, 'learning_rate': 1.9732041783587766e-05, 'epoch': 6.045454545454546}\n{'loss': 0.1762, 'grad_norm': 0.7841897010803223, 'learning_rate': 1.9273157090946188e-05, 'epoch': 6.068181818181818}\n{'loss': 0.1482, 'grad_norm': 0.7912801504135132, 'learning_rate': 1.8814272398304613e-05, 'epoch': 6.090909090909091}\n{'loss': 0.1715, 'grad_norm': 0.7800166010856628, 'learning_rate': 1.8355387705663038e-05, 'epoch': 6.113636363636363}\n{'loss': 0.1779, 'grad_norm': 1.398749828338623, 'learning_rate': 1.789650301302146e-05, 'epoch': 6.136363636363637}\n{'loss': 0.1784, 'grad_norm': 1.2869622707366943, 'learning_rate': 1.7437618320379885e-05, 'epoch': 6.159090909090909}\n{'loss': 0.1441, 'grad_norm': 1.4428164958953857, 'learning_rate': 1.697873362773831e-05, 'epoch': 6.181818181818182}\n{'loss': 0.1506, 'grad_norm': 1.681861162185669, 'learning_rate': 1.6519848935096732e-05, 'epoch': 6.204545454545454}\n{'loss': 0.1924, 'grad_norm': 1.2187737226486206, 'learning_rate': 1.6060964242455157e-05, 'epoch': 6.2272727272727275}\n{'loss': 0.1697, 'grad_norm': 1.7780858278274536, 'learning_rate': 1.5602079549813582e-05, 'epoch': 6.25}\n{'loss': 0.1467, 'grad_norm': 1.0295829772949219, 'learning_rate': 1.5143194857172006e-05, 'epoch': 6.2727272727272725}\n{'loss': 0.1828, 'grad_norm': 1.588790774345398, 'learning_rate': 1.4684310164530431e-05, 'epoch': 6.295454545454546}\n{'loss': 0.1771, 'grad_norm': 1.248122215270996, 'learning_rate': 1.4225425471888853e-05, 'epoch': 6.318181818181818}\n{'loss': 0.1664, 'grad_norm': 0.9159850478172302, 'learning_rate': 1.3766540779247278e-05, 'epoch': 6.340909090909091}\n{'loss': 0.1663, 'grad_norm': 0.9540239572525024, 'learning_rate': 1.3307656086605703e-05, 'epoch': 6.363636363636363}\n{'loss': 0.1921, 'grad_norm': 0.8175927996635437, 'learning_rate': 1.2848771393964125e-05, 'epoch': 6.386363636363637}\n{'loss': 0.1847, 'grad_norm': 0.8189874887466431, 'learning_rate': 1.238988670132255e-05, 'epoch': 6.409090909090909}\n{'loss': 0.205, 'grad_norm': 1.8584538698196411, 'learning_rate': 1.1931002008680975e-05, 'epoch': 6.431818181818182}\n{'loss': 0.1754, 'grad_norm': 0.957967221736908, 'learning_rate': 1.1472117316039398e-05, 'epoch': 6.454545454545454}\n{'loss': 0.2149, 'grad_norm': 1.030812382698059, 'learning_rate': 1.1013232623397822e-05, 'epoch': 6.4772727272727275}\n{'loss': 0.2012, 'grad_norm': 1.373032808303833, 'learning_rate': 1.0554347930756247e-05, 'epoch': 6.5}\n{'loss': 0.1636, 'grad_norm': 0.8526402711868286, 'learning_rate': 1.009546323811467e-05, 'epoch': 6.5227272727272725}\n{'loss': 0.173, 'grad_norm': 0.8453839421272278, 'learning_rate': 9.636578545473094e-06, 'epoch': 6.545454545454545}\n{'loss': 0.206, 'grad_norm': 0.8052152991294861, 'learning_rate': 9.177693852831519e-06, 'epoch': 6.568181818181818}\n{'loss': 0.1577, 'grad_norm': 0.8831124305725098, 'learning_rate': 8.718809160189943e-06, 'epoch': 6.590909090909091}\n{'loss': 0.1568, 'grad_norm': 0.8046630620956421, 'learning_rate': 8.259924467548366e-06, 'epoch': 6.613636363636363}\n{'loss': 0.1545, 'grad_norm': 1.112425684928894, 'learning_rate': 7.801039774906791e-06, 'epoch': 6.636363636363637}\n{'loss': 0.177, 'grad_norm': 0.849029004573822, 'learning_rate': 7.3421550822652154e-06, 'epoch': 6.659090909090909}\n{'loss': 0.1891, 'grad_norm': 0.9805278778076172, 'learning_rate': 6.883270389623639e-06, 'epoch': 6.681818181818182}\n{'loss': 0.1848, 'grad_norm': 1.3600677251815796, 'learning_rate': 6.424385696982062e-06, 'epoch': 6.704545454545455}\n{'loss': 0.1583, 'grad_norm': 1.2115899324417114, 'learning_rate': 5.9655010043404875e-06, 'epoch': 6.7272727272727275}\n{'loss': 0.2005, 'grad_norm': 1.1896629333496094, 'learning_rate': 5.506616311698911e-06, 'epoch': 6.75}\n{'loss': 0.187, 'grad_norm': 0.9411560893058777, 'learning_rate': 5.047731619057335e-06, 'epoch': 6.7727272727272725}\n{'loss': 0.1657, 'grad_norm': 0.8818305730819702, 'learning_rate': 4.5888469264157595e-06, 'epoch': 6.795454545454545}\n{'loss': 0.142, 'grad_norm': 1.0032188892364502, 'learning_rate': 4.129962233774183e-06, 'epoch': 6.818181818181818}\n{'loss': 0.1734, 'grad_norm': 0.7947171926498413, 'learning_rate': 3.6710775411326077e-06, 'epoch': 6.840909090909091}\n{'loss': 0.1794, 'grad_norm': 1.1347455978393555, 'learning_rate': 3.212192848491031e-06, 'epoch': 6.863636363636363}\n{'loss': 0.1396, 'grad_norm': 0.7957169413566589, 'learning_rate': 2.7533081558494555e-06, 'epoch': 6.886363636363637}\n{'loss': 0.2028, 'grad_norm': 0.8176308870315552, 'learning_rate': 2.2944234632078798e-06, 'epoch': 6.909090909090909}\n{'loss': 0.1795, 'grad_norm': 1.3870675563812256, 'learning_rate': 1.8355387705663039e-06, 'epoch': 6.931818181818182}\n{'loss': 0.1814, 'grad_norm': 1.1428182125091553, 'learning_rate': 1.3766540779247277e-06, 'epoch': 6.954545454545455}\n{'loss': 0.1976, 'grad_norm': 0.8685696125030518, 'learning_rate': 9.177693852831519e-07, 'epoch': 6.9772727272727275}\n{'loss': 0.2194, 'grad_norm': 2.6022021770477295, 'learning_rate': 4.5888469264157596e-07, 'epoch': 7.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8627    0.8768    0.8697      3311\n     PERCENT     0.8280    0.8953    0.8603        86\n        TYPE     0.9411    0.9648    0.9528     11299\n      VOLUME     0.7561    0.7381    0.7470        42\n\n   micro avg     0.9224    0.9440    0.9331     14738\n   macro avg     0.8470    0.8687    0.8575     14738\nweighted avg     0.9223    0.9440    0.9330     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.25961360335350037, 'eval_f1_macro': 0.8574517301176857, 'eval_precision': 0.9224240816867789, 'eval_recall': 0.9439544035825757, 'eval_f1': 0.933065057008719, 'eval_accuracy': 0.924648926288181, 'eval_runtime': 1.4358, 'eval_samples_per_second': 3837.513, 'eval_steps_per_second': 7.661, 'epoch': 7.0}\n{'train_runtime': 42.9953, 'train_samples_per_second': 3588.627, 'train_steps_per_second': 7.164, 'train_loss': 0.4125330168270058, 'epoch': 7.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nThe speedups for torchdynamo mostly come with GPU Ampere or higher and which is not detected here.\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8578    0.8674    0.8626      3311\n     PERCENT     0.8211    0.9070    0.8619        86\n        TYPE     0.9402    0.9638    0.9518     11299\n      VOLUME     0.8000    0.7619    0.7805        42\n\n   micro avg     0.9207    0.9412    0.9309     14738\n   macro avg     0.8548    0.8750    0.8642     14738\nweighted avg     0.9206    0.9412    0.9308     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.26288139820098877, 'eval_f1_macro': 0.8641995282078734, 'eval_precision': 0.9207487056949423, 'eval_recall': 0.9412403311168408, 'eval_f1': 0.9308817608374715, 'eval_accuracy': 0.9224086115512814, 'eval_runtime': 1.5052, 'eval_samples_per_second': 3660.749, 'eval_steps_per_second': 7.308, 'epoch': 7.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/3364797558.py:66: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 2.2565, 'grad_norm': 7.164239883422852, 'learning_rate': 0.0, 'epoch': 0.022727272727272728}\n{'loss': 2.2475, 'grad_norm': 7.511013507843018, 'learning_rate': 4.100356769732791e-06, 'epoch': 0.045454545454545456}\n{'loss': 2.2334, 'grad_norm': 7.131618499755859, 'learning_rate': 8.200713539465582e-06, 'epoch': 0.06818181818181818}\n{'loss': 2.2175, 'grad_norm': 7.69386100769043, 'learning_rate': 1.2301070309198374e-05, 'epoch': 0.09090909090909091}\n{'loss': 2.1839, 'grad_norm': 7.213454723358154, 'learning_rate': 1.6401427078931164e-05, 'epoch': 0.11363636363636363}\n{'loss': 2.1544, 'grad_norm': 6.853677272796631, 'learning_rate': 2.0501783848663958e-05, 'epoch': 0.13636363636363635}\n{'loss': 2.1139, 'grad_norm': 7.001660346984863, 'learning_rate': 2.4602140618396748e-05, 'epoch': 0.1590909090909091}\n{'loss': 2.061, 'grad_norm': 6.665991306304932, 'learning_rate': 2.8702497388129538e-05, 'epoch': 0.18181818181818182}\n{'loss': 2.0006, 'grad_norm': 6.564401626586914, 'learning_rate': 3.280285415786233e-05, 'epoch': 0.20454545454545456}\n{'loss': 1.9042, 'grad_norm': 6.615825653076172, 'learning_rate': 3.690321092759512e-05, 'epoch': 0.22727272727272727}\n{'loss': 1.8407, 'grad_norm': 6.493690013885498, 'learning_rate': 4.1003567697327915e-05, 'epoch': 0.25}\n{'loss': 1.7785, 'grad_norm': 5.945095539093018, 'learning_rate': 4.510392446706071e-05, 'epoch': 0.2727272727272727}\n{'loss': 1.73, 'grad_norm': 5.34287166595459, 'learning_rate': 4.9204281236793496e-05, 'epoch': 0.29545454545454547}\n{'loss': 1.6398, 'grad_norm': 4.940625190734863, 'learning_rate': 5.330463800652629e-05, 'epoch': 0.3181818181818182}\n{'loss': 1.5683, 'grad_norm': 4.5823893547058105, 'learning_rate': 5.7404994776259076e-05, 'epoch': 0.3409090909090909}\n{'loss': 1.4764, 'grad_norm': 4.3892621994018555, 'learning_rate': 6.150535154599188e-05, 'epoch': 0.36363636363636365}\n{'loss': 1.392, 'grad_norm': 3.7976791858673096, 'learning_rate': 6.560570831572466e-05, 'epoch': 0.38636363636363635}\n{'loss': 1.3032, 'grad_norm': 3.3995349407196045, 'learning_rate': 6.970606508545745e-05, 'epoch': 0.4090909090909091}\n{'loss': 1.259, 'grad_norm': 2.670966386795044, 'learning_rate': 7.380642185519024e-05, 'epoch': 0.4318181818181818}\n{'loss': 1.2424, 'grad_norm': 2.150805711746216, 'learning_rate': 7.790677862492304e-05, 'epoch': 0.45454545454545453}\n{'loss': 1.1858, 'grad_norm': 1.9960098266601562, 'learning_rate': 8.200713539465583e-05, 'epoch': 0.4772727272727273}\n{'loss': 1.1112, 'grad_norm': 1.6969050168991089, 'learning_rate': 8.610749216438861e-05, 'epoch': 0.5}\n{'loss': 1.1103, 'grad_norm': 1.837999701499939, 'learning_rate': 9.020784893412142e-05, 'epoch': 0.5227272727272727}\n{'loss': 1.0916, 'grad_norm': 2.08520770072937, 'learning_rate': 9.43082057038542e-05, 'epoch': 0.5454545454545454}\n{'loss': 1.1065, 'grad_norm': 2.058367967605591, 'learning_rate': 9.840856247358699e-05, 'epoch': 0.5681818181818182}\n{'loss': 0.985, 'grad_norm': 1.6854890584945679, 'learning_rate': 0.00010250891924331977, 'epoch': 0.5909090909090909}\n{'loss': 1.1025, 'grad_norm': 1.6371477842330933, 'learning_rate': 0.00010660927601305258, 'epoch': 0.6136363636363636}\n{'loss': 0.9898, 'grad_norm': 1.2813963890075684, 'learning_rate': 0.00011070963278278537, 'epoch': 0.6363636363636364}\n{'loss': 1.0035, 'grad_norm': 1.4893522262573242, 'learning_rate': 0.00011480998955251815, 'epoch': 0.6590909090909091}\n{'loss': 1.0005, 'grad_norm': 1.618364691734314, 'learning_rate': 0.00011891034632225095, 'epoch': 0.6818181818181818}\n{'loss': 0.9067, 'grad_norm': 2.1643710136413574, 'learning_rate': 0.00012301070309198375, 'epoch': 0.7045454545454546}\n{'loss': 0.9045, 'grad_norm': 2.3137619495391846, 'learning_rate': 0.00012711105986171653, 'epoch': 0.7272727272727273}\n{'loss': 0.8477, 'grad_norm': 1.5098527669906616, 'learning_rate': 0.00012665217516907496, 'epoch': 0.75}\n{'loss': 0.8401, 'grad_norm': 1.3586794137954712, 'learning_rate': 0.00012619329047643338, 'epoch': 0.7727272727272727}\n{'loss': 0.7938, 'grad_norm': 1.3764874935150146, 'learning_rate': 0.0001257344057837918, 'epoch': 0.7954545454545454}\n{'loss': 0.779, 'grad_norm': 1.171402096748352, 'learning_rate': 0.00012527552109115023, 'epoch': 0.8181818181818182}\n{'loss': 0.7557, 'grad_norm': 0.9864138960838318, 'learning_rate': 0.00012481663639850866, 'epoch': 0.8409090909090909}\n{'loss': 0.6856, 'grad_norm': 0.8736937046051025, 'learning_rate': 0.00012435775170586708, 'epoch': 0.8636363636363636}\n{'loss': 0.6646, 'grad_norm': 0.9612028002738953, 'learning_rate': 0.0001238988670132255, 'epoch': 0.8863636363636364}\n{'loss': 0.6048, 'grad_norm': 0.7780366539955139, 'learning_rate': 0.00012343998232058393, 'epoch': 0.9090909090909091}\n{'loss': 0.6348, 'grad_norm': 1.0747501850128174, 'learning_rate': 0.00012298109762794236, 'epoch': 0.9318181818181818}\n{'loss': 0.6211, 'grad_norm': 1.0524146556854248, 'learning_rate': 0.00012252221293530078, 'epoch': 0.9545454545454546}\n{'loss': 0.6187, 'grad_norm': 0.8192756175994873, 'learning_rate': 0.0001220633282426592, 'epoch': 0.9772727272727273}\n{'loss': 0.5768, 'grad_norm': 3.3002970218658447, 'learning_rate': 0.00012160444355001763, 'epoch': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.7377    0.6120    0.6690      3456\n     PERCENT     0.9615    0.3247    0.4854        77\n        TYPE     0.8401    0.9045    0.8711     11282\n      VOLUME     0.0000    0.0000    0.0000        41\n\n   micro avg     0.8208    0.8310    0.8259     14856\n   macro avg     0.6348    0.4603    0.5064     14856\nweighted avg     0.8146    0.8310    0.8197     14856\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.5810263156890869, 'eval_f1_macro': 0.5063914337405973, 'eval_precision': 0.820811170212766, 'eval_recall': 0.8309773828756059, 'eval_f1': 0.8258629917045759, 'eval_accuracy': 0.8274586597040905, 'eval_runtime': 1.4392, 'eval_samples_per_second': 3828.404, 'eval_steps_per_second': 7.643, 'epoch': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.5715, 'grad_norm': 1.1148438453674316, 'learning_rate': 0.00012114555885737605, 'epoch': 1.0227272727272727}\n{'loss': 0.6139, 'grad_norm': 0.9471710920333862, 'learning_rate': 0.00012068667416473446, 'epoch': 1.0454545454545454}\n{'loss': 0.5832, 'grad_norm': 0.9598967432975769, 'learning_rate': 0.0001202277894720929, 'epoch': 1.0681818181818181}\n{'loss': 0.5496, 'grad_norm': 1.9662500619888306, 'learning_rate': 0.00011976890477945131, 'epoch': 1.0909090909090908}\n{'loss': 0.5359, 'grad_norm': 0.8557508587837219, 'learning_rate': 0.00011931002008680975, 'epoch': 1.1136363636363635}\n{'loss': 0.5419, 'grad_norm': 0.8560353517532349, 'learning_rate': 0.00011885113539416816, 'epoch': 1.1363636363636362}\n{'loss': 0.56, 'grad_norm': 1.0516318082809448, 'learning_rate': 0.0001183922507015266, 'epoch': 1.1590909090909092}\n{'loss': 0.4405, 'grad_norm': 0.7264191508293152, 'learning_rate': 0.00011793336600888501, 'epoch': 1.1818181818181819}\n{'loss': 0.5285, 'grad_norm': 0.7724646925926208, 'learning_rate': 0.00011747448131624345, 'epoch': 1.2045454545454546}\n{'loss': 0.4666, 'grad_norm': 0.9544641375541687, 'learning_rate': 0.00011701559662360186, 'epoch': 1.2272727272727273}\n{'loss': 0.5396, 'grad_norm': 0.7937765121459961, 'learning_rate': 0.0001165567119309603, 'epoch': 1.25}\n{'loss': 0.4318, 'grad_norm': 0.803612232208252, 'learning_rate': 0.00011609782723831871, 'epoch': 1.2727272727272727}\n{'loss': 0.5399, 'grad_norm': 0.9360643625259399, 'learning_rate': 0.00011563894254567713, 'epoch': 1.2954545454545454}\n{'loss': 0.5191, 'grad_norm': 0.7446858286857605, 'learning_rate': 0.00011518005785303556, 'epoch': 1.3181818181818181}\n{'loss': 0.456, 'grad_norm': 0.9555624723434448, 'learning_rate': 0.00011472117316039397, 'epoch': 1.3409090909090908}\n{'loss': 0.458, 'grad_norm': 1.2197264432907104, 'learning_rate': 0.00011426228846775241, 'epoch': 1.3636363636363638}\n{'loss': 0.4502, 'grad_norm': 0.9880602359771729, 'learning_rate': 0.00011380340377511082, 'epoch': 1.3863636363636362}\n{'loss': 0.491, 'grad_norm': 0.7565778493881226, 'learning_rate': 0.00011334451908246926, 'epoch': 1.4090909090909092}\n{'loss': 0.4689, 'grad_norm': 0.8791390061378479, 'learning_rate': 0.00011288563438982767, 'epoch': 1.4318181818181819}\n{'loss': 0.4044, 'grad_norm': 0.821406900882721, 'learning_rate': 0.00011242674969718611, 'epoch': 1.4545454545454546}\n{'loss': 0.4071, 'grad_norm': 1.4851282835006714, 'learning_rate': 0.00011196786500454452, 'epoch': 1.4772727272727273}\n{'loss': 0.4646, 'grad_norm': 0.9625024795532227, 'learning_rate': 0.00011150898031190296, 'epoch': 1.5}\n{'loss': 0.4587, 'grad_norm': 0.8716332912445068, 'learning_rate': 0.00011105009561926137, 'epoch': 1.5227272727272727}\n{'loss': 0.4093, 'grad_norm': 0.8881617784500122, 'learning_rate': 0.00011059121092661981, 'epoch': 1.5454545454545454}\n{'loss': 0.464, 'grad_norm': 0.9638216495513916, 'learning_rate': 0.00011013232623397822, 'epoch': 1.5681818181818183}\n{'loss': 0.486, 'grad_norm': 1.290756106376648, 'learning_rate': 0.00010967344154133665, 'epoch': 1.5909090909090908}\n{'loss': 0.4568, 'grad_norm': 1.6501126289367676, 'learning_rate': 0.00010921455684869507, 'epoch': 1.6136363636363638}\n{'loss': 0.4873, 'grad_norm': 1.7342325448989868, 'learning_rate': 0.00010875567215605348, 'epoch': 1.6363636363636362}\n{'loss': 0.3446, 'grad_norm': 0.9378679990768433, 'learning_rate': 0.00010829678746341192, 'epoch': 1.6590909090909092}\n{'loss': 0.4112, 'grad_norm': 1.4988044500350952, 'learning_rate': 0.00010783790277077033, 'epoch': 1.6818181818181817}\n{'loss': 0.4171, 'grad_norm': 1.0027745962142944, 'learning_rate': 0.00010737901807812877, 'epoch': 1.7045454545454546}\n{'loss': 0.4356, 'grad_norm': 0.9805078506469727, 'learning_rate': 0.00010692013338548718, 'epoch': 1.7272727272727273}\n{'loss': 0.4569, 'grad_norm': 0.7532325983047485, 'learning_rate': 0.00010646124869284562, 'epoch': 1.75}\n{'loss': 0.3597, 'grad_norm': 0.8010135889053345, 'learning_rate': 0.00010600236400020403, 'epoch': 1.7727272727272727}\n{'loss': 0.3697, 'grad_norm': 0.9239104986190796, 'learning_rate': 0.00010554347930756247, 'epoch': 1.7954545454545454}\n{'loss': 0.3961, 'grad_norm': 0.7971876859664917, 'learning_rate': 0.00010508459461492089, 'epoch': 1.8181818181818183}\n{'loss': 0.4349, 'grad_norm': 0.8256801962852478, 'learning_rate': 0.00010462570992227931, 'epoch': 1.8409090909090908}\n{'loss': 0.3809, 'grad_norm': 0.9570580720901489, 'learning_rate': 0.00010416682522963774, 'epoch': 1.8636363636363638}\n{'loss': 0.3896, 'grad_norm': 0.7673600316047668, 'learning_rate': 0.00010370794053699616, 'epoch': 1.8863636363636362}\n{'loss': 0.3934, 'grad_norm': 0.8068567514419556, 'learning_rate': 0.00010324905584435459, 'epoch': 1.9090909090909092}\n{'loss': 0.4106, 'grad_norm': 0.843640923500061, 'learning_rate': 0.000102790171151713, 'epoch': 1.9318181818181817}\n{'loss': 0.3564, 'grad_norm': 0.7144103050231934, 'learning_rate': 0.00010233128645907144, 'epoch': 1.9545454545454546}\n{'loss': 0.3916, 'grad_norm': 0.8292807936668396, 'learning_rate': 0.00010187240176642985, 'epoch': 1.9772727272727273}\n{'loss': 0.4184, 'grad_norm': 3.3406593799591064, 'learning_rate': 0.00010141351707378829, 'epoch': 2.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8342    0.7543    0.7923      3456\n     PERCENT     0.6727    0.9610    0.7914        77\n        TYPE     0.9020    0.9596    0.9299     11282\n      VOLUME     0.3030    0.2439    0.2703        41\n\n   micro avg     0.8852    0.9099    0.8974     14856\n   macro avg     0.6780    0.7297    0.6960     14856\nweighted avg     0.8834    0.9099    0.8954     14856\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.35720089077949524, 'eval_f1_macro': 0.6959759698194015, 'eval_precision': 0.8851997380484611, 'eval_recall': 0.9098680667743673, 'eval_f1': 0.8973644028413994, 'eval_accuracy': 0.8947454308093995, 'eval_runtime': 1.4366, 'eval_samples_per_second': 3835.401, 'eval_steps_per_second': 7.657, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.334, 'grad_norm': 0.6672646999359131, 'learning_rate': 0.0001009546323811467, 'epoch': 2.022727272727273}\n{'loss': 0.3436, 'grad_norm': 0.8725665807723999, 'learning_rate': 0.00010049574768850514, 'epoch': 2.0454545454545454}\n{'loss': 0.3699, 'grad_norm': 0.8083518743515015, 'learning_rate': 0.00010003686299586355, 'epoch': 2.0681818181818183}\n{'loss': 0.3428, 'grad_norm': 0.7045484185218811, 'learning_rate': 9.957797830322199e-05, 'epoch': 2.090909090909091}\n{'loss': 0.307, 'grad_norm': 0.8847514390945435, 'learning_rate': 9.91190936105804e-05, 'epoch': 2.1136363636363638}\n{'loss': 0.3423, 'grad_norm': 0.8145873546600342, 'learning_rate': 9.866020891793882e-05, 'epoch': 2.1363636363636362}\n{'loss': 0.273, 'grad_norm': 1.0673586130142212, 'learning_rate': 9.820132422529725e-05, 'epoch': 2.159090909090909}\n{'loss': 0.285, 'grad_norm': 0.749280571937561, 'learning_rate': 9.774243953265567e-05, 'epoch': 2.1818181818181817}\n{'loss': 0.3357, 'grad_norm': 0.8648505806922913, 'learning_rate': 9.72835548400141e-05, 'epoch': 2.2045454545454546}\n{'loss': 0.2798, 'grad_norm': 0.7498027682304382, 'learning_rate': 9.682467014737251e-05, 'epoch': 2.227272727272727}\n{'loss': 0.3892, 'grad_norm': 1.401950716972351, 'learning_rate': 9.636578545473095e-05, 'epoch': 2.25}\n{'loss': 0.2748, 'grad_norm': 0.745432436466217, 'learning_rate': 9.590690076208936e-05, 'epoch': 2.2727272727272725}\n{'loss': 0.2894, 'grad_norm': 0.8052772879600525, 'learning_rate': 9.54480160694478e-05, 'epoch': 2.2954545454545454}\n{'loss': 0.3283, 'grad_norm': 0.6868861317634583, 'learning_rate': 9.498913137680621e-05, 'epoch': 2.3181818181818183}\n{'loss': 0.327, 'grad_norm': 0.8459412455558777, 'learning_rate': 9.453024668416465e-05, 'epoch': 2.340909090909091}\n{'loss': 0.3163, 'grad_norm': 1.0018548965454102, 'learning_rate': 9.407136199152306e-05, 'epoch': 2.3636363636363638}\n{'loss': 0.276, 'grad_norm': 0.7319700121879578, 'learning_rate': 9.361247729888149e-05, 'epoch': 2.3863636363636362}\n{'loss': 0.2765, 'grad_norm': 1.5526560544967651, 'learning_rate': 9.315359260623991e-05, 'epoch': 2.409090909090909}\n{'loss': 0.3189, 'grad_norm': 0.9400184750556946, 'learning_rate': 9.269470791359834e-05, 'epoch': 2.4318181818181817}\n{'loss': 0.2766, 'grad_norm': 0.9485817551612854, 'learning_rate': 9.223582322095676e-05, 'epoch': 2.4545454545454546}\n{'loss': 0.3709, 'grad_norm': 0.969002902507782, 'learning_rate': 9.177693852831519e-05, 'epoch': 2.4772727272727275}\n{'loss': 0.2801, 'grad_norm': 0.6816707253456116, 'learning_rate': 9.131805383567361e-05, 'epoch': 2.5}\n{'loss': 0.2631, 'grad_norm': 0.8907052874565125, 'learning_rate': 9.085916914303204e-05, 'epoch': 2.5227272727272725}\n{'loss': 0.3652, 'grad_norm': 1.51020085811615, 'learning_rate': 9.040028445039046e-05, 'epoch': 2.5454545454545454}\n{'loss': 0.2997, 'grad_norm': 0.8084184527397156, 'learning_rate': 8.994139975774887e-05, 'epoch': 2.5681818181818183}\n{'loss': 0.3347, 'grad_norm': 0.8532792329788208, 'learning_rate': 8.948251506510731e-05, 'epoch': 2.590909090909091}\n{'loss': 0.2789, 'grad_norm': 1.1585943698883057, 'learning_rate': 8.902363037246572e-05, 'epoch': 2.6136363636363638}\n{'loss': 0.3492, 'grad_norm': 1.019490361213684, 'learning_rate': 8.856474567982416e-05, 'epoch': 2.6363636363636362}\n{'loss': 0.3153, 'grad_norm': 0.9730309844017029, 'learning_rate': 8.810586098718257e-05, 'epoch': 2.659090909090909}\n{'loss': 0.3046, 'grad_norm': 0.8799071311950684, 'learning_rate': 8.7646976294541e-05, 'epoch': 2.6818181818181817}\n{'loss': 0.3036, 'grad_norm': 1.3640820980072021, 'learning_rate': 8.718809160189943e-05, 'epoch': 2.7045454545454546}\n{'loss': 0.2736, 'grad_norm': 0.9825440049171448, 'learning_rate': 8.672920690925785e-05, 'epoch': 2.7272727272727275}\n{'loss': 0.2982, 'grad_norm': 0.8774937391281128, 'learning_rate': 8.627032221661628e-05, 'epoch': 2.75}\n{'loss': 0.3888, 'grad_norm': 1.053182601928711, 'learning_rate': 8.58114375239747e-05, 'epoch': 2.7727272727272725}\n{'loss': 0.2879, 'grad_norm': 0.8611935973167419, 'learning_rate': 8.535255283133313e-05, 'epoch': 2.7954545454545454}\n{'loss': 0.32, 'grad_norm': 0.9982030987739563, 'learning_rate': 8.489366813869155e-05, 'epoch': 2.8181818181818183}\n{'loss': 0.335, 'grad_norm': 1.6035739183425903, 'learning_rate': 8.443478344604998e-05, 'epoch': 2.840909090909091}\n{'loss': 0.344, 'grad_norm': 1.1633734703063965, 'learning_rate': 8.397589875340839e-05, 'epoch': 2.8636363636363638}\n{'loss': 0.3001, 'grad_norm': 1.0866949558258057, 'learning_rate': 8.351701406076683e-05, 'epoch': 2.8863636363636362}\n{'loss': 0.2906, 'grad_norm': 0.8427598476409912, 'learning_rate': 8.305812936812524e-05, 'epoch': 2.909090909090909}\n{'loss': 0.3058, 'grad_norm': 1.1749141216278076, 'learning_rate': 8.259924467548366e-05, 'epoch': 2.9318181818181817}\n{'loss': 0.3561, 'grad_norm': 0.9632834792137146, 'learning_rate': 8.214035998284209e-05, 'epoch': 2.9545454545454546}\n{'loss': 0.3008, 'grad_norm': 0.7988190054893494, 'learning_rate': 8.168147529020051e-05, 'epoch': 2.9772727272727275}\n{'loss': 0.2008, 'grad_norm': 2.904228448867798, 'learning_rate': 8.122259059755894e-05, 'epoch': 3.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8428    0.8380    0.8404      3456\n     PERCENT     0.8132    0.9610    0.8810        77\n        TYPE     0.9274    0.9633    0.9450     11282\n      VOLUME     0.8462    0.8049    0.8250        41\n\n   micro avg     0.9075    0.9337    0.9204     14856\n   macro avg     0.8574    0.8918    0.8728     14856\nweighted avg     0.9069    0.9337    0.9200     14856\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.2953597903251648, 'eval_f1_macro': 0.8728373581571695, 'eval_precision': 0.9074910042525351, 'eval_recall': 0.9336968228325255, 'eval_f1': 0.9204074184665406, 'eval_accuracy': 0.9147628372497825, 'eval_runtime': 1.4823, 'eval_samples_per_second': 3717.173, 'eval_steps_per_second': 7.421, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.2789, 'grad_norm': 0.7988533973693848, 'learning_rate': 8.076370590491736e-05, 'epoch': 3.022727272727273}\n{'loss': 0.2224, 'grad_norm': 1.0044447183609009, 'learning_rate': 8.030482121227579e-05, 'epoch': 3.0454545454545454}\n{'loss': 0.2749, 'grad_norm': 0.987493634223938, 'learning_rate': 7.984593651963421e-05, 'epoch': 3.0681818181818183}\n{'loss': 0.3152, 'grad_norm': 0.9776114225387573, 'learning_rate': 7.938705182699264e-05, 'epoch': 3.090909090909091}\n{'loss': 0.2359, 'grad_norm': 1.537093997001648, 'learning_rate': 7.892816713435106e-05, 'epoch': 3.1136363636363638}\n{'loss': 0.311, 'grad_norm': 1.1131030321121216, 'learning_rate': 7.846928244170949e-05, 'epoch': 3.1363636363636362}\n{'loss': 0.2172, 'grad_norm': 0.8497730493545532, 'learning_rate': 7.80103977490679e-05, 'epoch': 3.159090909090909}\n{'loss': 0.2851, 'grad_norm': 0.8534199595451355, 'learning_rate': 7.755151305642634e-05, 'epoch': 3.1818181818181817}\n{'loss': 0.219, 'grad_norm': 0.9790048003196716, 'learning_rate': 7.709262836378475e-05, 'epoch': 3.2045454545454546}\n{'loss': 0.2159, 'grad_norm': 0.7658417820930481, 'learning_rate': 7.663374367114318e-05, 'epoch': 3.227272727272727}\n{'loss': 0.3076, 'grad_norm': 1.469308614730835, 'learning_rate': 7.61748589785016e-05, 'epoch': 3.25}\n{'loss': 0.2409, 'grad_norm': 1.3932665586471558, 'learning_rate': 7.571597428586003e-05, 'epoch': 3.2727272727272725}\n{'loss': 0.2918, 'grad_norm': 0.8757953643798828, 'learning_rate': 7.525708959321845e-05, 'epoch': 3.2954545454545454}\n{'loss': 0.2544, 'grad_norm': 0.8509025573730469, 'learning_rate': 7.479820490057688e-05, 'epoch': 3.3181818181818183}\n{'loss': 0.183, 'grad_norm': 1.1839450597763062, 'learning_rate': 7.43393202079353e-05, 'epoch': 3.340909090909091}\n{'loss': 0.2626, 'grad_norm': 0.9731354713439941, 'learning_rate': 7.388043551529373e-05, 'epoch': 3.3636363636363638}\n{'loss': 0.2389, 'grad_norm': 1.7961524724960327, 'learning_rate': 7.342155082265215e-05, 'epoch': 3.3863636363636362}\n{'loss': 0.2765, 'grad_norm': 1.4949359893798828, 'learning_rate': 7.296266613001058e-05, 'epoch': 3.409090909090909}\n{'loss': 0.2013, 'grad_norm': 1.194690465927124, 'learning_rate': 7.2503781437369e-05, 'epoch': 3.4318181818181817}\n{'loss': 0.2364, 'grad_norm': 0.7750551104545593, 'learning_rate': 7.204489674472741e-05, 'epoch': 3.4545454545454546}\n{'loss': 0.2366, 'grad_norm': 1.1381945610046387, 'learning_rate': 7.158601205208584e-05, 'epoch': 3.4772727272727275}\n{'loss': 0.2157, 'grad_norm': 1.1299083232879639, 'learning_rate': 7.112712735944426e-05, 'epoch': 3.5}\n{'loss': 0.2534, 'grad_norm': 1.0492969751358032, 'learning_rate': 7.066824266680269e-05, 'epoch': 3.5227272727272725}\n{'loss': 0.2808, 'grad_norm': 1.3536258935928345, 'learning_rate': 7.020935797416112e-05, 'epoch': 3.5454545454545454}\n{'loss': 0.2855, 'grad_norm': 1.1150130033493042, 'learning_rate': 6.975047328151954e-05, 'epoch': 3.5681818181818183}\n{'loss': 0.2595, 'grad_norm': 1.718399167060852, 'learning_rate': 6.929158858887797e-05, 'epoch': 3.590909090909091}\n{'loss': 0.259, 'grad_norm': 1.1491681337356567, 'learning_rate': 6.883270389623639e-05, 'epoch': 3.6136363636363638}\n{'loss': 0.2202, 'grad_norm': 0.9552371501922607, 'learning_rate': 6.837381920359482e-05, 'epoch': 3.6363636363636362}\n{'loss': 0.2612, 'grad_norm': 1.1406153440475464, 'learning_rate': 6.791493451095324e-05, 'epoch': 3.659090909090909}\n{'loss': 0.2431, 'grad_norm': 0.8938426971435547, 'learning_rate': 6.745604981831167e-05, 'epoch': 3.6818181818181817}\n{'loss': 0.228, 'grad_norm': 1.238021969795227, 'learning_rate': 6.699716512567009e-05, 'epoch': 3.7045454545454546}\n{'loss': 0.2473, 'grad_norm': 1.157893180847168, 'learning_rate': 6.653828043302852e-05, 'epoch': 3.7272727272727275}\n{'loss': 0.2018, 'grad_norm': 0.7044116258621216, 'learning_rate': 6.607939574038693e-05, 'epoch': 3.75}\n{'loss': 0.2479, 'grad_norm': 1.7578002214431763, 'learning_rate': 6.562051104774535e-05, 'epoch': 3.7727272727272725}\n{'loss': 0.2775, 'grad_norm': 1.2578043937683105, 'learning_rate': 6.516162635510378e-05, 'epoch': 3.7954545454545454}\n{'loss': 0.2502, 'grad_norm': 1.3866307735443115, 'learning_rate': 6.47027416624622e-05, 'epoch': 3.8181818181818183}\n{'loss': 0.241, 'grad_norm': 1.05925452709198, 'learning_rate': 6.424385696982063e-05, 'epoch': 3.840909090909091}\n{'loss': 0.1923, 'grad_norm': 0.634819746017456, 'learning_rate': 6.378497227717905e-05, 'epoch': 3.8636363636363638}\n{'loss': 0.2281, 'grad_norm': 1.7120051383972168, 'learning_rate': 6.332608758453748e-05, 'epoch': 3.8863636363636362}\n{'loss': 0.2938, 'grad_norm': 1.043116569519043, 'learning_rate': 6.28672028918959e-05, 'epoch': 3.909090909090909}\n{'loss': 0.2966, 'grad_norm': 0.8699020743370056, 'learning_rate': 6.240831819925433e-05, 'epoch': 3.9318181818181817}\n{'loss': 0.2854, 'grad_norm': 1.1074624061584473, 'learning_rate': 6.194943350661275e-05, 'epoch': 3.9545454545454546}\n{'loss': 0.2048, 'grad_norm': 0.6472676396369934, 'learning_rate': 6.149054881397118e-05, 'epoch': 3.9772727272727275}\n{'loss': 0.4003, 'grad_norm': 7.858445167541504, 'learning_rate': 6.10316641213296e-05, 'epoch': 4.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8478    0.8594    0.8536      3456\n     PERCENT     0.8315    0.9610    0.8916        77\n        TYPE     0.9328    0.9629    0.9476     11282\n      VOLUME     0.8462    0.8049    0.8250        41\n\n   micro avg     0.9125    0.9383    0.9253     14856\n   macro avg     0.8646    0.8970    0.8794     14856\nweighted avg     0.9123    0.9383    0.9251     14856\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.275774747133255, 'eval_f1_macro': 0.8794383819898703, 'eval_precision': 0.9125425504058654, 'eval_recall': 0.9383414108777598, 'eval_f1': 0.9252621797424665, 'eval_accuracy': 0.9202567449956484, 'eval_runtime': 1.5436, 'eval_samples_per_second': 3569.532, 'eval_steps_per_second': 7.126, 'epoch': 4.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.2167, 'grad_norm': 0.7812066674232483, 'learning_rate': 6.057277942868802e-05, 'epoch': 4.0227272727272725}\n{'loss': 0.2175, 'grad_norm': 0.8998268246650696, 'learning_rate': 6.011389473604645e-05, 'epoch': 4.045454545454546}\n{'loss': 0.2334, 'grad_norm': 0.8283409476280212, 'learning_rate': 5.965501004340487e-05, 'epoch': 4.068181818181818}\n{'loss': 0.2195, 'grad_norm': 0.8620960712432861, 'learning_rate': 5.91961253507633e-05, 'epoch': 4.090909090909091}\n{'loss': 0.2176, 'grad_norm': 0.8524383902549744, 'learning_rate': 5.8737240658121723e-05, 'epoch': 4.113636363636363}\n{'loss': 0.2041, 'grad_norm': 1.1401032209396362, 'learning_rate': 5.827835596548015e-05, 'epoch': 4.136363636363637}\n{'loss': 0.2547, 'grad_norm': 1.3989790678024292, 'learning_rate': 5.781947127283857e-05, 'epoch': 4.159090909090909}\n{'loss': 0.2025, 'grad_norm': 0.8214173316955566, 'learning_rate': 5.7360586580196985e-05, 'epoch': 4.181818181818182}\n{'loss': 0.2572, 'grad_norm': 0.8566178679466248, 'learning_rate': 5.690170188755541e-05, 'epoch': 4.204545454545454}\n{'loss': 0.2462, 'grad_norm': 1.0982760190963745, 'learning_rate': 5.6442817194913836e-05, 'epoch': 4.2272727272727275}\n{'loss': 0.2388, 'grad_norm': 1.3527498245239258, 'learning_rate': 5.598393250227226e-05, 'epoch': 4.25}\n{'loss': 0.1777, 'grad_norm': 0.8224505186080933, 'learning_rate': 5.5525047809630686e-05, 'epoch': 4.2727272727272725}\n{'loss': 0.2189, 'grad_norm': 1.6358540058135986, 'learning_rate': 5.506616311698911e-05, 'epoch': 4.295454545454546}\n{'loss': 0.2022, 'grad_norm': 0.8416111469268799, 'learning_rate': 5.4607278424347536e-05, 'epoch': 4.318181818181818}\n{'loss': 0.1802, 'grad_norm': 0.8622047901153564, 'learning_rate': 5.414839373170596e-05, 'epoch': 4.340909090909091}\n{'loss': 0.2256, 'grad_norm': 1.4332454204559326, 'learning_rate': 5.3689509039064386e-05, 'epoch': 4.363636363636363}\n{'loss': 0.1949, 'grad_norm': 0.815883457660675, 'learning_rate': 5.323062434642281e-05, 'epoch': 4.386363636363637}\n{'loss': 0.188, 'grad_norm': 1.5577075481414795, 'learning_rate': 5.277173965378124e-05, 'epoch': 4.409090909090909}\n{'loss': 0.2002, 'grad_norm': 1.072386622428894, 'learning_rate': 5.2312854961139655e-05, 'epoch': 4.431818181818182}\n{'loss': 0.1971, 'grad_norm': 1.758360743522644, 'learning_rate': 5.185397026849808e-05, 'epoch': 4.454545454545454}\n{'loss': 0.1861, 'grad_norm': 1.4775785207748413, 'learning_rate': 5.13950855758565e-05, 'epoch': 4.4772727272727275}\n{'loss': 0.2245, 'grad_norm': 1.333595871925354, 'learning_rate': 5.0936200883214924e-05, 'epoch': 4.5}\n{'loss': 0.2283, 'grad_norm': 1.769163727760315, 'learning_rate': 5.047731619057335e-05, 'epoch': 4.5227272727272725}\n{'loss': 0.2156, 'grad_norm': 1.0716294050216675, 'learning_rate': 5.0018431497931774e-05, 'epoch': 4.545454545454545}\n{'loss': 0.222, 'grad_norm': 2.006258010864258, 'learning_rate': 4.95595468052902e-05, 'epoch': 4.568181818181818}\n{'loss': 0.1977, 'grad_norm': 1.067252516746521, 'learning_rate': 4.9100662112648624e-05, 'epoch': 4.590909090909091}\n{'loss': 0.2072, 'grad_norm': 1.4279369115829468, 'learning_rate': 4.864177742000705e-05, 'epoch': 4.613636363636363}\n{'loss': 0.2477, 'grad_norm': 1.4787994623184204, 'learning_rate': 4.8182892727365475e-05, 'epoch': 4.636363636363637}\n{'loss': 0.2146, 'grad_norm': 1.3920283317565918, 'learning_rate': 4.77240080347239e-05, 'epoch': 4.659090909090909}\n{'loss': 0.2074, 'grad_norm': 0.8129209876060486, 'learning_rate': 4.7265123342082325e-05, 'epoch': 4.681818181818182}\n{'loss': 0.2462, 'grad_norm': 1.0040932893753052, 'learning_rate': 4.680623864944074e-05, 'epoch': 4.704545454545455}\n{'loss': 0.2008, 'grad_norm': 1.1173466444015503, 'learning_rate': 4.634735395679917e-05, 'epoch': 4.7272727272727275}\n{'loss': 0.1899, 'grad_norm': 0.8861849308013916, 'learning_rate': 4.5888469264157594e-05, 'epoch': 4.75}\n{'loss': 0.2035, 'grad_norm': 2.5291526317596436, 'learning_rate': 4.542958457151602e-05, 'epoch': 4.7727272727272725}\n{'loss': 0.2802, 'grad_norm': 1.0124326944351196, 'learning_rate': 4.497069987887444e-05, 'epoch': 4.795454545454545}\n{'loss': 0.1605, 'grad_norm': 1.2819983959197998, 'learning_rate': 4.451181518623286e-05, 'epoch': 4.818181818181818}\n{'loss': 0.1899, 'grad_norm': 1.0195726156234741, 'learning_rate': 4.405293049359129e-05, 'epoch': 4.840909090909091}\n{'loss': 0.1906, 'grad_norm': 0.725531816482544, 'learning_rate': 4.359404580094971e-05, 'epoch': 4.863636363636363}\n{'loss': 0.2116, 'grad_norm': 1.5945430994033813, 'learning_rate': 4.313516110830814e-05, 'epoch': 4.886363636363637}\n{'loss': 0.2238, 'grad_norm': 1.29276704788208, 'learning_rate': 4.267627641566656e-05, 'epoch': 4.909090909090909}\n{'loss': 0.2211, 'grad_norm': 1.1831960678100586, 'learning_rate': 4.221739172302499e-05, 'epoch': 4.931818181818182}\n{'loss': 0.1984, 'grad_norm': 0.8965880870819092, 'learning_rate': 4.175850703038341e-05, 'epoch': 4.954545454545455}\n{'loss': 0.1888, 'grad_norm': 1.080494999885559, 'learning_rate': 4.129962233774183e-05, 'epoch': 4.9772727272727275}\n{'loss': 0.3925, 'grad_norm': 7.22615385055542, 'learning_rate': 4.084073764510026e-05, 'epoch': 5.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8641    0.8608    0.8624      3456\n     PERCENT     0.8391    0.9481    0.8902        77\n        TYPE     0.9365    0.9651    0.9506     11282\n      VOLUME     0.7857    0.8049    0.7952        41\n\n   micro avg     0.9191    0.9403    0.9296     14856\n   macro avg     0.8563    0.8947    0.8746     14856\nweighted avg     0.9187    0.9403    0.9293     14856\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.26927950978279114, 'eval_f1_macro': 0.8746133515649213, 'eval_precision': 0.9191340965916568, 'eval_recall': 0.9402934841141626, 'eval_f1': 0.929593398549278, 'eval_accuracy': 0.9241187989556136, 'eval_runtime': 1.4464, 'eval_samples_per_second': 3809.4, 'eval_steps_per_second': 7.605, 'epoch': 5.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.14, 'grad_norm': 0.5856692790985107, 'learning_rate': 4.038185295245868e-05, 'epoch': 5.0227272727272725}\n{'loss': 0.2187, 'grad_norm': 1.0851457118988037, 'learning_rate': 3.992296825981711e-05, 'epoch': 5.045454545454546}\n{'loss': 0.176, 'grad_norm': 0.9370812773704529, 'learning_rate': 3.946408356717553e-05, 'epoch': 5.068181818181818}\n{'loss': 0.2041, 'grad_norm': 1.0069321393966675, 'learning_rate': 3.900519887453395e-05, 'epoch': 5.090909090909091}\n{'loss': 0.1928, 'grad_norm': 1.4001494646072388, 'learning_rate': 3.8546314181892376e-05, 'epoch': 5.113636363636363}\n{'loss': 0.1741, 'grad_norm': 1.4505316019058228, 'learning_rate': 3.80874294892508e-05, 'epoch': 5.136363636363637}\n{'loss': 0.1973, 'grad_norm': 0.7976064682006836, 'learning_rate': 3.7628544796609226e-05, 'epoch': 5.159090909090909}\n{'loss': 0.1948, 'grad_norm': 1.5453100204467773, 'learning_rate': 3.716966010396765e-05, 'epoch': 5.181818181818182}\n{'loss': 0.1776, 'grad_norm': 1.0906232595443726, 'learning_rate': 3.6710775411326076e-05, 'epoch': 5.204545454545454}\n{'loss': 0.2088, 'grad_norm': 1.3083020448684692, 'learning_rate': 3.62518907186845e-05, 'epoch': 5.2272727272727275}\n{'loss': 0.1537, 'grad_norm': 1.1283296346664429, 'learning_rate': 3.579300602604292e-05, 'epoch': 5.25}\n{'loss': 0.183, 'grad_norm': 0.8197370767593384, 'learning_rate': 3.5334121333401345e-05, 'epoch': 5.2727272727272725}\n{'loss': 0.2256, 'grad_norm': 1.0166118144989014, 'learning_rate': 3.487523664075977e-05, 'epoch': 5.295454545454546}\n{'loss': 0.1623, 'grad_norm': 1.2434438467025757, 'learning_rate': 3.4416351948118195e-05, 'epoch': 5.318181818181818}\n{'loss': 0.2027, 'grad_norm': 0.9130126237869263, 'learning_rate': 3.395746725547662e-05, 'epoch': 5.340909090909091}\n{'loss': 0.202, 'grad_norm': 1.277622103691101, 'learning_rate': 3.3498582562835046e-05, 'epoch': 5.363636363636363}\n{'loss': 0.1877, 'grad_norm': 0.7615070343017578, 'learning_rate': 3.3039697870193464e-05, 'epoch': 5.386363636363637}\n{'loss': 0.1973, 'grad_norm': 1.172488808631897, 'learning_rate': 3.258081317755189e-05, 'epoch': 5.409090909090909}\n{'loss': 0.1682, 'grad_norm': 0.6883675456047058, 'learning_rate': 3.2121928484910314e-05, 'epoch': 5.431818181818182}\n{'loss': 0.2258, 'grad_norm': 1.5603666305541992, 'learning_rate': 3.166304379226874e-05, 'epoch': 5.454545454545454}\n{'loss': 0.1807, 'grad_norm': 0.7357986569404602, 'learning_rate': 3.1204159099627165e-05, 'epoch': 5.4772727272727275}\n{'loss': 0.163, 'grad_norm': 1.2422977685928345, 'learning_rate': 3.074527440698559e-05, 'epoch': 5.5}\n{'loss': 0.196, 'grad_norm': 1.4175251722335815, 'learning_rate': 3.028638971434401e-05, 'epoch': 5.5227272727272725}\n{'loss': 0.2413, 'grad_norm': 1.190057635307312, 'learning_rate': 2.9827505021702437e-05, 'epoch': 5.545454545454545}\n{'loss': 0.2067, 'grad_norm': 1.5691838264465332, 'learning_rate': 2.9368620329060862e-05, 'epoch': 5.568181818181818}\n{'loss': 0.1719, 'grad_norm': 1.1010249853134155, 'learning_rate': 2.8909735636419283e-05, 'epoch': 5.590909090909091}\n{'loss': 0.2014, 'grad_norm': 0.9662072658538818, 'learning_rate': 2.8450850943777705e-05, 'epoch': 5.613636363636363}\n{'loss': 0.1724, 'grad_norm': 1.5644947290420532, 'learning_rate': 2.799196625113613e-05, 'epoch': 5.636363636363637}\n{'loss': 0.2274, 'grad_norm': 1.1948105096817017, 'learning_rate': 2.7533081558494556e-05, 'epoch': 5.659090909090909}\n{'loss': 0.19, 'grad_norm': 0.8478941917419434, 'learning_rate': 2.707419686585298e-05, 'epoch': 5.681818181818182}\n{'loss': 0.175, 'grad_norm': 1.0315910577774048, 'learning_rate': 2.6615312173211406e-05, 'epoch': 5.704545454545455}\n{'loss': 0.2032, 'grad_norm': 1.1827728748321533, 'learning_rate': 2.6156427480569828e-05, 'epoch': 5.7272727272727275}\n{'loss': 0.2054, 'grad_norm': 1.1116349697113037, 'learning_rate': 2.569754278792825e-05, 'epoch': 5.75}\n{'loss': 0.1771, 'grad_norm': 0.8441083431243896, 'learning_rate': 2.5238658095286674e-05, 'epoch': 5.7727272727272725}\n{'loss': 0.212, 'grad_norm': 1.262425184249878, 'learning_rate': 2.47797734026451e-05, 'epoch': 5.795454545454545}\n{'loss': 0.2104, 'grad_norm': 1.00762939453125, 'learning_rate': 2.4320888710003525e-05, 'epoch': 5.818181818181818}\n{'loss': 0.1892, 'grad_norm': 1.1602697372436523, 'learning_rate': 2.386200401736195e-05, 'epoch': 5.840909090909091}\n{'loss': 0.1865, 'grad_norm': 1.1465296745300293, 'learning_rate': 2.340311932472037e-05, 'epoch': 5.863636363636363}\n{'loss': 0.1633, 'grad_norm': 0.8186178803443909, 'learning_rate': 2.2944234632078797e-05, 'epoch': 5.886363636363637}\n{'loss': 0.1953, 'grad_norm': 0.8213074803352356, 'learning_rate': 2.248534993943722e-05, 'epoch': 5.909090909090909}\n{'loss': 0.178, 'grad_norm': 1.5513125658035278, 'learning_rate': 2.2026465246795644e-05, 'epoch': 5.931818181818182}\n{'loss': 0.2189, 'grad_norm': 1.3938144445419312, 'learning_rate': 2.156758055415407e-05, 'epoch': 5.954545454545455}\n{'loss': 0.1836, 'grad_norm': 1.253282904624939, 'learning_rate': 2.1108695861512494e-05, 'epoch': 5.9772727272727275}\n{'loss': 0.1398, 'grad_norm': 1.809430480003357, 'learning_rate': 2.0649811168870916e-05, 'epoch': 6.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8511    0.8796    0.8651      3456\n     PERCENT     0.8391    0.9481    0.8902        77\n        TYPE     0.9405    0.9611    0.9507     11282\n      VOLUME     0.7907    0.8293    0.8095        41\n\n   micro avg     0.9185    0.9417    0.9300     14856\n   macro avg     0.8553    0.9045    0.8789     14856\nweighted avg     0.9188    0.9417    0.9301     14856\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.2635519206523895, 'eval_f1_macro': 0.8788900963136779, 'eval_precision': 0.9185214365438907, 'eval_recall': 0.9417070543887991, 'eval_f1': 0.9299697543789676, 'eval_accuracy': 0.9252067014795474, 'eval_runtime': 1.4907, 'eval_samples_per_second': 3696.239, 'eval_steps_per_second': 7.379, 'epoch': 6.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.1262, 'grad_norm': 1.1402965784072876, 'learning_rate': 2.019092647622934e-05, 'epoch': 6.0227272727272725}\n{'loss': 0.1705, 'grad_norm': 0.8325167894363403, 'learning_rate': 1.9732041783587766e-05, 'epoch': 6.045454545454546}\n{'loss': 0.1586, 'grad_norm': 0.7489922642707825, 'learning_rate': 1.9273157090946188e-05, 'epoch': 6.068181818181818}\n{'loss': 0.1902, 'grad_norm': 0.8460971713066101, 'learning_rate': 1.8814272398304613e-05, 'epoch': 6.090909090909091}\n{'loss': 0.172, 'grad_norm': 0.7842855453491211, 'learning_rate': 1.8355387705663038e-05, 'epoch': 6.113636363636363}\n{'loss': 0.1437, 'grad_norm': 0.9711630344390869, 'learning_rate': 1.789650301302146e-05, 'epoch': 6.136363636363637}\n{'loss': 0.1611, 'grad_norm': 0.8316162824630737, 'learning_rate': 1.7437618320379885e-05, 'epoch': 6.159090909090909}\n{'loss': 0.2098, 'grad_norm': 1.44268000125885, 'learning_rate': 1.697873362773831e-05, 'epoch': 6.181818181818182}\n{'loss': 0.2423, 'grad_norm': 1.3266240358352661, 'learning_rate': 1.6519848935096732e-05, 'epoch': 6.204545454545454}\n{'loss': 0.1622, 'grad_norm': 1.2361814975738525, 'learning_rate': 1.6060964242455157e-05, 'epoch': 6.2272727272727275}\n{'loss': 0.1999, 'grad_norm': 0.9548004865646362, 'learning_rate': 1.5602079549813582e-05, 'epoch': 6.25}\n{'loss': 0.2075, 'grad_norm': 1.5650840997695923, 'learning_rate': 1.5143194857172006e-05, 'epoch': 6.2727272727272725}\n{'loss': 0.1534, 'grad_norm': 0.7234406471252441, 'learning_rate': 1.4684310164530431e-05, 'epoch': 6.295454545454546}\n{'loss': 0.1803, 'grad_norm': 0.8519050478935242, 'learning_rate': 1.4225425471888853e-05, 'epoch': 6.318181818181818}\n{'loss': 0.1893, 'grad_norm': 0.6613771915435791, 'learning_rate': 1.3766540779247278e-05, 'epoch': 6.340909090909091}\n{'loss': 0.204, 'grad_norm': 0.8187761306762695, 'learning_rate': 1.3307656086605703e-05, 'epoch': 6.363636363636363}\n{'loss': 0.1302, 'grad_norm': 1.0456273555755615, 'learning_rate': 1.2848771393964125e-05, 'epoch': 6.386363636363637}\n{'loss': 0.1878, 'grad_norm': 0.8646640181541443, 'learning_rate': 1.238988670132255e-05, 'epoch': 6.409090909090909}\n{'loss': 0.1864, 'grad_norm': 1.1005948781967163, 'learning_rate': 1.1931002008680975e-05, 'epoch': 6.431818181818182}\n{'loss': 0.141, 'grad_norm': 0.7866102457046509, 'learning_rate': 1.1472117316039398e-05, 'epoch': 6.454545454545454}\n{'loss': 0.1742, 'grad_norm': 0.7955217361450195, 'learning_rate': 1.1013232623397822e-05, 'epoch': 6.4772727272727275}\n{'loss': 0.1677, 'grad_norm': 1.4183064699172974, 'learning_rate': 1.0554347930756247e-05, 'epoch': 6.5}\n{'loss': 0.1475, 'grad_norm': 0.9414851069450378, 'learning_rate': 1.009546323811467e-05, 'epoch': 6.5227272727272725}\n{'loss': 0.2206, 'grad_norm': 0.7883038520812988, 'learning_rate': 9.636578545473094e-06, 'epoch': 6.545454545454545}\n{'loss': 0.1812, 'grad_norm': 1.290234923362732, 'learning_rate': 9.177693852831519e-06, 'epoch': 6.568181818181818}\n{'loss': 0.168, 'grad_norm': 0.9797707200050354, 'learning_rate': 8.718809160189943e-06, 'epoch': 6.590909090909091}\n{'loss': 0.1609, 'grad_norm': 1.1300629377365112, 'learning_rate': 8.259924467548366e-06, 'epoch': 6.613636363636363}\n{'loss': 0.1566, 'grad_norm': 0.8246753215789795, 'learning_rate': 7.801039774906791e-06, 'epoch': 6.636363636363637}\n{'loss': 0.1937, 'grad_norm': 1.0487011671066284, 'learning_rate': 7.3421550822652154e-06, 'epoch': 6.659090909090909}\n{'loss': 0.2028, 'grad_norm': 0.9998772740364075, 'learning_rate': 6.883270389623639e-06, 'epoch': 6.681818181818182}\n{'loss': 0.2013, 'grad_norm': 1.0534147024154663, 'learning_rate': 6.424385696982062e-06, 'epoch': 6.704545454545455}\n{'loss': 0.1768, 'grad_norm': 0.9607346653938293, 'learning_rate': 5.9655010043404875e-06, 'epoch': 6.7272727272727275}\n{'loss': 0.1463, 'grad_norm': 0.6088905334472656, 'learning_rate': 5.506616311698911e-06, 'epoch': 6.75}\n{'loss': 0.1725, 'grad_norm': 0.8994831442832947, 'learning_rate': 5.047731619057335e-06, 'epoch': 6.7727272727272725}\n{'loss': 0.2187, 'grad_norm': 1.2327247858047485, 'learning_rate': 4.5888469264157595e-06, 'epoch': 6.795454545454545}\n{'loss': 0.1911, 'grad_norm': 0.8069788217544556, 'learning_rate': 4.129962233774183e-06, 'epoch': 6.818181818181818}\n{'loss': 0.189, 'grad_norm': 0.9797911643981934, 'learning_rate': 3.6710775411326077e-06, 'epoch': 6.840909090909091}\n{'loss': 0.1952, 'grad_norm': 1.0789730548858643, 'learning_rate': 3.212192848491031e-06, 'epoch': 6.863636363636363}\n{'loss': 0.1627, 'grad_norm': 0.7990397214889526, 'learning_rate': 2.7533081558494555e-06, 'epoch': 6.886363636363637}\n{'loss': 0.1701, 'grad_norm': 1.070415735244751, 'learning_rate': 2.2944234632078798e-06, 'epoch': 6.909090909090909}\n{'loss': 0.2059, 'grad_norm': 1.0140960216522217, 'learning_rate': 1.8355387705663039e-06, 'epoch': 6.931818181818182}\n{'loss': 0.2063, 'grad_norm': 1.5126723051071167, 'learning_rate': 1.3766540779247277e-06, 'epoch': 6.954545454545455}\n{'loss': 0.1515, 'grad_norm': 0.9892438054084778, 'learning_rate': 9.177693852831519e-07, 'epoch': 6.9772727272727275}\n{'loss': 0.0589, 'grad_norm': 2.9924614429473877, 'learning_rate': 4.5888469264157596e-07, 'epoch': 7.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8702    0.8727    0.8714      3456\n     PERCENT     0.8391    0.9481    0.8902        77\n        TYPE     0.9396    0.9644    0.9518     11282\n      VOLUME     0.7907    0.8293    0.8095        41\n\n   micro avg     0.9227    0.9426    0.9325     14856\n   macro avg     0.8599    0.9036    0.8807     14856\nweighted avg     0.9225    0.9426    0.9324     14856\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.26154813170433044, 'eval_f1_macro': 0.880747474686205, 'eval_precision': 0.922706905640485, 'eval_recall': 0.9425821217016693, 'eval_f1': 0.9325386254661694, 'eval_accuracy': 0.9267841601392516, 'eval_runtime': 1.4479, 'eval_samples_per_second': 3805.62, 'eval_steps_per_second': 7.597, 'epoch': 7.0}\n{'train_runtime': 43.0766, 'train_samples_per_second': 3581.849, 'train_steps_per_second': 7.15, 'train_loss': 0.41644532262504874, 'epoch': 7.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n[I 2025-09-26 18:12:00,987] Trial 5 pruned. \n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8702    0.8727    0.8714      3456\n     PERCENT     0.8391    0.9481    0.8902        77\n        TYPE     0.9396    0.9644    0.9518     11282\n      VOLUME     0.7907    0.8293    0.8095        41\n\n   micro avg     0.9227    0.9426    0.9325     14856\n   macro avg     0.8599    0.9036    0.8807     14856\nweighted avg     0.9225    0.9426    0.9324     14856\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.26154813170433044, 'eval_f1_macro': 0.880747474686205, 'eval_precision': 0.922706905640485, 'eval_recall': 0.9425821217016693, 'eval_f1': 0.9325386254661694, 'eval_accuracy': 0.9267841601392516, 'eval_runtime': 1.5108, 'eval_samples_per_second': 3647.183, 'eval_steps_per_second': 7.281, 'epoch': 7.0}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/27552 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34be690e4d6747ee8633ecd2ddc00bcd"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nThe speedups for torchdynamo mostly come with GPU Ampere or higher and which is not detected here.\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/tmp/ipykernel_36/3364797558.py:66: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 2.3657, 'grad_norm': 7.645390510559082, 'learning_rate': 0.0, 'epoch': 0.022727272727272728}\n{'loss': 2.3712, 'grad_norm': 7.825218200683594, 'learning_rate': 5.851377717708392e-06, 'epoch': 0.045454545454545456}\n{'loss': 2.3409, 'grad_norm': 7.6815361976623535, 'learning_rate': 1.1702755435416784e-05, 'epoch': 0.06818181818181818}\n{'loss': 2.3317, 'grad_norm': 7.916087627410889, 'learning_rate': 1.755413315312518e-05, 'epoch': 0.09090909090909091}\n{'loss': 2.2662, 'grad_norm': 7.483981609344482, 'learning_rate': 2.340551087083357e-05, 'epoch': 0.11363636363636363}\n{'loss': 2.2165, 'grad_norm': 7.488437652587891, 'learning_rate': 2.9256888588541966e-05, 'epoch': 0.13636363636363635}\n{'loss': 2.1461, 'grad_norm': 7.118978977203369, 'learning_rate': 3.510826630625036e-05, 'epoch': 0.1590909090909091}\n{'loss': 2.037, 'grad_norm': 7.2049689292907715, 'learning_rate': 4.095964402395875e-05, 'epoch': 0.18181818181818182}\n{'loss': 1.9912, 'grad_norm': 6.7496232986450195, 'learning_rate': 4.681102174166714e-05, 'epoch': 0.20454545454545456}\n{'loss': 1.8778, 'grad_norm': 6.604808807373047, 'learning_rate': 5.2662399459375535e-05, 'epoch': 0.22727272727272727}\n{'loss': 1.7245, 'grad_norm': 6.366133689880371, 'learning_rate': 5.851377717708393e-05, 'epoch': 0.25}\n{'loss': 1.6274, 'grad_norm': 5.838237285614014, 'learning_rate': 6.436515489479233e-05, 'epoch': 0.2727272727272727}\n{'loss': 1.5152, 'grad_norm': 5.076303958892822, 'learning_rate': 7.021653261250071e-05, 'epoch': 0.29545454545454547}\n{'loss': 1.4322, 'grad_norm': 4.220292091369629, 'learning_rate': 7.606791033020911e-05, 'epoch': 0.3181818181818182}\n{'loss': 1.3514, 'grad_norm': 3.259899139404297, 'learning_rate': 8.19192880479175e-05, 'epoch': 0.3409090909090909}\n{'loss': 1.2988, 'grad_norm': 2.3852033615112305, 'learning_rate': 8.777066576562589e-05, 'epoch': 0.36363636363636365}\n{'loss': 1.2361, 'grad_norm': 2.008878469467163, 'learning_rate': 9.362204348333428e-05, 'epoch': 0.38636363636363635}\n{'loss': 1.1711, 'grad_norm': 1.6651026010513306, 'learning_rate': 9.947342120104267e-05, 'epoch': 0.4090909090909091}\n{'loss': 1.1695, 'grad_norm': 2.1088974475860596, 'learning_rate': 0.00010532479891875107, 'epoch': 0.4318181818181818}\n{'loss': 1.1057, 'grad_norm': 1.9122040271759033, 'learning_rate': 0.00010465818626736656, 'epoch': 0.45454545454545453}\n{'loss': 1.1286, 'grad_norm': 1.8788706064224243, 'learning_rate': 0.00010399157361598208, 'epoch': 0.4772727272727273}\n{'loss': 1.0537, 'grad_norm': 1.5934600830078125, 'learning_rate': 0.00010332496096459757, 'epoch': 0.5}\n{'loss': 1.081, 'grad_norm': 1.4169281721115112, 'learning_rate': 0.00010265834831321306, 'epoch': 0.5227272727272727}\n{'loss': 1.0159, 'grad_norm': 1.3742201328277588, 'learning_rate': 0.00010199173566182858, 'epoch': 0.5454545454545454}\n{'loss': 1.0459, 'grad_norm': 1.6279385089874268, 'learning_rate': 0.00010132512301044407, 'epoch': 0.5681818181818182}\n{'loss': 0.9829, 'grad_norm': 1.43763267993927, 'learning_rate': 0.00010065851035905956, 'epoch': 0.5909090909090909}\n{'loss': 0.9459, 'grad_norm': 1.7774522304534912, 'learning_rate': 9.999189770767507e-05, 'epoch': 0.6136363636363636}\n{'loss': 0.9188, 'grad_norm': 1.5407171249389648, 'learning_rate': 9.932528505629057e-05, 'epoch': 0.6363636363636364}\n{'loss': 0.9094, 'grad_norm': 1.2352283000946045, 'learning_rate': 9.865867240490606e-05, 'epoch': 0.6590909090909091}\n{'loss': 0.8489, 'grad_norm': 1.0621639490127563, 'learning_rate': 9.799205975352157e-05, 'epoch': 0.6818181818181818}\n{'loss': 0.8228, 'grad_norm': 0.9532398581504822, 'learning_rate': 9.732544710213707e-05, 'epoch': 0.7045454545454546}\n{'loss': 0.7897, 'grad_norm': 0.923295259475708, 'learning_rate': 9.665883445075256e-05, 'epoch': 0.7272727272727273}\n{'loss': 0.8299, 'grad_norm': 1.433128833770752, 'learning_rate': 9.599222179936806e-05, 'epoch': 0.75}\n{'loss': 0.8485, 'grad_norm': 1.3865623474121094, 'learning_rate': 9.532560914798357e-05, 'epoch': 0.7727272727272727}\n{'loss': 0.7478, 'grad_norm': 0.9305353164672852, 'learning_rate': 9.465899649659906e-05, 'epoch': 0.7954545454545454}\n{'loss': 0.723, 'grad_norm': 0.9930396676063538, 'learning_rate': 9.399238384521456e-05, 'epoch': 0.8181818181818182}\n{'loss': 0.7383, 'grad_norm': 1.1211178302764893, 'learning_rate': 9.332577119383006e-05, 'epoch': 0.8409090909090909}\n{'loss': 0.6986, 'grad_norm': 1.0770193338394165, 'learning_rate': 9.265915854244556e-05, 'epoch': 0.8636363636363636}\n{'loss': 0.6853, 'grad_norm': 0.9346197247505188, 'learning_rate': 9.199254589106106e-05, 'epoch': 0.8863636363636364}\n{'loss': 0.6889, 'grad_norm': 1.0140502452850342, 'learning_rate': 9.132593323967656e-05, 'epoch': 0.9090909090909091}\n{'loss': 0.7374, 'grad_norm': 1.270734429359436, 'learning_rate': 9.065932058829207e-05, 'epoch': 0.9318181818181818}\n{'loss': 0.6457, 'grad_norm': 0.924951434135437, 'learning_rate': 8.999270793690756e-05, 'epoch': 0.9545454545454546}\n{'loss': 0.6453, 'grad_norm': 0.8638846278190613, 'learning_rate': 8.932609528552306e-05, 'epoch': 0.9772727272727273}\n{'loss': 0.6303, 'grad_norm': 1.9294188022613525, 'learning_rate': 8.865948263413857e-05, 'epoch': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.7072    0.6088    0.6544      3142\n     PERCENT     0.6667    0.2121    0.3218        66\n        TYPE     0.8181    0.9007    0.8574     11415\n      VOLUME     0.0000    0.0000    0.0000        70\n\n   micro avg     0.7983    0.8309    0.8142     14693\n   macro avg     0.5480    0.4304    0.4584     14693\nweighted avg     0.7898    0.8309    0.8075     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.5907840728759766, 'eval_f1_macro': 0.45839619618171334, 'eval_precision': 0.7982737200026155, 'eval_recall': 0.8308718437351119, 'eval_f1': 0.8142466484359367, 'eval_accuracy': 0.8195124605506584, 'eval_runtime': 1.4351, 'eval_samples_per_second': 3840.071, 'eval_steps_per_second': 7.665, 'epoch': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.5711, 'grad_norm': 0.9777377247810364, 'learning_rate': 8.799286998275406e-05, 'epoch': 1.0227272727272727}\n{'loss': 0.5935, 'grad_norm': 0.7500412464141846, 'learning_rate': 8.732625733136956e-05, 'epoch': 1.0454545454545454}\n{'loss': 0.6025, 'grad_norm': 0.6327736973762512, 'learning_rate': 8.665964467998507e-05, 'epoch': 1.0681818181818181}\n{'loss': 0.5488, 'grad_norm': 0.8140043616294861, 'learning_rate': 8.599303202860056e-05, 'epoch': 1.0909090909090908}\n{'loss': 0.5324, 'grad_norm': 0.6982956528663635, 'learning_rate': 8.532641937721606e-05, 'epoch': 1.1136363636363635}\n{'loss': 0.5861, 'grad_norm': 0.7959696054458618, 'learning_rate': 8.465980672583155e-05, 'epoch': 1.1363636363636362}\n{'loss': 0.5658, 'grad_norm': 1.1338497400283813, 'learning_rate': 8.399319407444706e-05, 'epoch': 1.1590909090909092}\n{'loss': 0.5628, 'grad_norm': 0.9004883170127869, 'learning_rate': 8.332658142306255e-05, 'epoch': 1.1818181818181819}\n{'loss': 0.5468, 'grad_norm': 0.8742319345474243, 'learning_rate': 8.265996877167805e-05, 'epoch': 1.2045454545454546}\n{'loss': 0.5262, 'grad_norm': 0.9650866389274597, 'learning_rate': 8.199335612029356e-05, 'epoch': 1.2272727272727273}\n{'loss': 0.5381, 'grad_norm': 0.939781129360199, 'learning_rate': 8.132674346890905e-05, 'epoch': 1.25}\n{'loss': 0.5385, 'grad_norm': 0.5970568060874939, 'learning_rate': 8.066013081752455e-05, 'epoch': 1.2727272727272727}\n{'loss': 0.5024, 'grad_norm': 1.1867080926895142, 'learning_rate': 7.999351816614006e-05, 'epoch': 1.2954545454545454}\n{'loss': 0.4809, 'grad_norm': 1.1437599658966064, 'learning_rate': 7.932690551475555e-05, 'epoch': 1.3181818181818181}\n{'loss': 0.5147, 'grad_norm': 0.8887328505516052, 'learning_rate': 7.866029286337106e-05, 'epoch': 1.3409090909090908}\n{'loss': 0.5057, 'grad_norm': 0.6584604978561401, 'learning_rate': 7.799368021198655e-05, 'epoch': 1.3636363636363638}\n{'loss': 0.5326, 'grad_norm': 0.7992748022079468, 'learning_rate': 7.732706756060205e-05, 'epoch': 1.3863636363636362}\n{'loss': 0.434, 'grad_norm': 0.817222535610199, 'learning_rate': 7.666045490921756e-05, 'epoch': 1.4090909090909092}\n{'loss': 0.5093, 'grad_norm': 1.030531406402588, 'learning_rate': 7.599384225783305e-05, 'epoch': 1.4318181818181819}\n{'loss': 0.5267, 'grad_norm': 0.8173587918281555, 'learning_rate': 7.532722960644855e-05, 'epoch': 1.4545454545454546}\n{'loss': 0.4605, 'grad_norm': 0.8051937818527222, 'learning_rate': 7.466061695506406e-05, 'epoch': 1.4772727272727273}\n{'loss': 0.4954, 'grad_norm': 0.8390501737594604, 'learning_rate': 7.399400430367955e-05, 'epoch': 1.5}\n{'loss': 0.5192, 'grad_norm': 1.3577640056610107, 'learning_rate': 7.332739165229505e-05, 'epoch': 1.5227272727272727}\n{'loss': 0.4085, 'grad_norm': 1.0562714338302612, 'learning_rate': 7.266077900091054e-05, 'epoch': 1.5454545454545454}\n{'loss': 0.4129, 'grad_norm': 0.8425236344337463, 'learning_rate': 7.199416634952605e-05, 'epoch': 1.5681818181818183}\n{'loss': 0.4538, 'grad_norm': 1.0512696504592896, 'learning_rate': 7.132755369814154e-05, 'epoch': 1.5909090909090908}\n{'loss': 0.4503, 'grad_norm': 0.7272679805755615, 'learning_rate': 7.066094104675704e-05, 'epoch': 1.6136363636363638}\n{'loss': 0.4846, 'grad_norm': 1.4526517391204834, 'learning_rate': 6.999432839537255e-05, 'epoch': 1.6363636363636362}\n{'loss': 0.4057, 'grad_norm': 0.7852120995521545, 'learning_rate': 6.932771574398804e-05, 'epoch': 1.6590909090909092}\n{'loss': 0.4823, 'grad_norm': 1.2795166969299316, 'learning_rate': 6.866110309260354e-05, 'epoch': 1.6818181818181817}\n{'loss': 0.3969, 'grad_norm': 0.7853308320045471, 'learning_rate': 6.799449044121905e-05, 'epoch': 1.7045454545454546}\n{'loss': 0.4118, 'grad_norm': 0.8868638873100281, 'learning_rate': 6.732787778983454e-05, 'epoch': 1.7272727272727273}\n{'loss': 0.5037, 'grad_norm': 0.7603915929794312, 'learning_rate': 6.666126513845004e-05, 'epoch': 1.75}\n{'loss': 0.4734, 'grad_norm': 0.902265727519989, 'learning_rate': 6.599465248706555e-05, 'epoch': 1.7727272727272727}\n{'loss': 0.4164, 'grad_norm': 0.8389226198196411, 'learning_rate': 6.532803983568104e-05, 'epoch': 1.7954545454545454}\n{'loss': 0.4407, 'grad_norm': 0.8442175388336182, 'learning_rate': 6.466142718429655e-05, 'epoch': 1.8181818181818183}\n{'loss': 0.4279, 'grad_norm': 0.8190523982048035, 'learning_rate': 6.399481453291205e-05, 'epoch': 1.8409090909090908}\n{'loss': 0.4451, 'grad_norm': 0.903360903263092, 'learning_rate': 6.332820188152754e-05, 'epoch': 1.8636363636363638}\n{'loss': 0.4119, 'grad_norm': 0.9950006604194641, 'learning_rate': 6.266158923014303e-05, 'epoch': 1.8863636363636362}\n{'loss': 0.4966, 'grad_norm': 0.8243328332901001, 'learning_rate': 6.199497657875855e-05, 'epoch': 1.9090909090909092}\n{'loss': 0.3725, 'grad_norm': 1.0889085531234741, 'learning_rate': 6.132836392737404e-05, 'epoch': 1.9318181818181817}\n{'loss': 0.448, 'grad_norm': 0.9926789402961731, 'learning_rate': 6.066175127598954e-05, 'epoch': 1.9545454545454546}\n{'loss': 0.3898, 'grad_norm': 0.697513997554779, 'learning_rate': 5.9995138624605045e-05, 'epoch': 1.9772727272727273}\n{'loss': 0.4799, 'grad_norm': 3.580507755279541, 'learning_rate': 5.932852597322054e-05, 'epoch': 2.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8184    0.6954    0.7519      3142\n     PERCENT     0.4444    0.9697    0.6095        66\n        TYPE     0.8931    0.9497    0.9206     11415\n      VOLUME     0.0800    0.0286    0.0421        70\n\n   micro avg     0.8741    0.8910    0.8825     14693\n   macro avg     0.5590    0.6609    0.5810     14693\nweighted avg     0.8713    0.8910    0.8789     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.40634942054748535, 'eval_f1_macro': 0.581020961266394, 'eval_precision': 0.8741403485344195, 'eval_recall': 0.891036548016062, 'eval_f1': 0.8825075834175936, 'eval_accuracy': 0.8778974861247143, 'eval_runtime': 1.4325, 'eval_samples_per_second': 3847.197, 'eval_steps_per_second': 7.679, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.3922, 'grad_norm': 1.5818936824798584, 'learning_rate': 5.8661913321836034e-05, 'epoch': 2.022727272727273}\n{'loss': 0.4147, 'grad_norm': 1.0147595405578613, 'learning_rate': 5.7995300670451545e-05, 'epoch': 2.0454545454545454}\n{'loss': 0.4383, 'grad_norm': 1.2101892232894897, 'learning_rate': 5.732868801906704e-05, 'epoch': 2.0681818181818183}\n{'loss': 0.3716, 'grad_norm': 1.1384961605072021, 'learning_rate': 5.6662075367682534e-05, 'epoch': 2.090909090909091}\n{'loss': 0.3576, 'grad_norm': 0.7577794790267944, 'learning_rate': 5.599546271629803e-05, 'epoch': 2.1136363636363638}\n{'loss': 0.3301, 'grad_norm': 0.6997553706169128, 'learning_rate': 5.532885006491354e-05, 'epoch': 2.1363636363636362}\n{'loss': 0.3577, 'grad_norm': 1.0401523113250732, 'learning_rate': 5.4662237413529035e-05, 'epoch': 2.159090909090909}\n{'loss': 0.392, 'grad_norm': 1.3222103118896484, 'learning_rate': 5.399562476214453e-05, 'epoch': 2.1818181818181817}\n{'loss': 0.4224, 'grad_norm': 1.0297483205795288, 'learning_rate': 5.332901211076004e-05, 'epoch': 2.2045454545454546}\n{'loss': 0.3962, 'grad_norm': 0.7238923907279968, 'learning_rate': 5.2662399459375535e-05, 'epoch': 2.227272727272727}\n{'loss': 0.3786, 'grad_norm': 0.9695888757705688, 'learning_rate': 5.199578680799104e-05, 'epoch': 2.25}\n{'loss': 0.3437, 'grad_norm': 0.9702352285385132, 'learning_rate': 5.132917415660653e-05, 'epoch': 2.2727272727272725}\n{'loss': 0.403, 'grad_norm': 1.9584323167800903, 'learning_rate': 5.0662561505222036e-05, 'epoch': 2.2954545454545454}\n{'loss': 0.3517, 'grad_norm': 0.7251641750335693, 'learning_rate': 4.9995948853837534e-05, 'epoch': 2.3181818181818183}\n{'loss': 0.3338, 'grad_norm': 1.269139289855957, 'learning_rate': 4.932933620245303e-05, 'epoch': 2.340909090909091}\n{'loss': 0.384, 'grad_norm': 1.0068398714065552, 'learning_rate': 4.8662723551068536e-05, 'epoch': 2.3636363636363638}\n{'loss': 0.3594, 'grad_norm': 0.6736631989479065, 'learning_rate': 4.799611089968403e-05, 'epoch': 2.3863636363636362}\n{'loss': 0.4017, 'grad_norm': 0.9644448757171631, 'learning_rate': 4.732949824829953e-05, 'epoch': 2.409090909090909}\n{'loss': 0.3566, 'grad_norm': 0.7703995704650879, 'learning_rate': 4.666288559691503e-05, 'epoch': 2.4318181818181817}\n{'loss': 0.3753, 'grad_norm': 0.9321251511573792, 'learning_rate': 4.599627294553053e-05, 'epoch': 2.4545454545454546}\n{'loss': 0.392, 'grad_norm': 0.7095531225204468, 'learning_rate': 4.532966029414603e-05, 'epoch': 2.4772727272727275}\n{'loss': 0.3472, 'grad_norm': 0.9109854698181152, 'learning_rate': 4.466304764276153e-05, 'epoch': 2.5}\n{'loss': 0.42, 'grad_norm': 1.0009891986846924, 'learning_rate': 4.399643499137703e-05, 'epoch': 2.5227272727272725}\n{'loss': 0.3833, 'grad_norm': 0.9310914278030396, 'learning_rate': 4.332982233999253e-05, 'epoch': 2.5454545454545454}\n{'loss': 0.3884, 'grad_norm': 1.0452704429626465, 'learning_rate': 4.266320968860803e-05, 'epoch': 2.5681818181818183}\n{'loss': 0.444, 'grad_norm': 1.3564374446868896, 'learning_rate': 4.199659703722353e-05, 'epoch': 2.590909090909091}\n{'loss': 0.3235, 'grad_norm': 0.7379257678985596, 'learning_rate': 4.132998438583903e-05, 'epoch': 2.6136363636363638}\n{'loss': 0.3317, 'grad_norm': 1.2634257078170776, 'learning_rate': 4.0663371734454525e-05, 'epoch': 2.6363636363636362}\n{'loss': 0.365, 'grad_norm': 1.2004631757736206, 'learning_rate': 3.999675908307003e-05, 'epoch': 2.659090909090909}\n{'loss': 0.3139, 'grad_norm': 0.8588228225708008, 'learning_rate': 3.933014643168553e-05, 'epoch': 2.6818181818181817}\n{'loss': 0.3574, 'grad_norm': 1.051140308380127, 'learning_rate': 3.8663533780301026e-05, 'epoch': 2.7045454545454546}\n{'loss': 0.3409, 'grad_norm': 1.0727747678756714, 'learning_rate': 3.7996921128916523e-05, 'epoch': 2.7272727272727275}\n{'loss': 0.3448, 'grad_norm': 0.7157582640647888, 'learning_rate': 3.733030847753203e-05, 'epoch': 2.75}\n{'loss': 0.319, 'grad_norm': 0.8159298896789551, 'learning_rate': 3.6663695826147526e-05, 'epoch': 2.7727272727272725}\n{'loss': 0.375, 'grad_norm': 0.8570704460144043, 'learning_rate': 3.5997083174763024e-05, 'epoch': 2.7954545454545454}\n{'loss': 0.3886, 'grad_norm': 0.8896685838699341, 'learning_rate': 3.533047052337852e-05, 'epoch': 2.8181818181818183}\n{'loss': 0.3628, 'grad_norm': 1.0811247825622559, 'learning_rate': 3.466385787199402e-05, 'epoch': 2.840909090909091}\n{'loss': 0.3122, 'grad_norm': 0.7484732270240784, 'learning_rate': 3.3997245220609525e-05, 'epoch': 2.8636363636363638}\n{'loss': 0.301, 'grad_norm': 1.2694668769836426, 'learning_rate': 3.333063256922502e-05, 'epoch': 2.8863636363636362}\n{'loss': 0.365, 'grad_norm': 1.5217385292053223, 'learning_rate': 3.266401991784052e-05, 'epoch': 2.909090909090909}\n{'loss': 0.3527, 'grad_norm': 0.981169581413269, 'learning_rate': 3.1997407266456025e-05, 'epoch': 2.9318181818181817}\n{'loss': 0.3941, 'grad_norm': 1.1326425075531006, 'learning_rate': 3.1330794615071516e-05, 'epoch': 2.9545454545454546}\n{'loss': 0.3606, 'grad_norm': 1.2355929613113403, 'learning_rate': 3.066418196368702e-05, 'epoch': 2.9772727272727275}\n{'loss': 0.1526, 'grad_norm': 3.910722017288208, 'learning_rate': 2.9997569312302522e-05, 'epoch': 3.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8101    0.7683    0.7886      3142\n     PERCENT     0.4776    0.9697    0.6400        66\n        TYPE     0.9102    0.9596    0.9342     11415\n      VOLUME     0.2571    0.1286    0.1714        70\n\n   micro avg     0.8852    0.9148    0.8998     14693\n   macro avg     0.6138    0.7065    0.6336     14693\nweighted avg     0.8837    0.9148    0.8981     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.35539641976356506, 'eval_f1_macro': 0.633575702019143, 'eval_precision': 0.8852081138040042, 'eval_recall': 0.9147893554753964, 'eval_f1': 0.8997556648927268, 'eval_accuracy': 0.8949287191206878, 'eval_runtime': 1.4924, 'eval_samples_per_second': 3692.784, 'eval_steps_per_second': 7.371, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.3288, 'grad_norm': 1.0131293535232544, 'learning_rate': 2.9330956660918017e-05, 'epoch': 3.022727272727273}\n{'loss': 0.3432, 'grad_norm': 0.726641833782196, 'learning_rate': 2.866434400953352e-05, 'epoch': 3.0454545454545454}\n{'loss': 0.3283, 'grad_norm': 1.0657941102981567, 'learning_rate': 2.7997731358149016e-05, 'epoch': 3.0681818181818183}\n{'loss': 0.2891, 'grad_norm': 0.6657111048698425, 'learning_rate': 2.7331118706764517e-05, 'epoch': 3.090909090909091}\n{'loss': 0.3809, 'grad_norm': 1.3079378604888916, 'learning_rate': 2.666450605538002e-05, 'epoch': 3.1136363636363638}\n{'loss': 0.296, 'grad_norm': 0.860171377658844, 'learning_rate': 2.599789340399552e-05, 'epoch': 3.1363636363636362}\n{'loss': 0.3328, 'grad_norm': 2.077012062072754, 'learning_rate': 2.5331280752611018e-05, 'epoch': 3.159090909090909}\n{'loss': 0.3106, 'grad_norm': 1.2112102508544922, 'learning_rate': 2.4664668101226516e-05, 'epoch': 3.1818181818181817}\n{'loss': 0.3856, 'grad_norm': 0.756148099899292, 'learning_rate': 2.3998055449842014e-05, 'epoch': 3.2045454545454546}\n{'loss': 0.2892, 'grad_norm': 0.7384164929389954, 'learning_rate': 2.3331442798457515e-05, 'epoch': 3.227272727272727}\n{'loss': 0.3578, 'grad_norm': 0.8329156041145325, 'learning_rate': 2.2664830147073016e-05, 'epoch': 3.25}\n{'loss': 0.329, 'grad_norm': 0.8400287628173828, 'learning_rate': 2.1998217495688514e-05, 'epoch': 3.2727272727272725}\n{'loss': 0.3289, 'grad_norm': 1.0116822719573975, 'learning_rate': 2.1331604844304016e-05, 'epoch': 3.2954545454545454}\n{'loss': 0.3433, 'grad_norm': 0.73699951171875, 'learning_rate': 2.0664992192919514e-05, 'epoch': 3.3181818181818183}\n{'loss': 0.3198, 'grad_norm': 0.839706540107727, 'learning_rate': 1.9998379541535015e-05, 'epoch': 3.340909090909091}\n{'loss': 0.3134, 'grad_norm': 0.83143150806427, 'learning_rate': 1.9331766890150513e-05, 'epoch': 3.3636363636363638}\n{'loss': 0.275, 'grad_norm': 0.7408241033554077, 'learning_rate': 1.8665154238766014e-05, 'epoch': 3.3863636363636362}\n{'loss': 0.2864, 'grad_norm': 0.6066403388977051, 'learning_rate': 1.7998541587381512e-05, 'epoch': 3.409090909090909}\n{'loss': 0.3105, 'grad_norm': 1.0564160346984863, 'learning_rate': 1.733192893599701e-05, 'epoch': 3.4318181818181817}\n{'loss': 0.4144, 'grad_norm': 1.2261625528335571, 'learning_rate': 1.666531628461251e-05, 'epoch': 3.4545454545454546}\n{'loss': 0.2896, 'grad_norm': 0.7983506917953491, 'learning_rate': 1.5998703633228013e-05, 'epoch': 3.4772727272727275}\n{'loss': 0.3512, 'grad_norm': 0.962498128414154, 'learning_rate': 1.533209098184351e-05, 'epoch': 3.5}\n{'loss': 0.3514, 'grad_norm': 0.7489296793937683, 'learning_rate': 1.4665478330459008e-05, 'epoch': 3.5227272727272725}\n{'loss': 0.3085, 'grad_norm': 0.7968779802322388, 'learning_rate': 1.3998865679074508e-05, 'epoch': 3.5454545454545454}\n{'loss': 0.3283, 'grad_norm': 0.8758310079574585, 'learning_rate': 1.333225302769001e-05, 'epoch': 3.5681818181818183}\n{'loss': 0.3754, 'grad_norm': 0.806853175163269, 'learning_rate': 1.2665640376305509e-05, 'epoch': 3.590909090909091}\n{'loss': 0.3707, 'grad_norm': 0.7971122860908508, 'learning_rate': 1.1999027724921007e-05, 'epoch': 3.6136363636363638}\n{'loss': 0.2935, 'grad_norm': 0.7064879536628723, 'learning_rate': 1.1332415073536508e-05, 'epoch': 3.6363636363636362}\n{'loss': 0.2812, 'grad_norm': 1.1019598245620728, 'learning_rate': 1.0665802422152008e-05, 'epoch': 3.659090909090909}\n{'loss': 0.286, 'grad_norm': 1.3406784534454346, 'learning_rate': 9.999189770767507e-06, 'epoch': 3.6818181818181817}\n{'loss': 0.271, 'grad_norm': 0.730186939239502, 'learning_rate': 9.332577119383007e-06, 'epoch': 3.7045454545454546}\n{'loss': 0.2905, 'grad_norm': 0.7954539060592651, 'learning_rate': 8.665964467998505e-06, 'epoch': 3.7272727272727275}\n{'loss': 0.2944, 'grad_norm': 0.9749075770378113, 'learning_rate': 7.999351816614006e-06, 'epoch': 3.75}\n{'loss': 0.3415, 'grad_norm': 0.8506003022193909, 'learning_rate': 7.332739165229504e-06, 'epoch': 3.7727272727272725}\n{'loss': 0.3372, 'grad_norm': 0.8926067352294922, 'learning_rate': 6.666126513845005e-06, 'epoch': 3.7954545454545454}\n{'loss': 0.307, 'grad_norm': 0.6687082648277283, 'learning_rate': 5.9995138624605034e-06, 'epoch': 3.8181818181818183}\n{'loss': 0.2927, 'grad_norm': 0.7603479623794556, 'learning_rate': 5.332901211076004e-06, 'epoch': 3.840909090909091}\n{'loss': 0.4275, 'grad_norm': 1.0621063709259033, 'learning_rate': 4.6662885596915035e-06, 'epoch': 3.8636363636363638}\n{'loss': 0.3666, 'grad_norm': 1.4223283529281616, 'learning_rate': 3.999675908307003e-06, 'epoch': 3.8863636363636362}\n{'loss': 0.3248, 'grad_norm': 1.1362179517745972, 'learning_rate': 3.3330632569225023e-06, 'epoch': 3.909090909090909}\n{'loss': 0.2927, 'grad_norm': 1.2051615715026855, 'learning_rate': 2.666450605538002e-06, 'epoch': 3.9318181818181817}\n{'loss': 0.3623, 'grad_norm': 1.2912338972091675, 'learning_rate': 1.9998379541535016e-06, 'epoch': 3.9545454545454546}\n{'loss': 0.3184, 'grad_norm': 0.6769882440567017, 'learning_rate': 1.333225302769001e-06, 'epoch': 3.9772727272727275}\n{'loss': 0.5423, 'grad_norm': 4.743626594543457, 'learning_rate': 6.666126513845005e-07, 'epoch': 4.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8156    0.7810    0.7979      3142\n     PERCENT     0.5079    0.9697    0.6667        66\n        TYPE     0.9155    0.9597    0.9371     11415\n      VOLUME     0.3415    0.2000    0.2523        70\n\n   micro avg     0.8907    0.9179    0.9041     14693\n   macro avg     0.6451    0.7276    0.6635     14693\nweighted avg     0.8896    0.9179    0.9028     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.3423650562763214, 'eval_f1_macro': 0.6634809060809379, 'eval_precision': 0.8907013604543653, 'eval_recall': 0.9179200980058532, 'eval_f1': 0.9041059158706217, 'eval_accuracy': 0.8983567308738709, 'eval_runtime': 1.4481, 'eval_samples_per_second': 3805.642, 'eval_steps_per_second': 7.596, 'epoch': 4.0}\n{'train_runtime': 24.3965, 'train_samples_per_second': 3613.804, 'train_steps_per_second': 7.214, 'train_loss': 0.6119836164130406, 'epoch': 4.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nThe speedups for torchdynamo mostly come with GPU Ampere or higher and which is not detected here.\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8156    0.7810    0.7979      3142\n     PERCENT     0.5079    0.9697    0.6667        66\n        TYPE     0.9155    0.9597    0.9371     11415\n      VOLUME     0.3415    0.2000    0.2523        70\n\n   micro avg     0.8907    0.9179    0.9041     14693\n   macro avg     0.6451    0.7276    0.6635     14693\nweighted avg     0.8896    0.9179    0.9028     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.3423650562763214, 'eval_f1_macro': 0.6634809060809379, 'eval_precision': 0.8907013604543653, 'eval_recall': 0.9179200980058532, 'eval_f1': 0.9041059158706217, 'eval_accuracy': 0.8983567308738709, 'eval_runtime': 1.5065, 'eval_samples_per_second': 3658.168, 'eval_steps_per_second': 7.302, 'epoch': 4.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/3364797558.py:66: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 2.1128, 'grad_norm': 6.952020645141602, 'learning_rate': 0.0, 'epoch': 0.022727272727272728}\n{'loss': 2.1098, 'grad_norm': 7.102254390716553, 'learning_rate': 5.851377717708392e-06, 'epoch': 0.045454545454545456}\n{'loss': 2.0993, 'grad_norm': 7.103538513183594, 'learning_rate': 1.1702755435416784e-05, 'epoch': 0.06818181818181818}\n{'loss': 2.0732, 'grad_norm': 6.836056232452393, 'learning_rate': 1.755413315312518e-05, 'epoch': 0.09090909090909091}\n{'loss': 2.0529, 'grad_norm': 6.727117538452148, 'learning_rate': 2.340551087083357e-05, 'epoch': 0.11363636363636363}\n{'loss': 1.975, 'grad_norm': 6.69104528427124, 'learning_rate': 2.9256888588541966e-05, 'epoch': 0.13636363636363635}\n{'loss': 1.9277, 'grad_norm': 6.430558204650879, 'learning_rate': 3.510826630625036e-05, 'epoch': 0.1590909090909091}\n{'loss': 1.8836, 'grad_norm': 5.841019630432129, 'learning_rate': 4.095964402395875e-05, 'epoch': 0.18181818181818182}\n{'loss': 1.7779, 'grad_norm': 6.09973669052124, 'learning_rate': 4.681102174166714e-05, 'epoch': 0.20454545454545456}\n{'loss': 1.7183, 'grad_norm': 5.430098533630371, 'learning_rate': 5.2662399459375535e-05, 'epoch': 0.22727272727272727}\n{'loss': 1.6014, 'grad_norm': 5.354484558105469, 'learning_rate': 5.851377717708393e-05, 'epoch': 0.25}\n{'loss': 1.5564, 'grad_norm': 4.442960739135742, 'learning_rate': 6.436515489479233e-05, 'epoch': 0.2727272727272727}\n{'loss': 1.4303, 'grad_norm': 4.096538543701172, 'learning_rate': 7.021653261250071e-05, 'epoch': 0.29545454545454547}\n{'loss': 1.3131, 'grad_norm': 3.6639552116394043, 'learning_rate': 7.606791033020911e-05, 'epoch': 0.3181818181818182}\n{'loss': 1.3255, 'grad_norm': 2.6597580909729004, 'learning_rate': 8.19192880479175e-05, 'epoch': 0.3409090909090909}\n{'loss': 1.2305, 'grad_norm': 2.0888147354125977, 'learning_rate': 8.777066576562589e-05, 'epoch': 0.36363636363636365}\n{'loss': 1.1706, 'grad_norm': 1.7206388711929321, 'learning_rate': 9.362204348333428e-05, 'epoch': 0.38636363636363635}\n{'loss': 1.1616, 'grad_norm': 1.8025681972503662, 'learning_rate': 9.947342120104267e-05, 'epoch': 0.4090909090909091}\n{'loss': 1.1137, 'grad_norm': 1.7508580684661865, 'learning_rate': 0.00010532479891875107, 'epoch': 0.4318181818181818}\n{'loss': 1.122, 'grad_norm': 1.8496726751327515, 'learning_rate': 0.00010465818626736656, 'epoch': 0.45454545454545453}\n{'loss': 1.0673, 'grad_norm': 1.4921813011169434, 'learning_rate': 0.00010399157361598208, 'epoch': 0.4772727272727273}\n{'loss': 0.9814, 'grad_norm': 0.9811789989471436, 'learning_rate': 0.00010332496096459757, 'epoch': 0.5}\n{'loss': 1.0045, 'grad_norm': 1.1409035921096802, 'learning_rate': 0.00010265834831321306, 'epoch': 0.5227272727272727}\n{'loss': 1.0473, 'grad_norm': 1.153436541557312, 'learning_rate': 0.00010199173566182858, 'epoch': 0.5454545454545454}\n{'loss': 0.9896, 'grad_norm': 1.4248766899108887, 'learning_rate': 0.00010132512301044407, 'epoch': 0.5681818181818182}\n{'loss': 0.9211, 'grad_norm': 1.5519378185272217, 'learning_rate': 0.00010065851035905956, 'epoch': 0.5909090909090909}\n{'loss': 0.9667, 'grad_norm': 1.3217064142227173, 'learning_rate': 9.999189770767507e-05, 'epoch': 0.6136363636363636}\n{'loss': 0.873, 'grad_norm': 1.3709990978240967, 'learning_rate': 9.932528505629057e-05, 'epoch': 0.6363636363636364}\n{'loss': 0.9159, 'grad_norm': 1.1479737758636475, 'learning_rate': 9.865867240490606e-05, 'epoch': 0.6590909090909091}\n{'loss': 0.8809, 'grad_norm': 1.0382304191589355, 'learning_rate': 9.799205975352157e-05, 'epoch': 0.6818181818181818}\n{'loss': 0.827, 'grad_norm': 1.1465702056884766, 'learning_rate': 9.732544710213707e-05, 'epoch': 0.7045454545454546}\n{'loss': 0.8395, 'grad_norm': 1.1593035459518433, 'learning_rate': 9.665883445075256e-05, 'epoch': 0.7272727272727273}\n{'loss': 0.7673, 'grad_norm': 1.030357837677002, 'learning_rate': 9.599222179936806e-05, 'epoch': 0.75}\n{'loss': 0.7671, 'grad_norm': 0.8858033418655396, 'learning_rate': 9.532560914798357e-05, 'epoch': 0.7727272727272727}\n{'loss': 0.7221, 'grad_norm': 0.9372810125350952, 'learning_rate': 9.465899649659906e-05, 'epoch': 0.7954545454545454}\n{'loss': 0.7062, 'grad_norm': 0.8876421451568604, 'learning_rate': 9.399238384521456e-05, 'epoch': 0.8181818181818182}\n{'loss': 0.6786, 'grad_norm': 1.414621353149414, 'learning_rate': 9.332577119383006e-05, 'epoch': 0.8409090909090909}\n{'loss': 0.7024, 'grad_norm': 1.16575288772583, 'learning_rate': 9.265915854244556e-05, 'epoch': 0.8636363636363636}\n{'loss': 0.6361, 'grad_norm': 1.0953619480133057, 'learning_rate': 9.199254589106106e-05, 'epoch': 0.8863636363636364}\n{'loss': 0.6391, 'grad_norm': 1.0228852033615112, 'learning_rate': 9.132593323967656e-05, 'epoch': 0.9090909090909091}\n{'loss': 0.7155, 'grad_norm': 1.0131945610046387, 'learning_rate': 9.065932058829207e-05, 'epoch': 0.9318181818181818}\n{'loss': 0.6465, 'grad_norm': 0.9060910940170288, 'learning_rate': 8.999270793690756e-05, 'epoch': 0.9545454545454546}\n{'loss': 0.6548, 'grad_norm': 1.3677449226379395, 'learning_rate': 8.932609528552306e-05, 'epoch': 0.9772727272727273}\n{'loss': 0.486, 'grad_norm': 2.216392755508423, 'learning_rate': 8.865948263413857e-05, 'epoch': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.7235    0.5564    0.6290      3404\n     PERCENT     0.9302    0.5634    0.7018        71\n        TYPE     0.8226    0.8958    0.8577     11194\n      VOLUME     0.0000    0.0000    0.0000        56\n\n   micro avg     0.8055    0.8124    0.8089     14725\n   macro avg     0.6191    0.5039    0.5471     14725\nweighted avg     0.7971    0.8124    0.8008     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.6155798435211182, 'eval_f1_macro': 0.547115438150497, 'eval_precision': 0.8054676452764123, 'eval_recall': 0.8123599320882853, 'eval_f1': 0.8088991073843657, 'eval_accuracy': 0.8137367757496026, 'eval_runtime': 1.4696, 'eval_samples_per_second': 3749.9, 'eval_steps_per_second': 7.485, 'epoch': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.579, 'grad_norm': 0.9144540429115295, 'learning_rate': 8.799286998275406e-05, 'epoch': 1.0227272727272727}\n{'loss': 0.5566, 'grad_norm': 1.0259613990783691, 'learning_rate': 8.732625733136956e-05, 'epoch': 1.0454545454545454}\n{'loss': 0.5503, 'grad_norm': 0.7469369173049927, 'learning_rate': 8.665964467998507e-05, 'epoch': 1.0681818181818181}\n{'loss': 0.5163, 'grad_norm': 0.7788971662521362, 'learning_rate': 8.599303202860056e-05, 'epoch': 1.0909090909090908}\n{'loss': 0.5741, 'grad_norm': 1.172982931137085, 'learning_rate': 8.532641937721606e-05, 'epoch': 1.1136363636363635}\n{'loss': 0.4894, 'grad_norm': 1.4627400636672974, 'learning_rate': 8.465980672583155e-05, 'epoch': 1.1363636363636362}\n{'loss': 0.5146, 'grad_norm': 1.5735795497894287, 'learning_rate': 8.399319407444706e-05, 'epoch': 1.1590909090909092}\n{'loss': 0.564, 'grad_norm': 0.9564589262008667, 'learning_rate': 8.332658142306255e-05, 'epoch': 1.1818181818181819}\n{'loss': 0.5398, 'grad_norm': 0.8847936987876892, 'learning_rate': 8.265996877167805e-05, 'epoch': 1.2045454545454546}\n{'loss': 0.5873, 'grad_norm': 1.086578607559204, 'learning_rate': 8.199335612029356e-05, 'epoch': 1.2272727272727273}\n{'loss': 0.5406, 'grad_norm': 0.7945085763931274, 'learning_rate': 8.132674346890905e-05, 'epoch': 1.25}\n{'loss': 0.5224, 'grad_norm': 0.929637610912323, 'learning_rate': 8.066013081752455e-05, 'epoch': 1.2727272727272727}\n{'loss': 0.4562, 'grad_norm': 0.9617682695388794, 'learning_rate': 7.999351816614006e-05, 'epoch': 1.2954545454545454}\n{'loss': 0.5081, 'grad_norm': 0.6794573664665222, 'learning_rate': 7.932690551475555e-05, 'epoch': 1.3181818181818181}\n{'loss': 0.5398, 'grad_norm': 0.6382072567939758, 'learning_rate': 7.866029286337106e-05, 'epoch': 1.3409090909090908}\n{'loss': 0.5359, 'grad_norm': 0.6402159929275513, 'learning_rate': 7.799368021198655e-05, 'epoch': 1.3636363636363638}\n{'loss': 0.4587, 'grad_norm': 0.730262041091919, 'learning_rate': 7.732706756060205e-05, 'epoch': 1.3863636363636362}\n{'loss': 0.4625, 'grad_norm': 0.700261116027832, 'learning_rate': 7.666045490921756e-05, 'epoch': 1.4090909090909092}\n{'loss': 0.5122, 'grad_norm': 0.6532706618309021, 'learning_rate': 7.599384225783305e-05, 'epoch': 1.4318181818181819}\n{'loss': 0.4975, 'grad_norm': 0.6741843819618225, 'learning_rate': 7.532722960644855e-05, 'epoch': 1.4545454545454546}\n{'loss': 0.4854, 'grad_norm': 0.8321124315261841, 'learning_rate': 7.466061695506406e-05, 'epoch': 1.4772727272727273}\n{'loss': 0.4795, 'grad_norm': 0.6647241115570068, 'learning_rate': 7.399400430367955e-05, 'epoch': 1.5}\n{'loss': 0.5173, 'grad_norm': 0.67368483543396, 'learning_rate': 7.332739165229505e-05, 'epoch': 1.5227272727272727}\n{'loss': 0.415, 'grad_norm': 0.909382164478302, 'learning_rate': 7.266077900091054e-05, 'epoch': 1.5454545454545454}\n{'loss': 0.4844, 'grad_norm': 1.1043503284454346, 'learning_rate': 7.199416634952605e-05, 'epoch': 1.5681818181818183}\n{'loss': 0.441, 'grad_norm': 0.677193820476532, 'learning_rate': 7.132755369814154e-05, 'epoch': 1.5909090909090908}\n{'loss': 0.4336, 'grad_norm': 0.7342970967292786, 'learning_rate': 7.066094104675704e-05, 'epoch': 1.6136363636363638}\n{'loss': 0.4733, 'grad_norm': 0.6802711486816406, 'learning_rate': 6.999432839537255e-05, 'epoch': 1.6363636363636362}\n{'loss': 0.4886, 'grad_norm': 0.7246816754341125, 'learning_rate': 6.932771574398804e-05, 'epoch': 1.6590909090909092}\n{'loss': 0.461, 'grad_norm': 0.8168865442276001, 'learning_rate': 6.866110309260354e-05, 'epoch': 1.6818181818181817}\n{'loss': 0.4285, 'grad_norm': 0.6880490779876709, 'learning_rate': 6.799449044121905e-05, 'epoch': 1.7045454545454546}\n{'loss': 0.4888, 'grad_norm': 0.707352340221405, 'learning_rate': 6.732787778983454e-05, 'epoch': 1.7272727272727273}\n{'loss': 0.4078, 'grad_norm': 0.6415349841117859, 'learning_rate': 6.666126513845004e-05, 'epoch': 1.75}\n{'loss': 0.364, 'grad_norm': 0.7698026299476624, 'learning_rate': 6.599465248706555e-05, 'epoch': 1.7727272727272727}\n{'loss': 0.4762, 'grad_norm': 0.9443417191505432, 'learning_rate': 6.532803983568104e-05, 'epoch': 1.7954545454545454}\n{'loss': 0.4478, 'grad_norm': 0.7212379574775696, 'learning_rate': 6.466142718429655e-05, 'epoch': 1.8181818181818183}\n{'loss': 0.4726, 'grad_norm': 0.8011828660964966, 'learning_rate': 6.399481453291205e-05, 'epoch': 1.8409090909090908}\n{'loss': 0.4433, 'grad_norm': 0.7292231917381287, 'learning_rate': 6.332820188152754e-05, 'epoch': 1.8636363636363638}\n{'loss': 0.4673, 'grad_norm': 0.7973496913909912, 'learning_rate': 6.266158923014303e-05, 'epoch': 1.8863636363636362}\n{'loss': 0.4503, 'grad_norm': 1.1817960739135742, 'learning_rate': 6.199497657875855e-05, 'epoch': 1.9090909090909092}\n{'loss': 0.3944, 'grad_norm': 0.6618143916130066, 'learning_rate': 6.132836392737404e-05, 'epoch': 1.9318181818181817}\n{'loss': 0.3915, 'grad_norm': 0.9787834882736206, 'learning_rate': 6.066175127598954e-05, 'epoch': 1.9545454545454546}\n{'loss': 0.3964, 'grad_norm': 0.9322474002838135, 'learning_rate': 5.9995138624605045e-05, 'epoch': 1.9772727272727273}\n{'loss': 0.309, 'grad_norm': 3.088287115097046, 'learning_rate': 5.932852597322054e-05, 'epoch': 2.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.7968    0.7039    0.7475      3404\n     PERCENT     0.4930    0.9859    0.6573        71\n        TYPE     0.8853    0.9532    0.9180     11194\n      VOLUME     0.2000    0.0357    0.0606        56\n\n   micro avg     0.8637    0.8922    0.8777     14725\n   macro avg     0.5938    0.6697    0.5958     14725\nweighted avg     0.8604    0.8922    0.8741     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.4129004180431366, 'eval_f1_macro': 0.5958389372648869, 'eval_precision': 0.8637170468739728, 'eval_recall': 0.8922241086587437, 'eval_f1': 0.8777391769107429, 'eval_accuracy': 0.8754590801951433, 'eval_runtime': 1.4579, 'eval_samples_per_second': 3780.053, 'eval_steps_per_second': 7.545, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.3444, 'grad_norm': 1.004979133605957, 'learning_rate': 5.8661913321836034e-05, 'epoch': 2.022727272727273}\n{'loss': 0.4257, 'grad_norm': 0.8088529706001282, 'learning_rate': 5.7995300670451545e-05, 'epoch': 2.0454545454545454}\n{'loss': 0.3602, 'grad_norm': 0.9553201794624329, 'learning_rate': 5.732868801906704e-05, 'epoch': 2.0681818181818183}\n{'loss': 0.4004, 'grad_norm': 0.9697668552398682, 'learning_rate': 5.6662075367682534e-05, 'epoch': 2.090909090909091}\n{'loss': 0.4146, 'grad_norm': 0.7217499017715454, 'learning_rate': 5.599546271629803e-05, 'epoch': 2.1136363636363638}\n{'loss': 0.3656, 'grad_norm': 0.7960792779922485, 'learning_rate': 5.532885006491354e-05, 'epoch': 2.1363636363636362}\n{'loss': 0.3847, 'grad_norm': 1.295203447341919, 'learning_rate': 5.4662237413529035e-05, 'epoch': 2.159090909090909}\n{'loss': 0.4359, 'grad_norm': 1.2941522598266602, 'learning_rate': 5.399562476214453e-05, 'epoch': 2.1818181818181817}\n{'loss': 0.3935, 'grad_norm': 0.7459471225738525, 'learning_rate': 5.332901211076004e-05, 'epoch': 2.2045454545454546}\n{'loss': 0.3648, 'grad_norm': 0.9810815453529358, 'learning_rate': 5.2662399459375535e-05, 'epoch': 2.227272727272727}\n{'loss': 0.4382, 'grad_norm': 1.6705615520477295, 'learning_rate': 5.199578680799104e-05, 'epoch': 2.25}\n{'loss': 0.3707, 'grad_norm': 1.2159401178359985, 'learning_rate': 5.132917415660653e-05, 'epoch': 2.2727272727272725}\n{'loss': 0.3537, 'grad_norm': 1.926841378211975, 'learning_rate': 5.0662561505222036e-05, 'epoch': 2.2954545454545454}\n{'loss': 0.3476, 'grad_norm': 1.521535038948059, 'learning_rate': 4.9995948853837534e-05, 'epoch': 2.3181818181818183}\n{'loss': 0.3557, 'grad_norm': 0.8567618727684021, 'learning_rate': 4.932933620245303e-05, 'epoch': 2.340909090909091}\n{'loss': 0.4271, 'grad_norm': 0.9329760074615479, 'learning_rate': 4.8662723551068536e-05, 'epoch': 2.3636363636363638}\n{'loss': 0.3803, 'grad_norm': 0.6914113163948059, 'learning_rate': 4.799611089968403e-05, 'epoch': 2.3863636363636362}\n{'loss': 0.3587, 'grad_norm': 1.1072951555252075, 'learning_rate': 4.732949824829953e-05, 'epoch': 2.409090909090909}\n{'loss': 0.3363, 'grad_norm': 0.6021178364753723, 'learning_rate': 4.666288559691503e-05, 'epoch': 2.4318181818181817}\n{'loss': 0.3602, 'grad_norm': 0.8716993927955627, 'learning_rate': 4.599627294553053e-05, 'epoch': 2.4545454545454546}\n{'loss': 0.3472, 'grad_norm': 1.1394513845443726, 'learning_rate': 4.532966029414603e-05, 'epoch': 2.4772727272727275}\n{'loss': 0.3138, 'grad_norm': 0.736396849155426, 'learning_rate': 4.466304764276153e-05, 'epoch': 2.5}\n{'loss': 0.3475, 'grad_norm': 0.8368832468986511, 'learning_rate': 4.399643499137703e-05, 'epoch': 2.5227272727272725}\n{'loss': 0.3255, 'grad_norm': 1.208599328994751, 'learning_rate': 4.332982233999253e-05, 'epoch': 2.5454545454545454}\n{'loss': 0.3992, 'grad_norm': 0.7287291288375854, 'learning_rate': 4.266320968860803e-05, 'epoch': 2.5681818181818183}\n{'loss': 0.3466, 'grad_norm': 0.8754295110702515, 'learning_rate': 4.199659703722353e-05, 'epoch': 2.590909090909091}\n{'loss': 0.3895, 'grad_norm': 0.9659467935562134, 'learning_rate': 4.132998438583903e-05, 'epoch': 2.6136363636363638}\n{'loss': 0.2841, 'grad_norm': 0.8400519490242004, 'learning_rate': 4.0663371734454525e-05, 'epoch': 2.6363636363636362}\n{'loss': 0.3125, 'grad_norm': 0.6874405145645142, 'learning_rate': 3.999675908307003e-05, 'epoch': 2.659090909090909}\n{'loss': 0.343, 'grad_norm': 0.8539609313011169, 'learning_rate': 3.933014643168553e-05, 'epoch': 2.6818181818181817}\n{'loss': 0.3704, 'grad_norm': 1.145560622215271, 'learning_rate': 3.8663533780301026e-05, 'epoch': 2.7045454545454546}\n{'loss': 0.2973, 'grad_norm': 0.75974041223526, 'learning_rate': 3.7996921128916523e-05, 'epoch': 2.7272727272727275}\n{'loss': 0.3676, 'grad_norm': 0.83819979429245, 'learning_rate': 3.733030847753203e-05, 'epoch': 2.75}\n{'loss': 0.3795, 'grad_norm': 0.8424506783485413, 'learning_rate': 3.6663695826147526e-05, 'epoch': 2.7727272727272725}\n{'loss': 0.3359, 'grad_norm': 0.8242154717445374, 'learning_rate': 3.5997083174763024e-05, 'epoch': 2.7954545454545454}\n{'loss': 0.3555, 'grad_norm': 0.9549502730369568, 'learning_rate': 3.533047052337852e-05, 'epoch': 2.8181818181818183}\n{'loss': 0.3674, 'grad_norm': 1.1369190216064453, 'learning_rate': 3.466385787199402e-05, 'epoch': 2.840909090909091}\n{'loss': 0.4084, 'grad_norm': 0.9590129256248474, 'learning_rate': 3.3997245220609525e-05, 'epoch': 2.8636363636363638}\n{'loss': 0.3873, 'grad_norm': 0.8297798037528992, 'learning_rate': 3.333063256922502e-05, 'epoch': 2.8863636363636362}\n{'loss': 0.3144, 'grad_norm': 1.2379869222640991, 'learning_rate': 3.266401991784052e-05, 'epoch': 2.909090909090909}\n{'loss': 0.3088, 'grad_norm': 1.9976304769515991, 'learning_rate': 3.1997407266456025e-05, 'epoch': 2.9318181818181817}\n{'loss': 0.354, 'grad_norm': 0.9679529070854187, 'learning_rate': 3.1330794615071516e-05, 'epoch': 2.9545454545454546}\n{'loss': 0.3171, 'grad_norm': 1.2741212844848633, 'learning_rate': 3.066418196368702e-05, 'epoch': 2.9772727272727275}\n{'loss': 0.2429, 'grad_norm': 2.785439968109131, 'learning_rate': 2.9997569312302522e-05, 'epoch': 3.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8280    0.7468    0.7853      3404\n     PERCENT     0.6970    0.9718    0.8118        71\n        TYPE     0.8997    0.9602    0.9290     11194\n      VOLUME     0.3654    0.3393    0.3519        56\n\n   micro avg     0.8820    0.9085    0.8951     14725\n   macro avg     0.6975    0.7545    0.7195     14725\nweighted avg     0.8801    0.9085    0.8930     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.36747750639915466, 'eval_f1_macro': 0.7194664439671568, 'eval_precision': 0.8820465484275071, 'eval_recall': 0.9085229202037352, 'eval_f1': 0.8950889870199383, 'eval_accuracy': 0.8910815107164392, 'eval_runtime': 1.4405, 'eval_samples_per_second': 3825.671, 'eval_steps_per_second': 7.636, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.3209, 'grad_norm': 0.8076686263084412, 'learning_rate': 2.9330956660918017e-05, 'epoch': 3.022727272727273}\n{'loss': 0.3498, 'grad_norm': 0.8945527672767639, 'learning_rate': 2.866434400953352e-05, 'epoch': 3.0454545454545454}\n{'loss': 0.3453, 'grad_norm': 1.111914873123169, 'learning_rate': 2.7997731358149016e-05, 'epoch': 3.0681818181818183}\n{'loss': 0.2879, 'grad_norm': 1.130085825920105, 'learning_rate': 2.7331118706764517e-05, 'epoch': 3.090909090909091}\n{'loss': 0.2744, 'grad_norm': 0.9624016880989075, 'learning_rate': 2.666450605538002e-05, 'epoch': 3.1136363636363638}\n{'loss': 0.3738, 'grad_norm': 1.538406491279602, 'learning_rate': 2.599789340399552e-05, 'epoch': 3.1363636363636362}\n{'loss': 0.3814, 'grad_norm': 2.0179495811462402, 'learning_rate': 2.5331280752611018e-05, 'epoch': 3.159090909090909}\n{'loss': 0.3219, 'grad_norm': 1.3883670568466187, 'learning_rate': 2.4664668101226516e-05, 'epoch': 3.1818181818181817}\n{'loss': 0.3354, 'grad_norm': 1.2730225324630737, 'learning_rate': 2.3998055449842014e-05, 'epoch': 3.2045454545454546}\n{'loss': 0.3459, 'grad_norm': 0.8821980953216553, 'learning_rate': 2.3331442798457515e-05, 'epoch': 3.227272727272727}\n{'loss': 0.2665, 'grad_norm': 1.9660149812698364, 'learning_rate': 2.2664830147073016e-05, 'epoch': 3.25}\n{'loss': 0.3009, 'grad_norm': 1.1684839725494385, 'learning_rate': 2.1998217495688514e-05, 'epoch': 3.2727272727272725}\n{'loss': 0.3525, 'grad_norm': 1.041248083114624, 'learning_rate': 2.1331604844304016e-05, 'epoch': 3.2954545454545454}\n{'loss': 0.3436, 'grad_norm': 0.8818075060844421, 'learning_rate': 2.0664992192919514e-05, 'epoch': 3.3181818181818183}\n{'loss': 0.2901, 'grad_norm': 1.3875536918640137, 'learning_rate': 1.9998379541535015e-05, 'epoch': 3.340909090909091}\n{'loss': 0.2952, 'grad_norm': 0.7730871438980103, 'learning_rate': 1.9331766890150513e-05, 'epoch': 3.3636363636363638}\n{'loss': 0.3676, 'grad_norm': 1.006567358970642, 'learning_rate': 1.8665154238766014e-05, 'epoch': 3.3863636363636362}\n{'loss': 0.3176, 'grad_norm': 0.8524971008300781, 'learning_rate': 1.7998541587381512e-05, 'epoch': 3.409090909090909}\n{'loss': 0.3257, 'grad_norm': 0.8741116523742676, 'learning_rate': 1.733192893599701e-05, 'epoch': 3.4318181818181817}\n{'loss': 0.2831, 'grad_norm': 0.7518835067749023, 'learning_rate': 1.666531628461251e-05, 'epoch': 3.4545454545454546}\n{'loss': 0.35, 'grad_norm': 1.1212955713272095, 'learning_rate': 1.5998703633228013e-05, 'epoch': 3.4772727272727275}\n{'loss': 0.2474, 'grad_norm': 0.8852189779281616, 'learning_rate': 1.533209098184351e-05, 'epoch': 3.5}\n{'loss': 0.3396, 'grad_norm': 0.6838225722312927, 'learning_rate': 1.4665478330459008e-05, 'epoch': 3.5227272727272725}\n{'loss': 0.3376, 'grad_norm': 0.989879310131073, 'learning_rate': 1.3998865679074508e-05, 'epoch': 3.5454545454545454}\n{'loss': 0.369, 'grad_norm': 1.1412550210952759, 'learning_rate': 1.333225302769001e-05, 'epoch': 3.5681818181818183}\n{'loss': 0.3307, 'grad_norm': 1.0834243297576904, 'learning_rate': 1.2665640376305509e-05, 'epoch': 3.590909090909091}\n{'loss': 0.3173, 'grad_norm': 0.6940737366676331, 'learning_rate': 1.1999027724921007e-05, 'epoch': 3.6136363636363638}\n{'loss': 0.334, 'grad_norm': 0.8600618839263916, 'learning_rate': 1.1332415073536508e-05, 'epoch': 3.6363636363636362}\n{'loss': 0.2805, 'grad_norm': 0.7227350473403931, 'learning_rate': 1.0665802422152008e-05, 'epoch': 3.659090909090909}\n{'loss': 0.3074, 'grad_norm': 1.2467784881591797, 'learning_rate': 9.999189770767507e-06, 'epoch': 3.6818181818181817}\n{'loss': 0.3694, 'grad_norm': 1.3885278701782227, 'learning_rate': 9.332577119383007e-06, 'epoch': 3.7045454545454546}\n{'loss': 0.3214, 'grad_norm': 0.7569252252578735, 'learning_rate': 8.665964467998505e-06, 'epoch': 3.7272727272727275}\n{'loss': 0.2761, 'grad_norm': 0.9475364089012146, 'learning_rate': 7.999351816614006e-06, 'epoch': 3.75}\n{'loss': 0.2888, 'grad_norm': 1.035887598991394, 'learning_rate': 7.332739165229504e-06, 'epoch': 3.7727272727272725}\n{'loss': 0.2867, 'grad_norm': 0.9377576112747192, 'learning_rate': 6.666126513845005e-06, 'epoch': 3.7954545454545454}\n{'loss': 0.3082, 'grad_norm': 0.9096723198890686, 'learning_rate': 5.9995138624605034e-06, 'epoch': 3.8181818181818183}\n{'loss': 0.3051, 'grad_norm': 1.9819172620773315, 'learning_rate': 5.332901211076004e-06, 'epoch': 3.840909090909091}\n{'loss': 0.3444, 'grad_norm': 1.4695762395858765, 'learning_rate': 4.6662885596915035e-06, 'epoch': 3.8636363636363638}\n{'loss': 0.33, 'grad_norm': 0.8440032601356506, 'learning_rate': 3.999675908307003e-06, 'epoch': 3.8863636363636362}\n{'loss': 0.3057, 'grad_norm': 0.7401445508003235, 'learning_rate': 3.3330632569225023e-06, 'epoch': 3.909090909090909}\n{'loss': 0.2999, 'grad_norm': 0.9578970670700073, 'learning_rate': 2.666450605538002e-06, 'epoch': 3.9318181818181817}\n{'loss': 0.3331, 'grad_norm': 1.0067633390426636, 'learning_rate': 1.9998379541535016e-06, 'epoch': 3.9545454545454546}\n{'loss': 0.2882, 'grad_norm': 0.9324207901954651, 'learning_rate': 1.333225302769001e-06, 'epoch': 3.9772727272727275}\n{'loss': 0.3281, 'grad_norm': 2.5222673416137695, 'learning_rate': 6.666126513845005e-07, 'epoch': 4.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8295    0.7806    0.8043      3404\n     PERCENT     0.7473    0.9577    0.8395        71\n        TYPE     0.9073    0.9621    0.9339     11194\n      VOLUME     0.4211    0.4286    0.4248        56\n\n   micro avg     0.8882    0.9181    0.9029     14725\n   macro avg     0.7263    0.7822    0.7506     14725\nweighted avg     0.8867    0.9181    0.9016     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.34987306594848633, 'eval_f1_macro': 0.7506266005245008, 'eval_precision': 0.8881808028381841, 'eval_recall': 0.9180984719864177, 'eval_f1': 0.9028918720363323, 'eval_accuracy': 0.8982075316559777, 'eval_runtime': 1.4409, 'eval_samples_per_second': 3824.729, 'eval_steps_per_second': 7.634, 'epoch': 4.0}\n{'train_runtime': 24.3345, 'train_samples_per_second': 3623.008, 'train_steps_per_second': 7.233, 'train_loss': 0.5865082866935567, 'epoch': 4.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nThe speedups for torchdynamo mostly come with GPU Ampere or higher and which is not detected here.\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8295    0.7806    0.8043      3404\n     PERCENT     0.7473    0.9577    0.8395        71\n        TYPE     0.9073    0.9621    0.9339     11194\n      VOLUME     0.4211    0.4286    0.4248        56\n\n   micro avg     0.8882    0.9181    0.9029     14725\n   macro avg     0.7263    0.7822    0.7506     14725\nweighted avg     0.8867    0.9181    0.9016     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.34987306594848633, 'eval_f1_macro': 0.7506266005245008, 'eval_precision': 0.8881808028381841, 'eval_recall': 0.9180984719864177, 'eval_f1': 0.9028918720363323, 'eval_accuracy': 0.8982075316559777, 'eval_runtime': 1.5247, 'eval_samples_per_second': 3614.373, 'eval_steps_per_second': 7.214, 'epoch': 4.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/3364797558.py:66: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 2.1859, 'grad_norm': 6.638328552246094, 'learning_rate': 0.0, 'epoch': 0.022727272727272728}\n{'loss': 2.193, 'grad_norm': 6.709334850311279, 'learning_rate': 5.851377717708392e-06, 'epoch': 0.045454545454545456}\n{'loss': 2.1857, 'grad_norm': 6.708355903625488, 'learning_rate': 1.1702755435416784e-05, 'epoch': 0.06818181818181818}\n{'loss': 2.1507, 'grad_norm': 6.458714485168457, 'learning_rate': 1.755413315312518e-05, 'epoch': 0.09090909090909091}\n{'loss': 2.1224, 'grad_norm': 6.4665961265563965, 'learning_rate': 2.340551087083357e-05, 'epoch': 0.11363636363636363}\n{'loss': 2.0691, 'grad_norm': 6.734390735626221, 'learning_rate': 2.9256888588541966e-05, 'epoch': 0.13636363636363635}\n{'loss': 2.0302, 'grad_norm': 5.987710475921631, 'learning_rate': 3.510826630625036e-05, 'epoch': 0.1590909090909091}\n{'loss': 1.9725, 'grad_norm': 6.137629985809326, 'learning_rate': 4.095964402395875e-05, 'epoch': 0.18181818181818182}\n{'loss': 1.8986, 'grad_norm': 5.926784992218018, 'learning_rate': 4.681102174166714e-05, 'epoch': 0.20454545454545456}\n{'loss': 1.8198, 'grad_norm': 5.67391300201416, 'learning_rate': 5.2662399459375535e-05, 'epoch': 0.22727272727272727}\n{'loss': 1.7595, 'grad_norm': 5.159557819366455, 'learning_rate': 5.851377717708393e-05, 'epoch': 0.25}\n{'loss': 1.673, 'grad_norm': 4.908365726470947, 'learning_rate': 6.436515489479233e-05, 'epoch': 0.2727272727272727}\n{'loss': 1.5637, 'grad_norm': 4.6135783195495605, 'learning_rate': 7.021653261250071e-05, 'epoch': 0.29545454545454547}\n{'loss': 1.4892, 'grad_norm': 4.037976264953613, 'learning_rate': 7.606791033020911e-05, 'epoch': 0.3181818181818182}\n{'loss': 1.4413, 'grad_norm': 3.3773488998413086, 'learning_rate': 8.19192880479175e-05, 'epoch': 0.3409090909090909}\n{'loss': 1.3172, 'grad_norm': 3.0557165145874023, 'learning_rate': 8.777066576562589e-05, 'epoch': 0.36363636363636365}\n{'loss': 1.268, 'grad_norm': 2.419921398162842, 'learning_rate': 9.362204348333428e-05, 'epoch': 0.38636363636363635}\n{'loss': 1.1779, 'grad_norm': 2.0107357501983643, 'learning_rate': 9.947342120104267e-05, 'epoch': 0.4090909090909091}\n{'loss': 1.1365, 'grad_norm': 1.6935710906982422, 'learning_rate': 0.00010532479891875107, 'epoch': 0.4318181818181818}\n{'loss': 1.1416, 'grad_norm': 1.7577049732208252, 'learning_rate': 0.00010465818626736656, 'epoch': 0.45454545454545453}\n{'loss': 1.0631, 'grad_norm': 1.453685998916626, 'learning_rate': 0.00010399157361598208, 'epoch': 0.4772727272727273}\n{'loss': 1.0418, 'grad_norm': 1.3373439311981201, 'learning_rate': 0.00010332496096459757, 'epoch': 0.5}\n{'loss': 1.0051, 'grad_norm': 1.1587345600128174, 'learning_rate': 0.00010265834831321306, 'epoch': 0.5227272727272727}\n{'loss': 0.9343, 'grad_norm': 1.261723518371582, 'learning_rate': 0.00010199173566182858, 'epoch': 0.5454545454545454}\n{'loss': 0.9644, 'grad_norm': 1.782395601272583, 'learning_rate': 0.00010132512301044407, 'epoch': 0.5681818181818182}\n{'loss': 0.9675, 'grad_norm': 1.5167425870895386, 'learning_rate': 0.00010065851035905956, 'epoch': 0.5909090909090909}\n{'loss': 0.9674, 'grad_norm': 1.36681067943573, 'learning_rate': 9.999189770767507e-05, 'epoch': 0.6136363636363636}\n{'loss': 0.907, 'grad_norm': 1.1271940469741821, 'learning_rate': 9.932528505629057e-05, 'epoch': 0.6363636363636364}\n{'loss': 0.8577, 'grad_norm': 1.2921174764633179, 'learning_rate': 9.865867240490606e-05, 'epoch': 0.6590909090909091}\n{'loss': 0.8293, 'grad_norm': 0.9955193996429443, 'learning_rate': 9.799205975352157e-05, 'epoch': 0.6818181818181818}\n{'loss': 0.8409, 'grad_norm': 1.1356396675109863, 'learning_rate': 9.732544710213707e-05, 'epoch': 0.7045454545454546}\n{'loss': 0.8987, 'grad_norm': 1.6239341497421265, 'learning_rate': 9.665883445075256e-05, 'epoch': 0.7272727272727273}\n{'loss': 0.7822, 'grad_norm': 0.7441524267196655, 'learning_rate': 9.599222179936806e-05, 'epoch': 0.75}\n{'loss': 0.7693, 'grad_norm': 0.8383941650390625, 'learning_rate': 9.532560914798357e-05, 'epoch': 0.7727272727272727}\n{'loss': 0.8445, 'grad_norm': 0.8141987919807434, 'learning_rate': 9.465899649659906e-05, 'epoch': 0.7954545454545454}\n{'loss': 0.7641, 'grad_norm': 0.9322550892829895, 'learning_rate': 9.399238384521456e-05, 'epoch': 0.8181818181818182}\n{'loss': 0.6655, 'grad_norm': 1.0964155197143555, 'learning_rate': 9.332577119383006e-05, 'epoch': 0.8409090909090909}\n{'loss': 0.7085, 'grad_norm': 0.8995802402496338, 'learning_rate': 9.265915854244556e-05, 'epoch': 0.8636363636363636}\n{'loss': 0.688, 'grad_norm': 0.8315728306770325, 'learning_rate': 9.199254589106106e-05, 'epoch': 0.8863636363636364}\n{'loss': 0.6571, 'grad_norm': 0.869720458984375, 'learning_rate': 9.132593323967656e-05, 'epoch': 0.9090909090909091}\n{'loss': 0.6115, 'grad_norm': 0.956270158290863, 'learning_rate': 9.065932058829207e-05, 'epoch': 0.9318181818181818}\n{'loss': 0.6258, 'grad_norm': 0.7583837509155273, 'learning_rate': 8.999270793690756e-05, 'epoch': 0.9545454545454546}\n{'loss': 0.6657, 'grad_norm': 1.131469488143921, 'learning_rate': 8.932609528552306e-05, 'epoch': 0.9772727272727273}\n{'loss': 0.7383, 'grad_norm': 2.450622320175171, 'learning_rate': 8.865948263413857e-05, 'epoch': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.6822    0.6741    0.6781      3311\n     PERCENT     1.0000    0.0349    0.0674        86\n        TYPE     0.8301    0.9020    0.8646     11299\n      VOLUME     0.0000    0.0000    0.0000        42\n\n   micro avg     0.7990    0.8432    0.8205     14738\n   macro avg     0.6281    0.4028    0.4025     14738\nweighted avg     0.7955    0.8432    0.8156     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.5807954668998718, 'eval_f1_macro': 0.4025243546842069, 'eval_precision': 0.7990098373304185, 'eval_recall': 0.8431944632921698, 'eval_f1': 0.8205077415734046, 'eval_accuracy': 0.8235615540134419, 'eval_runtime': 1.4353, 'eval_samples_per_second': 3838.972, 'eval_steps_per_second': 7.664, 'epoch': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.5901, 'grad_norm': 1.1315943002700806, 'learning_rate': 8.799286998275406e-05, 'epoch': 1.0227272727272727}\n{'loss': 0.6093, 'grad_norm': 0.8721634745597839, 'learning_rate': 8.732625733136956e-05, 'epoch': 1.0454545454545454}\n{'loss': 0.5939, 'grad_norm': 0.9521905183792114, 'learning_rate': 8.665964467998507e-05, 'epoch': 1.0681818181818181}\n{'loss': 0.5744, 'grad_norm': 0.7489674687385559, 'learning_rate': 8.599303202860056e-05, 'epoch': 1.0909090909090908}\n{'loss': 0.5399, 'grad_norm': 1.0063300132751465, 'learning_rate': 8.532641937721606e-05, 'epoch': 1.1136363636363635}\n{'loss': 0.618, 'grad_norm': 1.080849289894104, 'learning_rate': 8.465980672583155e-05, 'epoch': 1.1363636363636362}\n{'loss': 0.4718, 'grad_norm': 0.7861858606338501, 'learning_rate': 8.399319407444706e-05, 'epoch': 1.1590909090909092}\n{'loss': 0.5423, 'grad_norm': 0.8475505709648132, 'learning_rate': 8.332658142306255e-05, 'epoch': 1.1818181818181819}\n{'loss': 0.5227, 'grad_norm': 0.620098352432251, 'learning_rate': 8.265996877167805e-05, 'epoch': 1.2045454545454546}\n{'loss': 0.5797, 'grad_norm': 0.719285786151886, 'learning_rate': 8.199335612029356e-05, 'epoch': 1.2272727272727273}\n{'loss': 0.5554, 'grad_norm': 0.896011233329773, 'learning_rate': 8.132674346890905e-05, 'epoch': 1.25}\n{'loss': 0.5175, 'grad_norm': 0.8312715888023376, 'learning_rate': 8.066013081752455e-05, 'epoch': 1.2727272727272727}\n{'loss': 0.4928, 'grad_norm': 0.9375213980674744, 'learning_rate': 7.999351816614006e-05, 'epoch': 1.2954545454545454}\n{'loss': 0.5676, 'grad_norm': 0.9107185006141663, 'learning_rate': 7.932690551475555e-05, 'epoch': 1.3181818181818181}\n{'loss': 0.4623, 'grad_norm': 1.2203747034072876, 'learning_rate': 7.866029286337106e-05, 'epoch': 1.3409090909090908}\n{'loss': 0.4999, 'grad_norm': 0.7431745529174805, 'learning_rate': 7.799368021198655e-05, 'epoch': 1.3636363636363638}\n{'loss': 0.5095, 'grad_norm': 0.9735323190689087, 'learning_rate': 7.732706756060205e-05, 'epoch': 1.3863636363636362}\n{'loss': 0.4868, 'grad_norm': 0.8933054804801941, 'learning_rate': 7.666045490921756e-05, 'epoch': 1.4090909090909092}\n{'loss': 0.4764, 'grad_norm': 0.7622267007827759, 'learning_rate': 7.599384225783305e-05, 'epoch': 1.4318181818181819}\n{'loss': 0.4573, 'grad_norm': 0.8742111921310425, 'learning_rate': 7.532722960644855e-05, 'epoch': 1.4545454545454546}\n{'loss': 0.4398, 'grad_norm': 0.9605450630187988, 'learning_rate': 7.466061695506406e-05, 'epoch': 1.4772727272727273}\n{'loss': 0.4763, 'grad_norm': 0.7232942581176758, 'learning_rate': 7.399400430367955e-05, 'epoch': 1.5}\n{'loss': 0.4469, 'grad_norm': 0.7828166484832764, 'learning_rate': 7.332739165229505e-05, 'epoch': 1.5227272727272727}\n{'loss': 0.425, 'grad_norm': 0.969081699848175, 'learning_rate': 7.266077900091054e-05, 'epoch': 1.5454545454545454}\n{'loss': 0.4547, 'grad_norm': 1.2994543313980103, 'learning_rate': 7.199416634952605e-05, 'epoch': 1.5681818181818183}\n{'loss': 0.4258, 'grad_norm': 1.0239582061767578, 'learning_rate': 7.132755369814154e-05, 'epoch': 1.5909090909090908}\n{'loss': 0.4617, 'grad_norm': 0.819246768951416, 'learning_rate': 7.066094104675704e-05, 'epoch': 1.6136363636363638}\n{'loss': 0.46, 'grad_norm': 0.8991714715957642, 'learning_rate': 6.999432839537255e-05, 'epoch': 1.6363636363636362}\n{'loss': 0.4232, 'grad_norm': 0.958899199962616, 'learning_rate': 6.932771574398804e-05, 'epoch': 1.6590909090909092}\n{'loss': 0.4549, 'grad_norm': 1.3226417303085327, 'learning_rate': 6.866110309260354e-05, 'epoch': 1.6818181818181817}\n{'loss': 0.4318, 'grad_norm': 1.255859375, 'learning_rate': 6.799449044121905e-05, 'epoch': 1.7045454545454546}\n{'loss': 0.4487, 'grad_norm': 1.3182079792022705, 'learning_rate': 6.732787778983454e-05, 'epoch': 1.7272727272727273}\n{'loss': 0.4422, 'grad_norm': 0.8880485892295837, 'learning_rate': 6.666126513845004e-05, 'epoch': 1.75}\n{'loss': 0.4375, 'grad_norm': 0.7637177109718323, 'learning_rate': 6.599465248706555e-05, 'epoch': 1.7727272727272727}\n{'loss': 0.3756, 'grad_norm': 1.3658790588378906, 'learning_rate': 6.532803983568104e-05, 'epoch': 1.7954545454545454}\n{'loss': 0.4153, 'grad_norm': 1.1549391746520996, 'learning_rate': 6.466142718429655e-05, 'epoch': 1.8181818181818183}\n{'loss': 0.3559, 'grad_norm': 0.6944644451141357, 'learning_rate': 6.399481453291205e-05, 'epoch': 1.8409090909090908}\n{'loss': 0.4241, 'grad_norm': 0.9532603025436401, 'learning_rate': 6.332820188152754e-05, 'epoch': 1.8636363636363638}\n{'loss': 0.4308, 'grad_norm': 0.7845696210861206, 'learning_rate': 6.266158923014303e-05, 'epoch': 1.8863636363636362}\n{'loss': 0.4658, 'grad_norm': 1.0031695365905762, 'learning_rate': 6.199497657875855e-05, 'epoch': 1.9090909090909092}\n{'loss': 0.4625, 'grad_norm': 0.9402486681938171, 'learning_rate': 6.132836392737404e-05, 'epoch': 1.9318181818181817}\n{'loss': 0.4363, 'grad_norm': 1.0195198059082031, 'learning_rate': 6.066175127598954e-05, 'epoch': 1.9545454545454546}\n{'loss': 0.3652, 'grad_norm': 0.6792839169502258, 'learning_rate': 5.9995138624605045e-05, 'epoch': 1.9772727272727273}\n{'loss': 0.3286, 'grad_norm': 3.863676071166992, 'learning_rate': 5.932852597322054e-05, 'epoch': 2.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8093    0.7650    0.7865      3311\n     PERCENT     0.5263    0.9302    0.6723        86\n        TYPE     0.9065    0.9495    0.9275     11299\n      VOLUME     0.0000    0.0000    0.0000        42\n\n   micro avg     0.8825    0.9052    0.8937     14738\n   macro avg     0.5605    0.6612    0.5966     14738\nweighted avg     0.8799    0.9052    0.8917     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.3790608048439026, 'eval_f1_macro': 0.5965747248243253, 'eval_precision': 0.8824579970895621, 'eval_recall': 0.9052110191342109, 'eval_f1': 0.8936897106109325, 'eval_accuracy': 0.8866728594065898, 'eval_runtime': 1.4372, 'eval_samples_per_second': 3833.865, 'eval_steps_per_second': 7.654, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.3656, 'grad_norm': 0.8663960695266724, 'learning_rate': 5.8661913321836034e-05, 'epoch': 2.022727272727273}\n{'loss': 0.3725, 'grad_norm': 1.0404480695724487, 'learning_rate': 5.7995300670451545e-05, 'epoch': 2.0454545454545454}\n{'loss': 0.4006, 'grad_norm': 0.9644685983657837, 'learning_rate': 5.732868801906704e-05, 'epoch': 2.0681818181818183}\n{'loss': 0.4285, 'grad_norm': 0.836452066898346, 'learning_rate': 5.6662075367682534e-05, 'epoch': 2.090909090909091}\n{'loss': 0.3806, 'grad_norm': 0.7536564469337463, 'learning_rate': 5.599546271629803e-05, 'epoch': 2.1136363636363638}\n{'loss': 0.2917, 'grad_norm': 0.8854473233222961, 'learning_rate': 5.532885006491354e-05, 'epoch': 2.1363636363636362}\n{'loss': 0.37, 'grad_norm': 0.7791414260864258, 'learning_rate': 5.4662237413529035e-05, 'epoch': 2.159090909090909}\n{'loss': 0.3985, 'grad_norm': 1.1177654266357422, 'learning_rate': 5.399562476214453e-05, 'epoch': 2.1818181818181817}\n{'loss': 0.3891, 'grad_norm': 0.8863660097122192, 'learning_rate': 5.332901211076004e-05, 'epoch': 2.2045454545454546}\n{'loss': 0.4207, 'grad_norm': 1.1703444719314575, 'learning_rate': 5.2662399459375535e-05, 'epoch': 2.227272727272727}\n{'loss': 0.3985, 'grad_norm': 0.7663838863372803, 'learning_rate': 5.199578680799104e-05, 'epoch': 2.25}\n{'loss': 0.3513, 'grad_norm': 0.8671143651008606, 'learning_rate': 5.132917415660653e-05, 'epoch': 2.2727272727272725}\n{'loss': 0.4539, 'grad_norm': 1.0740532875061035, 'learning_rate': 5.0662561505222036e-05, 'epoch': 2.2954545454545454}\n{'loss': 0.3738, 'grad_norm': 1.1611213684082031, 'learning_rate': 4.9995948853837534e-05, 'epoch': 2.3181818181818183}\n{'loss': 0.2873, 'grad_norm': 0.9332537055015564, 'learning_rate': 4.932933620245303e-05, 'epoch': 2.340909090909091}\n{'loss': 0.3496, 'grad_norm': 0.7252811193466187, 'learning_rate': 4.8662723551068536e-05, 'epoch': 2.3636363636363638}\n{'loss': 0.3211, 'grad_norm': 0.6570513248443604, 'learning_rate': 4.799611089968403e-05, 'epoch': 2.3863636363636362}\n{'loss': 0.4178, 'grad_norm': 0.8873445391654968, 'learning_rate': 4.732949824829953e-05, 'epoch': 2.409090909090909}\n{'loss': 0.3551, 'grad_norm': 0.9928715825080872, 'learning_rate': 4.666288559691503e-05, 'epoch': 2.4318181818181817}\n{'loss': 0.3023, 'grad_norm': 0.7881658673286438, 'learning_rate': 4.599627294553053e-05, 'epoch': 2.4545454545454546}\n{'loss': 0.3466, 'grad_norm': 0.9760343432426453, 'learning_rate': 4.532966029414603e-05, 'epoch': 2.4772727272727275}\n{'loss': 0.3467, 'grad_norm': 0.7994053363800049, 'learning_rate': 4.466304764276153e-05, 'epoch': 2.5}\n{'loss': 0.3526, 'grad_norm': 1.0227417945861816, 'learning_rate': 4.399643499137703e-05, 'epoch': 2.5227272727272725}\n{'loss': 0.3527, 'grad_norm': 0.7901764512062073, 'learning_rate': 4.332982233999253e-05, 'epoch': 2.5454545454545454}\n{'loss': 0.3361, 'grad_norm': 1.0070624351501465, 'learning_rate': 4.266320968860803e-05, 'epoch': 2.5681818181818183}\n{'loss': 0.3366, 'grad_norm': 1.7167103290557861, 'learning_rate': 4.199659703722353e-05, 'epoch': 2.590909090909091}\n{'loss': 0.3578, 'grad_norm': 0.9075615406036377, 'learning_rate': 4.132998438583903e-05, 'epoch': 2.6136363636363638}\n{'loss': 0.3856, 'grad_norm': 1.6510134935379028, 'learning_rate': 4.0663371734454525e-05, 'epoch': 2.6363636363636362}\n{'loss': 0.3417, 'grad_norm': 0.9259747266769409, 'learning_rate': 3.999675908307003e-05, 'epoch': 2.659090909090909}\n{'loss': 0.2726, 'grad_norm': 1.0526219606399536, 'learning_rate': 3.933014643168553e-05, 'epoch': 2.6818181818181817}\n{'loss': 0.3067, 'grad_norm': 0.6838200688362122, 'learning_rate': 3.8663533780301026e-05, 'epoch': 2.7045454545454546}\n{'loss': 0.4068, 'grad_norm': 0.8716959953308105, 'learning_rate': 3.7996921128916523e-05, 'epoch': 2.7272727272727275}\n{'loss': 0.3441, 'grad_norm': 0.872789740562439, 'learning_rate': 3.733030847753203e-05, 'epoch': 2.75}\n{'loss': 0.3823, 'grad_norm': 1.979337453842163, 'learning_rate': 3.6663695826147526e-05, 'epoch': 2.7727272727272725}\n{'loss': 0.3383, 'grad_norm': 1.1842445135116577, 'learning_rate': 3.5997083174763024e-05, 'epoch': 2.7954545454545454}\n{'loss': 0.3978, 'grad_norm': 1.9683430194854736, 'learning_rate': 3.533047052337852e-05, 'epoch': 2.8181818181818183}\n{'loss': 0.3846, 'grad_norm': 1.3664406538009644, 'learning_rate': 3.466385787199402e-05, 'epoch': 2.840909090909091}\n{'loss': 0.3141, 'grad_norm': 0.7077093124389648, 'learning_rate': 3.3997245220609525e-05, 'epoch': 2.8636363636363638}\n{'loss': 0.3448, 'grad_norm': 0.8222599625587463, 'learning_rate': 3.333063256922502e-05, 'epoch': 2.8863636363636362}\n{'loss': 0.3214, 'grad_norm': 0.8168908357620239, 'learning_rate': 3.266401991784052e-05, 'epoch': 2.909090909090909}\n{'loss': 0.3688, 'grad_norm': 0.9618103504180908, 'learning_rate': 3.1997407266456025e-05, 'epoch': 2.9318181818181817}\n{'loss': 0.3547, 'grad_norm': 0.9016295671463013, 'learning_rate': 3.1330794615071516e-05, 'epoch': 2.9545454545454546}\n{'loss': 0.3615, 'grad_norm': 0.850643515586853, 'learning_rate': 3.066418196368702e-05, 'epoch': 2.9772727272727275}\n{'loss': 0.2301, 'grad_norm': 3.0996859073638916, 'learning_rate': 2.9997569312302522e-05, 'epoch': 3.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8193    0.8188    0.8190      3311\n     PERCENT     0.6320    0.9186    0.7488        86\n        TYPE     0.9195    0.9573    0.9380     11299\n      VOLUME     0.4286    0.2857    0.3429        42\n\n   micro avg     0.8945    0.9241    0.9090     14738\n   macro avg     0.6998    0.7451    0.7122     14738\nweighted avg     0.8939    0.9241    0.9085     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.33237624168395996, 'eval_f1_macro': 0.7121862062658714, 'eval_precision': 0.8944568501247866, 'eval_recall': 0.924073822771068, 'eval_f1': 0.9090241623281271, 'eval_accuracy': 0.9029014807933993, 'eval_runtime': 1.8313, 'eval_samples_per_second': 3008.829, 'eval_steps_per_second': 6.007, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.2996, 'grad_norm': 0.8451358079910278, 'learning_rate': 2.9330956660918017e-05, 'epoch': 3.022727272727273}\n{'loss': 0.3389, 'grad_norm': 1.245808720588684, 'learning_rate': 2.866434400953352e-05, 'epoch': 3.0454545454545454}\n{'loss': 0.3317, 'grad_norm': 0.8709924221038818, 'learning_rate': 2.7997731358149016e-05, 'epoch': 3.0681818181818183}\n{'loss': 0.3249, 'grad_norm': 0.9658929109573364, 'learning_rate': 2.7331118706764517e-05, 'epoch': 3.090909090909091}\n{'loss': 0.3418, 'grad_norm': 0.9131034016609192, 'learning_rate': 2.666450605538002e-05, 'epoch': 3.1136363636363638}\n{'loss': 0.2897, 'grad_norm': 1.2826428413391113, 'learning_rate': 2.599789340399552e-05, 'epoch': 3.1363636363636362}\n{'loss': 0.3674, 'grad_norm': 1.1664239168167114, 'learning_rate': 2.5331280752611018e-05, 'epoch': 3.159090909090909}\n{'loss': 0.2789, 'grad_norm': 0.6716945171356201, 'learning_rate': 2.4664668101226516e-05, 'epoch': 3.1818181818181817}\n{'loss': 0.3338, 'grad_norm': 1.131143569946289, 'learning_rate': 2.3998055449842014e-05, 'epoch': 3.2045454545454546}\n{'loss': 0.336, 'grad_norm': 1.160888671875, 'learning_rate': 2.3331442798457515e-05, 'epoch': 3.227272727272727}\n{'loss': 0.3377, 'grad_norm': 0.6502121090888977, 'learning_rate': 2.2664830147073016e-05, 'epoch': 3.25}\n{'loss': 0.3128, 'grad_norm': 0.6397055387496948, 'learning_rate': 2.1998217495688514e-05, 'epoch': 3.2727272727272725}\n{'loss': 0.286, 'grad_norm': 0.715739369392395, 'learning_rate': 2.1331604844304016e-05, 'epoch': 3.2954545454545454}\n{'loss': 0.3711, 'grad_norm': 0.7491579055786133, 'learning_rate': 2.0664992192919514e-05, 'epoch': 3.3181818181818183}\n{'loss': 0.3119, 'grad_norm': 0.7035548090934753, 'learning_rate': 1.9998379541535015e-05, 'epoch': 3.340909090909091}\n{'loss': 0.2814, 'grad_norm': 0.8206082582473755, 'learning_rate': 1.9331766890150513e-05, 'epoch': 3.3636363636363638}\n{'loss': 0.2821, 'grad_norm': 0.7728168964385986, 'learning_rate': 1.8665154238766014e-05, 'epoch': 3.3863636363636362}\n{'loss': 0.3369, 'grad_norm': 0.7534307837486267, 'learning_rate': 1.7998541587381512e-05, 'epoch': 3.409090909090909}\n{'loss': 0.2915, 'grad_norm': 1.1564406156539917, 'learning_rate': 1.733192893599701e-05, 'epoch': 3.4318181818181817}\n{'loss': 0.3353, 'grad_norm': 0.7743235230445862, 'learning_rate': 1.666531628461251e-05, 'epoch': 3.4545454545454546}\n{'loss': 0.2779, 'grad_norm': 0.8011021018028259, 'learning_rate': 1.5998703633228013e-05, 'epoch': 3.4772727272727275}\n{'loss': 0.2966, 'grad_norm': 0.7296733856201172, 'learning_rate': 1.533209098184351e-05, 'epoch': 3.5}\n{'loss': 0.3378, 'grad_norm': 0.9243581891059875, 'learning_rate': 1.4665478330459008e-05, 'epoch': 3.5227272727272725}\n{'loss': 0.3264, 'grad_norm': 1.0186121463775635, 'learning_rate': 1.3998865679074508e-05, 'epoch': 3.5454545454545454}\n{'loss': 0.3155, 'grad_norm': 1.3659981489181519, 'learning_rate': 1.333225302769001e-05, 'epoch': 3.5681818181818183}\n{'loss': 0.342, 'grad_norm': 0.747199296951294, 'learning_rate': 1.2665640376305509e-05, 'epoch': 3.590909090909091}\n{'loss': 0.3324, 'grad_norm': 0.8548928499221802, 'learning_rate': 1.1999027724921007e-05, 'epoch': 3.6136363636363638}\n{'loss': 0.2969, 'grad_norm': 0.7760257124900818, 'learning_rate': 1.1332415073536508e-05, 'epoch': 3.6363636363636362}\n{'loss': 0.3092, 'grad_norm': 0.8222271203994751, 'learning_rate': 1.0665802422152008e-05, 'epoch': 3.659090909090909}\n{'loss': 0.3097, 'grad_norm': 0.843411386013031, 'learning_rate': 9.999189770767507e-06, 'epoch': 3.6818181818181817}\n{'loss': 0.296, 'grad_norm': 0.8212318420410156, 'learning_rate': 9.332577119383007e-06, 'epoch': 3.7045454545454546}\n{'loss': 0.3168, 'grad_norm': 0.9294070601463318, 'learning_rate': 8.665964467998505e-06, 'epoch': 3.7272727272727275}\n{'loss': 0.365, 'grad_norm': 0.9442683458328247, 'learning_rate': 7.999351816614006e-06, 'epoch': 3.75}\n{'loss': 0.2954, 'grad_norm': 1.0482755899429321, 'learning_rate': 7.332739165229504e-06, 'epoch': 3.7727272727272725}\n{'loss': 0.3585, 'grad_norm': 0.8862652778625488, 'learning_rate': 6.666126513845005e-06, 'epoch': 3.7954545454545454}\n{'loss': 0.3117, 'grad_norm': 0.8069753646850586, 'learning_rate': 5.9995138624605034e-06, 'epoch': 3.8181818181818183}\n{'loss': 0.364, 'grad_norm': 1.1945433616638184, 'learning_rate': 5.332901211076004e-06, 'epoch': 3.840909090909091}\n{'loss': 0.3436, 'grad_norm': 1.0980987548828125, 'learning_rate': 4.6662885596915035e-06, 'epoch': 3.8636363636363638}\n{'loss': 0.2658, 'grad_norm': 0.8067209124565125, 'learning_rate': 3.999675908307003e-06, 'epoch': 3.8863636363636362}\n{'loss': 0.322, 'grad_norm': 0.7018123865127563, 'learning_rate': 3.3330632569225023e-06, 'epoch': 3.909090909090909}\n{'loss': 0.3471, 'grad_norm': 1.0264297723770142, 'learning_rate': 2.666450605538002e-06, 'epoch': 3.9318181818181817}\n{'loss': 0.3295, 'grad_norm': 0.7198464274406433, 'learning_rate': 1.9998379541535016e-06, 'epoch': 3.9545454545454546}\n{'loss': 0.3095, 'grad_norm': 0.8534238338470459, 'learning_rate': 1.333225302769001e-06, 'epoch': 3.9772727272727275}\n{'loss': 0.0526, 'grad_norm': 1.5464951992034912, 'learning_rate': 6.666126513845005e-07, 'epoch': 4.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8407    0.8130    0.8267      3311\n     PERCENT     0.6752    0.9186    0.7783        86\n        TYPE     0.9203    0.9620    0.9407     11299\n      VOLUME     0.4412    0.3571    0.3947        42\n\n   micro avg     0.9006    0.9266    0.9134     14738\n   macro avg     0.7194    0.7627    0.7351     14738\nweighted avg     0.8996    0.9266    0.9126     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.3234369158744812, 'eval_f1_macro': 0.7351086631407969, 'eval_precision': 0.9005539435505143, 'eval_recall': 0.9265843398018727, 'eval_f1': 0.9133837201524981, 'eval_accuracy': 0.9065624829244303, 'eval_runtime': 1.4286, 'eval_samples_per_second': 3856.95, 'eval_steps_per_second': 7.7, 'epoch': 4.0}\n{'train_runtime': 24.3802, 'train_samples_per_second': 3616.38, 'train_steps_per_second': 7.219, 'train_loss': 0.5958778605034406, 'epoch': 4.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n[I 2025-09-26 18:13:21,869] Trial 6 pruned. \n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8407    0.8130    0.8267      3311\n     PERCENT     0.6752    0.9186    0.7783        86\n        TYPE     0.9203    0.9620    0.9407     11299\n      VOLUME     0.4412    0.3571    0.3947        42\n\n   micro avg     0.9006    0.9266    0.9134     14738\n   macro avg     0.7194    0.7627    0.7351     14738\nweighted avg     0.8996    0.9266    0.9126     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.3234369158744812, 'eval_f1_macro': 0.7351086631407969, 'eval_precision': 0.9005539435505143, 'eval_recall': 0.9265843398018727, 'eval_f1': 0.9133837201524981, 'eval_accuracy': 0.9065624829244303, 'eval_runtime': 1.5017, 'eval_samples_per_second': 3669.108, 'eval_steps_per_second': 7.325, 'epoch': 4.0}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/27552 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"188f84ab9d064b49bbc887cb81b58e1c"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nThe speedups for torchdynamo mostly come with GPU Ampere or higher and which is not detected here.\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/tmp/ipykernel_36/3364797558.py:66: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 2.3574, 'grad_norm': 7.250397205352783, 'learning_rate': 0.0, 'epoch': 0.022727272727272728}\n{'loss': 2.352, 'grad_norm': 7.356775760650635, 'learning_rate': 1.1528509127345877e-05, 'epoch': 0.045454545454545456}\n{'loss': 2.3131, 'grad_norm': 7.243195533752441, 'learning_rate': 2.3057018254691754e-05, 'epoch': 0.06818181818181818}\n{'loss': 2.2743, 'grad_norm': 7.415351867675781, 'learning_rate': 3.458552738203763e-05, 'epoch': 0.09090909090909091}\n{'loss': 2.1917, 'grad_norm': 6.953155040740967, 'learning_rate': 4.611403650938351e-05, 'epoch': 0.11363636363636363}\n{'loss': 2.093, 'grad_norm': 6.789633750915527, 'learning_rate': 5.76425456367294e-05, 'epoch': 0.13636363636363635}\n{'loss': 1.9915, 'grad_norm': 6.3507080078125, 'learning_rate': 6.917105476407527e-05, 'epoch': 0.1590909090909091}\n{'loss': 1.8296, 'grad_norm': 6.200675964355469, 'learning_rate': 8.069956389142115e-05, 'epoch': 0.18181818181818182}\n{'loss': 1.6913, 'grad_norm': 5.473141670227051, 'learning_rate': 9.222807301876702e-05, 'epoch': 0.20454545454545456}\n{'loss': 1.5193, 'grad_norm': 4.8218793869018555, 'learning_rate': 0.0001037565821461129, 'epoch': 0.22727272727272727}\n{'loss': 1.343, 'grad_norm': 3.9860310554504395, 'learning_rate': 0.0001152850912734588, 'epoch': 0.25}\n{'loss': 1.2435, 'grad_norm': 2.7379863262176514, 'learning_rate': 0.00012681360040080468, 'epoch': 0.2727272727272727}\n{'loss': 1.1903, 'grad_norm': 1.8327410221099854, 'learning_rate': 0.00013834210952815053, 'epoch': 0.29545454545454547}\n{'loss': 1.1805, 'grad_norm': 1.95756196975708, 'learning_rate': 0.0001498706186554964, 'epoch': 0.3181818181818182}\n{'loss': 1.1475, 'grad_norm': 2.1493752002716064, 'learning_rate': 0.0001613991277828423, 'epoch': 0.3409090909090909}\n{'loss': 1.1409, 'grad_norm': 2.1091597080230713, 'learning_rate': 0.00017292763691018818, 'epoch': 0.36363636363636365}\n{'loss': 1.0436, 'grad_norm': 1.1993889808654785, 'learning_rate': 0.00018445614603753403, 'epoch': 0.38636363636363635}\n{'loss': 1.0084, 'grad_norm': 1.3296701908111572, 'learning_rate': 0.00019598465516487993, 'epoch': 0.4090909090909091}\n{'loss': 1.0067, 'grad_norm': 1.7998287677764893, 'learning_rate': 0.0002075131642922258, 'epoch': 0.4318181818181818}\n{'loss': 0.9527, 'grad_norm': 2.279337167739868, 'learning_rate': 0.00021904167341957169, 'epoch': 0.45454545454545453}\n{'loss': 0.955, 'grad_norm': 2.110239267349243, 'learning_rate': 0.0002305701825469176, 'epoch': 0.4772727272727273}\n{'loss': 0.8509, 'grad_norm': 1.4622232913970947, 'learning_rate': 0.00024209869167426346, 'epoch': 0.5}\n{'loss': 0.8606, 'grad_norm': 1.1586601734161377, 'learning_rate': 0.00025362720080160937, 'epoch': 0.5227272727272727}\n{'loss': 0.8027, 'grad_norm': 1.187408447265625, 'learning_rate': 0.0002651557099289552, 'epoch': 0.5454545454545454}\n{'loss': 0.7782, 'grad_norm': 0.843745231628418, 'learning_rate': 0.00027668421905630106, 'epoch': 0.5681818181818182}\n{'loss': 0.728, 'grad_norm': 0.9011489152908325, 'learning_rate': 0.00028821272818364694, 'epoch': 0.5909090909090909}\n{'loss': 0.6367, 'grad_norm': 1.8674534559249878, 'learning_rate': 0.0002997412373109928, 'epoch': 0.6136363636363636}\n{'loss': 0.6321, 'grad_norm': 1.3044308423995972, 'learning_rate': 0.00031126974643833874, 'epoch': 0.6363636363636364}\n{'loss': 0.6674, 'grad_norm': 0.9825687408447266, 'learning_rate': 0.0003227982555656846, 'epoch': 0.6590909090909091}\n{'loss': 0.5826, 'grad_norm': 1.2014342546463013, 'learning_rate': 0.0003343267646930305, 'epoch': 0.6818181818181818}\n{'loss': 0.5779, 'grad_norm': 0.736567497253418, 'learning_rate': 0.00034585527382037637, 'epoch': 0.7045454545454546}\n{'loss': 0.5683, 'grad_norm': 1.0681090354919434, 'learning_rate': 0.00035738378294772224, 'epoch': 0.7272727272727273}\n{'loss': 0.5542, 'grad_norm': 0.8428437113761902, 'learning_rate': 0.00036891229207506806, 'epoch': 0.75}\n{'loss': 0.569, 'grad_norm': 0.778445303440094, 'learning_rate': 0.000380440801202414, 'epoch': 0.7727272727272727}\n{'loss': 0.5119, 'grad_norm': 1.093088984489441, 'learning_rate': 0.00039196931032975987, 'epoch': 0.7954545454545454}\n{'loss': 0.4937, 'grad_norm': 1.288403034210205, 'learning_rate': 0.00040349781945710574, 'epoch': 0.8181818181818182}\n{'loss': 0.4746, 'grad_norm': 1.4991345405578613, 'learning_rate': 0.0004150263285844516, 'epoch': 0.8409090909090909}\n{'loss': 0.4505, 'grad_norm': 0.712866485118866, 'learning_rate': 0.0004137129541269059, 'epoch': 0.8636363636363636}\n{'loss': 0.448, 'grad_norm': 1.6235727071762085, 'learning_rate': 0.0004123995796693601, 'epoch': 0.8863636363636364}\n{'loss': 0.4078, 'grad_norm': 0.7650490999221802, 'learning_rate': 0.0004110862052118144, 'epoch': 0.9090909090909091}\n{'loss': 0.4741, 'grad_norm': 0.807784914970398, 'learning_rate': 0.0004097728307542687, 'epoch': 0.9318181818181818}\n{'loss': 0.4083, 'grad_norm': 1.0933144092559814, 'learning_rate': 0.00040845945629672297, 'epoch': 0.9545454545454546}\n{'loss': 0.3879, 'grad_norm': 1.1529698371887207, 'learning_rate': 0.00040714608183917725, 'epoch': 0.9772727272727273}\n{'loss': 0.3079, 'grad_norm': 3.0358736515045166, 'learning_rate': 0.0004058327073816315, 'epoch': 1.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8128    0.6881    0.7453      3142\n     PERCENT     0.3902    0.9697    0.5565        66\n        TYPE     0.8873    0.9553    0.9201     11415\n      VOLUME     0.1818    0.0286    0.0494        70\n\n   micro avg     0.8683    0.8938    0.8809     14693\n   macro avg     0.5680    0.6604    0.5678     14693\nweighted avg     0.8658    0.8938    0.8769     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.40716180205345154, 'eval_f1_macro': 0.5678059423836168, 'eval_precision': 0.8682975206611571, 'eval_recall': 0.8938269924453821, 'eval_f1': 0.8808773224226978, 'eval_accuracy': 0.8778430732397432, 'eval_runtime': 1.4392, 'eval_samples_per_second': 3829.341, 'eval_steps_per_second': 7.643, 'epoch': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.3561, 'grad_norm': 2.231590986251831, 'learning_rate': 0.00040451933292408575, 'epoch': 1.0227272727272727}\n{'loss': 0.39, 'grad_norm': 2.7148563861846924, 'learning_rate': 0.00040320595846654, 'epoch': 1.0454545454545454}\n{'loss': 0.3987, 'grad_norm': 1.116493582725525, 'learning_rate': 0.0004018925840089943, 'epoch': 1.0681818181818181}\n{'loss': 0.3248, 'grad_norm': 1.5386050939559937, 'learning_rate': 0.0004005792095514486, 'epoch': 1.0909090909090908}\n{'loss': 0.313, 'grad_norm': 2.284263849258423, 'learning_rate': 0.0003992658350939028, 'epoch': 1.1136363636363635}\n{'loss': 0.3648, 'grad_norm': 0.9362279176712036, 'learning_rate': 0.0003979524606363571, 'epoch': 1.1363636363636362}\n{'loss': 0.3691, 'grad_norm': 2.063089609146118, 'learning_rate': 0.00039663908617881133, 'epoch': 1.1590909090909092}\n{'loss': 0.3181, 'grad_norm': 1.1605722904205322, 'learning_rate': 0.0003953257117212656, 'epoch': 1.1818181818181819}\n{'loss': 0.3149, 'grad_norm': 0.9217676520347595, 'learning_rate': 0.0003940123372637199, 'epoch': 1.2045454545454546}\n{'loss': 0.351, 'grad_norm': 0.9863308668136597, 'learning_rate': 0.00039269896280617417, 'epoch': 1.2272727272727273}\n{'loss': 0.3177, 'grad_norm': 1.4403034448623657, 'learning_rate': 0.00039138558834862845, 'epoch': 1.25}\n{'loss': 0.3405, 'grad_norm': 0.7580083012580872, 'learning_rate': 0.0003900722138910827, 'epoch': 1.2727272727272727}\n{'loss': 0.3112, 'grad_norm': 0.6975209712982178, 'learning_rate': 0.00038875883943353696, 'epoch': 1.2954545454545454}\n{'loss': 0.2851, 'grad_norm': 1.577014684677124, 'learning_rate': 0.00038744546497599124, 'epoch': 1.3181818181818181}\n{'loss': 0.3069, 'grad_norm': 1.0363112688064575, 'learning_rate': 0.00038613209051844546, 'epoch': 1.3409090909090908}\n{'loss': 0.3186, 'grad_norm': 0.9198427200317383, 'learning_rate': 0.00038481871606089974, 'epoch': 1.3636363636363638}\n{'loss': 0.3339, 'grad_norm': 1.3081891536712646, 'learning_rate': 0.000383505341603354, 'epoch': 1.3863636363636362}\n{'loss': 0.284, 'grad_norm': 1.5616345405578613, 'learning_rate': 0.0003821919671458083, 'epoch': 1.4090909090909092}\n{'loss': 0.2845, 'grad_norm': 0.6553836464881897, 'learning_rate': 0.0003808785926882626, 'epoch': 1.4318181818181819}\n{'loss': 0.3003, 'grad_norm': 1.3409011363983154, 'learning_rate': 0.0003795652182307168, 'epoch': 1.4545454545454546}\n{'loss': 0.2598, 'grad_norm': 1.1148349046707153, 'learning_rate': 0.0003782518437731711, 'epoch': 1.4772727272727273}\n{'loss': 0.2798, 'grad_norm': 0.9406514167785645, 'learning_rate': 0.0003769384693156253, 'epoch': 1.5}\n{'loss': 0.3344, 'grad_norm': 0.8929327726364136, 'learning_rate': 0.00037562509485807965, 'epoch': 1.5227272727272727}\n{'loss': 0.2708, 'grad_norm': 0.847224771976471, 'learning_rate': 0.00037431172040053393, 'epoch': 1.5454545454545454}\n{'loss': 0.2407, 'grad_norm': 1.0523054599761963, 'learning_rate': 0.00037299834594298816, 'epoch': 1.5681818181818183}\n{'loss': 0.2446, 'grad_norm': 1.3634170293807983, 'learning_rate': 0.00037168497148544244, 'epoch': 1.5909090909090908}\n{'loss': 0.2522, 'grad_norm': 0.9228197336196899, 'learning_rate': 0.00037037159702789667, 'epoch': 1.6136363636363638}\n{'loss': 0.2869, 'grad_norm': 0.9133161902427673, 'learning_rate': 0.00036905822257035095, 'epoch': 1.6363636363636362}\n{'loss': 0.2394, 'grad_norm': 1.0839594602584839, 'learning_rate': 0.0003677448481128052, 'epoch': 1.6590909090909092}\n{'loss': 0.2897, 'grad_norm': 1.1708087921142578, 'learning_rate': 0.0003664314736552595, 'epoch': 1.6818181818181817}\n{'loss': 0.2588, 'grad_norm': 1.5075796842575073, 'learning_rate': 0.0003651180991977138, 'epoch': 1.7045454545454546}\n{'loss': 0.2449, 'grad_norm': 1.2958754301071167, 'learning_rate': 0.000363804724740168, 'epoch': 1.7272727272727273}\n{'loss': 0.3137, 'grad_norm': 1.077057957649231, 'learning_rate': 0.0003624913502826223, 'epoch': 1.75}\n{'loss': 0.2965, 'grad_norm': 0.8313250541687012, 'learning_rate': 0.0003611779758250766, 'epoch': 1.7727272727272727}\n{'loss': 0.2585, 'grad_norm': 0.9799855947494507, 'learning_rate': 0.0003598646013675308, 'epoch': 1.7954545454545454}\n{'loss': 0.2538, 'grad_norm': 1.2168500423431396, 'learning_rate': 0.00035855122690998514, 'epoch': 1.8181818181818183}\n{'loss': 0.2528, 'grad_norm': 1.0854228734970093, 'learning_rate': 0.00035723785245243936, 'epoch': 1.8409090909090908}\n{'loss': 0.2755, 'grad_norm': 1.2989875078201294, 'learning_rate': 0.00035592447799489364, 'epoch': 1.8636363636363638}\n{'loss': 0.218, 'grad_norm': 1.0016261339187622, 'learning_rate': 0.0003546111035373479, 'epoch': 1.8863636363636362}\n{'loss': 0.3075, 'grad_norm': 1.0942212343215942, 'learning_rate': 0.00035329772907980215, 'epoch': 1.9090909090909092}\n{'loss': 0.1992, 'grad_norm': 0.7029539942741394, 'learning_rate': 0.00035198435462225643, 'epoch': 1.9318181818181817}\n{'loss': 0.2995, 'grad_norm': 1.0753282308578491, 'learning_rate': 0.0003506709801647107, 'epoch': 1.9545454545454546}\n{'loss': 0.2355, 'grad_norm': 0.7308863997459412, 'learning_rate': 0.000349357605707165, 'epoch': 1.9772727272727273}\n{'loss': 0.4587, 'grad_norm': 4.195779800415039, 'learning_rate': 0.00034804423124961927, 'epoch': 2.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8607    0.8498    0.8552      3142\n     PERCENT     0.7561    0.9394    0.8378        66\n        TYPE     0.9400    0.9636    0.9517     11415\n      VOLUME     0.8571    0.7714    0.8120        70\n\n   micro avg     0.9222    0.9383    0.9302     14693\n   macro avg     0.8535    0.8811    0.8642     14693\nweighted avg     0.9218    0.9383    0.9299     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.25914254784584045, 'eval_f1_macro': 0.8641923766469528, 'eval_precision': 0.9222021539902334, 'eval_recall': 0.9382699244538215, 'eval_f1': 0.9301666554213616, 'eval_accuracy': 0.9212645554467298, 'eval_runtime': 1.4639, 'eval_samples_per_second': 3764.721, 'eval_steps_per_second': 7.514, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.197, 'grad_norm': 1.2411259412765503, 'learning_rate': 0.0003467308567920735, 'epoch': 2.022727272727273}\n{'loss': 0.2031, 'grad_norm': 0.6945043206214905, 'learning_rate': 0.0003454174823345278, 'epoch': 2.0454545454545454}\n{'loss': 0.1971, 'grad_norm': 1.110337495803833, 'learning_rate': 0.000344104107876982, 'epoch': 2.0681818181818183}\n{'loss': 0.1558, 'grad_norm': 0.8861263394355774, 'learning_rate': 0.0003427907334194363, 'epoch': 2.090909090909091}\n{'loss': 0.1481, 'grad_norm': 0.7769474387168884, 'learning_rate': 0.0003414773589618906, 'epoch': 2.1136363636363638}\n{'loss': 0.1234, 'grad_norm': 0.9287026524543762, 'learning_rate': 0.00034016398450434484, 'epoch': 2.1363636363636362}\n{'loss': 0.1604, 'grad_norm': 0.6508030295372009, 'learning_rate': 0.0003388506100467991, 'epoch': 2.159090909090909}\n{'loss': 0.1731, 'grad_norm': 0.9084595441818237, 'learning_rate': 0.00033753723558925335, 'epoch': 2.1818181818181817}\n{'loss': 0.164, 'grad_norm': 0.7812544107437134, 'learning_rate': 0.00033622386113170763, 'epoch': 2.2045454545454546}\n{'loss': 0.1758, 'grad_norm': 0.8531495332717896, 'learning_rate': 0.0003349104866741619, 'epoch': 2.227272727272727}\n{'loss': 0.1596, 'grad_norm': 1.2036528587341309, 'learning_rate': 0.00033359711221661614, 'epoch': 2.25}\n{'loss': 0.1457, 'grad_norm': 1.2246873378753662, 'learning_rate': 0.00033228373775907047, 'epoch': 2.2727272727272725}\n{'loss': 0.1922, 'grad_norm': 0.9680913686752319, 'learning_rate': 0.0003309703633015247, 'epoch': 2.2954545454545454}\n{'loss': 0.186, 'grad_norm': 1.3948928117752075, 'learning_rate': 0.000329656988843979, 'epoch': 2.3181818181818183}\n{'loss': 0.1708, 'grad_norm': 1.0562007427215576, 'learning_rate': 0.00032834361438643326, 'epoch': 2.340909090909091}\n{'loss': 0.1658, 'grad_norm': 1.0141130685806274, 'learning_rate': 0.0003270302399288875, 'epoch': 2.3636363636363638}\n{'loss': 0.1773, 'grad_norm': 0.8085273504257202, 'learning_rate': 0.00032571686547134177, 'epoch': 2.3863636363636362}\n{'loss': 0.2101, 'grad_norm': 0.9420124292373657, 'learning_rate': 0.00032440349101379605, 'epoch': 2.409090909090909}\n{'loss': 0.1357, 'grad_norm': 0.7676306366920471, 'learning_rate': 0.0003230901165562503, 'epoch': 2.4318181818181817}\n{'loss': 0.1683, 'grad_norm': 0.7535178661346436, 'learning_rate': 0.0003217767420987046, 'epoch': 2.4545454545454546}\n{'loss': 0.1764, 'grad_norm': 1.553889274597168, 'learning_rate': 0.00032046336764115883, 'epoch': 2.4772727272727275}\n{'loss': 0.1517, 'grad_norm': 0.718551754951477, 'learning_rate': 0.0003191499931836131, 'epoch': 2.5}\n{'loss': 0.2129, 'grad_norm': 1.148023009300232, 'learning_rate': 0.0003178366187260674, 'epoch': 2.5227272727272725}\n{'loss': 0.1741, 'grad_norm': 0.7159650921821594, 'learning_rate': 0.0003165232442685216, 'epoch': 2.5454545454545454}\n{'loss': 0.1772, 'grad_norm': 1.3152830600738525, 'learning_rate': 0.00031520986981097595, 'epoch': 2.5681818181818183}\n{'loss': 0.2143, 'grad_norm': 1.7736132144927979, 'learning_rate': 0.0003138964953534302, 'epoch': 2.590909090909091}\n{'loss': 0.1394, 'grad_norm': 1.151060700416565, 'learning_rate': 0.00031258312089588446, 'epoch': 2.6136363636363638}\n{'loss': 0.1366, 'grad_norm': 0.8902932405471802, 'learning_rate': 0.00031126974643833874, 'epoch': 2.6363636363636362}\n{'loss': 0.1501, 'grad_norm': 1.4392067193984985, 'learning_rate': 0.00030995637198079297, 'epoch': 2.659090909090909}\n{'loss': 0.1444, 'grad_norm': 1.0975366830825806, 'learning_rate': 0.00030864299752324725, 'epoch': 2.6818181818181817}\n{'loss': 0.1913, 'grad_norm': 0.9409332871437073, 'learning_rate': 0.0003073296230657015, 'epoch': 2.7045454545454546}\n{'loss': 0.1553, 'grad_norm': 1.1806542873382568, 'learning_rate': 0.0003060162486081558, 'epoch': 2.7272727272727275}\n{'loss': 0.1699, 'grad_norm': 1.1758836507797241, 'learning_rate': 0.00030470287415061004, 'epoch': 2.75}\n{'loss': 0.1384, 'grad_norm': 0.7341978549957275, 'learning_rate': 0.0003033894996930643, 'epoch': 2.7727272727272725}\n{'loss': 0.2149, 'grad_norm': 0.924065887928009, 'learning_rate': 0.0003020761252355186, 'epoch': 2.7954545454545454}\n{'loss': 0.1931, 'grad_norm': 1.0042158365249634, 'learning_rate': 0.0003007627507779728, 'epoch': 2.8181818181818183}\n{'loss': 0.1735, 'grad_norm': 1.2130907773971558, 'learning_rate': 0.0002994493763204271, 'epoch': 2.840909090909091}\n{'loss': 0.1688, 'grad_norm': 1.4134633541107178, 'learning_rate': 0.0002981360018628814, 'epoch': 2.8636363636363638}\n{'loss': 0.1464, 'grad_norm': 0.9900195598602295, 'learning_rate': 0.00029682262740533566, 'epoch': 2.8863636363636362}\n{'loss': 0.1811, 'grad_norm': 1.2526792287826538, 'learning_rate': 0.00029550925294778994, 'epoch': 2.909090909090909}\n{'loss': 0.1692, 'grad_norm': 1.2914881706237793, 'learning_rate': 0.00029419587849024417, 'epoch': 2.9318181818181817}\n{'loss': 0.2118, 'grad_norm': 1.9206308126449585, 'learning_rate': 0.00029288250403269845, 'epoch': 2.9545454545454546}\n{'loss': 0.1946, 'grad_norm': 1.0271128416061401, 'learning_rate': 0.00029156912957515273, 'epoch': 2.9772727272727275}\n{'loss': 0.0647, 'grad_norm': 4.151430606842041, 'learning_rate': 0.00029025575511760696, 'epoch': 3.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8894    0.8803    0.8848      3142\n     PERCENT     0.8857    0.9394    0.9118        66\n        TYPE     0.9451    0.9711    0.9579     11415\n      VOLUME     0.9306    0.9571    0.9437        70\n\n   micro avg     0.9332    0.9515    0.9422     14693\n   macro avg     0.9127    0.9370    0.9245     14693\nweighted avg     0.9328    0.9515    0.9420     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.23612628877162933, 'eval_f1_macro': 0.9245447971016583, 'eval_precision': 0.933182030572058, 'eval_recall': 0.9514734907779214, 'eval_f1': 0.94223899710184, 'eval_accuracy': 0.932147132440962, 'eval_runtime': 1.5222, 'eval_samples_per_second': 3620.496, 'eval_steps_per_second': 7.227, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.1228, 'grad_norm': 0.885921835899353, 'learning_rate': 0.0002889423806600613, 'epoch': 3.022727272727273}\n{'loss': 0.1084, 'grad_norm': 0.7944374084472656, 'learning_rate': 0.0002876290062025155, 'epoch': 3.0454545454545454}\n{'loss': 0.0987, 'grad_norm': 0.5789532661437988, 'learning_rate': 0.0002863156317449698, 'epoch': 3.0681818181818183}\n{'loss': 0.1136, 'grad_norm': 0.888323187828064, 'learning_rate': 0.0002850022572874241, 'epoch': 3.090909090909091}\n{'loss': 0.1138, 'grad_norm': 0.8078727126121521, 'learning_rate': 0.0002836888828298783, 'epoch': 3.1136363636363638}\n{'loss': 0.1009, 'grad_norm': 0.6948216557502747, 'learning_rate': 0.0002823755083723326, 'epoch': 3.1363636363636362}\n{'loss': 0.0934, 'grad_norm': 0.9551083445549011, 'learning_rate': 0.0002810621339147868, 'epoch': 3.159090909090909}\n{'loss': 0.1176, 'grad_norm': 0.901320219039917, 'learning_rate': 0.00027974875945724115, 'epoch': 3.1818181818181817}\n{'loss': 0.1514, 'grad_norm': 0.7749798893928528, 'learning_rate': 0.0002784353849996954, 'epoch': 3.2045454545454546}\n{'loss': 0.0871, 'grad_norm': 0.7696864604949951, 'learning_rate': 0.00027712201054214965, 'epoch': 3.227272727272727}\n{'loss': 0.1363, 'grad_norm': 0.9073066711425781, 'learning_rate': 0.00027580863608460393, 'epoch': 3.25}\n{'loss': 0.0916, 'grad_norm': 0.8684785962104797, 'learning_rate': 0.00027449526162705816, 'epoch': 3.2727272727272725}\n{'loss': 0.1214, 'grad_norm': 1.1398929357528687, 'learning_rate': 0.00027318188716951244, 'epoch': 3.2954545454545454}\n{'loss': 0.1245, 'grad_norm': 1.0876214504241943, 'learning_rate': 0.0002718685127119667, 'epoch': 3.3181818181818183}\n{'loss': 0.1054, 'grad_norm': 1.5321532487869263, 'learning_rate': 0.000270555138254421, 'epoch': 3.340909090909091}\n{'loss': 0.1002, 'grad_norm': 0.9235244393348694, 'learning_rate': 0.0002692417637968753, 'epoch': 3.3636363636363638}\n{'loss': 0.094, 'grad_norm': 0.8518208861351013, 'learning_rate': 0.0002679283893393295, 'epoch': 3.3863636363636362}\n{'loss': 0.1134, 'grad_norm': 1.5035715103149414, 'learning_rate': 0.0002666150148817838, 'epoch': 3.409090909090909}\n{'loss': 0.0885, 'grad_norm': 0.6975722908973694, 'learning_rate': 0.00026530164042423807, 'epoch': 3.4318181818181817}\n{'loss': 0.1594, 'grad_norm': 1.7315545082092285, 'learning_rate': 0.0002639882659666923, 'epoch': 3.4545454545454546}\n{'loss': 0.101, 'grad_norm': 0.7931614518165588, 'learning_rate': 0.00026267489150914663, 'epoch': 3.4772727272727275}\n{'loss': 0.1148, 'grad_norm': 0.856732964515686, 'learning_rate': 0.00026136151705160086, 'epoch': 3.5}\n{'loss': 0.1286, 'grad_norm': 0.7591090202331543, 'learning_rate': 0.00026004814259405514, 'epoch': 3.5227272727272725}\n{'loss': 0.0833, 'grad_norm': 0.8158361911773682, 'learning_rate': 0.0002587347681365094, 'epoch': 3.5454545454545454}\n{'loss': 0.0794, 'grad_norm': 0.7612578868865967, 'learning_rate': 0.00025742139367896364, 'epoch': 3.5681818181818183}\n{'loss': 0.127, 'grad_norm': 0.8362988233566284, 'learning_rate': 0.0002561080192214179, 'epoch': 3.590909090909091}\n{'loss': 0.1114, 'grad_norm': 0.7218947410583496, 'learning_rate': 0.0002547946447638722, 'epoch': 3.6136363636363638}\n{'loss': 0.0813, 'grad_norm': 0.6418426632881165, 'learning_rate': 0.0002534812703063265, 'epoch': 3.6363636363636362}\n{'loss': 0.0875, 'grad_norm': 0.904915452003479, 'learning_rate': 0.00025216789584878076, 'epoch': 3.659090909090909}\n{'loss': 0.0853, 'grad_norm': 0.7091395854949951, 'learning_rate': 0.000250854521391235, 'epoch': 3.6818181818181817}\n{'loss': 0.0897, 'grad_norm': 0.6785346865653992, 'learning_rate': 0.00024954114693368927, 'epoch': 3.7045454545454546}\n{'loss': 0.1107, 'grad_norm': 0.6814048886299133, 'learning_rate': 0.0002482277724761435, 'epoch': 3.7272727272727275}\n{'loss': 0.0719, 'grad_norm': 0.6835580468177795, 'learning_rate': 0.0002469143980185978, 'epoch': 3.75}\n{'loss': 0.1303, 'grad_norm': 0.9931079149246216, 'learning_rate': 0.0002456010235610521, 'epoch': 3.7727272727272725}\n{'loss': 0.1014, 'grad_norm': 0.8003393411636353, 'learning_rate': 0.00024428764910350634, 'epoch': 3.7954545454545454}\n{'loss': 0.1059, 'grad_norm': 1.2756121158599854, 'learning_rate': 0.00024297427464596062, 'epoch': 3.8181818181818183}\n{'loss': 0.1097, 'grad_norm': 1.5550715923309326, 'learning_rate': 0.00024166090018841487, 'epoch': 3.840909090909091}\n{'loss': 0.148, 'grad_norm': 1.1732350587844849, 'learning_rate': 0.00024034752573086912, 'epoch': 3.8636363636363638}\n{'loss': 0.0998, 'grad_norm': 0.9138693809509277, 'learning_rate': 0.00023903415127332338, 'epoch': 3.8863636363636362}\n{'loss': 0.1128, 'grad_norm': 1.3750358819961548, 'learning_rate': 0.00023772077681577769, 'epoch': 3.909090909090909}\n{'loss': 0.1145, 'grad_norm': 2.0606844425201416, 'learning_rate': 0.00023640740235823194, 'epoch': 3.9318181818181817}\n{'loss': 0.14, 'grad_norm': 1.0674939155578613, 'learning_rate': 0.00023509402790068622, 'epoch': 3.9545454545454546}\n{'loss': 0.1609, 'grad_norm': 1.3887951374053955, 'learning_rate': 0.00023378065344314047, 'epoch': 3.9772727272727275}\n{'loss': 0.2592, 'grad_norm': 3.978696584701538, 'learning_rate': 0.00023246727898559473, 'epoch': 4.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8904    0.8842    0.8873      3142\n     PERCENT     0.9242    0.9242    0.9242        66\n        TYPE     0.9465    0.9687    0.9575     11415\n      VOLUME     0.9067    0.9714    0.9379        70\n\n   micro avg     0.9345    0.9505    0.9424     14693\n   macro avg     0.9169    0.9371    0.9267     14693\nweighted avg     0.9342    0.9505    0.9422     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.2556321918964386, 'eval_f1_macro': 0.9267288557218039, 'eval_precision': 0.9344887580299786, 'eval_recall': 0.9504525964745116, 'eval_f1': 0.9424030772345379, 'eval_accuracy': 0.9328544999455871, 'eval_runtime': 1.4944, 'eval_samples_per_second': 3687.805, 'eval_steps_per_second': 7.361, 'epoch': 4.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.0745, 'grad_norm': 0.5409509539604187, 'learning_rate': 0.00023115390452804898, 'epoch': 4.0227272727272725}\n{'loss': 0.0688, 'grad_norm': 0.8874563574790955, 'learning_rate': 0.00022984053007050326, 'epoch': 4.045454545454546}\n{'loss': 0.0819, 'grad_norm': 0.6681721210479736, 'learning_rate': 0.00022852715561295757, 'epoch': 4.068181818181818}\n{'loss': 0.0735, 'grad_norm': 0.879976749420166, 'learning_rate': 0.00022721378115541182, 'epoch': 4.090909090909091}\n{'loss': 0.0932, 'grad_norm': 1.3207858800888062, 'learning_rate': 0.00022590040669786607, 'epoch': 4.113636363636363}\n{'loss': 0.0969, 'grad_norm': 0.634148359298706, 'learning_rate': 0.00022458703224032033, 'epoch': 4.136363636363637}\n{'loss': 0.067, 'grad_norm': 0.6942680478096008, 'learning_rate': 0.0002232736577827746, 'epoch': 4.159090909090909}\n{'loss': 0.0718, 'grad_norm': 0.6898046731948853, 'learning_rate': 0.00022196028332522886, 'epoch': 4.181818181818182}\n{'loss': 0.0835, 'grad_norm': 0.663784384727478, 'learning_rate': 0.00022064690886768311, 'epoch': 4.204545454545454}\n{'loss': 0.0747, 'grad_norm': 0.7949156761169434, 'learning_rate': 0.00021933353441013742, 'epoch': 4.2272727272727275}\n{'loss': 0.0935, 'grad_norm': 1.7336349487304688, 'learning_rate': 0.00021802015995259168, 'epoch': 4.25}\n{'loss': 0.0905, 'grad_norm': 1.1761715412139893, 'learning_rate': 0.00021670678549504596, 'epoch': 4.2727272727272725}\n{'loss': 0.0653, 'grad_norm': 0.8250424861907959, 'learning_rate': 0.0002153934110375002, 'epoch': 4.295454545454546}\n{'loss': 0.0644, 'grad_norm': 0.8185053467750549, 'learning_rate': 0.00021408003657995446, 'epoch': 4.318181818181818}\n{'loss': 0.0605, 'grad_norm': 0.7818199992179871, 'learning_rate': 0.00021276666212240872, 'epoch': 4.340909090909091}\n{'loss': 0.081, 'grad_norm': 1.254929542541504, 'learning_rate': 0.00021145328766486302, 'epoch': 4.363636363636363}\n{'loss': 0.0811, 'grad_norm': 0.9068291187286377, 'learning_rate': 0.0002101399132073173, 'epoch': 4.386363636363637}\n{'loss': 0.0524, 'grad_norm': 0.5685232877731323, 'learning_rate': 0.00020882653874977156, 'epoch': 4.409090909090909}\n{'loss': 0.0855, 'grad_norm': 0.9886905550956726, 'learning_rate': 0.0002075131642922258, 'epoch': 4.431818181818182}\n{'loss': 0.0697, 'grad_norm': 0.7876273989677429, 'learning_rate': 0.00020619978983468006, 'epoch': 4.454545454545454}\n{'loss': 0.0967, 'grad_norm': 1.3849488496780396, 'learning_rate': 0.00020488641537713434, 'epoch': 4.4772727272727275}\n{'loss': 0.0714, 'grad_norm': 0.7165346145629883, 'learning_rate': 0.00020357304091958862, 'epoch': 4.5}\n{'loss': 0.0809, 'grad_norm': 0.7064560055732727, 'learning_rate': 0.00020225966646204288, 'epoch': 4.5227272727272725}\n{'loss': 0.0503, 'grad_norm': 0.7119513750076294, 'learning_rate': 0.00020094629200449716, 'epoch': 4.545454545454545}\n{'loss': 0.0955, 'grad_norm': 0.7722999453544617, 'learning_rate': 0.0001996329175469514, 'epoch': 4.568181818181818}\n{'loss': 0.0806, 'grad_norm': 0.8627657890319824, 'learning_rate': 0.00019831954308940566, 'epoch': 4.590909090909091}\n{'loss': 0.0721, 'grad_norm': 0.5765437483787537, 'learning_rate': 0.00019700616863185994, 'epoch': 4.613636363636363}\n{'loss': 0.0883, 'grad_norm': 1.0153672695159912, 'learning_rate': 0.00019569279417431423, 'epoch': 4.636363636363637}\n{'loss': 0.0622, 'grad_norm': 0.5614734888076782, 'learning_rate': 0.00019437941971676848, 'epoch': 4.659090909090909}\n{'loss': 0.0435, 'grad_norm': 0.6773790121078491, 'learning_rate': 0.00019306604525922273, 'epoch': 4.681818181818182}\n{'loss': 0.0436, 'grad_norm': 0.9301131367683411, 'learning_rate': 0.000191752670801677, 'epoch': 4.704545454545455}\n{'loss': 0.0866, 'grad_norm': 0.8772626519203186, 'learning_rate': 0.0001904392963441313, 'epoch': 4.7272727272727275}\n{'loss': 0.0755, 'grad_norm': 0.9998959898948669, 'learning_rate': 0.00018912592188658555, 'epoch': 4.75}\n{'loss': 0.0656, 'grad_norm': 1.1692720651626587, 'learning_rate': 0.00018781254742903983, 'epoch': 4.7727272727272725}\n{'loss': 0.0785, 'grad_norm': 0.866370677947998, 'learning_rate': 0.00018649917297149408, 'epoch': 4.795454545454545}\n{'loss': 0.0794, 'grad_norm': 1.1059848070144653, 'learning_rate': 0.00018518579851394833, 'epoch': 4.818181818181818}\n{'loss': 0.0846, 'grad_norm': 1.26620352268219, 'learning_rate': 0.0001838724240564026, 'epoch': 4.840909090909091}\n{'loss': 0.1148, 'grad_norm': 1.0091866254806519, 'learning_rate': 0.0001825590495988569, 'epoch': 4.863636363636363}\n{'loss': 0.1118, 'grad_norm': 0.8894957900047302, 'learning_rate': 0.00018124567514131115, 'epoch': 4.886363636363637}\n{'loss': 0.0564, 'grad_norm': 0.6938208937644958, 'learning_rate': 0.0001799323006837654, 'epoch': 4.909090909090909}\n{'loss': 0.0781, 'grad_norm': 1.2957775592803955, 'learning_rate': 0.00017861892622621968, 'epoch': 4.931818181818182}\n{'loss': 0.0662, 'grad_norm': 1.0755457878112793, 'learning_rate': 0.00017730555176867396, 'epoch': 4.954545454545455}\n{'loss': 0.0872, 'grad_norm': 0.8576498627662659, 'learning_rate': 0.00017599217731112821, 'epoch': 4.9772727272727275}\n{'loss': 0.122, 'grad_norm': 4.373412132263184, 'learning_rate': 0.0001746788028535825, 'epoch': 5.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8818    0.8883    0.8850      3142\n     PERCENT     0.8971    0.9242    0.9104        66\n        TYPE     0.9498    0.9649    0.9573     11415\n      VOLUME     0.9315    0.9714    0.9510        70\n\n   micro avg     0.9350    0.9483    0.9416     14693\n   macro avg     0.9151    0.9372    0.9260     14693\nweighted avg     0.9349    0.9483    0.9416     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.27996501326560974, 'eval_f1_macro': 0.9259565928642292, 'eval_precision': 0.9350422762045363, 'eval_recall': 0.9483427482474648, 'eval_f1': 0.9416455482344992, 'eval_accuracy': 0.9317662422461639, 'eval_runtime': 1.5263, 'eval_samples_per_second': 3610.6, 'eval_steps_per_second': 7.207, 'epoch': 5.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.0746, 'grad_norm': 1.1140267848968506, 'learning_rate': 0.00017336542839603675, 'epoch': 5.0227272727272725}\n{'loss': 0.0673, 'grad_norm': 0.6546051502227783, 'learning_rate': 0.000172052053938491, 'epoch': 5.045454545454546}\n{'loss': 0.0704, 'grad_norm': 0.804387629032135, 'learning_rate': 0.0001707386794809453, 'epoch': 5.068181818181818}\n{'loss': 0.0485, 'grad_norm': 0.8313849568367004, 'learning_rate': 0.00016942530502339956, 'epoch': 5.090909090909091}\n{'loss': 0.0655, 'grad_norm': 0.9892646670341492, 'learning_rate': 0.00016811193056585382, 'epoch': 5.113636363636363}\n{'loss': 0.0766, 'grad_norm': 1.077057123184204, 'learning_rate': 0.00016679855610830807, 'epoch': 5.136363636363637}\n{'loss': 0.0456, 'grad_norm': 0.6127740740776062, 'learning_rate': 0.00016548518165076235, 'epoch': 5.159090909090909}\n{'loss': 0.0628, 'grad_norm': 0.85075843334198, 'learning_rate': 0.00016417180719321663, 'epoch': 5.181818181818182}\n{'loss': 0.0326, 'grad_norm': 0.4704872965812683, 'learning_rate': 0.00016285843273567088, 'epoch': 5.204545454545454}\n{'loss': 0.0544, 'grad_norm': 0.977545440196991, 'learning_rate': 0.00016154505827812516, 'epoch': 5.2272727272727275}\n{'loss': 0.0485, 'grad_norm': 0.5361312031745911, 'learning_rate': 0.00016023168382057942, 'epoch': 5.25}\n{'loss': 0.0549, 'grad_norm': 0.7575289011001587, 'learning_rate': 0.0001589183093630337, 'epoch': 5.2727272727272725}\n{'loss': 0.0893, 'grad_norm': 0.9930935502052307, 'learning_rate': 0.00015760493490548798, 'epoch': 5.295454545454546}\n{'loss': 0.056, 'grad_norm': 0.8302691578865051, 'learning_rate': 0.00015629156044794223, 'epoch': 5.318181818181818}\n{'loss': 0.0822, 'grad_norm': 0.8051927089691162, 'learning_rate': 0.00015497818599039648, 'epoch': 5.340909090909091}\n{'loss': 0.0439, 'grad_norm': 0.76517254114151, 'learning_rate': 0.00015366481153285074, 'epoch': 5.363636363636363}\n{'loss': 0.0494, 'grad_norm': 0.6909844875335693, 'learning_rate': 0.00015235143707530502, 'epoch': 5.386363636363637}\n{'loss': 0.0633, 'grad_norm': 0.6278815865516663, 'learning_rate': 0.0001510380626177593, 'epoch': 5.409090909090909}\n{'loss': 0.043, 'grad_norm': 0.633478581905365, 'learning_rate': 0.00014972468816021355, 'epoch': 5.431818181818182}\n{'loss': 0.0658, 'grad_norm': 1.0019221305847168, 'learning_rate': 0.00014841131370266783, 'epoch': 5.454545454545454}\n{'loss': 0.0587, 'grad_norm': 0.9357194304466248, 'learning_rate': 0.00014709793924512209, 'epoch': 5.4772727272727275}\n{'loss': 0.0379, 'grad_norm': 0.6326121091842651, 'learning_rate': 0.00014578456478757637, 'epoch': 5.5}\n{'loss': 0.0557, 'grad_norm': 0.7478108406066895, 'learning_rate': 0.00014447119033003065, 'epoch': 5.5227272727272725}\n{'loss': 0.0366, 'grad_norm': 0.7053356170654297, 'learning_rate': 0.0001431578158724849, 'epoch': 5.545454545454545}\n{'loss': 0.0375, 'grad_norm': 0.5072936415672302, 'learning_rate': 0.00014184444141493915, 'epoch': 5.568181818181818}\n{'loss': 0.0602, 'grad_norm': 0.7805851101875305, 'learning_rate': 0.0001405310669573934, 'epoch': 5.590909090909091}\n{'loss': 0.0695, 'grad_norm': 1.0232806205749512, 'learning_rate': 0.0001392176924998477, 'epoch': 5.613636363636363}\n{'loss': 0.0647, 'grad_norm': 0.5947944521903992, 'learning_rate': 0.00013790431804230197, 'epoch': 5.636363636363637}\n{'loss': 0.0443, 'grad_norm': 0.8447780609130859, 'learning_rate': 0.00013659094358475622, 'epoch': 5.659090909090909}\n{'loss': 0.0657, 'grad_norm': 1.0755383968353271, 'learning_rate': 0.0001352775691272105, 'epoch': 5.681818181818182}\n{'loss': 0.0601, 'grad_norm': 0.7750527262687683, 'learning_rate': 0.00013396419466966475, 'epoch': 5.704545454545455}\n{'loss': 0.0678, 'grad_norm': 0.7132036089897156, 'learning_rate': 0.00013265082021211903, 'epoch': 5.7272727272727275}\n{'loss': 0.0548, 'grad_norm': 0.8279179930686951, 'learning_rate': 0.00013133744575457331, 'epoch': 5.75}\n{'loss': 0.0648, 'grad_norm': 1.171064853668213, 'learning_rate': 0.00013002407129702757, 'epoch': 5.7727272727272725}\n{'loss': 0.0714, 'grad_norm': 1.5104763507843018, 'learning_rate': 0.00012871069683948182, 'epoch': 5.795454545454545}\n{'loss': 0.0501, 'grad_norm': 0.9344481229782104, 'learning_rate': 0.0001273973223819361, 'epoch': 5.818181818181818}\n{'loss': 0.0518, 'grad_norm': 0.7822635173797607, 'learning_rate': 0.00012608394792439038, 'epoch': 5.840909090909091}\n{'loss': 0.0711, 'grad_norm': 0.6275410056114197, 'learning_rate': 0.00012477057346684464, 'epoch': 5.863636363636363}\n{'loss': 0.0741, 'grad_norm': 1.0577622652053833, 'learning_rate': 0.0001234571990092989, 'epoch': 5.886363636363637}\n{'loss': 0.0653, 'grad_norm': 0.6763725876808167, 'learning_rate': 0.00012214382455175317, 'epoch': 5.909090909090909}\n{'loss': 0.0434, 'grad_norm': 0.5320225954055786, 'learning_rate': 0.00012083045009420744, 'epoch': 5.931818181818182}\n{'loss': 0.0662, 'grad_norm': 0.9984258413314819, 'learning_rate': 0.00011951707563666169, 'epoch': 5.954545454545455}\n{'loss': 0.0739, 'grad_norm': 0.9787626266479492, 'learning_rate': 0.00011820370117911597, 'epoch': 5.9772727272727275}\n{'loss': 0.007, 'grad_norm': 0.532150149345398, 'learning_rate': 0.00011689032672157024, 'epoch': 6.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8685    0.9061    0.8869      3142\n     PERCENT     0.8841    0.9242    0.9037        66\n        TYPE     0.9551    0.9606    0.9578     11415\n      VOLUME     0.9315    0.9714    0.9510        70\n\n   micro avg     0.9356    0.9488    0.9422     14693\n   macro avg     0.9098    0.9406    0.9249     14693\nweighted avg     0.9361    0.9488    0.9424     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.28746846318244934, 'eval_f1_macro': 0.9248694417304452, 'eval_precision': 0.9355747936380109, 'eval_recall': 0.948819165589056, 'eval_f1': 0.9421504358991687, 'eval_accuracy': 0.9319294809010774, 'eval_runtime': 1.4367, 'eval_samples_per_second': 3835.985, 'eval_steps_per_second': 7.657, 'epoch': 6.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.0356, 'grad_norm': 0.8251559734344482, 'learning_rate': 0.00011557695226402449, 'epoch': 6.0227272727272725}\n{'loss': 0.0461, 'grad_norm': 1.0278724431991577, 'learning_rate': 0.00011426357780647878, 'epoch': 6.045454545454546}\n{'loss': 0.043, 'grad_norm': 0.6629867553710938, 'learning_rate': 0.00011295020334893304, 'epoch': 6.068181818181818}\n{'loss': 0.0625, 'grad_norm': 0.5232085585594177, 'learning_rate': 0.0001116368288913873, 'epoch': 6.090909090909091}\n{'loss': 0.0578, 'grad_norm': 0.6709697842597961, 'learning_rate': 0.00011032345443384156, 'epoch': 6.113636363636363}\n{'loss': 0.0478, 'grad_norm': 0.5560698509216309, 'learning_rate': 0.00010901007997629584, 'epoch': 6.136363636363637}\n{'loss': 0.0427, 'grad_norm': 0.4498981535434723, 'learning_rate': 0.0001076967055187501, 'epoch': 6.159090909090909}\n{'loss': 0.0281, 'grad_norm': 0.3798620402812958, 'learning_rate': 0.00010638333106120436, 'epoch': 6.181818181818182}\n{'loss': 0.0368, 'grad_norm': 0.418971449136734, 'learning_rate': 0.00010506995660365865, 'epoch': 6.204545454545454}\n{'loss': 0.0338, 'grad_norm': 0.6728262305259705, 'learning_rate': 0.0001037565821461129, 'epoch': 6.2272727272727275}\n{'loss': 0.0354, 'grad_norm': 0.48622190952301025, 'learning_rate': 0.00010244320768856717, 'epoch': 6.25}\n{'loss': 0.0756, 'grad_norm': 0.6387645602226257, 'learning_rate': 0.00010112983323102144, 'epoch': 6.2727272727272725}\n{'loss': 0.0545, 'grad_norm': 0.6599149107933044, 'learning_rate': 9.98164587734757e-05, 'epoch': 6.295454545454546}\n{'loss': 0.0385, 'grad_norm': 0.5910841822624207, 'learning_rate': 9.850308431592997e-05, 'epoch': 6.318181818181818}\n{'loss': 0.062, 'grad_norm': 0.49622663855552673, 'learning_rate': 9.718970985838424e-05, 'epoch': 6.340909090909091}\n{'loss': 0.0416, 'grad_norm': 0.43500885367393494, 'learning_rate': 9.58763354008385e-05, 'epoch': 6.363636363636363}\n{'loss': 0.0402, 'grad_norm': 0.5338335037231445, 'learning_rate': 9.456296094329277e-05, 'epoch': 6.386363636363637}\n{'loss': 0.0307, 'grad_norm': 0.49029436707496643, 'learning_rate': 9.324958648574704e-05, 'epoch': 6.409090909090909}\n{'loss': 0.0415, 'grad_norm': 0.5362182259559631, 'learning_rate': 9.19362120282013e-05, 'epoch': 6.431818181818182}\n{'loss': 0.0399, 'grad_norm': 0.6967862248420715, 'learning_rate': 9.062283757065557e-05, 'epoch': 6.454545454545454}\n{'loss': 0.0517, 'grad_norm': 0.8161378502845764, 'learning_rate': 8.930946311310984e-05, 'epoch': 6.4772727272727275}\n{'loss': 0.0285, 'grad_norm': 0.6332553029060364, 'learning_rate': 8.799608865556411e-05, 'epoch': 6.5}\n{'loss': 0.0573, 'grad_norm': 0.9223048686981201, 'learning_rate': 8.668271419801837e-05, 'epoch': 6.5227272727272725}\n{'loss': 0.0235, 'grad_norm': 0.3857777714729309, 'learning_rate': 8.536933974047265e-05, 'epoch': 6.545454545454545}\n{'loss': 0.0538, 'grad_norm': 0.7667178511619568, 'learning_rate': 8.405596528292691e-05, 'epoch': 6.568181818181818}\n{'loss': 0.0504, 'grad_norm': 0.6287490725517273, 'learning_rate': 8.274259082538117e-05, 'epoch': 6.590909090909091}\n{'loss': 0.0333, 'grad_norm': 0.6021808385848999, 'learning_rate': 8.142921636783544e-05, 'epoch': 6.613636363636363}\n{'loss': 0.0503, 'grad_norm': 0.6925469636917114, 'learning_rate': 8.011584191028971e-05, 'epoch': 6.636363636363637}\n{'loss': 0.0472, 'grad_norm': 0.8107260465621948, 'learning_rate': 7.880246745274399e-05, 'epoch': 6.659090909090909}\n{'loss': 0.0395, 'grad_norm': 0.8137507438659668, 'learning_rate': 7.748909299519824e-05, 'epoch': 6.681818181818182}\n{'loss': 0.0393, 'grad_norm': 0.6777116060256958, 'learning_rate': 7.617571853765251e-05, 'epoch': 6.704545454545455}\n{'loss': 0.047, 'grad_norm': 1.2443292140960693, 'learning_rate': 7.486234408010678e-05, 'epoch': 6.7272727272727275}\n{'loss': 0.051, 'grad_norm': 0.6331733465194702, 'learning_rate': 7.354896962256104e-05, 'epoch': 6.75}\n{'loss': 0.0458, 'grad_norm': 0.5584779381752014, 'learning_rate': 7.223559516501532e-05, 'epoch': 6.7727272727272725}\n{'loss': 0.0627, 'grad_norm': 1.330036997795105, 'learning_rate': 7.092222070746958e-05, 'epoch': 6.795454545454545}\n{'loss': 0.0642, 'grad_norm': 0.7806262969970703, 'learning_rate': 6.960884624992386e-05, 'epoch': 6.818181818181818}\n{'loss': 0.0452, 'grad_norm': 0.5306403636932373, 'learning_rate': 6.829547179237811e-05, 'epoch': 6.840909090909091}\n{'loss': 0.0452, 'grad_norm': 0.641598105430603, 'learning_rate': 6.698209733483238e-05, 'epoch': 6.863636363636363}\n{'loss': 0.0378, 'grad_norm': 0.6361331939697266, 'learning_rate': 6.566872287728666e-05, 'epoch': 6.886363636363637}\n{'loss': 0.0347, 'grad_norm': 0.4588944613933563, 'learning_rate': 6.435534841974091e-05, 'epoch': 6.909090909090909}\n{'loss': 0.039, 'grad_norm': 0.6355764269828796, 'learning_rate': 6.304197396219519e-05, 'epoch': 6.931818181818182}\n{'loss': 0.0549, 'grad_norm': 1.032116413116455, 'learning_rate': 6.172859950464944e-05, 'epoch': 6.954545454545455}\n{'loss': 0.0545, 'grad_norm': 0.9270020723342896, 'learning_rate': 6.041522504710372e-05, 'epoch': 6.9772727272727275}\n{'loss': 0.009, 'grad_norm': 0.7678837180137634, 'learning_rate': 5.9101850589557985e-05, 'epoch': 7.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8802    0.8953    0.8877      3142\n     PERCENT     0.8841    0.9242    0.9037        66\n        TYPE     0.9533    0.9621    0.9577     11415\n      VOLUME     0.9306    0.9571    0.9437        70\n\n   micro avg     0.9371    0.9476    0.9423     14693\n   macro avg     0.9120    0.9347    0.9232     14693\nweighted avg     0.9372    0.9476    0.9424     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.3059209883213043, 'eval_f1_macro': 0.9231725897540848, 'eval_precision': 0.937134010903951, 'eval_recall': 0.9475940924249643, 'eval_f1': 0.9423350253807107, 'eval_accuracy': 0.9310044618565676, 'eval_runtime': 1.5675, 'eval_samples_per_second': 3515.706, 'eval_steps_per_second': 7.017, 'epoch': 7.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.0328, 'grad_norm': 0.5459415912628174, 'learning_rate': 5.7788476132012245e-05, 'epoch': 7.0227272727272725}\n{'loss': 0.0404, 'grad_norm': 0.6205876469612122, 'learning_rate': 5.647510167446652e-05, 'epoch': 7.045454545454546}\n{'loss': 0.064, 'grad_norm': 0.601428747177124, 'learning_rate': 5.516172721692078e-05, 'epoch': 7.068181818181818}\n{'loss': 0.0449, 'grad_norm': 0.6155222654342651, 'learning_rate': 5.384835275937505e-05, 'epoch': 7.090909090909091}\n{'loss': 0.0456, 'grad_norm': 0.8224418759346008, 'learning_rate': 5.2534978301829326e-05, 'epoch': 7.113636363636363}\n{'loss': 0.0262, 'grad_norm': 0.4944409132003784, 'learning_rate': 5.1221603844283586e-05, 'epoch': 7.136363636363637}\n{'loss': 0.0339, 'grad_norm': 0.58492112159729, 'learning_rate': 4.990822938673785e-05, 'epoch': 7.159090909090909}\n{'loss': 0.0244, 'grad_norm': 0.36591920256614685, 'learning_rate': 4.859485492919212e-05, 'epoch': 7.181818181818182}\n{'loss': 0.0446, 'grad_norm': 0.5818040370941162, 'learning_rate': 4.7281480471646386e-05, 'epoch': 7.204545454545454}\n{'loss': 0.0405, 'grad_norm': 0.6097785830497742, 'learning_rate': 4.596810601410065e-05, 'epoch': 7.2272727272727275}\n{'loss': 0.0453, 'grad_norm': 1.188145637512207, 'learning_rate': 4.465473155655492e-05, 'epoch': 7.25}\n{'loss': 0.0416, 'grad_norm': 0.5006996393203735, 'learning_rate': 4.334135709900919e-05, 'epoch': 7.2727272727272725}\n{'loss': 0.0401, 'grad_norm': 0.7890933156013489, 'learning_rate': 4.2027982641463454e-05, 'epoch': 7.295454545454546}\n{'loss': 0.051, 'grad_norm': 0.5000890493392944, 'learning_rate': 4.071460818391772e-05, 'epoch': 7.318181818181818}\n{'loss': 0.0407, 'grad_norm': 0.8522288799285889, 'learning_rate': 3.9401233726371994e-05, 'epoch': 7.340909090909091}\n{'loss': 0.0417, 'grad_norm': 0.5907299518585205, 'learning_rate': 3.8087859268826254e-05, 'epoch': 7.363636363636363}\n{'loss': 0.0393, 'grad_norm': 0.7443104982376099, 'learning_rate': 3.677448481128052e-05, 'epoch': 7.386363636363637}\n{'loss': 0.0617, 'grad_norm': 0.8277227878570557, 'learning_rate': 3.546111035373479e-05, 'epoch': 7.409090909090909}\n{'loss': 0.0308, 'grad_norm': 0.8870754837989807, 'learning_rate': 3.4147735896189055e-05, 'epoch': 7.431818181818182}\n{'loss': 0.0376, 'grad_norm': 0.7204237580299377, 'learning_rate': 3.283436143864333e-05, 'epoch': 7.454545454545454}\n{'loss': 0.0263, 'grad_norm': 0.44417163729667664, 'learning_rate': 3.1520986981097595e-05, 'epoch': 7.4772727272727275}\n{'loss': 0.0295, 'grad_norm': 0.8046765327453613, 'learning_rate': 3.020761252355186e-05, 'epoch': 7.5}\n{'loss': 0.0445, 'grad_norm': 0.7714933156967163, 'learning_rate': 2.8894238066006122e-05, 'epoch': 7.5227272727272725}\n{'loss': 0.0439, 'grad_norm': 0.4892451763153076, 'learning_rate': 2.758086360846039e-05, 'epoch': 7.545454545454545}\n{'loss': 0.0368, 'grad_norm': 0.703667938709259, 'learning_rate': 2.6267489150914663e-05, 'epoch': 7.568181818181818}\n{'loss': 0.0259, 'grad_norm': 0.8470655679702759, 'learning_rate': 2.4954114693368926e-05, 'epoch': 7.590909090909091}\n{'loss': 0.0451, 'grad_norm': 0.4655546545982361, 'learning_rate': 2.3640740235823193e-05, 'epoch': 7.613636363636363}\n{'loss': 0.0279, 'grad_norm': 0.5836359262466431, 'learning_rate': 2.232736577827746e-05, 'epoch': 7.636363636363637}\n{'loss': 0.0512, 'grad_norm': 1.0760917663574219, 'learning_rate': 2.1013991320731727e-05, 'epoch': 7.659090909090909}\n{'loss': 0.0302, 'grad_norm': 0.5435506701469421, 'learning_rate': 1.9700616863185997e-05, 'epoch': 7.681818181818182}\n{'loss': 0.0382, 'grad_norm': 0.547111988067627, 'learning_rate': 1.838724240564026e-05, 'epoch': 7.704545454545455}\n{'loss': 0.0314, 'grad_norm': 0.6246617436408997, 'learning_rate': 1.7073867948094528e-05, 'epoch': 7.7272727272727275}\n{'loss': 0.0201, 'grad_norm': 0.5668307542800903, 'learning_rate': 1.5760493490548798e-05, 'epoch': 7.75}\n{'loss': 0.0455, 'grad_norm': 0.7902052402496338, 'learning_rate': 1.4447119033003061e-05, 'epoch': 7.7727272727272725}\n{'loss': 0.0283, 'grad_norm': 0.5282987952232361, 'learning_rate': 1.3133744575457331e-05, 'epoch': 7.795454545454545}\n{'loss': 0.0284, 'grad_norm': 0.47620147466659546, 'learning_rate': 1.1820370117911597e-05, 'epoch': 7.818181818181818}\n{'loss': 0.0316, 'grad_norm': 0.4349745810031891, 'learning_rate': 1.0506995660365863e-05, 'epoch': 7.840909090909091}\n{'loss': 0.0497, 'grad_norm': 0.5249496102333069, 'learning_rate': 9.19362120282013e-06, 'epoch': 7.863636363636363}\n{'loss': 0.027, 'grad_norm': 0.5865548849105835, 'learning_rate': 7.880246745274399e-06, 'epoch': 7.886363636363637}\n{'loss': 0.0289, 'grad_norm': 0.7799494862556458, 'learning_rate': 6.566872287728666e-06, 'epoch': 7.909090909090909}\n{'loss': 0.0491, 'grad_norm': 0.7311457991600037, 'learning_rate': 5.253497830182932e-06, 'epoch': 7.931818181818182}\n{'loss': 0.0349, 'grad_norm': 0.6061345934867859, 'learning_rate': 3.9401233726371994e-06, 'epoch': 7.954545454545455}\n{'loss': 0.0415, 'grad_norm': 0.6255380511283875, 'learning_rate': 2.626748915091466e-06, 'epoch': 7.9772727272727275}\n{'loss': 0.0176, 'grad_norm': 1.7932629585266113, 'learning_rate': 1.313374457545733e-06, 'epoch': 8.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8804    0.8972    0.8887      3142\n     PERCENT     0.8841    0.9242    0.9037        66\n        TYPE     0.9499    0.9647    0.9572     11415\n      VOLUME     0.9306    0.9571    0.9437        70\n\n   micro avg     0.9346    0.9500    0.9423     14693\n   macro avg     0.9112    0.9358    0.9233     14693\nweighted avg     0.9346    0.9500    0.9423     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.3144630491733551, 'eval_f1_macro': 0.923327921960841, 'eval_precision': 0.934587573647563, 'eval_recall': 0.9500442387531478, 'eval_f1': 0.9422525228661108, 'eval_accuracy': 0.9319294809010774, 'eval_runtime': 1.852, 'eval_samples_per_second': 2975.642, 'eval_steps_per_second': 5.939, 'epoch': 8.0}\n{'train_runtime': 49.8896, 'train_samples_per_second': 3534.365, 'train_steps_per_second': 7.056, 'train_loss': 0.23067592061653902, 'epoch': 8.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nThe speedups for torchdynamo mostly come with GPU Ampere or higher and which is not detected here.\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8904    0.8842    0.8873      3142\n     PERCENT     0.9242    0.9242    0.9242        66\n        TYPE     0.9465    0.9687    0.9575     11415\n      VOLUME     0.9067    0.9714    0.9379        70\n\n   micro avg     0.9345    0.9505    0.9424     14693\n   macro avg     0.9169    0.9371    0.9267     14693\nweighted avg     0.9342    0.9505    0.9422     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.2556321918964386, 'eval_f1_macro': 0.9267288557218039, 'eval_precision': 0.9344887580299786, 'eval_recall': 0.9504525964745116, 'eval_f1': 0.9424030772345379, 'eval_accuracy': 0.9328544999455871, 'eval_runtime': 1.5388, 'eval_samples_per_second': 3581.465, 'eval_steps_per_second': 7.149, 'epoch': 8.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/3364797558.py:66: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 2.1632, 'grad_norm': 7.234492778778076, 'learning_rate': 0.0, 'epoch': 0.022727272727272728}\n{'loss': 2.1749, 'grad_norm': 7.372167110443115, 'learning_rate': 1.1528509127345877e-05, 'epoch': 0.045454545454545456}\n{'loss': 2.1319, 'grad_norm': 7.371554851531982, 'learning_rate': 2.3057018254691754e-05, 'epoch': 0.06818181818181818}\n{'loss': 2.0928, 'grad_norm': 7.065462112426758, 'learning_rate': 3.458552738203763e-05, 'epoch': 0.09090909090909091}\n{'loss': 2.0214, 'grad_norm': 6.88538122177124, 'learning_rate': 4.611403650938351e-05, 'epoch': 0.11363636363636363}\n{'loss': 1.9064, 'grad_norm': 6.747328281402588, 'learning_rate': 5.76425456367294e-05, 'epoch': 0.13636363636363635}\n{'loss': 1.7859, 'grad_norm': 6.235224723815918, 'learning_rate': 6.917105476407527e-05, 'epoch': 0.1590909090909091}\n{'loss': 1.6902, 'grad_norm': 5.3829345703125, 'learning_rate': 8.069956389142115e-05, 'epoch': 0.18181818181818182}\n{'loss': 1.5235, 'grad_norm': 5.202297210693359, 'learning_rate': 9.222807301876702e-05, 'epoch': 0.20454545454545456}\n{'loss': 1.4324, 'grad_norm': 3.9813554286956787, 'learning_rate': 0.0001037565821461129, 'epoch': 0.22727272727272727}\n{'loss': 1.2949, 'grad_norm': 3.3498129844665527, 'learning_rate': 0.0001152850912734588, 'epoch': 0.25}\n{'loss': 1.2839, 'grad_norm': 2.2364392280578613, 'learning_rate': 0.00012681360040080468, 'epoch': 0.2727272727272727}\n{'loss': 1.1786, 'grad_norm': 1.9614207744598389, 'learning_rate': 0.00013834210952815053, 'epoch': 0.29545454545454547}\n{'loss': 1.0751, 'grad_norm': 1.7803623676300049, 'learning_rate': 0.0001498706186554964, 'epoch': 0.3181818181818182}\n{'loss': 1.1232, 'grad_norm': 2.169931411743164, 'learning_rate': 0.0001613991277828423, 'epoch': 0.3409090909090909}\n{'loss': 1.0454, 'grad_norm': 1.2959893941879272, 'learning_rate': 0.00017292763691018818, 'epoch': 0.36363636363636365}\n{'loss': 0.9945, 'grad_norm': 1.0457301139831543, 'learning_rate': 0.00018445614603753403, 'epoch': 0.38636363636363635}\n{'loss': 0.9761, 'grad_norm': 1.6262694597244263, 'learning_rate': 0.00019598465516487993, 'epoch': 0.4090909090909091}\n{'loss': 0.9456, 'grad_norm': 1.6401329040527344, 'learning_rate': 0.0002075131642922258, 'epoch': 0.4318181818181818}\n{'loss': 0.9213, 'grad_norm': 1.2934516668319702, 'learning_rate': 0.00021904167341957169, 'epoch': 0.45454545454545453}\n{'loss': 0.8704, 'grad_norm': 0.9560326337814331, 'learning_rate': 0.0002305701825469176, 'epoch': 0.4772727272727273}\n{'loss': 0.7757, 'grad_norm': 0.9686195254325867, 'learning_rate': 0.00024209869167426346, 'epoch': 0.5}\n{'loss': 0.7485, 'grad_norm': 0.9111204743385315, 'learning_rate': 0.00025362720080160937, 'epoch': 0.5227272727272727}\n{'loss': 0.7745, 'grad_norm': 1.1180732250213623, 'learning_rate': 0.0002651557099289552, 'epoch': 0.5454545454545454}\n{'loss': 0.6862, 'grad_norm': 0.8905100226402283, 'learning_rate': 0.00027668421905630106, 'epoch': 0.5681818181818182}\n{'loss': 0.6007, 'grad_norm': 1.1221054792404175, 'learning_rate': 0.00028821272818364694, 'epoch': 0.5909090909090909}\n{'loss': 0.6667, 'grad_norm': 0.8629694581031799, 'learning_rate': 0.0002997412373109928, 'epoch': 0.6136363636363636}\n{'loss': 0.5912, 'grad_norm': 1.1183792352676392, 'learning_rate': 0.00031126974643833874, 'epoch': 0.6363636363636364}\n{'loss': 0.6655, 'grad_norm': 1.4662542343139648, 'learning_rate': 0.0003227982555656846, 'epoch': 0.6590909090909091}\n{'loss': 0.5959, 'grad_norm': 0.7702242732048035, 'learning_rate': 0.0003343267646930305, 'epoch': 0.6818181818181818}\n{'loss': 0.5441, 'grad_norm': 1.104917049407959, 'learning_rate': 0.00034585527382037637, 'epoch': 0.7045454545454546}\n{'loss': 0.5698, 'grad_norm': 0.9569418430328369, 'learning_rate': 0.00035738378294772224, 'epoch': 0.7272727272727273}\n{'loss': 0.5193, 'grad_norm': 0.995442807674408, 'learning_rate': 0.00036891229207506806, 'epoch': 0.75}\n{'loss': 0.515, 'grad_norm': 0.9377560019493103, 'learning_rate': 0.000380440801202414, 'epoch': 0.7727272727272727}\n{'loss': 0.4781, 'grad_norm': 1.4863280057907104, 'learning_rate': 0.00039196931032975987, 'epoch': 0.7954545454545454}\n{'loss': 0.4622, 'grad_norm': 0.8946642279624939, 'learning_rate': 0.00040349781945710574, 'epoch': 0.8181818181818182}\n{'loss': 0.4827, 'grad_norm': 2.710479736328125, 'learning_rate': 0.0004150263285844516, 'epoch': 0.8409090909090909}\n{'loss': 0.498, 'grad_norm': 2.279420852661133, 'learning_rate': 0.0004137129541269059, 'epoch': 0.8636363636363636}\n{'loss': 0.4325, 'grad_norm': 1.7181577682495117, 'learning_rate': 0.0004123995796693601, 'epoch': 0.8863636363636364}\n{'loss': 0.427, 'grad_norm': 0.9165122509002686, 'learning_rate': 0.0004110862052118144, 'epoch': 0.9090909090909091}\n{'loss': 0.5215, 'grad_norm': 2.5168004035949707, 'learning_rate': 0.0004097728307542687, 'epoch': 0.9318181818181818}\n{'loss': 0.4695, 'grad_norm': 1.7689012289047241, 'learning_rate': 0.00040845945629672297, 'epoch': 0.9545454545454546}\n{'loss': 0.4319, 'grad_norm': 1.3474185466766357, 'learning_rate': 0.00040714608183917725, 'epoch': 0.9772727272727273}\n{'loss': 0.2453, 'grad_norm': 2.8674612045288086, 'learning_rate': 0.0004058327073816315, 'epoch': 1.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.7338    0.7861    0.7590      3404\n     PERCENT     0.4494    1.0000    0.6201        71\n        TYPE     0.8958    0.9397    0.9172     11194\n      VOLUME     0.2500    0.0357    0.0625        56\n\n   micro avg     0.8530    0.9011    0.8764     14725\n   macro avg     0.5822    0.6904    0.5897     14725\nweighted avg     0.8538    0.9011    0.8760     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.4057704508304596, 'eval_f1_macro': 0.5897191503513834, 'eval_precision': 0.8529733204757313, 'eval_recall': 0.9010526315789473, 'eval_f1': 0.8763540290620873, 'eval_accuracy': 0.8748012936468783, 'eval_runtime': 1.6379, 'eval_samples_per_second': 3364.742, 'eval_steps_per_second': 6.716, 'epoch': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.3706, 'grad_norm': 2.2792673110961914, 'learning_rate': 0.00040451933292408575, 'epoch': 1.0227272727272727}\n{'loss': 0.3411, 'grad_norm': 2.21484637260437, 'learning_rate': 0.00040320595846654, 'epoch': 1.0454545454545454}\n{'loss': 0.34, 'grad_norm': 0.7868078947067261, 'learning_rate': 0.0004018925840089943, 'epoch': 1.0681818181818181}\n{'loss': 0.3406, 'grad_norm': 1.5188084840774536, 'learning_rate': 0.0004005792095514486, 'epoch': 1.0909090909090908}\n{'loss': 0.3854, 'grad_norm': 1.836228370666504, 'learning_rate': 0.0003992658350939028, 'epoch': 1.1136363636363635}\n{'loss': 0.2724, 'grad_norm': 1.0479227304458618, 'learning_rate': 0.0003979524606363571, 'epoch': 1.1363636363636362}\n{'loss': 0.334, 'grad_norm': 0.7696924209594727, 'learning_rate': 0.00039663908617881133, 'epoch': 1.1590909090909092}\n{'loss': 0.3715, 'grad_norm': 1.0195661783218384, 'learning_rate': 0.0003953257117212656, 'epoch': 1.1818181818181819}\n{'loss': 0.3218, 'grad_norm': 1.1322596073150635, 'learning_rate': 0.0003940123372637199, 'epoch': 1.2045454545454546}\n{'loss': 0.3664, 'grad_norm': 1.348838448524475, 'learning_rate': 0.00039269896280617417, 'epoch': 1.2272727272727273}\n{'loss': 0.3648, 'grad_norm': 1.24741792678833, 'learning_rate': 0.00039138558834862845, 'epoch': 1.25}\n{'loss': 0.3117, 'grad_norm': 0.8381794691085815, 'learning_rate': 0.0003900722138910827, 'epoch': 1.2727272727272727}\n{'loss': 0.2543, 'grad_norm': 0.8127071261405945, 'learning_rate': 0.00038875883943353696, 'epoch': 1.2954545454545454}\n{'loss': 0.3018, 'grad_norm': 0.7556384801864624, 'learning_rate': 0.00038744546497599124, 'epoch': 1.3181818181818181}\n{'loss': 0.3386, 'grad_norm': 0.8454381823539734, 'learning_rate': 0.00038613209051844546, 'epoch': 1.3409090909090908}\n{'loss': 0.353, 'grad_norm': 1.4185508489608765, 'learning_rate': 0.00038481871606089974, 'epoch': 1.3636363636363638}\n{'loss': 0.278, 'grad_norm': 0.7494645714759827, 'learning_rate': 0.000383505341603354, 'epoch': 1.3863636363636362}\n{'loss': 0.2786, 'grad_norm': 0.8317930102348328, 'learning_rate': 0.0003821919671458083, 'epoch': 1.4090909090909092}\n{'loss': 0.3222, 'grad_norm': 1.1914124488830566, 'learning_rate': 0.0003808785926882626, 'epoch': 1.4318181818181819}\n{'loss': 0.2982, 'grad_norm': 0.8759891986846924, 'learning_rate': 0.0003795652182307168, 'epoch': 1.4545454545454546}\n{'loss': 0.2916, 'grad_norm': 0.8455761671066284, 'learning_rate': 0.0003782518437731711, 'epoch': 1.4772727272727273}\n{'loss': 0.2991, 'grad_norm': 0.8808674812316895, 'learning_rate': 0.0003769384693156253, 'epoch': 1.5}\n{'loss': 0.3368, 'grad_norm': 1.3003185987472534, 'learning_rate': 0.00037562509485807965, 'epoch': 1.5227272727272727}\n{'loss': 0.2536, 'grad_norm': 1.8875048160552979, 'learning_rate': 0.00037431172040053393, 'epoch': 1.5454545454545454}\n{'loss': 0.2965, 'grad_norm': 1.5985699892044067, 'learning_rate': 0.00037299834594298816, 'epoch': 1.5681818181818183}\n{'loss': 0.2944, 'grad_norm': 1.6639870405197144, 'learning_rate': 0.00037168497148544244, 'epoch': 1.5909090909090908}\n{'loss': 0.2747, 'grad_norm': 1.58656644821167, 'learning_rate': 0.00037037159702789667, 'epoch': 1.6136363636363638}\n{'loss': 0.303, 'grad_norm': 0.8982933759689331, 'learning_rate': 0.00036905822257035095, 'epoch': 1.6363636363636362}\n{'loss': 0.303, 'grad_norm': 1.3788775205612183, 'learning_rate': 0.0003677448481128052, 'epoch': 1.6590909090909092}\n{'loss': 0.3074, 'grad_norm': 1.6899985074996948, 'learning_rate': 0.0003664314736552595, 'epoch': 1.6818181818181817}\n{'loss': 0.2763, 'grad_norm': 0.8930367827415466, 'learning_rate': 0.0003651180991977138, 'epoch': 1.7045454545454546}\n{'loss': 0.3166, 'grad_norm': 1.1471562385559082, 'learning_rate': 0.000363804724740168, 'epoch': 1.7272727272727273}\n{'loss': 0.2496, 'grad_norm': 1.4494059085845947, 'learning_rate': 0.0003624913502826223, 'epoch': 1.75}\n{'loss': 0.2244, 'grad_norm': 1.0702341794967651, 'learning_rate': 0.0003611779758250766, 'epoch': 1.7727272727272727}\n{'loss': 0.2904, 'grad_norm': 0.7358148694038391, 'learning_rate': 0.0003598646013675308, 'epoch': 1.7954545454545454}\n{'loss': 0.2839, 'grad_norm': 0.6838161945343018, 'learning_rate': 0.00035855122690998514, 'epoch': 1.8181818181818183}\n{'loss': 0.2969, 'grad_norm': 0.9434124827384949, 'learning_rate': 0.00035723785245243936, 'epoch': 1.8409090909090908}\n{'loss': 0.2631, 'grad_norm': 1.4166454076766968, 'learning_rate': 0.00035592447799489364, 'epoch': 1.8636363636363638}\n{'loss': 0.279, 'grad_norm': 1.0894274711608887, 'learning_rate': 0.0003546111035373479, 'epoch': 1.8863636363636362}\n{'loss': 0.2833, 'grad_norm': 1.016694188117981, 'learning_rate': 0.00035329772907980215, 'epoch': 1.9090909090909092}\n{'loss': 0.2268, 'grad_norm': 1.8624166250228882, 'learning_rate': 0.00035198435462225643, 'epoch': 1.9318181818181817}\n{'loss': 0.2679, 'grad_norm': 2.0988800525665283, 'learning_rate': 0.0003506709801647107, 'epoch': 1.9545454545454546}\n{'loss': 0.2493, 'grad_norm': 1.2357509136199951, 'learning_rate': 0.000349357605707165, 'epoch': 1.9772727272727273}\n{'loss': 0.1746, 'grad_norm': 2.8364176750183105, 'learning_rate': 0.00034804423124961927, 'epoch': 2.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8774    0.8431    0.8599      3404\n     PERCENT     0.9577    0.9577    0.9577        71\n        TYPE     0.9237    0.9723    0.9474     11194\n      VOLUME     0.7755    0.6786    0.7238        56\n\n   micro avg     0.9134    0.9413    0.9271     14725\n   macro avg     0.8836    0.8629    0.8722     14725\nweighted avg     0.9126    0.9413    0.9264     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.28388547897338867, 'eval_f1_macro': 0.8722158152722977, 'eval_precision': 0.9134045077105575, 'eval_recall': 0.9412563667232597, 'eval_f1': 0.9271213084049633, 'eval_accuracy': 0.9185989146521953, 'eval_runtime': 1.4362, 'eval_samples_per_second': 3837.139, 'eval_steps_per_second': 7.659, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.1766, 'grad_norm': 0.7477465867996216, 'learning_rate': 0.0003467308567920735, 'epoch': 2.022727272727273}\n{'loss': 0.2111, 'grad_norm': 0.9140433073043823, 'learning_rate': 0.0003454174823345278, 'epoch': 2.0454545454545454}\n{'loss': 0.1744, 'grad_norm': 1.3055897951126099, 'learning_rate': 0.000344104107876982, 'epoch': 2.0681818181818183}\n{'loss': 0.2196, 'grad_norm': 1.2122812271118164, 'learning_rate': 0.0003427907334194363, 'epoch': 2.090909090909091}\n{'loss': 0.1913, 'grad_norm': 0.6577664017677307, 'learning_rate': 0.0003414773589618906, 'epoch': 2.1136363636363638}\n{'loss': 0.1825, 'grad_norm': 1.0963096618652344, 'learning_rate': 0.00034016398450434484, 'epoch': 2.1363636363636362}\n{'loss': 0.174, 'grad_norm': 0.8305752277374268, 'learning_rate': 0.0003388506100467991, 'epoch': 2.159090909090909}\n{'loss': 0.2254, 'grad_norm': 0.6746777892112732, 'learning_rate': 0.00033753723558925335, 'epoch': 2.1818181818181817}\n{'loss': 0.2005, 'grad_norm': 0.9126541614532471, 'learning_rate': 0.00033622386113170763, 'epoch': 2.2045454545454546}\n{'loss': 0.1548, 'grad_norm': 0.652563750743866, 'learning_rate': 0.0003349104866741619, 'epoch': 2.227272727272727}\n{'loss': 0.2425, 'grad_norm': 2.1874465942382812, 'learning_rate': 0.00033359711221661614, 'epoch': 2.25}\n{'loss': 0.2038, 'grad_norm': 1.2641620635986328, 'learning_rate': 0.00033228373775907047, 'epoch': 2.2727272727272725}\n{'loss': 0.1536, 'grad_norm': 0.8117364048957825, 'learning_rate': 0.0003309703633015247, 'epoch': 2.2954545454545454}\n{'loss': 0.1639, 'grad_norm': 1.0410828590393066, 'learning_rate': 0.000329656988843979, 'epoch': 2.3181818181818183}\n{'loss': 0.1467, 'grad_norm': 0.708564043045044, 'learning_rate': 0.00032834361438643326, 'epoch': 2.340909090909091}\n{'loss': 0.1719, 'grad_norm': 0.7350467443466187, 'learning_rate': 0.0003270302399288875, 'epoch': 2.3636363636363638}\n{'loss': 0.1719, 'grad_norm': 0.9091885089874268, 'learning_rate': 0.00032571686547134177, 'epoch': 2.3863636363636362}\n{'loss': 0.1705, 'grad_norm': 0.7907254695892334, 'learning_rate': 0.00032440349101379605, 'epoch': 2.409090909090909}\n{'loss': 0.1546, 'grad_norm': 1.1847901344299316, 'learning_rate': 0.0003230901165562503, 'epoch': 2.4318181818181817}\n{'loss': 0.1615, 'grad_norm': 1.2098156213760376, 'learning_rate': 0.0003217767420987046, 'epoch': 2.4545454545454546}\n{'loss': 0.1806, 'grad_norm': 1.7509175539016724, 'learning_rate': 0.00032046336764115883, 'epoch': 2.4772727272727275}\n{'loss': 0.1578, 'grad_norm': 0.6650457382202148, 'learning_rate': 0.0003191499931836131, 'epoch': 2.5}\n{'loss': 0.1707, 'grad_norm': 0.8103145956993103, 'learning_rate': 0.0003178366187260674, 'epoch': 2.5227272727272725}\n{'loss': 0.1382, 'grad_norm': 1.3235127925872803, 'learning_rate': 0.0003165232442685216, 'epoch': 2.5454545454545454}\n{'loss': 0.1988, 'grad_norm': 0.9146490693092346, 'learning_rate': 0.00031520986981097595, 'epoch': 2.5681818181818183}\n{'loss': 0.1465, 'grad_norm': 0.8299514651298523, 'learning_rate': 0.0003138964953534302, 'epoch': 2.590909090909091}\n{'loss': 0.2205, 'grad_norm': 1.2836965322494507, 'learning_rate': 0.00031258312089588446, 'epoch': 2.6136363636363638}\n{'loss': 0.1445, 'grad_norm': 1.3260680437088013, 'learning_rate': 0.00031126974643833874, 'epoch': 2.6363636363636362}\n{'loss': 0.1484, 'grad_norm': 1.1575734615325928, 'learning_rate': 0.00030995637198079297, 'epoch': 2.659090909090909}\n{'loss': 0.1436, 'grad_norm': 0.9380964636802673, 'learning_rate': 0.00030864299752324725, 'epoch': 2.6818181818181817}\n{'loss': 0.1816, 'grad_norm': 0.8778814673423767, 'learning_rate': 0.0003073296230657015, 'epoch': 2.7045454545454546}\n{'loss': 0.1417, 'grad_norm': 0.8491044044494629, 'learning_rate': 0.0003060162486081558, 'epoch': 2.7272727272727275}\n{'loss': 0.2062, 'grad_norm': 0.8588978052139282, 'learning_rate': 0.00030470287415061004, 'epoch': 2.75}\n{'loss': 0.1948, 'grad_norm': 0.7644993662834167, 'learning_rate': 0.0003033894996930643, 'epoch': 2.7727272727272725}\n{'loss': 0.1609, 'grad_norm': 1.0419894456863403, 'learning_rate': 0.0003020761252355186, 'epoch': 2.7954545454545454}\n{'loss': 0.1599, 'grad_norm': 0.7181811332702637, 'learning_rate': 0.0003007627507779728, 'epoch': 2.8181818181818183}\n{'loss': 0.1682, 'grad_norm': 1.0495190620422363, 'learning_rate': 0.0002994493763204271, 'epoch': 2.840909090909091}\n{'loss': 0.1745, 'grad_norm': 0.9800789952278137, 'learning_rate': 0.0002981360018628814, 'epoch': 2.8636363636363638}\n{'loss': 0.1716, 'grad_norm': 0.803361177444458, 'learning_rate': 0.00029682262740533566, 'epoch': 2.8863636363636362}\n{'loss': 0.1399, 'grad_norm': 0.8096877932548523, 'learning_rate': 0.00029550925294778994, 'epoch': 2.909090909090909}\n{'loss': 0.1656, 'grad_norm': 1.4747955799102783, 'learning_rate': 0.00029419587849024417, 'epoch': 2.9318181818181817}\n{'loss': 0.1584, 'grad_norm': 0.8822271823883057, 'learning_rate': 0.00029288250403269845, 'epoch': 2.9545454545454546}\n{'loss': 0.144, 'grad_norm': 0.911625325679779, 'learning_rate': 0.00029156912957515273, 'epoch': 2.9772727272727275}\n{'loss': 0.0691, 'grad_norm': 0.9595015645027161, 'learning_rate': 0.00029025575511760696, 'epoch': 3.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.9190    0.8437    0.8798      3404\n     PERCENT     0.9565    0.9296    0.9429        71\n        TYPE     0.9306    0.9745    0.9520     11194\n      VOLUME     0.8519    0.8214    0.8364        56\n\n   micro avg     0.9280    0.9434    0.9356     14725\n   macro avg     0.9145    0.8923    0.9027     14725\nweighted avg     0.9277    0.9434    0.9348     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.2763146758079529, 'eval_f1_macro': 0.9027466438355437, 'eval_precision': 0.9279893119572479, 'eval_recall': 0.9434295415959253, 'eval_f1': 0.9356457316046471, 'eval_accuracy': 0.9262182755029327, 'eval_runtime': 1.9053, 'eval_samples_per_second': 2892.422, 'eval_steps_per_second': 5.773, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.1192, 'grad_norm': 1.1967097520828247, 'learning_rate': 0.0002889423806600613, 'epoch': 3.022727272727273}\n{'loss': 0.1208, 'grad_norm': 0.978456437587738, 'learning_rate': 0.0002876290062025155, 'epoch': 3.0454545454545454}\n{'loss': 0.1289, 'grad_norm': 1.0252923965454102, 'learning_rate': 0.0002863156317449698, 'epoch': 3.0681818181818183}\n{'loss': 0.0845, 'grad_norm': 0.9271978735923767, 'learning_rate': 0.0002850022572874241, 'epoch': 3.090909090909091}\n{'loss': 0.0991, 'grad_norm': 0.9370490312576294, 'learning_rate': 0.0002836888828298783, 'epoch': 3.1136363636363638}\n{'loss': 0.1539, 'grad_norm': 1.1825292110443115, 'learning_rate': 0.0002823755083723326, 'epoch': 3.1363636363636362}\n{'loss': 0.1146, 'grad_norm': 0.7869274020195007, 'learning_rate': 0.0002810621339147868, 'epoch': 3.159090909090909}\n{'loss': 0.1057, 'grad_norm': 0.653704047203064, 'learning_rate': 0.00027974875945724115, 'epoch': 3.1818181818181817}\n{'loss': 0.1053, 'grad_norm': 0.9052488207817078, 'learning_rate': 0.0002784353849996954, 'epoch': 3.2045454545454546}\n{'loss': 0.0957, 'grad_norm': 0.9926201105117798, 'learning_rate': 0.00027712201054214965, 'epoch': 3.227272727272727}\n{'loss': 0.0917, 'grad_norm': 1.4458954334259033, 'learning_rate': 0.00027580863608460393, 'epoch': 3.25}\n{'loss': 0.1011, 'grad_norm': 0.7186578512191772, 'learning_rate': 0.00027449526162705816, 'epoch': 3.2727272727272725}\n{'loss': 0.1113, 'grad_norm': 0.9842709898948669, 'learning_rate': 0.00027318188716951244, 'epoch': 3.2954545454545454}\n{'loss': 0.1184, 'grad_norm': 1.421773076057434, 'learning_rate': 0.0002718685127119667, 'epoch': 3.3181818181818183}\n{'loss': 0.0929, 'grad_norm': 0.6851098537445068, 'learning_rate': 0.000270555138254421, 'epoch': 3.340909090909091}\n{'loss': 0.1158, 'grad_norm': 0.9734802842140198, 'learning_rate': 0.0002692417637968753, 'epoch': 3.3636363636363638}\n{'loss': 0.1199, 'grad_norm': 1.5093729496002197, 'learning_rate': 0.0002679283893393295, 'epoch': 3.3863636363636362}\n{'loss': 0.1018, 'grad_norm': 0.8984352350234985, 'learning_rate': 0.0002666150148817838, 'epoch': 3.409090909090909}\n{'loss': 0.1364, 'grad_norm': 0.996955931186676, 'learning_rate': 0.00026530164042423807, 'epoch': 3.4318181818181817}\n{'loss': 0.1238, 'grad_norm': 2.5162224769592285, 'learning_rate': 0.0002639882659666923, 'epoch': 3.4545454545454546}\n{'loss': 0.1049, 'grad_norm': 1.1953502893447876, 'learning_rate': 0.00026267489150914663, 'epoch': 3.4772727272727275}\n{'loss': 0.0918, 'grad_norm': 1.7685970067977905, 'learning_rate': 0.00026136151705160086, 'epoch': 3.5}\n{'loss': 0.0967, 'grad_norm': 0.9661529660224915, 'learning_rate': 0.00026004814259405514, 'epoch': 3.5227272727272725}\n{'loss': 0.132, 'grad_norm': 1.2286779880523682, 'learning_rate': 0.0002587347681365094, 'epoch': 3.5454545454545454}\n{'loss': 0.1143, 'grad_norm': 1.1017481088638306, 'learning_rate': 0.00025742139367896364, 'epoch': 3.5681818181818183}\n{'loss': 0.1374, 'grad_norm': 1.2220557928085327, 'learning_rate': 0.0002561080192214179, 'epoch': 3.590909090909091}\n{'loss': 0.1012, 'grad_norm': 1.122139573097229, 'learning_rate': 0.0002547946447638722, 'epoch': 3.6136363636363638}\n{'loss': 0.0965, 'grad_norm': 0.8008243441581726, 'learning_rate': 0.0002534812703063265, 'epoch': 3.6363636363636362}\n{'loss': 0.1352, 'grad_norm': 1.3675662279129028, 'learning_rate': 0.00025216789584878076, 'epoch': 3.659090909090909}\n{'loss': 0.1064, 'grad_norm': 0.7856431603431702, 'learning_rate': 0.000250854521391235, 'epoch': 3.6818181818181817}\n{'loss': 0.1166, 'grad_norm': 0.9879252314567566, 'learning_rate': 0.00024954114693368927, 'epoch': 3.7045454545454546}\n{'loss': 0.1018, 'grad_norm': 0.7899501323699951, 'learning_rate': 0.0002482277724761435, 'epoch': 3.7272727272727275}\n{'loss': 0.1044, 'grad_norm': 0.9334518909454346, 'learning_rate': 0.0002469143980185978, 'epoch': 3.75}\n{'loss': 0.1019, 'grad_norm': 0.7334268689155579, 'learning_rate': 0.0002456010235610521, 'epoch': 3.7727272727272725}\n{'loss': 0.1008, 'grad_norm': 0.8740071654319763, 'learning_rate': 0.00024428764910350634, 'epoch': 3.7954545454545454}\n{'loss': 0.1492, 'grad_norm': 1.402345061302185, 'learning_rate': 0.00024297427464596062, 'epoch': 3.8181818181818183}\n{'loss': 0.1135, 'grad_norm': 2.150465250015259, 'learning_rate': 0.00024166090018841487, 'epoch': 3.840909090909091}\n{'loss': 0.1166, 'grad_norm': 1.4600932598114014, 'learning_rate': 0.00024034752573086912, 'epoch': 3.8636363636363638}\n{'loss': 0.116, 'grad_norm': 1.598484754562378, 'learning_rate': 0.00023903415127332338, 'epoch': 3.8863636363636362}\n{'loss': 0.1219, 'grad_norm': 1.9377570152282715, 'learning_rate': 0.00023772077681577769, 'epoch': 3.909090909090909}\n{'loss': 0.1208, 'grad_norm': 2.377748489379883, 'learning_rate': 0.00023640740235823194, 'epoch': 3.9318181818181817}\n{'loss': 0.1132, 'grad_norm': 1.4057153463363647, 'learning_rate': 0.00023509402790068622, 'epoch': 3.9545454545454546}\n{'loss': 0.1123, 'grad_norm': 0.7179428935050964, 'learning_rate': 0.00023378065344314047, 'epoch': 3.9772727272727275}\n{'loss': 0.105, 'grad_norm': 2.9747636318206787, 'learning_rate': 0.00023246727898559473, 'epoch': 4.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.9178    0.8599    0.8879      3404\n     PERCENT     0.9565    0.9296    0.9429        71\n        TYPE     0.9353    0.9755    0.9550     11194\n      VOLUME     0.8545    0.8393    0.8468        56\n\n   micro avg     0.9314    0.9480    0.9397     14725\n   macro avg     0.9161    0.9011    0.9082     14725\nweighted avg     0.9311    0.9480    0.9390     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.27717867493629456, 'eval_f1_macro': 0.9081550005691312, 'eval_precision': 0.9314117961035495, 'eval_recall': 0.9480475382003396, 'eval_f1': 0.9396560428095446, 'eval_accuracy': 0.930658334703722, 'eval_runtime': 1.6338, 'eval_samples_per_second': 3373.053, 'eval_steps_per_second': 6.733, 'epoch': 4.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.0898, 'grad_norm': 1.4360098838806152, 'learning_rate': 0.00023115390452804898, 'epoch': 4.0227272727272725}\n{'loss': 0.0841, 'grad_norm': 1.279845952987671, 'learning_rate': 0.00022984053007050326, 'epoch': 4.045454545454546}\n{'loss': 0.0888, 'grad_norm': 1.3091028928756714, 'learning_rate': 0.00022852715561295757, 'epoch': 4.068181818181818}\n{'loss': 0.0765, 'grad_norm': 0.8490116596221924, 'learning_rate': 0.00022721378115541182, 'epoch': 4.090909090909091}\n{'loss': 0.0649, 'grad_norm': 0.5663594007492065, 'learning_rate': 0.00022590040669786607, 'epoch': 4.113636363636363}\n{'loss': 0.0669, 'grad_norm': 0.8127403855323792, 'learning_rate': 0.00022458703224032033, 'epoch': 4.136363636363637}\n{'loss': 0.0874, 'grad_norm': 0.7577750086784363, 'learning_rate': 0.0002232736577827746, 'epoch': 4.159090909090909}\n{'loss': 0.073, 'grad_norm': 0.9388136863708496, 'learning_rate': 0.00022196028332522886, 'epoch': 4.181818181818182}\n{'loss': 0.0974, 'grad_norm': 1.4740036725997925, 'learning_rate': 0.00022064690886768311, 'epoch': 4.204545454545454}\n{'loss': 0.0958, 'grad_norm': 1.3266857862472534, 'learning_rate': 0.00021933353441013742, 'epoch': 4.2272727272727275}\n{'loss': 0.0841, 'grad_norm': 1.0626487731933594, 'learning_rate': 0.00021802015995259168, 'epoch': 4.25}\n{'loss': 0.052, 'grad_norm': 0.5153835415840149, 'learning_rate': 0.00021670678549504596, 'epoch': 4.2727272727272725}\n{'loss': 0.0705, 'grad_norm': 0.5973855257034302, 'learning_rate': 0.0002153934110375002, 'epoch': 4.295454545454546}\n{'loss': 0.0842, 'grad_norm': 0.9571816921234131, 'learning_rate': 0.00021408003657995446, 'epoch': 4.318181818181818}\n{'loss': 0.0563, 'grad_norm': 0.5253652930259705, 'learning_rate': 0.00021276666212240872, 'epoch': 4.340909090909091}\n{'loss': 0.0681, 'grad_norm': 1.3153163194656372, 'learning_rate': 0.00021145328766486302, 'epoch': 4.363636363636363}\n{'loss': 0.0656, 'grad_norm': 0.661461591720581, 'learning_rate': 0.0002101399132073173, 'epoch': 4.386363636363637}\n{'loss': 0.0573, 'grad_norm': 0.7056748867034912, 'learning_rate': 0.00020882653874977156, 'epoch': 4.409090909090909}\n{'loss': 0.0721, 'grad_norm': 0.8674911260604858, 'learning_rate': 0.0002075131642922258, 'epoch': 4.431818181818182}\n{'loss': 0.0699, 'grad_norm': 0.8324390649795532, 'learning_rate': 0.00020619978983468006, 'epoch': 4.454545454545454}\n{'loss': 0.0785, 'grad_norm': 0.8479166030883789, 'learning_rate': 0.00020488641537713434, 'epoch': 4.4772727272727275}\n{'loss': 0.0652, 'grad_norm': 0.6419006586074829, 'learning_rate': 0.00020357304091958862, 'epoch': 4.5}\n{'loss': 0.0785, 'grad_norm': 0.7397770881652832, 'learning_rate': 0.00020225966646204288, 'epoch': 4.5227272727272725}\n{'loss': 0.0737, 'grad_norm': 0.81606125831604, 'learning_rate': 0.00020094629200449716, 'epoch': 4.545454545454545}\n{'loss': 0.0826, 'grad_norm': 1.064512848854065, 'learning_rate': 0.0001996329175469514, 'epoch': 4.568181818181818}\n{'loss': 0.0963, 'grad_norm': 1.4960488080978394, 'learning_rate': 0.00019831954308940566, 'epoch': 4.590909090909091}\n{'loss': 0.0559, 'grad_norm': 1.1658719778060913, 'learning_rate': 0.00019700616863185994, 'epoch': 4.613636363636363}\n{'loss': 0.0864, 'grad_norm': 0.5456340312957764, 'learning_rate': 0.00019569279417431423, 'epoch': 4.636363636363637}\n{'loss': 0.0733, 'grad_norm': 1.3154683113098145, 'learning_rate': 0.00019437941971676848, 'epoch': 4.659090909090909}\n{'loss': 0.0881, 'grad_norm': 0.9889085292816162, 'learning_rate': 0.00019306604525922273, 'epoch': 4.681818181818182}\n{'loss': 0.0601, 'grad_norm': 0.711429238319397, 'learning_rate': 0.000191752670801677, 'epoch': 4.704545454545455}\n{'loss': 0.0833, 'grad_norm': 0.9971412420272827, 'learning_rate': 0.0001904392963441313, 'epoch': 4.7272727272727275}\n{'loss': 0.1096, 'grad_norm': 1.268610954284668, 'learning_rate': 0.00018912592188658555, 'epoch': 4.75}\n{'loss': 0.0641, 'grad_norm': 1.097248911857605, 'learning_rate': 0.00018781254742903983, 'epoch': 4.7727272727272725}\n{'loss': 0.1032, 'grad_norm': 0.8071097135543823, 'learning_rate': 0.00018649917297149408, 'epoch': 4.795454545454545}\n{'loss': 0.0967, 'grad_norm': 1.2506051063537598, 'learning_rate': 0.00018518579851394833, 'epoch': 4.818181818181818}\n{'loss': 0.0728, 'grad_norm': 0.6676967144012451, 'learning_rate': 0.0001838724240564026, 'epoch': 4.840909090909091}\n{'loss': 0.0874, 'grad_norm': 1.0846599340438843, 'learning_rate': 0.0001825590495988569, 'epoch': 4.863636363636363}\n{'loss': 0.0526, 'grad_norm': 0.8603024482727051, 'learning_rate': 0.00018124567514131115, 'epoch': 4.886363636363637}\n{'loss': 0.0944, 'grad_norm': 0.6970887780189514, 'learning_rate': 0.0001799323006837654, 'epoch': 4.909090909090909}\n{'loss': 0.0546, 'grad_norm': 0.9243808388710022, 'learning_rate': 0.00017861892622621968, 'epoch': 4.931818181818182}\n{'loss': 0.07, 'grad_norm': 0.8665964603424072, 'learning_rate': 0.00017730555176867396, 'epoch': 4.954545454545455}\n{'loss': 0.113, 'grad_norm': 0.8677573800086975, 'learning_rate': 0.00017599217731112821, 'epoch': 4.9772727272727275}\n{'loss': 0.0167, 'grad_norm': 1.0233557224273682, 'learning_rate': 0.0001746788028535825, 'epoch': 5.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.9027    0.8772    0.8897      3404\n     PERCENT     0.9429    0.9296    0.9362        71\n        TYPE     0.9372    0.9745    0.9555     11194\n      VOLUME     0.8654    0.8036    0.8333        56\n\n   micro avg     0.9294    0.9511    0.9401     14725\n   macro avg     0.9120    0.8962    0.9037     14725\nweighted avg     0.9290    0.9511    0.9397     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.2826681137084961, 'eval_f1_macro': 0.9036781129773773, 'eval_precision': 0.9293914659234189, 'eval_recall': 0.9511035653650255, 'eval_f1': 0.9401221722494462, 'eval_accuracy': 0.9320835388916296, 'eval_runtime': 1.525, 'eval_samples_per_second': 3613.719, 'eval_steps_per_second': 7.213, 'epoch': 5.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.0583, 'grad_norm': 0.8789810538291931, 'learning_rate': 0.00017336542839603675, 'epoch': 5.0227272727272725}\n{'loss': 0.0796, 'grad_norm': 0.8703426718711853, 'learning_rate': 0.000172052053938491, 'epoch': 5.045454545454546}\n{'loss': 0.0522, 'grad_norm': 0.6752282977104187, 'learning_rate': 0.0001707386794809453, 'epoch': 5.068181818181818}\n{'loss': 0.0494, 'grad_norm': 0.7800913453102112, 'learning_rate': 0.00016942530502339956, 'epoch': 5.090909090909091}\n{'loss': 0.0659, 'grad_norm': 0.5830095410346985, 'learning_rate': 0.00016811193056585382, 'epoch': 5.113636363636363}\n{'loss': 0.0588, 'grad_norm': 0.6544346213340759, 'learning_rate': 0.00016679855610830807, 'epoch': 5.136363636363637}\n{'loss': 0.071, 'grad_norm': 0.8305017948150635, 'learning_rate': 0.00016548518165076235, 'epoch': 5.159090909090909}\n{'loss': 0.0636, 'grad_norm': 1.3206651210784912, 'learning_rate': 0.00016417180719321663, 'epoch': 5.181818181818182}\n{'loss': 0.0505, 'grad_norm': 0.803171694278717, 'learning_rate': 0.00016285843273567088, 'epoch': 5.204545454545454}\n{'loss': 0.0607, 'grad_norm': 1.4191327095031738, 'learning_rate': 0.00016154505827812516, 'epoch': 5.2272727272727275}\n{'loss': 0.0599, 'grad_norm': 1.3799012899398804, 'learning_rate': 0.00016023168382057942, 'epoch': 5.25}\n{'loss': 0.0633, 'grad_norm': 0.9495078921318054, 'learning_rate': 0.0001589183093630337, 'epoch': 5.2727272727272725}\n{'loss': 0.0507, 'grad_norm': 0.6267384886741638, 'learning_rate': 0.00015760493490548798, 'epoch': 5.295454545454546}\n{'loss': 0.068, 'grad_norm': 1.2235627174377441, 'learning_rate': 0.00015629156044794223, 'epoch': 5.318181818181818}\n{'loss': 0.0751, 'grad_norm': 0.8131128549575806, 'learning_rate': 0.00015497818599039648, 'epoch': 5.340909090909091}\n{'loss': 0.0368, 'grad_norm': 0.8276673555374146, 'learning_rate': 0.00015366481153285074, 'epoch': 5.363636363636363}\n{'loss': 0.0565, 'grad_norm': 1.242232322692871, 'learning_rate': 0.00015235143707530502, 'epoch': 5.386363636363637}\n{'loss': 0.0596, 'grad_norm': 0.8667421340942383, 'learning_rate': 0.0001510380626177593, 'epoch': 5.409090909090909}\n{'loss': 0.0846, 'grad_norm': 0.7896885275840759, 'learning_rate': 0.00014972468816021355, 'epoch': 5.431818181818182}\n{'loss': 0.0385, 'grad_norm': 0.5520067811012268, 'learning_rate': 0.00014841131370266783, 'epoch': 5.454545454545454}\n{'loss': 0.0603, 'grad_norm': 1.5032485723495483, 'learning_rate': 0.00014709793924512209, 'epoch': 5.4772727272727275}\n{'loss': 0.0545, 'grad_norm': 1.5735069513320923, 'learning_rate': 0.00014578456478757637, 'epoch': 5.5}\n{'loss': 0.0611, 'grad_norm': 1.4540092945098877, 'learning_rate': 0.00014447119033003065, 'epoch': 5.5227272727272725}\n{'loss': 0.0737, 'grad_norm': 1.135878086090088, 'learning_rate': 0.0001431578158724849, 'epoch': 5.545454545454545}\n{'loss': 0.0514, 'grad_norm': 0.7809470295906067, 'learning_rate': 0.00014184444141493915, 'epoch': 5.568181818181818}\n{'loss': 0.0385, 'grad_norm': 0.6966434121131897, 'learning_rate': 0.0001405310669573934, 'epoch': 5.590909090909091}\n{'loss': 0.0377, 'grad_norm': 0.687178909778595, 'learning_rate': 0.0001392176924998477, 'epoch': 5.613636363636363}\n{'loss': 0.0674, 'grad_norm': 0.6149886846542358, 'learning_rate': 0.00013790431804230197, 'epoch': 5.636363636363637}\n{'loss': 0.0532, 'grad_norm': 1.4332859516143799, 'learning_rate': 0.00013659094358475622, 'epoch': 5.659090909090909}\n{'loss': 0.0733, 'grad_norm': 1.2026773691177368, 'learning_rate': 0.0001352775691272105, 'epoch': 5.681818181818182}\n{'loss': 0.0461, 'grad_norm': 0.48233336210250854, 'learning_rate': 0.00013396419466966475, 'epoch': 5.704545454545455}\n{'loss': 0.068, 'grad_norm': 1.044418454170227, 'learning_rate': 0.00013265082021211903, 'epoch': 5.7272727272727275}\n{'loss': 0.0682, 'grad_norm': 1.2352886199951172, 'learning_rate': 0.00013133744575457331, 'epoch': 5.75}\n{'loss': 0.0428, 'grad_norm': 0.7608051896095276, 'learning_rate': 0.00013002407129702757, 'epoch': 5.7727272727272725}\n{'loss': 0.0575, 'grad_norm': 1.0691837072372437, 'learning_rate': 0.00012871069683948182, 'epoch': 5.795454545454545}\n{'loss': 0.0468, 'grad_norm': 0.971474826335907, 'learning_rate': 0.0001273973223819361, 'epoch': 5.818181818181818}\n{'loss': 0.0433, 'grad_norm': 0.7085405588150024, 'learning_rate': 0.00012608394792439038, 'epoch': 5.840909090909091}\n{'loss': 0.0669, 'grad_norm': 1.1260654926300049, 'learning_rate': 0.00012477057346684464, 'epoch': 5.863636363636363}\n{'loss': 0.0576, 'grad_norm': 0.7769591808319092, 'learning_rate': 0.0001234571990092989, 'epoch': 5.886363636363637}\n{'loss': 0.0676, 'grad_norm': 0.8288699984550476, 'learning_rate': 0.00012214382455175317, 'epoch': 5.909090909090909}\n{'loss': 0.0792, 'grad_norm': 1.3410723209381104, 'learning_rate': 0.00012083045009420744, 'epoch': 5.931818181818182}\n{'loss': 0.0805, 'grad_norm': 1.1758660078048706, 'learning_rate': 0.00011951707563666169, 'epoch': 5.954545454545455}\n{'loss': 0.0498, 'grad_norm': 0.9957598447799683, 'learning_rate': 0.00011820370117911597, 'epoch': 5.9772727272727275}\n{'loss': 0.0041, 'grad_norm': 0.2329675406217575, 'learning_rate': 0.00011689032672157024, 'epoch': 6.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.9065    0.8828    0.8945      3404\n     PERCENT     0.9429    0.9296    0.9362        71\n        TYPE     0.9425    0.9702    0.9561     11194\n      VOLUME     0.8654    0.8036    0.8333        56\n\n   micro avg     0.9342    0.9491    0.9416     14725\n   macro avg     0.9143    0.8965    0.9050     14725\nweighted avg     0.9339    0.9491    0.9413     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.31039443612098694, 'eval_f1_macro': 0.905023513447161, 'eval_precision': 0.9342245989304813, 'eval_recall': 0.9491341256366723, 'eval_f1': 0.9416203469765875, 'eval_accuracy': 0.9325768788028285, 'eval_runtime': 1.4898, 'eval_samples_per_second': 3699.085, 'eval_steps_per_second': 7.383, 'epoch': 6.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.0481, 'grad_norm': 0.49092042446136475, 'learning_rate': 0.00011557695226402449, 'epoch': 6.0227272727272725}\n{'loss': 0.059, 'grad_norm': 0.8930780291557312, 'learning_rate': 0.00011426357780647878, 'epoch': 6.045454545454546}\n{'loss': 0.0591, 'grad_norm': 0.9498526453971863, 'learning_rate': 0.00011295020334893304, 'epoch': 6.068181818181818}\n{'loss': 0.0476, 'grad_norm': 0.8993210792541504, 'learning_rate': 0.0001116368288913873, 'epoch': 6.090909090909091}\n{'loss': 0.0534, 'grad_norm': 0.766686201095581, 'learning_rate': 0.00011032345443384156, 'epoch': 6.113636363636363}\n{'loss': 0.0322, 'grad_norm': 0.9945966601371765, 'learning_rate': 0.00010901007997629584, 'epoch': 6.136363636363637}\n{'loss': 0.0429, 'grad_norm': 0.7077810764312744, 'learning_rate': 0.0001076967055187501, 'epoch': 6.159090909090909}\n{'loss': 0.0562, 'grad_norm': 0.739857017993927, 'learning_rate': 0.00010638333106120436, 'epoch': 6.181818181818182}\n{'loss': 0.05, 'grad_norm': 0.8258619904518127, 'learning_rate': 0.00010506995660365865, 'epoch': 6.204545454545454}\n{'loss': 0.0265, 'grad_norm': 0.45968741178512573, 'learning_rate': 0.0001037565821461129, 'epoch': 6.2272727272727275}\n{'loss': 0.0474, 'grad_norm': 0.6147899031639099, 'learning_rate': 0.00010244320768856717, 'epoch': 6.25}\n{'loss': 0.031, 'grad_norm': 0.5563860535621643, 'learning_rate': 0.00010112983323102144, 'epoch': 6.2727272727272725}\n{'loss': 0.0335, 'grad_norm': 1.3424270153045654, 'learning_rate': 9.98164587734757e-05, 'epoch': 6.295454545454546}\n{'loss': 0.0296, 'grad_norm': 0.5167021155357361, 'learning_rate': 9.850308431592997e-05, 'epoch': 6.318181818181818}\n{'loss': 0.0407, 'grad_norm': 0.5867025256156921, 'learning_rate': 9.718970985838424e-05, 'epoch': 6.340909090909091}\n{'loss': 0.0447, 'grad_norm': 0.7566986680030823, 'learning_rate': 9.58763354008385e-05, 'epoch': 6.363636363636363}\n{'loss': 0.028, 'grad_norm': 0.6168007850646973, 'learning_rate': 9.456296094329277e-05, 'epoch': 6.386363636363637}\n{'loss': 0.0468, 'grad_norm': 0.5334513187408447, 'learning_rate': 9.324958648574704e-05, 'epoch': 6.409090909090909}\n{'loss': 0.049, 'grad_norm': 0.7578364014625549, 'learning_rate': 9.19362120282013e-05, 'epoch': 6.431818181818182}\n{'loss': 0.0723, 'grad_norm': 0.9815953969955444, 'learning_rate': 9.062283757065557e-05, 'epoch': 6.454545454545454}\n{'loss': 0.047, 'grad_norm': 0.6002040505409241, 'learning_rate': 8.930946311310984e-05, 'epoch': 6.4772727272727275}\n{'loss': 0.0401, 'grad_norm': 1.22873854637146, 'learning_rate': 8.799608865556411e-05, 'epoch': 6.5}\n{'loss': 0.0377, 'grad_norm': 0.9404480457305908, 'learning_rate': 8.668271419801837e-05, 'epoch': 6.5227272727272725}\n{'loss': 0.0384, 'grad_norm': 1.2772159576416016, 'learning_rate': 8.536933974047265e-05, 'epoch': 6.545454545454545}\n{'loss': 0.0404, 'grad_norm': 0.43664711713790894, 'learning_rate': 8.405596528292691e-05, 'epoch': 6.568181818181818}\n{'loss': 0.0295, 'grad_norm': 0.5690810084342957, 'learning_rate': 8.274259082538117e-05, 'epoch': 6.590909090909091}\n{'loss': 0.04, 'grad_norm': 0.6253973245620728, 'learning_rate': 8.142921636783544e-05, 'epoch': 6.613636363636363}\n{'loss': 0.0425, 'grad_norm': 0.5670291781425476, 'learning_rate': 8.011584191028971e-05, 'epoch': 6.636363636363637}\n{'loss': 0.0542, 'grad_norm': 0.6862300038337708, 'learning_rate': 7.880246745274399e-05, 'epoch': 6.659090909090909}\n{'loss': 0.0428, 'grad_norm': 0.9609952569007874, 'learning_rate': 7.748909299519824e-05, 'epoch': 6.681818181818182}\n{'loss': 0.0546, 'grad_norm': 0.7423112988471985, 'learning_rate': 7.617571853765251e-05, 'epoch': 6.704545454545455}\n{'loss': 0.056, 'grad_norm': 1.017886996269226, 'learning_rate': 7.486234408010678e-05, 'epoch': 6.7272727272727275}\n{'loss': 0.0907, 'grad_norm': 1.4884885549545288, 'learning_rate': 7.354896962256104e-05, 'epoch': 6.75}\n{'loss': 0.0609, 'grad_norm': 1.0812885761260986, 'learning_rate': 7.223559516501532e-05, 'epoch': 6.7727272727272725}\n{'loss': 0.0456, 'grad_norm': 0.831322193145752, 'learning_rate': 7.092222070746958e-05, 'epoch': 6.795454545454545}\n{'loss': 0.0407, 'grad_norm': 0.6330180168151855, 'learning_rate': 6.960884624992386e-05, 'epoch': 6.818181818181818}\n{'loss': 0.0419, 'grad_norm': 0.5546965599060059, 'learning_rate': 6.829547179237811e-05, 'epoch': 6.840909090909091}\n{'loss': 0.0461, 'grad_norm': 0.9235344529151917, 'learning_rate': 6.698209733483238e-05, 'epoch': 6.863636363636363}\n{'loss': 0.0602, 'grad_norm': 0.9100403189659119, 'learning_rate': 6.566872287728666e-05, 'epoch': 6.886363636363637}\n{'loss': 0.0532, 'grad_norm': 0.9775882959365845, 'learning_rate': 6.435534841974091e-05, 'epoch': 6.909090909090909}\n{'loss': 0.0404, 'grad_norm': 0.9574962854385376, 'learning_rate': 6.304197396219519e-05, 'epoch': 6.931818181818182}\n{'loss': 0.0212, 'grad_norm': 0.5595705509185791, 'learning_rate': 6.172859950464944e-05, 'epoch': 6.954545454545455}\n{'loss': 0.0497, 'grad_norm': 0.526803731918335, 'learning_rate': 6.041522504710372e-05, 'epoch': 6.9772727272727275}\n{'loss': 0.0083, 'grad_norm': 0.871880292892456, 'learning_rate': 5.9101850589557985e-05, 'epoch': 7.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.9026    0.8793    0.8908      3404\n     PERCENT     0.9429    0.9296    0.9362        71\n        TYPE     0.9427    0.9694    0.9559     11194\n      VOLUME     0.8868    0.8393    0.8624        56\n\n   micro avg     0.9336    0.9479    0.9407     14725\n   macro avg     0.9187    0.9044    0.9113     14725\nweighted avg     0.9332    0.9479    0.9404     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.31588754057884216, 'eval_f1_macro': 0.9113000095902941, 'eval_precision': 0.9335830379238846, 'eval_recall': 0.9479117147707979, 'eval_f1': 0.9406928157433616, 'eval_accuracy': 0.9313709367976758, 'eval_runtime': 1.463, 'eval_samples_per_second': 3766.938, 'eval_steps_per_second': 7.519, 'epoch': 7.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.031, 'grad_norm': 0.8065234422683716, 'learning_rate': 5.7788476132012245e-05, 'epoch': 7.0227272727272725}\n{'loss': 0.044, 'grad_norm': 1.1062041521072388, 'learning_rate': 5.647510167446652e-05, 'epoch': 7.045454545454546}\n{'loss': 0.0264, 'grad_norm': 0.7897689342498779, 'learning_rate': 5.516172721692078e-05, 'epoch': 7.068181818181818}\n{'loss': 0.0536, 'grad_norm': 0.5817291736602783, 'learning_rate': 5.384835275937505e-05, 'epoch': 7.090909090909091}\n{'loss': 0.0427, 'grad_norm': 0.6245102882385254, 'learning_rate': 5.2534978301829326e-05, 'epoch': 7.113636363636363}\n{'loss': 0.0433, 'grad_norm': 1.0255231857299805, 'learning_rate': 5.1221603844283586e-05, 'epoch': 7.136363636363637}\n{'loss': 0.0407, 'grad_norm': 1.0560451745986938, 'learning_rate': 4.990822938673785e-05, 'epoch': 7.159090909090909}\n{'loss': 0.0293, 'grad_norm': 0.45653897523880005, 'learning_rate': 4.859485492919212e-05, 'epoch': 7.181818181818182}\n{'loss': 0.037, 'grad_norm': 0.9242743253707886, 'learning_rate': 4.7281480471646386e-05, 'epoch': 7.204545454545454}\n{'loss': 0.0459, 'grad_norm': 0.6379827857017517, 'learning_rate': 4.596810601410065e-05, 'epoch': 7.2272727272727275}\n{'loss': 0.028, 'grad_norm': 0.6871861815452576, 'learning_rate': 4.465473155655492e-05, 'epoch': 7.25}\n{'loss': 0.0287, 'grad_norm': 0.5904135704040527, 'learning_rate': 4.334135709900919e-05, 'epoch': 7.2727272727272725}\n{'loss': 0.0544, 'grad_norm': 0.7932512760162354, 'learning_rate': 4.2027982641463454e-05, 'epoch': 7.295454545454546}\n{'loss': 0.042, 'grad_norm': 0.6296669840812683, 'learning_rate': 4.071460818391772e-05, 'epoch': 7.318181818181818}\n{'loss': 0.0466, 'grad_norm': 1.0240130424499512, 'learning_rate': 3.9401233726371994e-05, 'epoch': 7.340909090909091}\n{'loss': 0.0471, 'grad_norm': 1.2358866930007935, 'learning_rate': 3.8087859268826254e-05, 'epoch': 7.363636363636363}\n{'loss': 0.0265, 'grad_norm': 0.42447543144226074, 'learning_rate': 3.677448481128052e-05, 'epoch': 7.386363636363637}\n{'loss': 0.0191, 'grad_norm': 0.35284918546676636, 'learning_rate': 3.546111035373479e-05, 'epoch': 7.409090909090909}\n{'loss': 0.0509, 'grad_norm': 0.5527624487876892, 'learning_rate': 3.4147735896189055e-05, 'epoch': 7.431818181818182}\n{'loss': 0.0272, 'grad_norm': 0.748949408531189, 'learning_rate': 3.283436143864333e-05, 'epoch': 7.454545454545454}\n{'loss': 0.0535, 'grad_norm': 0.7601614594459534, 'learning_rate': 3.1520986981097595e-05, 'epoch': 7.4772727272727275}\n{'loss': 0.0469, 'grad_norm': 0.6551756262779236, 'learning_rate': 3.020761252355186e-05, 'epoch': 7.5}\n{'loss': 0.0369, 'grad_norm': 0.5867534279823303, 'learning_rate': 2.8894238066006122e-05, 'epoch': 7.5227272727272725}\n{'loss': 0.0419, 'grad_norm': 0.7657686471939087, 'learning_rate': 2.758086360846039e-05, 'epoch': 7.545454545454545}\n{'loss': 0.0365, 'grad_norm': 0.4584534764289856, 'learning_rate': 2.6267489150914663e-05, 'epoch': 7.568181818181818}\n{'loss': 0.0563, 'grad_norm': 0.6935437917709351, 'learning_rate': 2.4954114693368926e-05, 'epoch': 7.590909090909091}\n{'loss': 0.0392, 'grad_norm': 0.5142977833747864, 'learning_rate': 2.3640740235823193e-05, 'epoch': 7.613636363636363}\n{'loss': 0.0454, 'grad_norm': 1.0411721467971802, 'learning_rate': 2.232736577827746e-05, 'epoch': 7.636363636363637}\n{'loss': 0.0355, 'grad_norm': 0.519764244556427, 'learning_rate': 2.1013991320731727e-05, 'epoch': 7.659090909090909}\n{'loss': 0.0481, 'grad_norm': 0.8673493266105652, 'learning_rate': 1.9700616863185997e-05, 'epoch': 7.681818181818182}\n{'loss': 0.0532, 'grad_norm': 0.5518354177474976, 'learning_rate': 1.838724240564026e-05, 'epoch': 7.704545454545455}\n{'loss': 0.039, 'grad_norm': 1.261722445487976, 'learning_rate': 1.7073867948094528e-05, 'epoch': 7.7272727272727275}\n{'loss': 0.0302, 'grad_norm': 0.5012434124946594, 'learning_rate': 1.5760493490548798e-05, 'epoch': 7.75}\n{'loss': 0.0475, 'grad_norm': 0.7503372430801392, 'learning_rate': 1.4447119033003061e-05, 'epoch': 7.7727272727272725}\n{'loss': 0.0453, 'grad_norm': 0.5890247821807861, 'learning_rate': 1.3133744575457331e-05, 'epoch': 7.795454545454545}\n{'loss': 0.0304, 'grad_norm': 0.7082986235618591, 'learning_rate': 1.1820370117911597e-05, 'epoch': 7.818181818181818}\n{'loss': 0.0272, 'grad_norm': 0.4034644365310669, 'learning_rate': 1.0506995660365863e-05, 'epoch': 7.840909090909091}\n{'loss': 0.0499, 'grad_norm': 0.7009092569351196, 'learning_rate': 9.19362120282013e-06, 'epoch': 7.863636363636363}\n{'loss': 0.0541, 'grad_norm': 0.8139311671257019, 'learning_rate': 7.880246745274399e-06, 'epoch': 7.886363636363637}\n{'loss': 0.0201, 'grad_norm': 0.5626868605613708, 'learning_rate': 6.566872287728666e-06, 'epoch': 7.909090909090909}\n{'loss': 0.025, 'grad_norm': 0.5612605214118958, 'learning_rate': 5.253497830182932e-06, 'epoch': 7.931818181818182}\n{'loss': 0.0287, 'grad_norm': 0.5664801597595215, 'learning_rate': 3.9401233726371994e-06, 'epoch': 7.954545454545455}\n{'loss': 0.0718, 'grad_norm': 0.702916145324707, 'learning_rate': 2.626748915091466e-06, 'epoch': 7.9772727272727275}\n{'loss': 0.0247, 'grad_norm': 2.383171319961548, 'learning_rate': 1.313374457545733e-06, 'epoch': 8.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.9034    0.8851    0.8942      3404\n     PERCENT     0.9429    0.9296    0.9362        71\n        TYPE     0.9439    0.9687    0.9562     11194\n      VOLUME     0.8868    0.8393    0.8624        56\n\n   micro avg     0.9347    0.9487    0.9417     14725\n   macro avg     0.9193    0.9057    0.9122     14725\nweighted avg     0.9344    0.9487    0.9414     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.3191380500793457, 'eval_f1_macro': 0.9122325475199153, 'eval_precision': 0.9346982470226147, 'eval_recall': 0.9487266553480476, 'eval_f1': 0.9416602069360656, 'eval_accuracy': 0.9325220632571397, 'eval_runtime': 1.9589, 'eval_samples_per_second': 2813.326, 'eval_steps_per_second': 5.615, 'epoch': 8.0}\n{'train_runtime': 51.4952, 'train_samples_per_second': 3424.164, 'train_steps_per_second': 6.836, 'train_loss': 0.22345118889643345, 'epoch': 8.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nThe speedups for torchdynamo mostly come with GPU Ampere or higher and which is not detected here.\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.9034    0.8851    0.8942      3404\n     PERCENT     0.9429    0.9296    0.9362        71\n        TYPE     0.9439    0.9687    0.9562     11194\n      VOLUME     0.8868    0.8393    0.8624        56\n\n   micro avg     0.9347    0.9487    0.9417     14725\n   macro avg     0.9193    0.9057    0.9122     14725\nweighted avg     0.9344    0.9487    0.9414     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.3191380500793457, 'eval_f1_macro': 0.9122325475199153, 'eval_precision': 0.9346982470226147, 'eval_recall': 0.9487266553480476, 'eval_f1': 0.9416602069360656, 'eval_accuracy': 0.9325220632571397, 'eval_runtime': 1.5996, 'eval_samples_per_second': 3445.129, 'eval_steps_per_second': 6.877, 'epoch': 8.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/3364797558.py:66: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 2.3638, 'grad_norm': 7.670507431030273, 'learning_rate': 0.0, 'epoch': 0.022727272727272728}\n{'loss': 2.3771, 'grad_norm': 7.773203372955322, 'learning_rate': 1.1528509127345877e-05, 'epoch': 0.045454545454545456}\n{'loss': 2.3442, 'grad_norm': 7.699681758880615, 'learning_rate': 2.3057018254691754e-05, 'epoch': 0.06818181818181818}\n{'loss': 2.2824, 'grad_norm': 7.3981146812438965, 'learning_rate': 3.458552738203763e-05, 'epoch': 0.09090909090909091}\n{'loss': 2.2148, 'grad_norm': 7.308290004730225, 'learning_rate': 4.611403650938351e-05, 'epoch': 0.11363636363636363}\n{'loss': 2.1018, 'grad_norm': 7.46167516708374, 'learning_rate': 5.76425456367294e-05, 'epoch': 0.13636363636363635}\n{'loss': 2.0115, 'grad_norm': 6.4928297996521, 'learning_rate': 6.917105476407527e-05, 'epoch': 0.1590909090909091}\n{'loss': 1.8598, 'grad_norm': 6.404950141906738, 'learning_rate': 8.069956389142115e-05, 'epoch': 0.18181818181818182}\n{'loss': 1.7156, 'grad_norm': 5.800938606262207, 'learning_rate': 9.222807301876702e-05, 'epoch': 0.20454545454545456}\n{'loss': 1.5594, 'grad_norm': 4.97749137878418, 'learning_rate': 0.0001037565821461129, 'epoch': 0.22727272727272727}\n{'loss': 1.4618, 'grad_norm': 3.86838436126709, 'learning_rate': 0.0001152850912734588, 'epoch': 0.25}\n{'loss': 1.3565, 'grad_norm': 2.9391653537750244, 'learning_rate': 0.00012681360040080468, 'epoch': 0.2727272727272727}\n{'loss': 1.2356, 'grad_norm': 2.3116023540496826, 'learning_rate': 0.00013834210952815053, 'epoch': 0.29545454545454547}\n{'loss': 1.2138, 'grad_norm': 2.296880006790161, 'learning_rate': 0.0001498706186554964, 'epoch': 0.3181818181818182}\n{'loss': 1.1958, 'grad_norm': 2.5079684257507324, 'learning_rate': 0.0001613991277828423, 'epoch': 0.3409090909090909}\n{'loss': 1.0568, 'grad_norm': 1.601884126663208, 'learning_rate': 0.00017292763691018818, 'epoch': 0.36363636363636365}\n{'loss': 1.0324, 'grad_norm': 1.3266444206237793, 'learning_rate': 0.00018445614603753403, 'epoch': 0.38636363636363635}\n{'loss': 0.9812, 'grad_norm': 1.9410885572433472, 'learning_rate': 0.00019598465516487993, 'epoch': 0.4090909090909091}\n{'loss': 0.9593, 'grad_norm': 2.449084997177124, 'learning_rate': 0.0002075131642922258, 'epoch': 0.4318181818181818}\n{'loss': 0.9533, 'grad_norm': 1.8888076543807983, 'learning_rate': 0.00021904167341957169, 'epoch': 0.45454545454545453}\n{'loss': 0.8837, 'grad_norm': 1.1653828620910645, 'learning_rate': 0.0002305701825469176, 'epoch': 0.4772727272727273}\n{'loss': 0.859, 'grad_norm': 1.5515727996826172, 'learning_rate': 0.00024209869167426346, 'epoch': 0.5}\n{'loss': 0.7999, 'grad_norm': 1.685335397720337, 'learning_rate': 0.00025362720080160937, 'epoch': 0.5227272727272727}\n{'loss': 0.7226, 'grad_norm': 0.8885666728019714, 'learning_rate': 0.0002651557099289552, 'epoch': 0.5454545454545454}\n{'loss': 0.6769, 'grad_norm': 1.304060459136963, 'learning_rate': 0.00027668421905630106, 'epoch': 0.5681818181818182}\n{'loss': 0.733, 'grad_norm': 1.5016181468963623, 'learning_rate': 0.00028821272818364694, 'epoch': 0.5909090909090909}\n{'loss': 0.7124, 'grad_norm': 0.8640801906585693, 'learning_rate': 0.0002997412373109928, 'epoch': 0.6136363636363636}\n{'loss': 0.7166, 'grad_norm': 1.1996791362762451, 'learning_rate': 0.00031126974643833874, 'epoch': 0.6363636363636364}\n{'loss': 0.565, 'grad_norm': 1.245986819267273, 'learning_rate': 0.0003227982555656846, 'epoch': 0.6590909090909091}\n{'loss': 0.591, 'grad_norm': 1.0715041160583496, 'learning_rate': 0.0003343267646930305, 'epoch': 0.6818181818181818}\n{'loss': 0.6395, 'grad_norm': 1.8680001497268677, 'learning_rate': 0.00034585527382037637, 'epoch': 0.7045454545454546}\n{'loss': 0.6318, 'grad_norm': 1.0230580568313599, 'learning_rate': 0.00035738378294772224, 'epoch': 0.7272727272727273}\n{'loss': 0.586, 'grad_norm': 2.0083889961242676, 'learning_rate': 0.00036891229207506806, 'epoch': 0.75}\n{'loss': 0.5374, 'grad_norm': 1.9507397413253784, 'learning_rate': 0.000380440801202414, 'epoch': 0.7727272727272727}\n{'loss': 0.6239, 'grad_norm': 2.0418500900268555, 'learning_rate': 0.00039196931032975987, 'epoch': 0.7954545454545454}\n{'loss': 0.5579, 'grad_norm': 1.816123127937317, 'learning_rate': 0.00040349781945710574, 'epoch': 0.8181818181818182}\n{'loss': 0.462, 'grad_norm': 1.6860430240631104, 'learning_rate': 0.0004150263285844516, 'epoch': 0.8409090909090909}\n{'loss': 0.4913, 'grad_norm': 1.3736015558242798, 'learning_rate': 0.0004137129541269059, 'epoch': 0.8636363636363636}\n{'loss': 0.4663, 'grad_norm': 0.8911446928977966, 'learning_rate': 0.0004123995796693601, 'epoch': 0.8863636363636364}\n{'loss': 0.4298, 'grad_norm': 1.2663242816925049, 'learning_rate': 0.0004110862052118144, 'epoch': 0.9090909090909091}\n{'loss': 0.4205, 'grad_norm': 1.6566801071166992, 'learning_rate': 0.0004097728307542687, 'epoch': 0.9318181818181818}\n{'loss': 0.4107, 'grad_norm': 0.9459890127182007, 'learning_rate': 0.00040845945629672297, 'epoch': 0.9545454545454546}\n{'loss': 0.4429, 'grad_norm': 1.0556658506393433, 'learning_rate': 0.00040714608183917725, 'epoch': 0.9772727272727273}\n{'loss': 0.4139, 'grad_norm': 4.067317008972168, 'learning_rate': 0.0004058327073816315, 'epoch': 1.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.7361    0.8112    0.7718      3311\n     PERCENT     0.5364    0.9419    0.6835        86\n        TYPE     0.9144    0.9305    0.9224     11299\n      VOLUME     0.2500    0.0714    0.1111        42\n\n   micro avg     0.8677    0.9013    0.8842     14738\n   macro avg     0.6092    0.6888    0.6222     14738\nweighted avg     0.8703    0.9013    0.8849     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.3879649341106415, 'eval_f1_macro': 0.6222241414540538, 'eval_precision': 0.8676681907250163, 'eval_recall': 0.9013434658705387, 'eval_f1': 0.884185303514377, 'eval_accuracy': 0.8830118572755588, 'eval_runtime': 1.5403, 'eval_samples_per_second': 3577.332, 'eval_steps_per_second': 7.142, 'epoch': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.3679, 'grad_norm': 3.1199722290039062, 'learning_rate': 0.00040451933292408575, 'epoch': 1.0227272727272727}\n{'loss': 0.3855, 'grad_norm': 1.4887245893478394, 'learning_rate': 0.00040320595846654, 'epoch': 1.0454545454545454}\n{'loss': 0.3952, 'grad_norm': 1.5628015995025635, 'learning_rate': 0.0004018925840089943, 'epoch': 1.0681818181818181}\n{'loss': 0.3811, 'grad_norm': 1.5114214420318604, 'learning_rate': 0.0004005792095514486, 'epoch': 1.0909090909090908}\n{'loss': 0.3572, 'grad_norm': 0.9316183924674988, 'learning_rate': 0.0003992658350939028, 'epoch': 1.1136363636363635}\n{'loss': 0.4123, 'grad_norm': 0.7588236331939697, 'learning_rate': 0.0003979524606363571, 'epoch': 1.1363636363636362}\n{'loss': 0.2932, 'grad_norm': 0.8208940625190735, 'learning_rate': 0.00039663908617881133, 'epoch': 1.1590909090909092}\n{'loss': 0.3268, 'grad_norm': 0.8927778601646423, 'learning_rate': 0.0003953257117212656, 'epoch': 1.1818181818181819}\n{'loss': 0.327, 'grad_norm': 0.822843074798584, 'learning_rate': 0.0003940123372637199, 'epoch': 1.2045454545454546}\n{'loss': 0.3632, 'grad_norm': 0.8011183142662048, 'learning_rate': 0.00039269896280617417, 'epoch': 1.2272727272727273}\n{'loss': 0.3656, 'grad_norm': 0.9603262543678284, 'learning_rate': 0.00039138558834862845, 'epoch': 1.25}\n{'loss': 0.2915, 'grad_norm': 0.6837120652198792, 'learning_rate': 0.0003900722138910827, 'epoch': 1.2727272727272727}\n{'loss': 0.3021, 'grad_norm': 1.391955852508545, 'learning_rate': 0.00038875883943353696, 'epoch': 1.2954545454545454}\n{'loss': 0.3363, 'grad_norm': 0.7717019319534302, 'learning_rate': 0.00038744546497599124, 'epoch': 1.3181818181818181}\n{'loss': 0.2746, 'grad_norm': 1.0643894672393799, 'learning_rate': 0.00038613209051844546, 'epoch': 1.3409090909090908}\n{'loss': 0.3468, 'grad_norm': 1.596945881843567, 'learning_rate': 0.00038481871606089974, 'epoch': 1.3636363636363638}\n{'loss': 0.3143, 'grad_norm': 2.0107345581054688, 'learning_rate': 0.000383505341603354, 'epoch': 1.3863636363636362}\n{'loss': 0.3427, 'grad_norm': 1.0403342247009277, 'learning_rate': 0.0003821919671458083, 'epoch': 1.4090909090909092}\n{'loss': 0.2745, 'grad_norm': 1.0081151723861694, 'learning_rate': 0.0003808785926882626, 'epoch': 1.4318181818181819}\n{'loss': 0.2833, 'grad_norm': 1.037566065788269, 'learning_rate': 0.0003795652182307168, 'epoch': 1.4545454545454546}\n{'loss': 0.2408, 'grad_norm': 0.8048900365829468, 'learning_rate': 0.0003782518437731711, 'epoch': 1.4772727272727273}\n{'loss': 0.3043, 'grad_norm': 1.1168190240859985, 'learning_rate': 0.0003769384693156253, 'epoch': 1.5}\n{'loss': 0.2752, 'grad_norm': 0.6245654821395874, 'learning_rate': 0.00037562509485807965, 'epoch': 1.5227272727272727}\n{'loss': 0.2355, 'grad_norm': 0.9610766768455505, 'learning_rate': 0.00037431172040053393, 'epoch': 1.5454545454545454}\n{'loss': 0.2376, 'grad_norm': 1.0599353313446045, 'learning_rate': 0.00037299834594298816, 'epoch': 1.5681818181818183}\n{'loss': 0.2762, 'grad_norm': 1.0156760215759277, 'learning_rate': 0.00037168497148544244, 'epoch': 1.5909090909090908}\n{'loss': 0.2496, 'grad_norm': 0.9841946363449097, 'learning_rate': 0.00037037159702789667, 'epoch': 1.6136363636363638}\n{'loss': 0.2843, 'grad_norm': 0.7658679485321045, 'learning_rate': 0.00036905822257035095, 'epoch': 1.6363636363636362}\n{'loss': 0.2369, 'grad_norm': 1.0432833433151245, 'learning_rate': 0.0003677448481128052, 'epoch': 1.6590909090909092}\n{'loss': 0.2878, 'grad_norm': 1.1474685668945312, 'learning_rate': 0.0003664314736552595, 'epoch': 1.6818181818181817}\n{'loss': 0.2626, 'grad_norm': 0.9248504042625427, 'learning_rate': 0.0003651180991977138, 'epoch': 1.7045454545454546}\n{'loss': 0.2681, 'grad_norm': 0.8524587154388428, 'learning_rate': 0.000363804724740168, 'epoch': 1.7272727272727273}\n{'loss': 0.2643, 'grad_norm': 1.240555763244629, 'learning_rate': 0.0003624913502826223, 'epoch': 1.75}\n{'loss': 0.2673, 'grad_norm': 0.9288157224655151, 'learning_rate': 0.0003611779758250766, 'epoch': 1.7727272727272727}\n{'loss': 0.2207, 'grad_norm': 1.1480265855789185, 'learning_rate': 0.0003598646013675308, 'epoch': 1.7954545454545454}\n{'loss': 0.2609, 'grad_norm': 0.7096291184425354, 'learning_rate': 0.00035855122690998514, 'epoch': 1.8181818181818183}\n{'loss': 0.2262, 'grad_norm': 0.8945020437240601, 'learning_rate': 0.00035723785245243936, 'epoch': 1.8409090909090908}\n{'loss': 0.2734, 'grad_norm': 0.9749770164489746, 'learning_rate': 0.00035592447799489364, 'epoch': 1.8636363636363638}\n{'loss': 0.2504, 'grad_norm': 0.8601096868515015, 'learning_rate': 0.0003546111035373479, 'epoch': 1.8863636363636362}\n{'loss': 0.2565, 'grad_norm': 0.9815516471862793, 'learning_rate': 0.00035329772907980215, 'epoch': 1.9090909090909092}\n{'loss': 0.2889, 'grad_norm': 0.6416799426078796, 'learning_rate': 0.00035198435462225643, 'epoch': 1.9318181818181817}\n{'loss': 0.263, 'grad_norm': 0.7426105737686157, 'learning_rate': 0.0003506709801647107, 'epoch': 1.9545454545454546}\n{'loss': 0.2192, 'grad_norm': 0.7068193554878235, 'learning_rate': 0.000349357605707165, 'epoch': 1.9772727272727273}\n{'loss': 0.2158, 'grad_norm': 5.267147064208984, 'learning_rate': 0.00034804423124961927, 'epoch': 2.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8667    0.8816    0.8741      3311\n     PERCENT     0.8085    0.8837    0.8444        86\n        TYPE     0.9431    0.9635    0.9532     11299\n      VOLUME     0.7250    0.6905    0.7073        42\n\n   micro avg     0.9246    0.9439    0.9341     14738\n   macro avg     0.8358    0.8548    0.8448     14738\nweighted avg     0.9245    0.9439    0.9341     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.24409689009189606, 'eval_f1_macro': 0.8447616895208558, 'eval_precision': 0.9245646683503921, 'eval_recall': 0.9438865517709323, 'eval_f1': 0.9341257050765511, 'eval_accuracy': 0.926998524670783, 'eval_runtime': 1.6679, 'eval_samples_per_second': 3303.608, 'eval_steps_per_second': 6.595, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.1533, 'grad_norm': 0.6369402408599854, 'learning_rate': 0.0003467308567920735, 'epoch': 2.022727272727273}\n{'loss': 0.167, 'grad_norm': 0.7154141664505005, 'learning_rate': 0.0003454174823345278, 'epoch': 2.0454545454545454}\n{'loss': 0.1728, 'grad_norm': 0.8680553436279297, 'learning_rate': 0.000344104107876982, 'epoch': 2.0681818181818183}\n{'loss': 0.1915, 'grad_norm': 1.211386799812317, 'learning_rate': 0.0003427907334194363, 'epoch': 2.090909090909091}\n{'loss': 0.159, 'grad_norm': 0.713270366191864, 'learning_rate': 0.0003414773589618906, 'epoch': 2.1136363636363638}\n{'loss': 0.1337, 'grad_norm': 0.7711006999015808, 'learning_rate': 0.00034016398450434484, 'epoch': 2.1363636363636362}\n{'loss': 0.1548, 'grad_norm': 0.7704963088035583, 'learning_rate': 0.0003388506100467991, 'epoch': 2.159090909090909}\n{'loss': 0.1717, 'grad_norm': 0.8482580184936523, 'learning_rate': 0.00033753723558925335, 'epoch': 2.1818181818181817}\n{'loss': 0.2061, 'grad_norm': 0.9989238977432251, 'learning_rate': 0.00033622386113170763, 'epoch': 2.2045454545454546}\n{'loss': 0.1994, 'grad_norm': 1.422501802444458, 'learning_rate': 0.0003349104866741619, 'epoch': 2.227272727272727}\n{'loss': 0.2175, 'grad_norm': 2.397329568862915, 'learning_rate': 0.00033359711221661614, 'epoch': 2.25}\n{'loss': 0.1605, 'grad_norm': 2.0196266174316406, 'learning_rate': 0.00033228373775907047, 'epoch': 2.2727272727272725}\n{'loss': 0.1836, 'grad_norm': 1.2400109767913818, 'learning_rate': 0.0003309703633015247, 'epoch': 2.2954545454545454}\n{'loss': 0.1923, 'grad_norm': 1.5751745700836182, 'learning_rate': 0.000329656988843979, 'epoch': 2.3181818181818183}\n{'loss': 0.1169, 'grad_norm': 0.7612482309341431, 'learning_rate': 0.00032834361438643326, 'epoch': 2.340909090909091}\n{'loss': 0.1661, 'grad_norm': 1.0298810005187988, 'learning_rate': 0.0003270302399288875, 'epoch': 2.3636363636363638}\n{'loss': 0.1435, 'grad_norm': 0.9012694954872131, 'learning_rate': 0.00032571686547134177, 'epoch': 2.3863636363636362}\n{'loss': 0.2361, 'grad_norm': 1.7943272590637207, 'learning_rate': 0.00032440349101379605, 'epoch': 2.409090909090909}\n{'loss': 0.1883, 'grad_norm': 1.3613637685775757, 'learning_rate': 0.0003230901165562503, 'epoch': 2.4318181818181817}\n{'loss': 0.1178, 'grad_norm': 1.1037890911102295, 'learning_rate': 0.0003217767420987046, 'epoch': 2.4545454545454546}\n{'loss': 0.1796, 'grad_norm': 1.3050931692123413, 'learning_rate': 0.00032046336764115883, 'epoch': 2.4772727272727275}\n{'loss': 0.1467, 'grad_norm': 1.167904257774353, 'learning_rate': 0.0003191499931836131, 'epoch': 2.5}\n{'loss': 0.1461, 'grad_norm': 1.1591707468032837, 'learning_rate': 0.0003178366187260674, 'epoch': 2.5227272727272725}\n{'loss': 0.1452, 'grad_norm': 1.2982988357543945, 'learning_rate': 0.0003165232442685216, 'epoch': 2.5454545454545454}\n{'loss': 0.1792, 'grad_norm': 1.726807713508606, 'learning_rate': 0.00031520986981097595, 'epoch': 2.5681818181818183}\n{'loss': 0.1914, 'grad_norm': 1.6541457176208496, 'learning_rate': 0.0003138964953534302, 'epoch': 2.590909090909091}\n{'loss': 0.1813, 'grad_norm': 0.940739095211029, 'learning_rate': 0.00031258312089588446, 'epoch': 2.6136363636363638}\n{'loss': 0.1939, 'grad_norm': 0.8783186674118042, 'learning_rate': 0.00031126974643833874, 'epoch': 2.6363636363636362}\n{'loss': 0.1506, 'grad_norm': 1.0308778285980225, 'learning_rate': 0.00030995637198079297, 'epoch': 2.659090909090909}\n{'loss': 0.1249, 'grad_norm': 1.1046370267868042, 'learning_rate': 0.00030864299752324725, 'epoch': 2.6818181818181817}\n{'loss': 0.1476, 'grad_norm': 0.969147264957428, 'learning_rate': 0.0003073296230657015, 'epoch': 2.7045454545454546}\n{'loss': 0.2227, 'grad_norm': 1.0357028245925903, 'learning_rate': 0.0003060162486081558, 'epoch': 2.7272727272727275}\n{'loss': 0.1837, 'grad_norm': 1.1359683275222778, 'learning_rate': 0.00030470287415061004, 'epoch': 2.75}\n{'loss': 0.1643, 'grad_norm': 1.064677357673645, 'learning_rate': 0.0003033894996930643, 'epoch': 2.7727272727272725}\n{'loss': 0.1637, 'grad_norm': 1.5172739028930664, 'learning_rate': 0.0003020761252355186, 'epoch': 2.7954545454545454}\n{'loss': 0.1606, 'grad_norm': 1.4931588172912598, 'learning_rate': 0.0003007627507779728, 'epoch': 2.8181818181818183}\n{'loss': 0.1995, 'grad_norm': 1.0237174034118652, 'learning_rate': 0.0002994493763204271, 'epoch': 2.840909090909091}\n{'loss': 0.1414, 'grad_norm': 0.950398325920105, 'learning_rate': 0.0002981360018628814, 'epoch': 2.8636363636363638}\n{'loss': 0.169, 'grad_norm': 0.857103705406189, 'learning_rate': 0.00029682262740533566, 'epoch': 2.8863636363636362}\n{'loss': 0.1581, 'grad_norm': 0.9983513951301575, 'learning_rate': 0.00029550925294778994, 'epoch': 2.909090909090909}\n{'loss': 0.1807, 'grad_norm': 1.0258448123931885, 'learning_rate': 0.00029419587849024417, 'epoch': 2.9318181818181817}\n{'loss': 0.1503, 'grad_norm': 1.2731003761291504, 'learning_rate': 0.00029288250403269845, 'epoch': 2.9545454545454546}\n{'loss': 0.1769, 'grad_norm': 0.8429104685783386, 'learning_rate': 0.00029156912957515273, 'epoch': 2.9772727272727275}\n{'loss': 0.0916, 'grad_norm': 3.8032286167144775, 'learning_rate': 0.00029025575511760696, 'epoch': 3.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8254    0.9269    0.8732      3311\n     PERCENT     0.8105    0.8953    0.8508        86\n        TYPE     0.9509    0.9540    0.9524     11299\n      VOLUME     0.7561    0.7381    0.7470        42\n\n   micro avg     0.9188    0.9469    0.9326     14738\n   macro avg     0.8357    0.8786    0.8559     14738\nweighted avg     0.9213    0.9469    0.9335     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.25272926688194275, 'eval_f1_macro': 0.8558687345298958, 'eval_precision': 0.9187623436471363, 'eval_recall': 0.946939883294884, 'eval_f1': 0.9326383319967924, 'eval_accuracy': 0.9248128517567346, 'eval_runtime': 1.7017, 'eval_samples_per_second': 3237.893, 'eval_steps_per_second': 6.464, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.1012, 'grad_norm': 1.6937479972839355, 'learning_rate': 0.0002889423806600613, 'epoch': 3.022727272727273}\n{'loss': 0.1433, 'grad_norm': 2.2748918533325195, 'learning_rate': 0.0002876290062025155, 'epoch': 3.0454545454545454}\n{'loss': 0.1212, 'grad_norm': 1.3076744079589844, 'learning_rate': 0.0002863156317449698, 'epoch': 3.0681818181818183}\n{'loss': 0.1174, 'grad_norm': 0.9704256057739258, 'learning_rate': 0.0002850022572874241, 'epoch': 3.090909090909091}\n{'loss': 0.1261, 'grad_norm': 1.108528733253479, 'learning_rate': 0.0002836888828298783, 'epoch': 3.1136363636363638}\n{'loss': 0.0961, 'grad_norm': 0.8828861713409424, 'learning_rate': 0.0002823755083723326, 'epoch': 3.1363636363636362}\n{'loss': 0.1505, 'grad_norm': 1.8607138395309448, 'learning_rate': 0.0002810621339147868, 'epoch': 3.159090909090909}\n{'loss': 0.0729, 'grad_norm': 0.6196320652961731, 'learning_rate': 0.00027974875945724115, 'epoch': 3.1818181818181817}\n{'loss': 0.103, 'grad_norm': 0.8252779245376587, 'learning_rate': 0.0002784353849996954, 'epoch': 3.2045454545454546}\n{'loss': 0.124, 'grad_norm': 0.7992916107177734, 'learning_rate': 0.00027712201054214965, 'epoch': 3.227272727272727}\n{'loss': 0.1315, 'grad_norm': 1.3056175708770752, 'learning_rate': 0.00027580863608460393, 'epoch': 3.25}\n{'loss': 0.1279, 'grad_norm': 1.521378993988037, 'learning_rate': 0.00027449526162705816, 'epoch': 3.2727272727272725}\n{'loss': 0.1092, 'grad_norm': 1.1979918479919434, 'learning_rate': 0.00027318188716951244, 'epoch': 3.2954545454545454}\n{'loss': 0.1212, 'grad_norm': 1.0205104351043701, 'learning_rate': 0.0002718685127119667, 'epoch': 3.3181818181818183}\n{'loss': 0.0848, 'grad_norm': 0.8642860054969788, 'learning_rate': 0.000270555138254421, 'epoch': 3.340909090909091}\n{'loss': 0.0814, 'grad_norm': 1.0673601627349854, 'learning_rate': 0.0002692417637968753, 'epoch': 3.3636363636363638}\n{'loss': 0.0974, 'grad_norm': 1.451507568359375, 'learning_rate': 0.0002679283893393295, 'epoch': 3.3863636363636362}\n{'loss': 0.1024, 'grad_norm': 1.633846402168274, 'learning_rate': 0.0002666150148817838, 'epoch': 3.409090909090909}\n{'loss': 0.0844, 'grad_norm': 1.0575263500213623, 'learning_rate': 0.00026530164042423807, 'epoch': 3.4318181818181817}\n{'loss': 0.1113, 'grad_norm': 0.7084625959396362, 'learning_rate': 0.0002639882659666923, 'epoch': 3.4545454545454546}\n{'loss': 0.1096, 'grad_norm': 1.6841280460357666, 'learning_rate': 0.00026267489150914663, 'epoch': 3.4772727272727275}\n{'loss': 0.1148, 'grad_norm': 1.7986339330673218, 'learning_rate': 0.00026136151705160086, 'epoch': 3.5}\n{'loss': 0.1256, 'grad_norm': 2.409583330154419, 'learning_rate': 0.00026004814259405514, 'epoch': 3.5227272727272725}\n{'loss': 0.0983, 'grad_norm': 2.0835983753204346, 'learning_rate': 0.0002587347681365094, 'epoch': 3.5454545454545454}\n{'loss': 0.144, 'grad_norm': 1.4388962984085083, 'learning_rate': 0.00025742139367896364, 'epoch': 3.5681818181818183}\n{'loss': 0.1181, 'grad_norm': 1.10502028465271, 'learning_rate': 0.0002561080192214179, 'epoch': 3.590909090909091}\n{'loss': 0.1229, 'grad_norm': 1.3979169130325317, 'learning_rate': 0.0002547946447638722, 'epoch': 3.6136363636363638}\n{'loss': 0.1133, 'grad_norm': 1.5418310165405273, 'learning_rate': 0.0002534812703063265, 'epoch': 3.6363636363636362}\n{'loss': 0.0956, 'grad_norm': 0.9019609689712524, 'learning_rate': 0.00025216789584878076, 'epoch': 3.659090909090909}\n{'loss': 0.0966, 'grad_norm': 0.8675076365470886, 'learning_rate': 0.000250854521391235, 'epoch': 3.6818181818181817}\n{'loss': 0.0982, 'grad_norm': 1.032140851020813, 'learning_rate': 0.00024954114693368927, 'epoch': 3.7045454545454546}\n{'loss': 0.1282, 'grad_norm': 0.8332231044769287, 'learning_rate': 0.0002482277724761435, 'epoch': 3.7272727272727275}\n{'loss': 0.1225, 'grad_norm': 1.0050561428070068, 'learning_rate': 0.0002469143980185978, 'epoch': 3.75}\n{'loss': 0.1054, 'grad_norm': 1.2449259757995605, 'learning_rate': 0.0002456010235610521, 'epoch': 3.7727272727272725}\n{'loss': 0.1057, 'grad_norm': 0.7941155433654785, 'learning_rate': 0.00024428764910350634, 'epoch': 3.7954545454545454}\n{'loss': 0.112, 'grad_norm': 0.9854860901832581, 'learning_rate': 0.00024297427464596062, 'epoch': 3.8181818181818183}\n{'loss': 0.1413, 'grad_norm': 1.3134229183197021, 'learning_rate': 0.00024166090018841487, 'epoch': 3.840909090909091}\n{'loss': 0.1392, 'grad_norm': 1.9068318605422974, 'learning_rate': 0.00024034752573086912, 'epoch': 3.8636363636363638}\n{'loss': 0.0777, 'grad_norm': 1.2408077716827393, 'learning_rate': 0.00023903415127332338, 'epoch': 3.8863636363636362}\n{'loss': 0.1067, 'grad_norm': 0.931366503238678, 'learning_rate': 0.00023772077681577769, 'epoch': 3.909090909090909}\n{'loss': 0.1226, 'grad_norm': 0.9341089129447937, 'learning_rate': 0.00023640740235823194, 'epoch': 3.9318181818181817}\n{'loss': 0.1135, 'grad_norm': 0.7585567235946655, 'learning_rate': 0.00023509402790068622, 'epoch': 3.9545454545454546}\n{'loss': 0.1337, 'grad_norm': 1.4330767393112183, 'learning_rate': 0.00023378065344314047, 'epoch': 3.9772727272727275}\n{'loss': 0.0095, 'grad_norm': 0.40720072388648987, 'learning_rate': 0.00023246727898559473, 'epoch': 4.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8620    0.9166    0.8885      3311\n     PERCENT     0.8588    0.8488    0.8538        86\n        TYPE     0.9523    0.9603    0.9563     11299\n      VOLUME     0.7708    0.8810    0.8222        42\n\n   micro avg     0.9301    0.9496    0.9397     14738\n   macro avg     0.8610    0.9017    0.8802     14738\nweighted avg     0.9310    0.9496    0.9401     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.24588169157505035, 'eval_f1_macro': 0.8801933967432243, 'eval_precision': 0.9300857313750249, 'eval_recall': 0.9495861039489755, 'eval_f1': 0.9397347658217224, 'eval_accuracy': 0.9314245123217311, 'eval_runtime': 1.5798, 'eval_samples_per_second': 3487.885, 'eval_steps_per_second': 6.963, 'epoch': 4.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.0773, 'grad_norm': 0.5649972558021545, 'learning_rate': 0.00023115390452804898, 'epoch': 4.0227272727272725}\n{'loss': 0.0602, 'grad_norm': 0.7104460597038269, 'learning_rate': 0.00022984053007050326, 'epoch': 4.045454545454546}\n{'loss': 0.0829, 'grad_norm': 0.8806195855140686, 'learning_rate': 0.00022852715561295757, 'epoch': 4.068181818181818}\n{'loss': 0.0761, 'grad_norm': 0.8597816228866577, 'learning_rate': 0.00022721378115541182, 'epoch': 4.090909090909091}\n{'loss': 0.0713, 'grad_norm': 1.1884108781814575, 'learning_rate': 0.00022590040669786607, 'epoch': 4.113636363636363}\n{'loss': 0.0612, 'grad_norm': 0.6142575144767761, 'learning_rate': 0.00022458703224032033, 'epoch': 4.136363636363637}\n{'loss': 0.059, 'grad_norm': 0.8508815765380859, 'learning_rate': 0.0002232736577827746, 'epoch': 4.159090909090909}\n{'loss': 0.0636, 'grad_norm': 0.6292232275009155, 'learning_rate': 0.00022196028332522886, 'epoch': 4.181818181818182}\n{'loss': 0.0602, 'grad_norm': 0.914533793926239, 'learning_rate': 0.00022064690886768311, 'epoch': 4.204545454545454}\n{'loss': 0.0828, 'grad_norm': 0.8496381640434265, 'learning_rate': 0.00021933353441013742, 'epoch': 4.2272727272727275}\n{'loss': 0.0947, 'grad_norm': 0.7301746010780334, 'learning_rate': 0.00021802015995259168, 'epoch': 4.25}\n{'loss': 0.0833, 'grad_norm': 0.6353870630264282, 'learning_rate': 0.00021670678549504596, 'epoch': 4.2727272727272725}\n{'loss': 0.0726, 'grad_norm': 0.849731981754303, 'learning_rate': 0.0002153934110375002, 'epoch': 4.295454545454546}\n{'loss': 0.095, 'grad_norm': 0.8597650527954102, 'learning_rate': 0.00021408003657995446, 'epoch': 4.318181818181818}\n{'loss': 0.0798, 'grad_norm': 0.7384303212165833, 'learning_rate': 0.00021276666212240872, 'epoch': 4.340909090909091}\n{'loss': 0.0625, 'grad_norm': 0.9825718998908997, 'learning_rate': 0.00021145328766486302, 'epoch': 4.363636363636363}\n{'loss': 0.0896, 'grad_norm': 0.8273058533668518, 'learning_rate': 0.0002101399132073173, 'epoch': 4.386363636363637}\n{'loss': 0.0917, 'grad_norm': 0.947670578956604, 'learning_rate': 0.00020882653874977156, 'epoch': 4.409090909090909}\n{'loss': 0.0581, 'grad_norm': 0.5148491263389587, 'learning_rate': 0.0002075131642922258, 'epoch': 4.431818181818182}\n{'loss': 0.0625, 'grad_norm': 0.6643315553665161, 'learning_rate': 0.00020619978983468006, 'epoch': 4.454545454545454}\n{'loss': 0.0975, 'grad_norm': 0.7978259325027466, 'learning_rate': 0.00020488641537713434, 'epoch': 4.4772727272727275}\n{'loss': 0.0907, 'grad_norm': 0.7305710315704346, 'learning_rate': 0.00020357304091958862, 'epoch': 4.5}\n{'loss': 0.0667, 'grad_norm': 0.7417765855789185, 'learning_rate': 0.00020225966646204288, 'epoch': 4.5227272727272725}\n{'loss': 0.0611, 'grad_norm': 0.8798866271972656, 'learning_rate': 0.00020094629200449716, 'epoch': 4.545454545454545}\n{'loss': 0.079, 'grad_norm': 0.7620936036109924, 'learning_rate': 0.0001996329175469514, 'epoch': 4.568181818181818}\n{'loss': 0.0778, 'grad_norm': 0.8644634485244751, 'learning_rate': 0.00019831954308940566, 'epoch': 4.590909090909091}\n{'loss': 0.1158, 'grad_norm': 0.8727445602416992, 'learning_rate': 0.00019700616863185994, 'epoch': 4.613636363636363}\n{'loss': 0.0827, 'grad_norm': 0.6587417721748352, 'learning_rate': 0.00019569279417431423, 'epoch': 4.636363636363637}\n{'loss': 0.0909, 'grad_norm': 1.0704847574234009, 'learning_rate': 0.00019437941971676848, 'epoch': 4.659090909090909}\n{'loss': 0.0624, 'grad_norm': 1.3693854808807373, 'learning_rate': 0.00019306604525922273, 'epoch': 4.681818181818182}\n{'loss': 0.0748, 'grad_norm': 0.5664410591125488, 'learning_rate': 0.000191752670801677, 'epoch': 4.704545454545455}\n{'loss': 0.0626, 'grad_norm': 0.5810546278953552, 'learning_rate': 0.0001904392963441313, 'epoch': 4.7272727272727275}\n{'loss': 0.0937, 'grad_norm': 0.8796335458755493, 'learning_rate': 0.00018912592188658555, 'epoch': 4.75}\n{'loss': 0.0915, 'grad_norm': 2.1719956398010254, 'learning_rate': 0.00018781254742903983, 'epoch': 4.7727272727272725}\n{'loss': 0.0633, 'grad_norm': 1.109253168106079, 'learning_rate': 0.00018649917297149408, 'epoch': 4.795454545454545}\n{'loss': 0.0737, 'grad_norm': 1.1312952041625977, 'learning_rate': 0.00018518579851394833, 'epoch': 4.818181818181818}\n{'loss': 0.0602, 'grad_norm': 0.7294495701789856, 'learning_rate': 0.0001838724240564026, 'epoch': 4.840909090909091}\n{'loss': 0.068, 'grad_norm': 1.0249218940734863, 'learning_rate': 0.0001825590495988569, 'epoch': 4.863636363636363}\n{'loss': 0.0991, 'grad_norm': 1.2210884094238281, 'learning_rate': 0.00018124567514131115, 'epoch': 4.886363636363637}\n{'loss': 0.0527, 'grad_norm': 1.284934163093567, 'learning_rate': 0.0001799323006837654, 'epoch': 4.909090909090909}\n{'loss': 0.0828, 'grad_norm': 1.5644798278808594, 'learning_rate': 0.00017861892622621968, 'epoch': 4.931818181818182}\n{'loss': 0.0526, 'grad_norm': 0.8213620781898499, 'learning_rate': 0.00017730555176867396, 'epoch': 4.954545454545455}\n{'loss': 0.0622, 'grad_norm': 0.5011470317840576, 'learning_rate': 0.00017599217731112821, 'epoch': 4.9772727272727275}\n{'loss': 0.1458, 'grad_norm': 9.487948417663574, 'learning_rate': 0.0001746788028535825, 'epoch': 5.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.9028    0.8916    0.8971      3311\n     PERCENT     0.8506    0.8605    0.8555        86\n        TYPE     0.9488    0.9655    0.9571     11299\n      VOLUME     0.7500    0.8571    0.8000        42\n\n   micro avg     0.9375    0.9480    0.9427     14738\n   macro avg     0.8630    0.8937    0.8774     14738\nweighted avg     0.9373    0.9480    0.9426     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.2625235915184021, 'eval_f1_macro': 0.8774187946159389, 'eval_precision': 0.9374622559216265, 'eval_recall': 0.9479576604695346, 'eval_f1': 0.9426807462636214, 'eval_accuracy': 0.9331184088301185, 'eval_runtime': 1.5154, 'eval_samples_per_second': 3635.978, 'eval_steps_per_second': 7.259, 'epoch': 5.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.0381, 'grad_norm': 0.5167640447616577, 'learning_rate': 0.00017336542839603675, 'epoch': 5.0227272727272725}\n{'loss': 0.0624, 'grad_norm': 1.0835292339324951, 'learning_rate': 0.000172052053938491, 'epoch': 5.045454545454546}\n{'loss': 0.0554, 'grad_norm': 0.5698750615119934, 'learning_rate': 0.0001707386794809453, 'epoch': 5.068181818181818}\n{'loss': 0.0864, 'grad_norm': 1.3218618631362915, 'learning_rate': 0.00016942530502339956, 'epoch': 5.090909090909091}\n{'loss': 0.0645, 'grad_norm': 0.9492912292480469, 'learning_rate': 0.00016811193056585382, 'epoch': 5.113636363636363}\n{'loss': 0.0502, 'grad_norm': 0.6998538970947266, 'learning_rate': 0.00016679855610830807, 'epoch': 5.136363636363637}\n{'loss': 0.0596, 'grad_norm': 0.5210779309272766, 'learning_rate': 0.00016548518165076235, 'epoch': 5.159090909090909}\n{'loss': 0.0393, 'grad_norm': 0.6168485879898071, 'learning_rate': 0.00016417180719321663, 'epoch': 5.181818181818182}\n{'loss': 0.0607, 'grad_norm': 0.7453995943069458, 'learning_rate': 0.00016285843273567088, 'epoch': 5.204545454545454}\n{'loss': 0.0628, 'grad_norm': 1.1220308542251587, 'learning_rate': 0.00016154505827812516, 'epoch': 5.2272727272727275}\n{'loss': 0.0505, 'grad_norm': 0.5407316088676453, 'learning_rate': 0.00016023168382057942, 'epoch': 5.25}\n{'loss': 0.0362, 'grad_norm': 0.575732409954071, 'learning_rate': 0.0001589183093630337, 'epoch': 5.2727272727272725}\n{'loss': 0.0609, 'grad_norm': 0.682616114616394, 'learning_rate': 0.00015760493490548798, 'epoch': 5.295454545454546}\n{'loss': 0.0472, 'grad_norm': 0.9208241701126099, 'learning_rate': 0.00015629156044794223, 'epoch': 5.318181818181818}\n{'loss': 0.0492, 'grad_norm': 0.6111624240875244, 'learning_rate': 0.00015497818599039648, 'epoch': 5.340909090909091}\n{'loss': 0.0403, 'grad_norm': 0.5129817128181458, 'learning_rate': 0.00015366481153285074, 'epoch': 5.363636363636363}\n{'loss': 0.0552, 'grad_norm': 0.7262277007102966, 'learning_rate': 0.00015235143707530502, 'epoch': 5.386363636363637}\n{'loss': 0.0423, 'grad_norm': 0.5662570595741272, 'learning_rate': 0.0001510380626177593, 'epoch': 5.409090909090909}\n{'loss': 0.0475, 'grad_norm': 1.4562770128250122, 'learning_rate': 0.00014972468816021355, 'epoch': 5.431818181818182}\n{'loss': 0.0555, 'grad_norm': 0.7189443111419678, 'learning_rate': 0.00014841131370266783, 'epoch': 5.454545454545454}\n{'loss': 0.0931, 'grad_norm': 0.7228924632072449, 'learning_rate': 0.00014709793924512209, 'epoch': 5.4772727272727275}\n{'loss': 0.0522, 'grad_norm': 1.3338760137557983, 'learning_rate': 0.00014578456478757637, 'epoch': 5.5}\n{'loss': 0.0588, 'grad_norm': 0.8169716596603394, 'learning_rate': 0.00014447119033003065, 'epoch': 5.5227272727272725}\n{'loss': 0.0708, 'grad_norm': 1.3656225204467773, 'learning_rate': 0.0001431578158724849, 'epoch': 5.545454545454545}\n{'loss': 0.0483, 'grad_norm': 0.771219789981842, 'learning_rate': 0.00014184444141493915, 'epoch': 5.568181818181818}\n{'loss': 0.0534, 'grad_norm': 0.7161715030670166, 'learning_rate': 0.0001405310669573934, 'epoch': 5.590909090909091}\n{'loss': 0.0751, 'grad_norm': 0.8321157693862915, 'learning_rate': 0.0001392176924998477, 'epoch': 5.613636363636363}\n{'loss': 0.0611, 'grad_norm': 0.9176406860351562, 'learning_rate': 0.00013790431804230197, 'epoch': 5.636363636363637}\n{'loss': 0.0592, 'grad_norm': 0.8315119743347168, 'learning_rate': 0.00013659094358475622, 'epoch': 5.659090909090909}\n{'loss': 0.0528, 'grad_norm': 0.6150808334350586, 'learning_rate': 0.0001352775691272105, 'epoch': 5.681818181818182}\n{'loss': 0.035, 'grad_norm': 0.8394727110862732, 'learning_rate': 0.00013396419466966475, 'epoch': 5.704545454545455}\n{'loss': 0.0505, 'grad_norm': 1.4243370294570923, 'learning_rate': 0.00013265082021211903, 'epoch': 5.7272727272727275}\n{'loss': 0.0543, 'grad_norm': 0.7204797267913818, 'learning_rate': 0.00013133744575457331, 'epoch': 5.75}\n{'loss': 0.0514, 'grad_norm': 0.7419590353965759, 'learning_rate': 0.00013002407129702757, 'epoch': 5.7727272727272725}\n{'loss': 0.0619, 'grad_norm': 0.7629120945930481, 'learning_rate': 0.00012871069683948182, 'epoch': 5.795454545454545}\n{'loss': 0.0571, 'grad_norm': 0.6545950174331665, 'learning_rate': 0.0001273973223819361, 'epoch': 5.818181818181818}\n{'loss': 0.0556, 'grad_norm': 0.6487331390380859, 'learning_rate': 0.00012608394792439038, 'epoch': 5.840909090909091}\n{'loss': 0.0504, 'grad_norm': 1.0086263418197632, 'learning_rate': 0.00012477057346684464, 'epoch': 5.863636363636363}\n{'loss': 0.0302, 'grad_norm': 0.5667489171028137, 'learning_rate': 0.0001234571990092989, 'epoch': 5.886363636363637}\n{'loss': 0.0495, 'grad_norm': 0.7131364941596985, 'learning_rate': 0.00012214382455175317, 'epoch': 5.909090909090909}\n{'loss': 0.0777, 'grad_norm': 0.9855279922485352, 'learning_rate': 0.00012083045009420744, 'epoch': 5.931818181818182}\n{'loss': 0.051, 'grad_norm': 0.7277894020080566, 'learning_rate': 0.00011951707563666169, 'epoch': 5.954545454545455}\n{'loss': 0.0738, 'grad_norm': 1.3655996322631836, 'learning_rate': 0.00011820370117911597, 'epoch': 5.9772727272727275}\n{'loss': 0.0419, 'grad_norm': 2.676433563232422, 'learning_rate': 0.00011689032672157024, 'epoch': 6.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8932    0.9018    0.8975      3311\n     PERCENT     0.8810    0.8605    0.8706        86\n        TYPE     0.9493    0.9668    0.9580     11299\n      VOLUME     0.7843    0.9524    0.8602        42\n\n   micro avg     0.9359    0.9516    0.9436     14738\n   macro avg     0.8770    0.9204    0.8966     14738\nweighted avg     0.9359    0.9516    0.9436     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.2859674394130707, 'eval_f1_macro': 0.8965755148828114, 'eval_precision': 0.9358692025358692, 'eval_recall': 0.9515538064866332, 'eval_f1': 0.9436463344884434, 'eval_accuracy': 0.9339926779957379, 'eval_runtime': 1.6315, 'eval_samples_per_second': 3377.185, 'eval_steps_per_second': 6.742, 'epoch': 6.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.0512, 'grad_norm': 0.8916667103767395, 'learning_rate': 0.00011557695226402449, 'epoch': 6.0227272727272725}\n{'loss': 0.0275, 'grad_norm': 0.46792739629745483, 'learning_rate': 0.00011426357780647878, 'epoch': 6.045454545454546}\n{'loss': 0.0407, 'grad_norm': 0.7485008239746094, 'learning_rate': 0.00011295020334893304, 'epoch': 6.068181818181818}\n{'loss': 0.033, 'grad_norm': 0.40215158462524414, 'learning_rate': 0.0001116368288913873, 'epoch': 6.090909090909091}\n{'loss': 0.0475, 'grad_norm': 0.6305461525917053, 'learning_rate': 0.00011032345443384156, 'epoch': 6.113636363636363}\n{'loss': 0.0373, 'grad_norm': 0.5117029547691345, 'learning_rate': 0.00010901007997629584, 'epoch': 6.136363636363637}\n{'loss': 0.0413, 'grad_norm': 1.2016764879226685, 'learning_rate': 0.0001076967055187501, 'epoch': 6.159090909090909}\n{'loss': 0.0344, 'grad_norm': 0.5944808721542358, 'learning_rate': 0.00010638333106120436, 'epoch': 6.181818181818182}\n{'loss': 0.0417, 'grad_norm': 0.7872742414474487, 'learning_rate': 0.00010506995660365865, 'epoch': 6.204545454545454}\n{'loss': 0.0445, 'grad_norm': 0.7022386193275452, 'learning_rate': 0.0001037565821461129, 'epoch': 6.2272727272727275}\n{'loss': 0.0609, 'grad_norm': 0.7966223955154419, 'learning_rate': 0.00010244320768856717, 'epoch': 6.25}\n{'loss': 0.0358, 'grad_norm': 0.543823778629303, 'learning_rate': 0.00010112983323102144, 'epoch': 6.2727272727272725}\n{'loss': 0.0443, 'grad_norm': 0.40110260248184204, 'learning_rate': 9.98164587734757e-05, 'epoch': 6.295454545454546}\n{'loss': 0.0413, 'grad_norm': 1.131724238395691, 'learning_rate': 9.850308431592997e-05, 'epoch': 6.318181818181818}\n{'loss': 0.0364, 'grad_norm': 0.7408683896064758, 'learning_rate': 9.718970985838424e-05, 'epoch': 6.340909090909091}\n{'loss': 0.0412, 'grad_norm': 0.9480441212654114, 'learning_rate': 9.58763354008385e-05, 'epoch': 6.363636363636363}\n{'loss': 0.0389, 'grad_norm': 0.7824425101280212, 'learning_rate': 9.456296094329277e-05, 'epoch': 6.386363636363637}\n{'loss': 0.0597, 'grad_norm': 0.7553307414054871, 'learning_rate': 9.324958648574704e-05, 'epoch': 6.409090909090909}\n{'loss': 0.0515, 'grad_norm': 0.6319472789764404, 'learning_rate': 9.19362120282013e-05, 'epoch': 6.431818181818182}\n{'loss': 0.0342, 'grad_norm': 0.8828076124191284, 'learning_rate': 9.062283757065557e-05, 'epoch': 6.454545454545454}\n{'loss': 0.0626, 'grad_norm': 0.5847271084785461, 'learning_rate': 8.930946311310984e-05, 'epoch': 6.4772727272727275}\n{'loss': 0.0342, 'grad_norm': 0.5539966821670532, 'learning_rate': 8.799608865556411e-05, 'epoch': 6.5}\n{'loss': 0.0244, 'grad_norm': 0.31200817227363586, 'learning_rate': 8.668271419801837e-05, 'epoch': 6.5227272727272725}\n{'loss': 0.0422, 'grad_norm': 1.2734769582748413, 'learning_rate': 8.536933974047265e-05, 'epoch': 6.545454545454545}\n{'loss': 0.0603, 'grad_norm': 0.6261733770370483, 'learning_rate': 8.405596528292691e-05, 'epoch': 6.568181818181818}\n{'loss': 0.0405, 'grad_norm': 0.7202856540679932, 'learning_rate': 8.274259082538117e-05, 'epoch': 6.590909090909091}\n{'loss': 0.0382, 'grad_norm': 0.6178737878799438, 'learning_rate': 8.142921636783544e-05, 'epoch': 6.613636363636363}\n{'loss': 0.0383, 'grad_norm': 0.6640834808349609, 'learning_rate': 8.011584191028971e-05, 'epoch': 6.636363636363637}\n{'loss': 0.0564, 'grad_norm': 1.171848177909851, 'learning_rate': 7.880246745274399e-05, 'epoch': 6.659090909090909}\n{'loss': 0.0483, 'grad_norm': 0.7360194325447083, 'learning_rate': 7.748909299519824e-05, 'epoch': 6.681818181818182}\n{'loss': 0.0415, 'grad_norm': 0.6473333835601807, 'learning_rate': 7.617571853765251e-05, 'epoch': 6.704545454545455}\n{'loss': 0.056, 'grad_norm': 1.2511669397354126, 'learning_rate': 7.486234408010678e-05, 'epoch': 6.7272727272727275}\n{'loss': 0.0411, 'grad_norm': 0.7797732353210449, 'learning_rate': 7.354896962256104e-05, 'epoch': 6.75}\n{'loss': 0.0374, 'grad_norm': 0.5547311305999756, 'learning_rate': 7.223559516501532e-05, 'epoch': 6.7727272727272725}\n{'loss': 0.0479, 'grad_norm': 0.8108295202255249, 'learning_rate': 7.092222070746958e-05, 'epoch': 6.795454545454545}\n{'loss': 0.0329, 'grad_norm': 0.7008509635925293, 'learning_rate': 6.960884624992386e-05, 'epoch': 6.818181818181818}\n{'loss': 0.0486, 'grad_norm': 0.5611951947212219, 'learning_rate': 6.829547179237811e-05, 'epoch': 6.840909090909091}\n{'loss': 0.0536, 'grad_norm': 0.7641584873199463, 'learning_rate': 6.698209733483238e-05, 'epoch': 6.863636363636363}\n{'loss': 0.0275, 'grad_norm': 0.7817764282226562, 'learning_rate': 6.566872287728666e-05, 'epoch': 6.886363636363637}\n{'loss': 0.0439, 'grad_norm': 0.48247405886650085, 'learning_rate': 6.435534841974091e-05, 'epoch': 6.909090909090909}\n{'loss': 0.0422, 'grad_norm': 0.8521148562431335, 'learning_rate': 6.304197396219519e-05, 'epoch': 6.931818181818182}\n{'loss': 0.0394, 'grad_norm': 0.5559509992599487, 'learning_rate': 6.172859950464944e-05, 'epoch': 6.954545454545455}\n{'loss': 0.0494, 'grad_norm': 0.7915530800819397, 'learning_rate': 6.041522504710372e-05, 'epoch': 6.9772727272727275}\n{'loss': 0.109, 'grad_norm': 3.7479593753814697, 'learning_rate': 5.9101850589557985e-05, 'epoch': 7.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8815    0.9079    0.8945      3311\n     PERCENT     0.8523    0.8721    0.8621        86\n        TYPE     0.9511    0.9627    0.9569     11299\n      VOLUME     0.7872    0.8810    0.8315        42\n\n   micro avg     0.9342    0.9497    0.9419     14738\n   macro avg     0.8680    0.9059    0.8862     14738\nweighted avg     0.9344    0.9497    0.9420     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.29589226841926575, 'eval_f1_macro': 0.8862339842430644, 'eval_precision': 0.934187691896943, 'eval_recall': 0.9496539557606188, 'eval_f1': 0.9418573351278601, 'eval_accuracy': 0.9326266324244576, 'eval_runtime': 1.6737, 'eval_samples_per_second': 3292.115, 'eval_steps_per_second': 6.572, 'epoch': 7.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.0344, 'grad_norm': 0.5650684237480164, 'learning_rate': 5.7788476132012245e-05, 'epoch': 7.0227272727272725}\n{'loss': 0.0463, 'grad_norm': 0.7515209913253784, 'learning_rate': 5.647510167446652e-05, 'epoch': 7.045454545454546}\n{'loss': 0.0401, 'grad_norm': 0.5149407982826233, 'learning_rate': 5.516172721692078e-05, 'epoch': 7.068181818181818}\n{'loss': 0.0307, 'grad_norm': 0.6994478702545166, 'learning_rate': 5.384835275937505e-05, 'epoch': 7.090909090909091}\n{'loss': 0.0432, 'grad_norm': 0.7638376951217651, 'learning_rate': 5.2534978301829326e-05, 'epoch': 7.113636363636363}\n{'loss': 0.0567, 'grad_norm': 0.8921021819114685, 'learning_rate': 5.1221603844283586e-05, 'epoch': 7.136363636363637}\n{'loss': 0.0373, 'grad_norm': 0.6807998418807983, 'learning_rate': 4.990822938673785e-05, 'epoch': 7.159090909090909}\n{'loss': 0.0258, 'grad_norm': 0.606977641582489, 'learning_rate': 4.859485492919212e-05, 'epoch': 7.181818181818182}\n{'loss': 0.0263, 'grad_norm': 0.7737085223197937, 'learning_rate': 4.7281480471646386e-05, 'epoch': 7.204545454545454}\n{'loss': 0.0313, 'grad_norm': 0.8926442861557007, 'learning_rate': 4.596810601410065e-05, 'epoch': 7.2272727272727275}\n{'loss': 0.0313, 'grad_norm': 0.7411734461784363, 'learning_rate': 4.465473155655492e-05, 'epoch': 7.25}\n{'loss': 0.0191, 'grad_norm': 0.3187867999076843, 'learning_rate': 4.334135709900919e-05, 'epoch': 7.2727272727272725}\n{'loss': 0.0628, 'grad_norm': 1.002376914024353, 'learning_rate': 4.2027982641463454e-05, 'epoch': 7.295454545454546}\n{'loss': 0.0454, 'grad_norm': 0.6161662340164185, 'learning_rate': 4.071460818391772e-05, 'epoch': 7.318181818181818}\n{'loss': 0.0413, 'grad_norm': 0.5207065939903259, 'learning_rate': 3.9401233726371994e-05, 'epoch': 7.340909090909091}\n{'loss': 0.047, 'grad_norm': 0.6225259900093079, 'learning_rate': 3.8087859268826254e-05, 'epoch': 7.363636363636363}\n{'loss': 0.0494, 'grad_norm': 0.6734700798988342, 'learning_rate': 3.677448481128052e-05, 'epoch': 7.386363636363637}\n{'loss': 0.0251, 'grad_norm': 0.8340669870376587, 'learning_rate': 3.546111035373479e-05, 'epoch': 7.409090909090909}\n{'loss': 0.0341, 'grad_norm': 0.753546416759491, 'learning_rate': 3.4147735896189055e-05, 'epoch': 7.431818181818182}\n{'loss': 0.0363, 'grad_norm': 0.5380886793136597, 'learning_rate': 3.283436143864333e-05, 'epoch': 7.454545454545454}\n{'loss': 0.0304, 'grad_norm': 0.8278474807739258, 'learning_rate': 3.1520986981097595e-05, 'epoch': 7.4772727272727275}\n{'loss': 0.0379, 'grad_norm': 0.582339346408844, 'learning_rate': 3.020761252355186e-05, 'epoch': 7.5}\n{'loss': 0.047, 'grad_norm': 1.1331076622009277, 'learning_rate': 2.8894238066006122e-05, 'epoch': 7.5227272727272725}\n{'loss': 0.0389, 'grad_norm': 0.7309602499008179, 'learning_rate': 2.758086360846039e-05, 'epoch': 7.545454545454545}\n{'loss': 0.0202, 'grad_norm': 0.6051990389823914, 'learning_rate': 2.6267489150914663e-05, 'epoch': 7.568181818181818}\n{'loss': 0.029, 'grad_norm': 0.8022260665893555, 'learning_rate': 2.4954114693368926e-05, 'epoch': 7.590909090909091}\n{'loss': 0.0447, 'grad_norm': 0.702398419380188, 'learning_rate': 2.3640740235823193e-05, 'epoch': 7.613636363636363}\n{'loss': 0.0341, 'grad_norm': 0.46952590346336365, 'learning_rate': 2.232736577827746e-05, 'epoch': 7.636363636363637}\n{'loss': 0.0303, 'grad_norm': 0.5530394911766052, 'learning_rate': 2.1013991320731727e-05, 'epoch': 7.659090909090909}\n{'loss': 0.0255, 'grad_norm': 0.8642695546150208, 'learning_rate': 1.9700616863185997e-05, 'epoch': 7.681818181818182}\n{'loss': 0.0478, 'grad_norm': 1.0220750570297241, 'learning_rate': 1.838724240564026e-05, 'epoch': 7.704545454545455}\n{'loss': 0.0422, 'grad_norm': 1.192906141281128, 'learning_rate': 1.7073867948094528e-05, 'epoch': 7.7272727272727275}\n{'loss': 0.0427, 'grad_norm': 0.693665623664856, 'learning_rate': 1.5760493490548798e-05, 'epoch': 7.75}\n{'loss': 0.0258, 'grad_norm': 0.44468873739242554, 'learning_rate': 1.4447119033003061e-05, 'epoch': 7.7727272727272725}\n{'loss': 0.028, 'grad_norm': 0.46027517318725586, 'learning_rate': 1.3133744575457331e-05, 'epoch': 7.795454545454545}\n{'loss': 0.0372, 'grad_norm': 0.5239481925964355, 'learning_rate': 1.1820370117911597e-05, 'epoch': 7.818181818181818}\n{'loss': 0.0311, 'grad_norm': 0.499054878950119, 'learning_rate': 1.0506995660365863e-05, 'epoch': 7.840909090909091}\n{'loss': 0.0316, 'grad_norm': 0.4723260998725891, 'learning_rate': 9.19362120282013e-06, 'epoch': 7.863636363636363}\n{'loss': 0.0445, 'grad_norm': 0.779782235622406, 'learning_rate': 7.880246745274399e-06, 'epoch': 7.886363636363637}\n{'loss': 0.0338, 'grad_norm': 0.43857747316360474, 'learning_rate': 6.566872287728666e-06, 'epoch': 7.909090909090909}\n{'loss': 0.0462, 'grad_norm': 0.6573562622070312, 'learning_rate': 5.253497830182932e-06, 'epoch': 7.931818181818182}\n{'loss': 0.0332, 'grad_norm': 0.7295199036598206, 'learning_rate': 3.9401233726371994e-06, 'epoch': 7.954545454545455}\n{'loss': 0.0294, 'grad_norm': 0.7440495491027832, 'learning_rate': 2.626748915091466e-06, 'epoch': 7.9772727272727275}\n{'loss': 0.0138, 'grad_norm': 1.993905782699585, 'learning_rate': 1.313374457545733e-06, 'epoch': 8.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8914    0.9021    0.8967      3311\n     PERCENT     0.8333    0.8721    0.8523        86\n        TYPE     0.9507    0.9639    0.9572     11299\n      VOLUME     0.8222    0.8810    0.8506        42\n\n   micro avg     0.9363    0.9492    0.9427     14738\n   macro avg     0.8744    0.9048    0.8892     14738\nweighted avg     0.9363    0.9492    0.9427     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.30176493525505066, 'eval_f1_macro': 0.8892038290773119, 'eval_precision': 0.9362869763083924, 'eval_recall': 0.9492468448907586, 'eval_f1': 0.942722371967655, 'eval_accuracy': 0.9331184088301185, 'eval_runtime': 2.0332, 'eval_samples_per_second': 2710.062, 'eval_steps_per_second': 5.41, 'epoch': 8.0}\n{'train_runtime': 52.8803, 'train_samples_per_second': 3334.626, 'train_steps_per_second': 6.657, 'train_loss': 0.2305947959989267, 'epoch': 8.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nThe speedups for torchdynamo mostly come with GPU Ampere or higher and which is not detected here.\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8932    0.9018    0.8975      3311\n     PERCENT     0.8810    0.8605    0.8706        86\n        TYPE     0.9493    0.9668    0.9580     11299\n      VOLUME     0.7843    0.9524    0.8602        42\n\n   micro avg     0.9359    0.9516    0.9436     14738\n   macro avg     0.8770    0.9204    0.8966     14738\nweighted avg     0.9359    0.9516    0.9436     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.2859674394130707, 'eval_f1_macro': 0.8965755148828114, 'eval_precision': 0.9358692025358692, 'eval_recall': 0.9515538064866332, 'eval_f1': 0.9436463344884434, 'eval_accuracy': 0.9339926779957379, 'eval_runtime': 1.7839, 'eval_samples_per_second': 3088.819, 'eval_steps_per_second': 6.166, 'epoch': 8.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/3364797558.py:66: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 2.2782, 'grad_norm': 7.156409740447998, 'learning_rate': 0.0, 'epoch': 0.022727272727272728}\n{'loss': 2.2758, 'grad_norm': 7.490075588226318, 'learning_rate': 1.1528509127345877e-05, 'epoch': 0.045454545454545456}\n{'loss': 2.2503, 'grad_norm': 7.09873628616333, 'learning_rate': 2.3057018254691754e-05, 'epoch': 0.06818181818181818}\n{'loss': 2.1924, 'grad_norm': 7.52242374420166, 'learning_rate': 3.458552738203763e-05, 'epoch': 0.09090909090909091}\n{'loss': 2.1124, 'grad_norm': 7.02615213394165, 'learning_rate': 4.611403650938351e-05, 'epoch': 0.11363636363636363}\n{'loss': 2.0324, 'grad_norm': 6.553909778594971, 'learning_rate': 5.76425456367294e-05, 'epoch': 0.13636363636363635}\n{'loss': 1.9263, 'grad_norm': 6.56605863571167, 'learning_rate': 6.917105476407527e-05, 'epoch': 0.1590909090909091}\n{'loss': 1.7791, 'grad_norm': 5.86032247543335, 'learning_rate': 8.069956389142115e-05, 'epoch': 0.18181818181818182}\n{'loss': 1.6369, 'grad_norm': 5.420650482177734, 'learning_rate': 9.222807301876702e-05, 'epoch': 0.20454545454545456}\n{'loss': 1.4679, 'grad_norm': 4.948894023895264, 'learning_rate': 0.0001037565821461129, 'epoch': 0.22727272727272727}\n{'loss': 1.3357, 'grad_norm': 4.061728000640869, 'learning_rate': 0.0001152850912734588, 'epoch': 0.25}\n{'loss': 1.2525, 'grad_norm': 2.883694648742676, 'learning_rate': 0.00012681360040080468, 'epoch': 0.2727272727272727}\n{'loss': 1.2462, 'grad_norm': 1.8849124908447266, 'learning_rate': 0.00013834210952815053, 'epoch': 0.29545454545454547}\n{'loss': 1.1924, 'grad_norm': 1.8042011260986328, 'learning_rate': 0.0001498706186554964, 'epoch': 0.3181818181818182}\n{'loss': 1.1358, 'grad_norm': 2.1658239364624023, 'learning_rate': 0.0001613991277828423, 'epoch': 0.3409090909090909}\n{'loss': 1.0768, 'grad_norm': 1.5578306913375854, 'learning_rate': 0.00017292763691018818, 'epoch': 0.36363636363636365}\n{'loss': 1.0413, 'grad_norm': 1.2137980461120605, 'learning_rate': 0.00018445614603753403, 'epoch': 0.38636363636363635}\n{'loss': 0.961, 'grad_norm': 1.2200112342834473, 'learning_rate': 0.00019598465516487993, 'epoch': 0.4090909090909091}\n{'loss': 0.9387, 'grad_norm': 1.383013367652893, 'learning_rate': 0.0002075131642922258, 'epoch': 0.4318181818181818}\n{'loss': 0.926, 'grad_norm': 1.2582659721374512, 'learning_rate': 0.00021904167341957169, 'epoch': 0.45454545454545453}\n{'loss': 0.8676, 'grad_norm': 1.340328574180603, 'learning_rate': 0.0002305701825469176, 'epoch': 0.4772727272727273}\n{'loss': 0.8144, 'grad_norm': 1.0473999977111816, 'learning_rate': 0.00024209869167426346, 'epoch': 0.5}\n{'loss': 0.7591, 'grad_norm': 0.9846364855766296, 'learning_rate': 0.00025362720080160937, 'epoch': 0.5227272727272727}\n{'loss': 0.7094, 'grad_norm': 0.8424230217933655, 'learning_rate': 0.0002651557099289552, 'epoch': 0.5454545454545454}\n{'loss': 0.7066, 'grad_norm': 1.1666816473007202, 'learning_rate': 0.00027668421905630106, 'epoch': 0.5681818181818182}\n{'loss': 0.6503, 'grad_norm': 0.8783318400382996, 'learning_rate': 0.00028821272818364694, 'epoch': 0.5909090909090909}\n{'loss': 0.7558, 'grad_norm': 1.002307415008545, 'learning_rate': 0.0002997412373109928, 'epoch': 0.6136363636363636}\n{'loss': 0.6753, 'grad_norm': 0.9914138913154602, 'learning_rate': 0.00031126974643833874, 'epoch': 0.6363636363636364}\n{'loss': 0.7068, 'grad_norm': 1.0964579582214355, 'learning_rate': 0.0003227982555656846, 'epoch': 0.6590909090909091}\n{'loss': 0.6472, 'grad_norm': 1.0055012702941895, 'learning_rate': 0.0003343267646930305, 'epoch': 0.6818181818181818}\n{'loss': 0.5744, 'grad_norm': 0.7679641842842102, 'learning_rate': 0.00034585527382037637, 'epoch': 0.7045454545454546}\n{'loss': 0.5762, 'grad_norm': 1.4993678331375122, 'learning_rate': 0.00035738378294772224, 'epoch': 0.7272727272727273}\n{'loss': 0.5161, 'grad_norm': 1.9111531972885132, 'learning_rate': 0.00036891229207506806, 'epoch': 0.75}\n{'loss': 0.541, 'grad_norm': 0.8934192657470703, 'learning_rate': 0.000380440801202414, 'epoch': 0.7727272727272727}\n{'loss': 0.5234, 'grad_norm': 1.8739055395126343, 'learning_rate': 0.00039196931032975987, 'epoch': 0.7954545454545454}\n{'loss': 0.4695, 'grad_norm': 1.167259931564331, 'learning_rate': 0.00040349781945710574, 'epoch': 0.8181818181818182}\n{'loss': 0.4805, 'grad_norm': 0.6606035828590393, 'learning_rate': 0.0004150263285844516, 'epoch': 0.8409090909090909}\n{'loss': 0.4463, 'grad_norm': 1.363573431968689, 'learning_rate': 0.0004137129541269059, 'epoch': 0.8636363636363636}\n{'loss': 0.4196, 'grad_norm': 1.4519888162612915, 'learning_rate': 0.0004123995796693601, 'epoch': 0.8863636363636364}\n{'loss': 0.4031, 'grad_norm': 1.0417431592941284, 'learning_rate': 0.0004110862052118144, 'epoch': 0.9090909090909091}\n{'loss': 0.4114, 'grad_norm': 0.7664448618888855, 'learning_rate': 0.0004097728307542687, 'epoch': 0.9318181818181818}\n{'loss': 0.4083, 'grad_norm': 1.4413717985153198, 'learning_rate': 0.00040845945629672297, 'epoch': 0.9545454545454546}\n{'loss': 0.4211, 'grad_norm': 1.1105419397354126, 'learning_rate': 0.00040714608183917725, 'epoch': 0.9772727272727273}\n{'loss': 0.3753, 'grad_norm': 3.5490705966949463, 'learning_rate': 0.0004058327073816315, 'epoch': 1.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.7927    0.7812    0.7869      3456\n     PERCENT     0.5474    0.9740    0.7009        77\n        TYPE     0.8997    0.9465    0.9225     11282\n      VOLUME     0.4667    0.1707    0.2500        41\n\n   micro avg     0.8726    0.9060    0.8890     14856\n   macro avg     0.6766    0.7181    0.6651     14856\nweighted avg     0.8718    0.9060    0.8880     14856\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.3774685263633728, 'eval_f1_macro': 0.6650956403361995, 'eval_precision': 0.872552832879554, 'eval_recall': 0.9060312331717825, 'eval_f1': 0.8889769500033025, 'eval_accuracy': 0.8882180156657964, 'eval_runtime': 1.6166, 'eval_samples_per_second': 3408.403, 'eval_steps_per_second': 6.804, 'epoch': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.352, 'grad_norm': 1.2072384357452393, 'learning_rate': 0.00040451933292408575, 'epoch': 1.0227272727272727}\n{'loss': 0.401, 'grad_norm': 1.3529915809631348, 'learning_rate': 0.00040320595846654, 'epoch': 1.0454545454545454}\n{'loss': 0.3685, 'grad_norm': 1.6207181215286255, 'learning_rate': 0.0004018925840089943, 'epoch': 1.0681818181818181}\n{'loss': 0.3442, 'grad_norm': 1.839303731918335, 'learning_rate': 0.0004005792095514486, 'epoch': 1.0909090909090908}\n{'loss': 0.3242, 'grad_norm': 1.2815428972244263, 'learning_rate': 0.0003992658350939028, 'epoch': 1.1136363636363635}\n{'loss': 0.3405, 'grad_norm': 1.1440660953521729, 'learning_rate': 0.0003979524606363571, 'epoch': 1.1363636363636362}\n{'loss': 0.3564, 'grad_norm': 1.5483834743499756, 'learning_rate': 0.00039663908617881133, 'epoch': 1.1590909090909092}\n{'loss': 0.2549, 'grad_norm': 1.5829684734344482, 'learning_rate': 0.0003953257117212656, 'epoch': 1.1818181818181819}\n{'loss': 0.3233, 'grad_norm': 1.4479858875274658, 'learning_rate': 0.0003940123372637199, 'epoch': 1.2045454545454546}\n{'loss': 0.2739, 'grad_norm': 0.6858261823654175, 'learning_rate': 0.00039269896280617417, 'epoch': 1.2272727272727273}\n{'loss': 0.3142, 'grad_norm': 1.3583831787109375, 'learning_rate': 0.00039138558834862845, 'epoch': 1.25}\n{'loss': 0.2627, 'grad_norm': 1.0528000593185425, 'learning_rate': 0.0003900722138910827, 'epoch': 1.2727272727272727}\n{'loss': 0.3188, 'grad_norm': 1.1835321187973022, 'learning_rate': 0.00038875883943353696, 'epoch': 1.2954545454545454}\n{'loss': 0.3583, 'grad_norm': 0.7647125720977783, 'learning_rate': 0.00038744546497599124, 'epoch': 1.3181818181818181}\n{'loss': 0.3039, 'grad_norm': 1.387183427810669, 'learning_rate': 0.00038613209051844546, 'epoch': 1.3409090909090908}\n{'loss': 0.2821, 'grad_norm': 0.9228412508964539, 'learning_rate': 0.00038481871606089974, 'epoch': 1.3636363636363638}\n{'loss': 0.2813, 'grad_norm': 0.9050039649009705, 'learning_rate': 0.000383505341603354, 'epoch': 1.3863636363636362}\n{'loss': 0.3452, 'grad_norm': 1.0621201992034912, 'learning_rate': 0.0003821919671458083, 'epoch': 1.4090909090909092}\n{'loss': 0.2931, 'grad_norm': 0.9278982281684875, 'learning_rate': 0.0003808785926882626, 'epoch': 1.4318181818181819}\n{'loss': 0.2485, 'grad_norm': 1.7008051872253418, 'learning_rate': 0.0003795652182307168, 'epoch': 1.4545454545454546}\n{'loss': 0.2678, 'grad_norm': 1.4984867572784424, 'learning_rate': 0.0003782518437731711, 'epoch': 1.4772727272727273}\n{'loss': 0.3032, 'grad_norm': 1.224399209022522, 'learning_rate': 0.0003769384693156253, 'epoch': 1.5}\n{'loss': 0.2987, 'grad_norm': 1.3770225048065186, 'learning_rate': 0.00037562509485807965, 'epoch': 1.5227272727272727}\n{'loss': 0.243, 'grad_norm': 0.9518761038780212, 'learning_rate': 0.00037431172040053393, 'epoch': 1.5454545454545454}\n{'loss': 0.3146, 'grad_norm': 0.7345376014709473, 'learning_rate': 0.00037299834594298816, 'epoch': 1.5681818181818183}\n{'loss': 0.3311, 'grad_norm': 1.0602039098739624, 'learning_rate': 0.00037168497148544244, 'epoch': 1.5909090909090908}\n{'loss': 0.2812, 'grad_norm': 1.9086251258850098, 'learning_rate': 0.00037037159702789667, 'epoch': 1.6136363636363638}\n{'loss': 0.3006, 'grad_norm': 1.7959954738616943, 'learning_rate': 0.00036905822257035095, 'epoch': 1.6363636363636362}\n{'loss': 0.2333, 'grad_norm': 1.3276392221450806, 'learning_rate': 0.0003677448481128052, 'epoch': 1.6590909090909092}\n{'loss': 0.2736, 'grad_norm': 1.7537155151367188, 'learning_rate': 0.0003664314736552595, 'epoch': 1.6818181818181817}\n{'loss': 0.3254, 'grad_norm': 1.7048747539520264, 'learning_rate': 0.0003651180991977138, 'epoch': 1.7045454545454546}\n{'loss': 0.3106, 'grad_norm': 1.086323618888855, 'learning_rate': 0.000363804724740168, 'epoch': 1.7272727272727273}\n{'loss': 0.3159, 'grad_norm': 1.6038072109222412, 'learning_rate': 0.0003624913502826223, 'epoch': 1.75}\n{'loss': 0.2446, 'grad_norm': 1.7809209823608398, 'learning_rate': 0.0003611779758250766, 'epoch': 1.7727272727272727}\n{'loss': 0.2388, 'grad_norm': 1.5087910890579224, 'learning_rate': 0.0003598646013675308, 'epoch': 1.7954545454545454}\n{'loss': 0.2535, 'grad_norm': 0.9270419478416443, 'learning_rate': 0.00035855122690998514, 'epoch': 1.8181818181818183}\n{'loss': 0.3012, 'grad_norm': 2.507535934448242, 'learning_rate': 0.00035723785245243936, 'epoch': 1.8409090909090908}\n{'loss': 0.2503, 'grad_norm': 0.6898059248924255, 'learning_rate': 0.00035592447799489364, 'epoch': 1.8636363636363638}\n{'loss': 0.2586, 'grad_norm': 1.6125537157058716, 'learning_rate': 0.0003546111035373479, 'epoch': 1.8863636363636362}\n{'loss': 0.2433, 'grad_norm': 0.8997368812561035, 'learning_rate': 0.00035329772907980215, 'epoch': 1.9090909090909092}\n{'loss': 0.2886, 'grad_norm': 1.1307504177093506, 'learning_rate': 0.00035198435462225643, 'epoch': 1.9318181818181817}\n{'loss': 0.2149, 'grad_norm': 1.9038715362548828, 'learning_rate': 0.0003506709801647107, 'epoch': 1.9545454545454546}\n{'loss': 0.2683, 'grad_norm': 1.5820573568344116, 'learning_rate': 0.000349357605707165, 'epoch': 1.9772727272727273}\n{'loss': 0.2982, 'grad_norm': 3.9908993244171143, 'learning_rate': 0.00034804423124961927, 'epoch': 2.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8868    0.8594    0.8729      3456\n     PERCENT     0.8718    0.8831    0.8774        77\n        TYPE     0.9329    0.9695    0.9508     11282\n      VOLUME     0.6522    0.7317    0.6897        41\n\n   micro avg     0.9216    0.9428    0.9321     14856\n   macro avg     0.8359    0.8609    0.8477     14856\nweighted avg     0.9211    0.9428    0.9316     14856\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.26226019859313965, 'eval_f1_macro': 0.8477007895722232, 'eval_precision': 0.9215686274509803, 'eval_recall': 0.9427840603123318, 'eval_f1': 0.932055633193585, 'eval_accuracy': 0.9254242819843342, 'eval_runtime': 1.6102, 'eval_samples_per_second': 3422.026, 'eval_steps_per_second': 6.832, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.1858, 'grad_norm': 0.6682971119880676, 'learning_rate': 0.0003467308567920735, 'epoch': 2.022727272727273}\n{'loss': 0.2014, 'grad_norm': 1.0404503345489502, 'learning_rate': 0.0003454174823345278, 'epoch': 2.0454545454545454}\n{'loss': 0.2011, 'grad_norm': 1.8423912525177002, 'learning_rate': 0.000344104107876982, 'epoch': 2.0681818181818183}\n{'loss': 0.1968, 'grad_norm': 1.7258528470993042, 'learning_rate': 0.0003427907334194363, 'epoch': 2.090909090909091}\n{'loss': 0.171, 'grad_norm': 0.7965230941772461, 'learning_rate': 0.0003414773589618906, 'epoch': 2.1136363636363638}\n{'loss': 0.1781, 'grad_norm': 0.8776726722717285, 'learning_rate': 0.00034016398450434484, 'epoch': 2.1363636363636362}\n{'loss': 0.1602, 'grad_norm': 1.2712725400924683, 'learning_rate': 0.0003388506100467991, 'epoch': 2.159090909090909}\n{'loss': 0.1416, 'grad_norm': 0.9467733502388, 'learning_rate': 0.00033753723558925335, 'epoch': 2.1818181818181817}\n{'loss': 0.1814, 'grad_norm': 1.0226290225982666, 'learning_rate': 0.00033622386113170763, 'epoch': 2.2045454545454546}\n{'loss': 0.1627, 'grad_norm': 0.7087240219116211, 'learning_rate': 0.0003349104866741619, 'epoch': 2.227272727272727}\n{'loss': 0.2108, 'grad_norm': 1.2023085355758667, 'learning_rate': 0.00033359711221661614, 'epoch': 2.25}\n{'loss': 0.1393, 'grad_norm': 0.8576526641845703, 'learning_rate': 0.00033228373775907047, 'epoch': 2.2727272727272725}\n{'loss': 0.1334, 'grad_norm': 0.6542450189590454, 'learning_rate': 0.0003309703633015247, 'epoch': 2.2954545454545454}\n{'loss': 0.1847, 'grad_norm': 1.122370719909668, 'learning_rate': 0.000329656988843979, 'epoch': 2.3181818181818183}\n{'loss': 0.1775, 'grad_norm': 0.7591933608055115, 'learning_rate': 0.00032834361438643326, 'epoch': 2.340909090909091}\n{'loss': 0.1813, 'grad_norm': 1.3357881307601929, 'learning_rate': 0.0003270302399288875, 'epoch': 2.3636363636363638}\n{'loss': 0.1623, 'grad_norm': 0.8129757642745972, 'learning_rate': 0.00032571686547134177, 'epoch': 2.3863636363636362}\n{'loss': 0.1194, 'grad_norm': 0.8851486444473267, 'learning_rate': 0.00032440349101379605, 'epoch': 2.409090909090909}\n{'loss': 0.1652, 'grad_norm': 1.6720389127731323, 'learning_rate': 0.0003230901165562503, 'epoch': 2.4318181818181817}\n{'loss': 0.1796, 'grad_norm': 1.4474523067474365, 'learning_rate': 0.0003217767420987046, 'epoch': 2.4545454545454546}\n{'loss': 0.2222, 'grad_norm': 1.3148398399353027, 'learning_rate': 0.00032046336764115883, 'epoch': 2.4772727272727275}\n{'loss': 0.1353, 'grad_norm': 0.840122401714325, 'learning_rate': 0.0003191499931836131, 'epoch': 2.5}\n{'loss': 0.1224, 'grad_norm': 0.7609719634056091, 'learning_rate': 0.0003178366187260674, 'epoch': 2.5227272727272725}\n{'loss': 0.1449, 'grad_norm': 0.8186089992523193, 'learning_rate': 0.0003165232442685216, 'epoch': 2.5454545454545454}\n{'loss': 0.15, 'grad_norm': 1.8575010299682617, 'learning_rate': 0.00031520986981097595, 'epoch': 2.5681818181818183}\n{'loss': 0.1761, 'grad_norm': 1.2196077108383179, 'learning_rate': 0.0003138964953534302, 'epoch': 2.590909090909091}\n{'loss': 0.1192, 'grad_norm': 0.8642178177833557, 'learning_rate': 0.00031258312089588446, 'epoch': 2.6136363636363638}\n{'loss': 0.2066, 'grad_norm': 0.8249372243881226, 'learning_rate': 0.00031126974643833874, 'epoch': 2.6363636363636362}\n{'loss': 0.2073, 'grad_norm': 1.1990844011306763, 'learning_rate': 0.00030995637198079297, 'epoch': 2.659090909090909}\n{'loss': 0.1693, 'grad_norm': 1.2189221382141113, 'learning_rate': 0.00030864299752324725, 'epoch': 2.6818181818181817}\n{'loss': 0.182, 'grad_norm': 1.3924875259399414, 'learning_rate': 0.0003073296230657015, 'epoch': 2.7045454545454546}\n{'loss': 0.1239, 'grad_norm': 0.9817660450935364, 'learning_rate': 0.0003060162486081558, 'epoch': 2.7272727272727275}\n{'loss': 0.1619, 'grad_norm': 0.9568228125572205, 'learning_rate': 0.00030470287415061004, 'epoch': 2.75}\n{'loss': 0.2213, 'grad_norm': 0.9704039692878723, 'learning_rate': 0.0003033894996930643, 'epoch': 2.7727272727272725}\n{'loss': 0.1676, 'grad_norm': 1.4184787273406982, 'learning_rate': 0.0003020761252355186, 'epoch': 2.7954545454545454}\n{'loss': 0.177, 'grad_norm': 1.1684136390686035, 'learning_rate': 0.0003007627507779728, 'epoch': 2.8181818181818183}\n{'loss': 0.1781, 'grad_norm': 1.20774507522583, 'learning_rate': 0.0002994493763204271, 'epoch': 2.840909090909091}\n{'loss': 0.1874, 'grad_norm': 1.048127293586731, 'learning_rate': 0.0002981360018628814, 'epoch': 2.8636363636363638}\n{'loss': 0.1493, 'grad_norm': 1.0515753030776978, 'learning_rate': 0.00029682262740533566, 'epoch': 2.8863636363636362}\n{'loss': 0.1656, 'grad_norm': 1.0942128896713257, 'learning_rate': 0.00029550925294778994, 'epoch': 2.909090909090909}\n{'loss': 0.1925, 'grad_norm': 1.0889413356781006, 'learning_rate': 0.00029419587849024417, 'epoch': 2.9318181818181817}\n{'loss': 0.1978, 'grad_norm': 0.7887573838233948, 'learning_rate': 0.00029288250403269845, 'epoch': 2.9545454545454546}\n{'loss': 0.1761, 'grad_norm': 0.7859136462211609, 'learning_rate': 0.00029156912957515273, 'epoch': 2.9772727272727275}\n{'loss': 0.0964, 'grad_norm': 2.7971909046173096, 'learning_rate': 0.00029025575511760696, 'epoch': 3.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8775    0.8872    0.8823      3456\n     PERCENT     0.8588    0.9481    0.9012        77\n        TYPE     0.9460    0.9620    0.9539     11282\n      VOLUME     0.7692    0.7317    0.7500        41\n\n   micro avg     0.9292    0.9439    0.9365     14856\n   macro avg     0.8629    0.8822    0.8719     14856\nweighted avg     0.9292    0.9439    0.9364     14856\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.24876578152179718, 'eval_f1_macro': 0.8718697225474336, 'eval_precision': 0.9292246520874752, 'eval_recall': 0.9438610662358643, 'eval_f1': 0.9364856742135844, 'eval_accuracy': 0.9290687554395126, 'eval_runtime': 1.6506, 'eval_samples_per_second': 3338.229, 'eval_steps_per_second': 6.664, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.117, 'grad_norm': 0.7337208390235901, 'learning_rate': 0.0002889423806600613, 'epoch': 3.022727272727273}\n{'loss': 0.1095, 'grad_norm': 1.1748180389404297, 'learning_rate': 0.0002876290062025155, 'epoch': 3.0454545454545454}\n{'loss': 0.1097, 'grad_norm': 0.7494632005691528, 'learning_rate': 0.0002863156317449698, 'epoch': 3.0681818181818183}\n{'loss': 0.1602, 'grad_norm': 1.1549921035766602, 'learning_rate': 0.0002850022572874241, 'epoch': 3.090909090909091}\n{'loss': 0.1133, 'grad_norm': 0.8960215449333191, 'learning_rate': 0.0002836888828298783, 'epoch': 3.1136363636363638}\n{'loss': 0.1132, 'grad_norm': 1.1196602582931519, 'learning_rate': 0.0002823755083723326, 'epoch': 3.1363636363636362}\n{'loss': 0.0845, 'grad_norm': 0.7447257041931152, 'learning_rate': 0.0002810621339147868, 'epoch': 3.159090909090909}\n{'loss': 0.1234, 'grad_norm': 0.9211022853851318, 'learning_rate': 0.00027974875945724115, 'epoch': 3.1818181818181817}\n{'loss': 0.0853, 'grad_norm': 1.0759390592575073, 'learning_rate': 0.0002784353849996954, 'epoch': 3.2045454545454546}\n{'loss': 0.086, 'grad_norm': 1.5364856719970703, 'learning_rate': 0.00027712201054214965, 'epoch': 3.227272727272727}\n{'loss': 0.1047, 'grad_norm': 0.6802666783332825, 'learning_rate': 0.00027580863608460393, 'epoch': 3.25}\n{'loss': 0.0956, 'grad_norm': 1.0080857276916504, 'learning_rate': 0.00027449526162705816, 'epoch': 3.2727272727272725}\n{'loss': 0.121, 'grad_norm': 0.9080696105957031, 'learning_rate': 0.00027318188716951244, 'epoch': 3.2954545454545454}\n{'loss': 0.1126, 'grad_norm': 1.0254982709884644, 'learning_rate': 0.0002718685127119667, 'epoch': 3.3181818181818183}\n{'loss': 0.074, 'grad_norm': 0.6687481999397278, 'learning_rate': 0.000270555138254421, 'epoch': 3.340909090909091}\n{'loss': 0.1177, 'grad_norm': 1.5125303268432617, 'learning_rate': 0.0002692417637968753, 'epoch': 3.3636363636363638}\n{'loss': 0.0816, 'grad_norm': 0.7148250937461853, 'learning_rate': 0.0002679283893393295, 'epoch': 3.3863636363636362}\n{'loss': 0.1109, 'grad_norm': 1.3742386102676392, 'learning_rate': 0.0002666150148817838, 'epoch': 3.409090909090909}\n{'loss': 0.1032, 'grad_norm': 1.239991307258606, 'learning_rate': 0.00026530164042423807, 'epoch': 3.4318181818181817}\n{'loss': 0.113, 'grad_norm': 0.6113459467887878, 'learning_rate': 0.0002639882659666923, 'epoch': 3.4545454545454546}\n{'loss': 0.0862, 'grad_norm': 0.6153728365898132, 'learning_rate': 0.00026267489150914663, 'epoch': 3.4772727272727275}\n{'loss': 0.1006, 'grad_norm': 1.8666681051254272, 'learning_rate': 0.00026136151705160086, 'epoch': 3.5}\n{'loss': 0.0996, 'grad_norm': 0.7305238842964172, 'learning_rate': 0.00026004814259405514, 'epoch': 3.5227272727272725}\n{'loss': 0.1172, 'grad_norm': 1.6279575824737549, 'learning_rate': 0.0002587347681365094, 'epoch': 3.5454545454545454}\n{'loss': 0.1327, 'grad_norm': 1.0609767436981201, 'learning_rate': 0.00025742139367896364, 'epoch': 3.5681818181818183}\n{'loss': 0.1152, 'grad_norm': 1.352814793586731, 'learning_rate': 0.0002561080192214179, 'epoch': 3.590909090909091}\n{'loss': 0.1224, 'grad_norm': 0.7666193842887878, 'learning_rate': 0.0002547946447638722, 'epoch': 3.6136363636363638}\n{'loss': 0.0862, 'grad_norm': 1.026380181312561, 'learning_rate': 0.0002534812703063265, 'epoch': 3.6363636363636362}\n{'loss': 0.1019, 'grad_norm': 0.8555088043212891, 'learning_rate': 0.00025216789584878076, 'epoch': 3.659090909090909}\n{'loss': 0.1075, 'grad_norm': 0.8303797841072083, 'learning_rate': 0.000250854521391235, 'epoch': 3.6818181818181817}\n{'loss': 0.076, 'grad_norm': 1.3200136423110962, 'learning_rate': 0.00024954114693368927, 'epoch': 3.7045454545454546}\n{'loss': 0.1271, 'grad_norm': 0.9309272766113281, 'learning_rate': 0.0002482277724761435, 'epoch': 3.7272727272727275}\n{'loss': 0.0857, 'grad_norm': 1.0788986682891846, 'learning_rate': 0.0002469143980185978, 'epoch': 3.75}\n{'loss': 0.1126, 'grad_norm': 1.3358988761901855, 'learning_rate': 0.0002456010235610521, 'epoch': 3.7727272727272725}\n{'loss': 0.124, 'grad_norm': 0.7307984828948975, 'learning_rate': 0.00024428764910350634, 'epoch': 3.7954545454545454}\n{'loss': 0.1473, 'grad_norm': 0.9813876152038574, 'learning_rate': 0.00024297427464596062, 'epoch': 3.8181818181818183}\n{'loss': 0.1148, 'grad_norm': 0.9456943273544312, 'learning_rate': 0.00024166090018841487, 'epoch': 3.840909090909091}\n{'loss': 0.0774, 'grad_norm': 0.9249966144561768, 'learning_rate': 0.00024034752573086912, 'epoch': 3.8636363636363638}\n{'loss': 0.1241, 'grad_norm': 1.5481934547424316, 'learning_rate': 0.00023903415127332338, 'epoch': 3.8863636363636362}\n{'loss': 0.1409, 'grad_norm': 0.7490523457527161, 'learning_rate': 0.00023772077681577769, 'epoch': 3.909090909090909}\n{'loss': 0.1769, 'grad_norm': 1.2503759860992432, 'learning_rate': 0.00023640740235823194, 'epoch': 3.9318181818181817}\n{'loss': 0.1488, 'grad_norm': 0.7379011511802673, 'learning_rate': 0.00023509402790068622, 'epoch': 3.9545454545454546}\n{'loss': 0.0915, 'grad_norm': 0.8031612038612366, 'learning_rate': 0.00023378065344314047, 'epoch': 3.9772727272727275}\n{'loss': 0.1222, 'grad_norm': 5.017322063446045, 'learning_rate': 0.00023246727898559473, 'epoch': 4.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.9019    0.8756    0.8886      3456\n     PERCENT     0.8588    0.9481    0.9012        77\n        TYPE     0.9473    0.9637    0.9554     11282\n      VOLUME     0.8182    0.8780    0.8471        41\n\n   micro avg     0.9362    0.9429    0.9395     14856\n   macro avg     0.8816    0.9163    0.8981     14856\nweighted avg     0.9359    0.9429    0.9393     14856\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.25349605083465576, 'eval_f1_macro': 0.8980645672925439, 'eval_precision': 0.9362342089432525, 'eval_recall': 0.9428513731825525, 'eval_f1': 0.9395311399537177, 'eval_accuracy': 0.9310813751087903, 'eval_runtime': 1.4757, 'eval_samples_per_second': 3733.846, 'eval_steps_per_second': 7.454, 'epoch': 4.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.0814, 'grad_norm': 0.7529456615447998, 'learning_rate': 0.00023115390452804898, 'epoch': 4.0227272727272725}\n{'loss': 0.08, 'grad_norm': 0.9797042012214661, 'learning_rate': 0.00022984053007050326, 'epoch': 4.045454545454546}\n{'loss': 0.0788, 'grad_norm': 1.0105069875717163, 'learning_rate': 0.00022852715561295757, 'epoch': 4.068181818181818}\n{'loss': 0.097, 'grad_norm': 0.6935225129127502, 'learning_rate': 0.00022721378115541182, 'epoch': 4.090909090909091}\n{'loss': 0.0675, 'grad_norm': 0.5371397733688354, 'learning_rate': 0.00022590040669786607, 'epoch': 4.113636363636363}\n{'loss': 0.0623, 'grad_norm': 0.7617022395133972, 'learning_rate': 0.00022458703224032033, 'epoch': 4.136363636363637}\n{'loss': 0.0898, 'grad_norm': 1.1396363973617554, 'learning_rate': 0.0002232736577827746, 'epoch': 4.159090909090909}\n{'loss': 0.079, 'grad_norm': 1.2576241493225098, 'learning_rate': 0.00022196028332522886, 'epoch': 4.181818181818182}\n{'loss': 0.0994, 'grad_norm': 0.8999282121658325, 'learning_rate': 0.00022064690886768311, 'epoch': 4.204545454545454}\n{'loss': 0.1003, 'grad_norm': 0.9147118926048279, 'learning_rate': 0.00021933353441013742, 'epoch': 4.2272727272727275}\n{'loss': 0.0734, 'grad_norm': 0.683957576751709, 'learning_rate': 0.00021802015995259168, 'epoch': 4.25}\n{'loss': 0.0678, 'grad_norm': 0.5936267375946045, 'learning_rate': 0.00021670678549504596, 'epoch': 4.2727272727272725}\n{'loss': 0.0749, 'grad_norm': 0.8793847560882568, 'learning_rate': 0.0002153934110375002, 'epoch': 4.295454545454546}\n{'loss': 0.077, 'grad_norm': 0.7681232690811157, 'learning_rate': 0.00021408003657995446, 'epoch': 4.318181818181818}\n{'loss': 0.0555, 'grad_norm': 1.1336290836334229, 'learning_rate': 0.00021276666212240872, 'epoch': 4.340909090909091}\n{'loss': 0.0797, 'grad_norm': 0.9767717719078064, 'learning_rate': 0.00021145328766486302, 'epoch': 4.363636363636363}\n{'loss': 0.065, 'grad_norm': 1.1474169492721558, 'learning_rate': 0.0002101399132073173, 'epoch': 4.386363636363637}\n{'loss': 0.077, 'grad_norm': 0.7734829187393188, 'learning_rate': 0.00020882653874977156, 'epoch': 4.409090909090909}\n{'loss': 0.0651, 'grad_norm': 0.9825735688209534, 'learning_rate': 0.0002075131642922258, 'epoch': 4.431818181818182}\n{'loss': 0.0818, 'grad_norm': 0.775085985660553, 'learning_rate': 0.00020619978983468006, 'epoch': 4.454545454545454}\n{'loss': 0.0659, 'grad_norm': 1.1101903915405273, 'learning_rate': 0.00020488641537713434, 'epoch': 4.4772727272727275}\n{'loss': 0.0936, 'grad_norm': 1.2713127136230469, 'learning_rate': 0.00020357304091958862, 'epoch': 4.5}\n{'loss': 0.0944, 'grad_norm': 1.3632233142852783, 'learning_rate': 0.00020225966646204288, 'epoch': 4.5227272727272725}\n{'loss': 0.0652, 'grad_norm': 0.7207698822021484, 'learning_rate': 0.00020094629200449716, 'epoch': 4.545454545454545}\n{'loss': 0.0739, 'grad_norm': 0.9267800450325012, 'learning_rate': 0.0001996329175469514, 'epoch': 4.568181818181818}\n{'loss': 0.0825, 'grad_norm': 0.7751057147979736, 'learning_rate': 0.00019831954308940566, 'epoch': 4.590909090909091}\n{'loss': 0.0791, 'grad_norm': 0.7229424118995667, 'learning_rate': 0.00019700616863185994, 'epoch': 4.613636363636363}\n{'loss': 0.0934, 'grad_norm': 1.249635934829712, 'learning_rate': 0.00019569279417431423, 'epoch': 4.636363636363637}\n{'loss': 0.0582, 'grad_norm': 1.1998118162155151, 'learning_rate': 0.00019437941971676848, 'epoch': 4.659090909090909}\n{'loss': 0.0795, 'grad_norm': 0.6579775214195251, 'learning_rate': 0.00019306604525922273, 'epoch': 4.681818181818182}\n{'loss': 0.0909, 'grad_norm': 0.9783742427825928, 'learning_rate': 0.000191752670801677, 'epoch': 4.704545454545455}\n{'loss': 0.0662, 'grad_norm': 0.9715062975883484, 'learning_rate': 0.0001904392963441313, 'epoch': 4.7272727272727275}\n{'loss': 0.0755, 'grad_norm': 1.1760358810424805, 'learning_rate': 0.00018912592188658555, 'epoch': 4.75}\n{'loss': 0.0768, 'grad_norm': 1.3631329536437988, 'learning_rate': 0.00018781254742903983, 'epoch': 4.7727272727272725}\n{'loss': 0.107, 'grad_norm': 1.1240776777267456, 'learning_rate': 0.00018649917297149408, 'epoch': 4.795454545454545}\n{'loss': 0.0574, 'grad_norm': 0.6424270272254944, 'learning_rate': 0.00018518579851394833, 'epoch': 4.818181818181818}\n{'loss': 0.0728, 'grad_norm': 0.8177555203437805, 'learning_rate': 0.0001838724240564026, 'epoch': 4.840909090909091}\n{'loss': 0.0664, 'grad_norm': 1.0022844076156616, 'learning_rate': 0.0001825590495988569, 'epoch': 4.863636363636363}\n{'loss': 0.0954, 'grad_norm': 1.5110621452331543, 'learning_rate': 0.00018124567514131115, 'epoch': 4.886363636363637}\n{'loss': 0.0954, 'grad_norm': 1.0548285245895386, 'learning_rate': 0.0001799323006837654, 'epoch': 4.909090909090909}\n{'loss': 0.0856, 'grad_norm': 0.9340679049491882, 'learning_rate': 0.00017861892622621968, 'epoch': 4.931818181818182}\n{'loss': 0.0911, 'grad_norm': 0.9494011402130127, 'learning_rate': 0.00017730555176867396, 'epoch': 4.954545454545455}\n{'loss': 0.0742, 'grad_norm': 0.9130392670631409, 'learning_rate': 0.00017599217731112821, 'epoch': 4.9772727272727275}\n{'loss': 0.1405, 'grad_norm': 5.22123384475708, 'learning_rate': 0.0001746788028535825, 'epoch': 5.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8888    0.8924    0.8906      3456\n     PERCENT     0.8621    0.9740    0.9146        77\n        TYPE     0.9503    0.9627    0.9565     11282\n      VOLUME     0.9211    0.8537    0.8861        41\n\n   micro avg     0.9355    0.9461    0.9408     14856\n   macro avg     0.9055    0.9207    0.9119     14856\nweighted avg     0.9354    0.9461    0.9407     14856\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.27040788531303406, 'eval_f1_macro': 0.9119300590425983, 'eval_precision': 0.9355031948881789, 'eval_recall': 0.9460823909531503, 'eval_f1': 0.9407630522088354, 'eval_accuracy': 0.9315165361183638, 'eval_runtime': 1.7477, 'eval_samples_per_second': 3152.802, 'eval_steps_per_second': 6.294, 'epoch': 5.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.047, 'grad_norm': 0.6918768882751465, 'learning_rate': 0.00017336542839603675, 'epoch': 5.0227272727272725}\n{'loss': 0.0776, 'grad_norm': 0.948513925075531, 'learning_rate': 0.000172052053938491, 'epoch': 5.045454545454546}\n{'loss': 0.0523, 'grad_norm': 0.9215336441993713, 'learning_rate': 0.0001707386794809453, 'epoch': 5.068181818181818}\n{'loss': 0.062, 'grad_norm': 0.5717519521713257, 'learning_rate': 0.00016942530502339956, 'epoch': 5.090909090909091}\n{'loss': 0.0598, 'grad_norm': 0.9348907470703125, 'learning_rate': 0.00016811193056585382, 'epoch': 5.113636363636363}\n{'loss': 0.0383, 'grad_norm': 0.6022472381591797, 'learning_rate': 0.00016679855610830807, 'epoch': 5.136363636363637}\n{'loss': 0.0648, 'grad_norm': 1.6696007251739502, 'learning_rate': 0.00016548518165076235, 'epoch': 5.159090909090909}\n{'loss': 0.0501, 'grad_norm': 0.8978389501571655, 'learning_rate': 0.00016417180719321663, 'epoch': 5.181818181818182}\n{'loss': 0.0381, 'grad_norm': 0.6931160092353821, 'learning_rate': 0.00016285843273567088, 'epoch': 5.204545454545454}\n{'loss': 0.0796, 'grad_norm': 0.7970535755157471, 'learning_rate': 0.00016154505827812516, 'epoch': 5.2272727272727275}\n{'loss': 0.0454, 'grad_norm': 0.7236596345901489, 'learning_rate': 0.00016023168382057942, 'epoch': 5.25}\n{'loss': 0.0555, 'grad_norm': 0.7421037554740906, 'learning_rate': 0.0001589183093630337, 'epoch': 5.2727272727272725}\n{'loss': 0.0712, 'grad_norm': 0.7749707102775574, 'learning_rate': 0.00015760493490548798, 'epoch': 5.295454545454546}\n{'loss': 0.0565, 'grad_norm': 1.0863219499588013, 'learning_rate': 0.00015629156044794223, 'epoch': 5.318181818181818}\n{'loss': 0.0447, 'grad_norm': 0.8518471717834473, 'learning_rate': 0.00015497818599039648, 'epoch': 5.340909090909091}\n{'loss': 0.0873, 'grad_norm': 0.6729999780654907, 'learning_rate': 0.00015366481153285074, 'epoch': 5.363636363636363}\n{'loss': 0.0552, 'grad_norm': 0.607609212398529, 'learning_rate': 0.00015235143707530502, 'epoch': 5.386363636363637}\n{'loss': 0.0592, 'grad_norm': 1.2313990592956543, 'learning_rate': 0.0001510380626177593, 'epoch': 5.409090909090909}\n{'loss': 0.0613, 'grad_norm': 1.334778070449829, 'learning_rate': 0.00014972468816021355, 'epoch': 5.431818181818182}\n{'loss': 0.0675, 'grad_norm': 0.7808644771575928, 'learning_rate': 0.00014841131370266783, 'epoch': 5.454545454545454}\n{'loss': 0.0482, 'grad_norm': 0.48409944772720337, 'learning_rate': 0.00014709793924512209, 'epoch': 5.4772727272727275}\n{'loss': 0.0408, 'grad_norm': 0.6478849053382874, 'learning_rate': 0.00014578456478757637, 'epoch': 5.5}\n{'loss': 0.0418, 'grad_norm': 0.8292718529701233, 'learning_rate': 0.00014447119033003065, 'epoch': 5.5227272727272725}\n{'loss': 0.0788, 'grad_norm': 1.1850773096084595, 'learning_rate': 0.0001431578158724849, 'epoch': 5.545454545454545}\n{'loss': 0.0575, 'grad_norm': 1.320920467376709, 'learning_rate': 0.00014184444141493915, 'epoch': 5.568181818181818}\n{'loss': 0.0503, 'grad_norm': 1.2974618673324585, 'learning_rate': 0.0001405310669573934, 'epoch': 5.590909090909091}\n{'loss': 0.0613, 'grad_norm': 0.6447230577468872, 'learning_rate': 0.0001392176924998477, 'epoch': 5.613636363636363}\n{'loss': 0.055, 'grad_norm': 0.9358185529708862, 'learning_rate': 0.00013790431804230197, 'epoch': 5.636363636363637}\n{'loss': 0.0659, 'grad_norm': 0.5964784026145935, 'learning_rate': 0.00013659094358475622, 'epoch': 5.659090909090909}\n{'loss': 0.0602, 'grad_norm': 0.8057883977890015, 'learning_rate': 0.0001352775691272105, 'epoch': 5.681818181818182}\n{'loss': 0.0435, 'grad_norm': 0.7608327269554138, 'learning_rate': 0.00013396419466966475, 'epoch': 5.704545454545455}\n{'loss': 0.0647, 'grad_norm': 1.6194818019866943, 'learning_rate': 0.00013265082021211903, 'epoch': 5.7272727272727275}\n{'loss': 0.0771, 'grad_norm': 1.281813144683838, 'learning_rate': 0.00013133744575457331, 'epoch': 5.75}\n{'loss': 0.042, 'grad_norm': 0.5129402875900269, 'learning_rate': 0.00013002407129702757, 'epoch': 5.7727272727272725}\n{'loss': 0.0807, 'grad_norm': 0.8798231482505798, 'learning_rate': 0.00012871069683948182, 'epoch': 5.795454545454545}\n{'loss': 0.0726, 'grad_norm': 1.4427043199539185, 'learning_rate': 0.0001273973223819361, 'epoch': 5.818181818181818}\n{'loss': 0.0356, 'grad_norm': 0.6285783648490906, 'learning_rate': 0.00012608394792439038, 'epoch': 5.840909090909091}\n{'loss': 0.0639, 'grad_norm': 0.5643734931945801, 'learning_rate': 0.00012477057346684464, 'epoch': 5.863636363636363}\n{'loss': 0.0497, 'grad_norm': 0.8493040204048157, 'learning_rate': 0.0001234571990092989, 'epoch': 5.886363636363637}\n{'loss': 0.0711, 'grad_norm': 0.8551921844482422, 'learning_rate': 0.00012214382455175317, 'epoch': 5.909090909090909}\n{'loss': 0.0394, 'grad_norm': 1.2378572225570679, 'learning_rate': 0.00012083045009420744, 'epoch': 5.931818181818182}\n{'loss': 0.0663, 'grad_norm': 0.9767071008682251, 'learning_rate': 0.00011951707563666169, 'epoch': 5.954545454545455}\n{'loss': 0.0699, 'grad_norm': 0.7884912490844727, 'learning_rate': 0.00011820370117911597, 'epoch': 5.9772727272727275}\n{'loss': 0.0108, 'grad_norm': 0.6293638944625854, 'learning_rate': 0.00011689032672157024, 'epoch': 6.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.9052    0.8811    0.8930      3456\n     PERCENT     0.8621    0.9740    0.9146        77\n        TYPE     0.9475    0.9668    0.9571     11282\n      VOLUME     0.9231    0.8780    0.9000        41\n\n   micro avg     0.9374    0.9467    0.9420     14856\n   macro avg     0.9094    0.9250    0.9162     14856\nweighted avg     0.9371    0.9467    0.9418     14856\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.29588502645492554, 'eval_f1_macro': 0.9161620020646891, 'eval_precision': 0.9374125174965007, 'eval_recall': 0.9466882067851373, 'eval_f1': 0.9420275293881243, 'eval_accuracy': 0.9326588337684943, 'eval_runtime': 1.5464, 'eval_samples_per_second': 3563.138, 'eval_steps_per_second': 7.113, 'epoch': 6.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.0319, 'grad_norm': 0.5494998097419739, 'learning_rate': 0.00011557695226402449, 'epoch': 6.0227272727272725}\n{'loss': 0.0483, 'grad_norm': 0.7719839215278625, 'learning_rate': 0.00011426357780647878, 'epoch': 6.045454545454546}\n{'loss': 0.0412, 'grad_norm': 0.9158718585968018, 'learning_rate': 0.00011295020334893304, 'epoch': 6.068181818181818}\n{'loss': 0.048, 'grad_norm': 0.902879536151886, 'learning_rate': 0.0001116368288913873, 'epoch': 6.090909090909091}\n{'loss': 0.052, 'grad_norm': 1.1214816570281982, 'learning_rate': 0.00011032345443384156, 'epoch': 6.113636363636363}\n{'loss': 0.0352, 'grad_norm': 0.5876176357269287, 'learning_rate': 0.00010901007997629584, 'epoch': 6.136363636363637}\n{'loss': 0.0592, 'grad_norm': 0.6298372745513916, 'learning_rate': 0.0001076967055187501, 'epoch': 6.159090909090909}\n{'loss': 0.0563, 'grad_norm': 0.7020531892776489, 'learning_rate': 0.00010638333106120436, 'epoch': 6.181818181818182}\n{'loss': 0.0602, 'grad_norm': 0.9580276608467102, 'learning_rate': 0.00010506995660365865, 'epoch': 6.204545454545454}\n{'loss': 0.0523, 'grad_norm': 0.8272255063056946, 'learning_rate': 0.0001037565821461129, 'epoch': 6.2272727272727275}\n{'loss': 0.047, 'grad_norm': 0.8383483290672302, 'learning_rate': 0.00010244320768856717, 'epoch': 6.25}\n{'loss': 0.0473, 'grad_norm': 1.2375675439834595, 'learning_rate': 0.00010112983323102144, 'epoch': 6.2727272727272725}\n{'loss': 0.0435, 'grad_norm': 0.7317711710929871, 'learning_rate': 9.98164587734757e-05, 'epoch': 6.295454545454546}\n{'loss': 0.0375, 'grad_norm': 0.7448713183403015, 'learning_rate': 9.850308431592997e-05, 'epoch': 6.318181818181818}\n{'loss': 0.0561, 'grad_norm': 0.5996705293655396, 'learning_rate': 9.718970985838424e-05, 'epoch': 6.340909090909091}\n{'loss': 0.0648, 'grad_norm': 0.659507155418396, 'learning_rate': 9.58763354008385e-05, 'epoch': 6.363636363636363}\n{'loss': 0.0303, 'grad_norm': 0.5724464058876038, 'learning_rate': 9.456296094329277e-05, 'epoch': 6.386363636363637}\n{'loss': 0.0488, 'grad_norm': 1.0637404918670654, 'learning_rate': 9.324958648574704e-05, 'epoch': 6.409090909090909}\n{'loss': 0.0486, 'grad_norm': 0.8441620469093323, 'learning_rate': 9.19362120282013e-05, 'epoch': 6.431818181818182}\n{'loss': 0.0228, 'grad_norm': 0.41539305448532104, 'learning_rate': 9.062283757065557e-05, 'epoch': 6.454545454545454}\n{'loss': 0.0337, 'grad_norm': 0.5848812460899353, 'learning_rate': 8.930946311310984e-05, 'epoch': 6.4772727272727275}\n{'loss': 0.0581, 'grad_norm': 0.6930155754089355, 'learning_rate': 8.799608865556411e-05, 'epoch': 6.5}\n{'loss': 0.0352, 'grad_norm': 0.657923698425293, 'learning_rate': 8.668271419801837e-05, 'epoch': 6.5227272727272725}\n{'loss': 0.0577, 'grad_norm': 0.8772431015968323, 'learning_rate': 8.536933974047265e-05, 'epoch': 6.545454545454545}\n{'loss': 0.0469, 'grad_norm': 0.7240455150604248, 'learning_rate': 8.405596528292691e-05, 'epoch': 6.568181818181818}\n{'loss': 0.0431, 'grad_norm': 0.5908586382865906, 'learning_rate': 8.274259082538117e-05, 'epoch': 6.590909090909091}\n{'loss': 0.0313, 'grad_norm': 0.44372105598449707, 'learning_rate': 8.142921636783544e-05, 'epoch': 6.613636363636363}\n{'loss': 0.0251, 'grad_norm': 0.4082832932472229, 'learning_rate': 8.011584191028971e-05, 'epoch': 6.636363636363637}\n{'loss': 0.039, 'grad_norm': 0.6535755395889282, 'learning_rate': 7.880246745274399e-05, 'epoch': 6.659090909090909}\n{'loss': 0.0453, 'grad_norm': 0.7956117987632751, 'learning_rate': 7.748909299519824e-05, 'epoch': 6.681818181818182}\n{'loss': 0.0454, 'grad_norm': 0.5993505120277405, 'learning_rate': 7.617571853765251e-05, 'epoch': 6.704545454545455}\n{'loss': 0.0599, 'grad_norm': 1.568400502204895, 'learning_rate': 7.486234408010678e-05, 'epoch': 6.7272727272727275}\n{'loss': 0.0269, 'grad_norm': 0.5466545224189758, 'learning_rate': 7.354896962256104e-05, 'epoch': 6.75}\n{'loss': 0.0446, 'grad_norm': 1.0567835569381714, 'learning_rate': 7.223559516501532e-05, 'epoch': 6.7727272727272725}\n{'loss': 0.0504, 'grad_norm': 0.75931715965271, 'learning_rate': 7.092222070746958e-05, 'epoch': 6.795454545454545}\n{'loss': 0.0623, 'grad_norm': 1.0484192371368408, 'learning_rate': 6.960884624992386e-05, 'epoch': 6.818181818181818}\n{'loss': 0.0506, 'grad_norm': 0.6391110420227051, 'learning_rate': 6.829547179237811e-05, 'epoch': 6.840909090909091}\n{'loss': 0.0554, 'grad_norm': 0.701969563961029, 'learning_rate': 6.698209733483238e-05, 'epoch': 6.863636363636363}\n{'loss': 0.0307, 'grad_norm': 0.5335464477539062, 'learning_rate': 6.566872287728666e-05, 'epoch': 6.886363636363637}\n{'loss': 0.0433, 'grad_norm': 0.8590601086616516, 'learning_rate': 6.435534841974091e-05, 'epoch': 6.909090909090909}\n{'loss': 0.0723, 'grad_norm': 0.9725872278213501, 'learning_rate': 6.304197396219519e-05, 'epoch': 6.931818181818182}\n{'loss': 0.0489, 'grad_norm': 1.4347933530807495, 'learning_rate': 6.172859950464944e-05, 'epoch': 6.954545454545455}\n{'loss': 0.0441, 'grad_norm': 0.8601751327514648, 'learning_rate': 6.041522504710372e-05, 'epoch': 6.9772727272727275}\n{'loss': 0.0073, 'grad_norm': 0.38918057084083557, 'learning_rate': 5.9101850589557985e-05, 'epoch': 7.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.9015    0.8788    0.8900      3456\n     PERCENT     0.8621    0.9740    0.9146        77\n        TYPE     0.9480    0.9648    0.9563     11282\n      VOLUME     0.9231    0.8780    0.9000        41\n\n   micro avg     0.9370    0.9446    0.9408     14856\n   macro avg     0.9087    0.9239    0.9152     14856\nweighted avg     0.9367    0.9446    0.9405     14856\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.30993643403053284, 'eval_f1_macro': 0.9152330199126166, 'eval_precision': 0.9369700206984042, 'eval_recall': 0.944601507808293, 'eval_f1': 0.9407702879361781, 'eval_accuracy': 0.9317341166231505, 'eval_runtime': 1.6969, 'eval_samples_per_second': 3247.139, 'eval_steps_per_second': 6.482, 'epoch': 7.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.0389, 'grad_norm': 0.704299807548523, 'learning_rate': 5.7788476132012245e-05, 'epoch': 7.0227272727272725}\n{'loss': 0.0318, 'grad_norm': 0.9082050919532776, 'learning_rate': 5.647510167446652e-05, 'epoch': 7.045454545454546}\n{'loss': 0.0349, 'grad_norm': 0.3345215916633606, 'learning_rate': 5.516172721692078e-05, 'epoch': 7.068181818181818}\n{'loss': 0.0553, 'grad_norm': 0.6998898386955261, 'learning_rate': 5.384835275937505e-05, 'epoch': 7.090909090909091}\n{'loss': 0.0284, 'grad_norm': 0.4745097756385803, 'learning_rate': 5.2534978301829326e-05, 'epoch': 7.113636363636363}\n{'loss': 0.0265, 'grad_norm': 0.6818349957466125, 'learning_rate': 5.1221603844283586e-05, 'epoch': 7.136363636363637}\n{'loss': 0.0429, 'grad_norm': 0.5865401029586792, 'learning_rate': 4.990822938673785e-05, 'epoch': 7.159090909090909}\n{'loss': 0.032, 'grad_norm': 0.8192000389099121, 'learning_rate': 4.859485492919212e-05, 'epoch': 7.181818181818182}\n{'loss': 0.0408, 'grad_norm': 0.6475058794021606, 'learning_rate': 4.7281480471646386e-05, 'epoch': 7.204545454545454}\n{'loss': 0.045, 'grad_norm': 0.6389324069023132, 'learning_rate': 4.596810601410065e-05, 'epoch': 7.2272727272727275}\n{'loss': 0.0368, 'grad_norm': 0.7752991318702698, 'learning_rate': 4.465473155655492e-05, 'epoch': 7.25}\n{'loss': 0.0253, 'grad_norm': 0.5532496571540833, 'learning_rate': 4.334135709900919e-05, 'epoch': 7.2727272727272725}\n{'loss': 0.0488, 'grad_norm': 0.5716313719749451, 'learning_rate': 4.2027982641463454e-05, 'epoch': 7.295454545454546}\n{'loss': 0.0273, 'grad_norm': 0.5823782086372375, 'learning_rate': 4.071460818391772e-05, 'epoch': 7.318181818181818}\n{'loss': 0.0366, 'grad_norm': 0.6306896209716797, 'learning_rate': 3.9401233726371994e-05, 'epoch': 7.340909090909091}\n{'loss': 0.0427, 'grad_norm': 0.52669757604599, 'learning_rate': 3.8087859268826254e-05, 'epoch': 7.363636363636363}\n{'loss': 0.0495, 'grad_norm': 0.552137553691864, 'learning_rate': 3.677448481128052e-05, 'epoch': 7.386363636363637}\n{'loss': 0.0367, 'grad_norm': 0.40202975273132324, 'learning_rate': 3.546111035373479e-05, 'epoch': 7.409090909090909}\n{'loss': 0.0442, 'grad_norm': 0.5254996418952942, 'learning_rate': 3.4147735896189055e-05, 'epoch': 7.431818181818182}\n{'loss': 0.0283, 'grad_norm': 0.5852202773094177, 'learning_rate': 3.283436143864333e-05, 'epoch': 7.454545454545454}\n{'loss': 0.0393, 'grad_norm': 0.9215177297592163, 'learning_rate': 3.1520986981097595e-05, 'epoch': 7.4772727272727275}\n{'loss': 0.0385, 'grad_norm': 0.7344589829444885, 'learning_rate': 3.020761252355186e-05, 'epoch': 7.5}\n{'loss': 0.0338, 'grad_norm': 0.5979115962982178, 'learning_rate': 2.8894238066006122e-05, 'epoch': 7.5227272727272725}\n{'loss': 0.0443, 'grad_norm': 0.44610080122947693, 'learning_rate': 2.758086360846039e-05, 'epoch': 7.545454545454545}\n{'loss': 0.0453, 'grad_norm': 0.8095473647117615, 'learning_rate': 2.6267489150914663e-05, 'epoch': 7.568181818181818}\n{'loss': 0.042, 'grad_norm': 1.207905650138855, 'learning_rate': 2.4954114693368926e-05, 'epoch': 7.590909090909091}\n{'loss': 0.0482, 'grad_norm': 0.49465519189834595, 'learning_rate': 2.3640740235823193e-05, 'epoch': 7.613636363636363}\n{'loss': 0.0333, 'grad_norm': 0.5435630083084106, 'learning_rate': 2.232736577827746e-05, 'epoch': 7.636363636363637}\n{'loss': 0.0503, 'grad_norm': 0.942737877368927, 'learning_rate': 2.1013991320731727e-05, 'epoch': 7.659090909090909}\n{'loss': 0.032, 'grad_norm': 0.488313227891922, 'learning_rate': 1.9700616863185997e-05, 'epoch': 7.681818181818182}\n{'loss': 0.024, 'grad_norm': 0.5443795323371887, 'learning_rate': 1.838724240564026e-05, 'epoch': 7.704545454545455}\n{'loss': 0.0387, 'grad_norm': 0.9746381640434265, 'learning_rate': 1.7073867948094528e-05, 'epoch': 7.7272727272727275}\n{'loss': 0.0371, 'grad_norm': 0.6281225681304932, 'learning_rate': 1.5760493490548798e-05, 'epoch': 7.75}\n{'loss': 0.031, 'grad_norm': 0.5608714818954468, 'learning_rate': 1.4447119033003061e-05, 'epoch': 7.7727272727272725}\n{'loss': 0.0271, 'grad_norm': 0.5869696140289307, 'learning_rate': 1.3133744575457331e-05, 'epoch': 7.795454545454545}\n{'loss': 0.0241, 'grad_norm': 0.4824102222919464, 'learning_rate': 1.1820370117911597e-05, 'epoch': 7.818181818181818}\n{'loss': 0.0355, 'grad_norm': 1.1080302000045776, 'learning_rate': 1.0506995660365863e-05, 'epoch': 7.840909090909091}\n{'loss': 0.0296, 'grad_norm': 0.5397768616676331, 'learning_rate': 9.19362120282013e-06, 'epoch': 7.863636363636363}\n{'loss': 0.0394, 'grad_norm': 0.779248833656311, 'learning_rate': 7.880246745274399e-06, 'epoch': 7.886363636363637}\n{'loss': 0.0311, 'grad_norm': 0.45201271772384644, 'learning_rate': 6.566872287728666e-06, 'epoch': 7.909090909090909}\n{'loss': 0.0435, 'grad_norm': 0.7204669713973999, 'learning_rate': 5.253497830182932e-06, 'epoch': 7.931818181818182}\n{'loss': 0.0379, 'grad_norm': 0.7736884355545044, 'learning_rate': 3.9401233726371994e-06, 'epoch': 7.954545454545455}\n{'loss': 0.0289, 'grad_norm': 0.8195141553878784, 'learning_rate': 2.626748915091466e-06, 'epoch': 7.9772727272727275}\n{'loss': 0.0145, 'grad_norm': 1.2206294536590576, 'learning_rate': 1.313374457545733e-06, 'epoch': 8.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8979    0.8857    0.8918      3456\n     PERCENT     0.8621    0.9740    0.9146        77\n        TYPE     0.9494    0.9630    0.9562     11282\n      VOLUME     0.9231    0.8780    0.9000        41\n\n   micro avg     0.9371    0.9449    0.9410     14856\n   macro avg     0.9081    0.9252    0.9156     14856\nweighted avg     0.9369    0.9449    0.9408     14856\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.3142237961292267, 'eval_f1_macro': 0.9156443847775696, 'eval_precision': 0.9371119567394353, 'eval_recall': 0.9448707592891761, 'eval_f1': 0.9409753645047763, 'eval_accuracy': 0.9315165361183638, 'eval_runtime': 2.0435, 'eval_samples_per_second': 2696.321, 'eval_steps_per_second': 5.383, 'epoch': 8.0}\n{'train_runtime': 53.0252, 'train_samples_per_second': 3325.51, 'train_steps_per_second': 6.638, 'train_loss': 0.2270089143461188, 'epoch': 8.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nThe speedups for torchdynamo mostly come with GPU Ampere or higher and which is not detected here.\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.9052    0.8811    0.8930      3456\n     PERCENT     0.8621    0.9740    0.9146        77\n        TYPE     0.9475    0.9668    0.9571     11282\n      VOLUME     0.9231    0.8780    0.9000        41\n\n   micro avg     0.9374    0.9467    0.9420     14856\n   macro avg     0.9094    0.9250    0.9162     14856\nweighted avg     0.9371    0.9467    0.9418     14856\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.29588502645492554, 'eval_f1_macro': 0.9161620020646891, 'eval_precision': 0.9374125174965007, 'eval_recall': 0.9466882067851373, 'eval_f1': 0.9420275293881243, 'eval_accuracy': 0.9326588337684943, 'eval_runtime': 1.5776, 'eval_samples_per_second': 3492.612, 'eval_steps_per_second': 6.973, 'epoch': 8.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/3364797558.py:66: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 2.1596, 'grad_norm': 7.335656642913818, 'learning_rate': 0.0, 'epoch': 0.022727272727272728}\n{'loss': 2.167, 'grad_norm': 7.082701206207275, 'learning_rate': 1.1528509127345877e-05, 'epoch': 0.045454545454545456}\n{'loss': 2.1391, 'grad_norm': 6.846900463104248, 'learning_rate': 2.3057018254691754e-05, 'epoch': 0.06818181818181818}\n{'loss': 2.0886, 'grad_norm': 7.1180901527404785, 'learning_rate': 3.458552738203763e-05, 'epoch': 0.09090909090909091}\n{'loss': 2.0099, 'grad_norm': 6.961860179901123, 'learning_rate': 4.611403650938351e-05, 'epoch': 0.11363636363636363}\n{'loss': 1.9088, 'grad_norm': 6.605220317840576, 'learning_rate': 5.76425456367294e-05, 'epoch': 0.13636363636363635}\n{'loss': 1.8117, 'grad_norm': 5.935586452484131, 'learning_rate': 6.917105476407527e-05, 'epoch': 0.1590909090909091}\n{'loss': 1.6901, 'grad_norm': 5.597349643707275, 'learning_rate': 8.069956389142115e-05, 'epoch': 0.18181818181818182}\n{'loss': 1.5351, 'grad_norm': 5.0380167961120605, 'learning_rate': 9.222807301876702e-05, 'epoch': 0.20454545454545456}\n{'loss': 1.4625, 'grad_norm': 3.8757619857788086, 'learning_rate': 0.0001037565821461129, 'epoch': 0.22727272727272727}\n{'loss': 1.3069, 'grad_norm': 3.0041470527648926, 'learning_rate': 0.0001152850912734588, 'epoch': 0.25}\n{'loss': 1.3007, 'grad_norm': 2.278554916381836, 'learning_rate': 0.00012681360040080468, 'epoch': 0.2727272727272727}\n{'loss': 1.2081, 'grad_norm': 2.096656084060669, 'learning_rate': 0.00013834210952815053, 'epoch': 0.29545454545454547}\n{'loss': 1.1968, 'grad_norm': 2.691216468811035, 'learning_rate': 0.0001498706186554964, 'epoch': 0.3181818181818182}\n{'loss': 1.234, 'grad_norm': 2.6602935791015625, 'learning_rate': 0.0001613991277828423, 'epoch': 0.3409090909090909}\n{'loss': 1.0448, 'grad_norm': 1.9724178314208984, 'learning_rate': 0.00017292763691018818, 'epoch': 0.36363636363636365}\n{'loss': 1.019, 'grad_norm': 1.28628408908844, 'learning_rate': 0.00018445614603753403, 'epoch': 0.38636363636363635}\n{'loss': 1.0112, 'grad_norm': 1.774106502532959, 'learning_rate': 0.00019598465516487993, 'epoch': 0.4090909090909091}\n{'loss': 1.0175, 'grad_norm': 2.480654716491699, 'learning_rate': 0.0002075131642922258, 'epoch': 0.4318181818181818}\n{'loss': 0.9521, 'grad_norm': 2.882650136947632, 'learning_rate': 0.00021904167341957169, 'epoch': 0.45454545454545453}\n{'loss': 0.9169, 'grad_norm': 1.9044400453567505, 'learning_rate': 0.0002305701825469176, 'epoch': 0.4772727272727273}\n{'loss': 0.8794, 'grad_norm': 1.2119276523590088, 'learning_rate': 0.00024209869167426346, 'epoch': 0.5}\n{'loss': 0.8385, 'grad_norm': 1.221640706062317, 'learning_rate': 0.00025362720080160937, 'epoch': 0.5227272727272727}\n{'loss': 0.801, 'grad_norm': 1.3026994466781616, 'learning_rate': 0.0002651557099289552, 'epoch': 0.5454545454545454}\n{'loss': 0.761, 'grad_norm': 1.4445589780807495, 'learning_rate': 0.00027668421905630106, 'epoch': 0.5681818181818182}\n{'loss': 0.7176, 'grad_norm': 0.7873523235321045, 'learning_rate': 0.00028821272818364694, 'epoch': 0.5909090909090909}\n{'loss': 0.7125, 'grad_norm': 1.5840638875961304, 'learning_rate': 0.0002997412373109928, 'epoch': 0.6136363636363636}\n{'loss': 0.6662, 'grad_norm': 1.5685783624649048, 'learning_rate': 0.00031126974643833874, 'epoch': 0.6363636363636364}\n{'loss': 0.6804, 'grad_norm': 0.9559311270713806, 'learning_rate': 0.0003227982555656846, 'epoch': 0.6590909090909091}\n{'loss': 0.6066, 'grad_norm': 1.0290366411209106, 'learning_rate': 0.0003343267646930305, 'epoch': 0.6818181818181818}\n{'loss': 0.5804, 'grad_norm': 0.9713935256004333, 'learning_rate': 0.00034585527382037637, 'epoch': 0.7045454545454546}\n{'loss': 0.5439, 'grad_norm': 0.8501381874084473, 'learning_rate': 0.00035738378294772224, 'epoch': 0.7272727272727273}\n{'loss': 0.5206, 'grad_norm': 1.0164210796356201, 'learning_rate': 0.00036891229207506806, 'epoch': 0.75}\n{'loss': 0.5092, 'grad_norm': 1.409511685371399, 'learning_rate': 0.000380440801202414, 'epoch': 0.7727272727272727}\n{'loss': 0.5353, 'grad_norm': 0.8474075794219971, 'learning_rate': 0.00039196931032975987, 'epoch': 0.7954545454545454}\n{'loss': 0.4687, 'grad_norm': 1.2693020105361938, 'learning_rate': 0.00040349781945710574, 'epoch': 0.8181818181818182}\n{'loss': 0.5245, 'grad_norm': 0.7679874897003174, 'learning_rate': 0.0004150263285844516, 'epoch': 0.8409090909090909}\n{'loss': 0.4522, 'grad_norm': 1.35274338722229, 'learning_rate': 0.0004137129541269059, 'epoch': 0.8636363636363636}\n{'loss': 0.4864, 'grad_norm': 0.8039836287498474, 'learning_rate': 0.0004123995796693601, 'epoch': 0.8863636363636364}\n{'loss': 0.4621, 'grad_norm': 1.3606711626052856, 'learning_rate': 0.0004110862052118144, 'epoch': 0.9090909090909091}\n{'loss': 0.4069, 'grad_norm': 1.2409417629241943, 'learning_rate': 0.0004097728307542687, 'epoch': 0.9318181818181818}\n{'loss': 0.3962, 'grad_norm': 1.0865172147750854, 'learning_rate': 0.00040845945629672297, 'epoch': 0.9545454545454546}\n{'loss': 0.4557, 'grad_norm': 0.7984904646873474, 'learning_rate': 0.00040714608183917725, 'epoch': 0.9772727272727273}\n{'loss': 0.3908, 'grad_norm': 2.2524938583374023, 'learning_rate': 0.0004058327073816315, 'epoch': 1.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8210    0.7603    0.7895      3221\n     PERCENT     0.4880    0.9310    0.6403        87\n        TYPE     0.9084    0.9533    0.9303     11501\n      VOLUME     0.3333    0.1186    0.1750        59\n\n   micro avg     0.8859    0.9081    0.8968     14868\n   macro avg     0.6377    0.6908    0.6338     14868\nweighted avg     0.8847    0.9081    0.8951     14868\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.3545272946357727, 'eval_f1_macro': 0.6337756397681198, 'eval_precision': 0.8858923884514436, 'eval_recall': 0.9080575733118106, 'eval_f1': 0.8968380496877907, 'eval_accuracy': 0.8931090613130765, 'eval_runtime': 1.5126, 'eval_samples_per_second': 3642.671, 'eval_steps_per_second': 7.272, 'epoch': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.3733, 'grad_norm': 0.9944977760314941, 'learning_rate': 0.00040451933292408575, 'epoch': 1.0227272727272727}\n{'loss': 0.4261, 'grad_norm': 1.3559318780899048, 'learning_rate': 0.00040320595846654, 'epoch': 1.0454545454545454}\n{'loss': 0.3242, 'grad_norm': 0.7914711236953735, 'learning_rate': 0.0004018925840089943, 'epoch': 1.0681818181818181}\n{'loss': 0.4008, 'grad_norm': 2.0076141357421875, 'learning_rate': 0.0004005792095514486, 'epoch': 1.0909090909090908}\n{'loss': 0.3915, 'grad_norm': 0.7978798151016235, 'learning_rate': 0.0003992658350939028, 'epoch': 1.1136363636363635}\n{'loss': 0.3426, 'grad_norm': 0.8132458329200745, 'learning_rate': 0.0003979524606363571, 'epoch': 1.1363636363636362}\n{'loss': 0.3402, 'grad_norm': 1.2143852710723877, 'learning_rate': 0.00039663908617881133, 'epoch': 1.1590909090909092}\n{'loss': 0.3467, 'grad_norm': 1.1121386289596558, 'learning_rate': 0.0003953257117212656, 'epoch': 1.1818181818181819}\n{'loss': 0.3433, 'grad_norm': 1.7047982215881348, 'learning_rate': 0.0003940123372637199, 'epoch': 1.2045454545454546}\n{'loss': 0.3133, 'grad_norm': 2.157705068588257, 'learning_rate': 0.00039269896280617417, 'epoch': 1.2272727272727273}\n{'loss': 0.3458, 'grad_norm': 1.7971060276031494, 'learning_rate': 0.00039138558834862845, 'epoch': 1.25}\n{'loss': 0.3502, 'grad_norm': 1.3238329887390137, 'learning_rate': 0.0003900722138910827, 'epoch': 1.2727272727272727}\n{'loss': 0.2922, 'grad_norm': 1.7756258249282837, 'learning_rate': 0.00038875883943353696, 'epoch': 1.2954545454545454}\n{'loss': 0.3581, 'grad_norm': 1.66962730884552, 'learning_rate': 0.00038744546497599124, 'epoch': 1.3181818181818181}\n{'loss': 0.2804, 'grad_norm': 1.1428217887878418, 'learning_rate': 0.00038613209051844546, 'epoch': 1.3409090909090908}\n{'loss': 0.3395, 'grad_norm': 1.9894416332244873, 'learning_rate': 0.00038481871606089974, 'epoch': 1.3636363636363638}\n{'loss': 0.2922, 'grad_norm': 2.490238666534424, 'learning_rate': 0.000383505341603354, 'epoch': 1.3863636363636362}\n{'loss': 0.291, 'grad_norm': 2.1153945922851562, 'learning_rate': 0.0003821919671458083, 'epoch': 1.4090909090909092}\n{'loss': 0.2706, 'grad_norm': 0.6937937140464783, 'learning_rate': 0.0003808785926882626, 'epoch': 1.4318181818181819}\n{'loss': 0.2734, 'grad_norm': 1.4717063903808594, 'learning_rate': 0.0003795652182307168, 'epoch': 1.4545454545454546}\n{'loss': 0.2478, 'grad_norm': 0.7634606957435608, 'learning_rate': 0.0003782518437731711, 'epoch': 1.4772727272727273}\n{'loss': 0.2971, 'grad_norm': 1.9512306451797485, 'learning_rate': 0.0003769384693156253, 'epoch': 1.5}\n{'loss': 0.3029, 'grad_norm': 1.6957272291183472, 'learning_rate': 0.00037562509485807965, 'epoch': 1.5227272727272727}\n{'loss': 0.3161, 'grad_norm': 0.8147920966148376, 'learning_rate': 0.00037431172040053393, 'epoch': 1.5454545454545454}\n{'loss': 0.2759, 'grad_norm': 1.226904034614563, 'learning_rate': 0.00037299834594298816, 'epoch': 1.5681818181818183}\n{'loss': 0.3232, 'grad_norm': 0.8773705363273621, 'learning_rate': 0.00037168497148544244, 'epoch': 1.5909090909090908}\n{'loss': 0.2851, 'grad_norm': 1.3975718021392822, 'learning_rate': 0.00037037159702789667, 'epoch': 1.6136363636363638}\n{'loss': 0.3035, 'grad_norm': 1.1346889734268188, 'learning_rate': 0.00036905822257035095, 'epoch': 1.6363636363636362}\n{'loss': 0.3226, 'grad_norm': 1.0532525777816772, 'learning_rate': 0.0003677448481128052, 'epoch': 1.6590909090909092}\n{'loss': 0.2866, 'grad_norm': 0.7911175489425659, 'learning_rate': 0.0003664314736552595, 'epoch': 1.6818181818181817}\n{'loss': 0.2953, 'grad_norm': 0.8963973522186279, 'learning_rate': 0.0003651180991977138, 'epoch': 1.7045454545454546}\n{'loss': 0.2456, 'grad_norm': 0.7999047636985779, 'learning_rate': 0.000363804724740168, 'epoch': 1.7272727272727273}\n{'loss': 0.2672, 'grad_norm': 1.1760427951812744, 'learning_rate': 0.0003624913502826223, 'epoch': 1.75}\n{'loss': 0.274, 'grad_norm': 1.16007399559021, 'learning_rate': 0.0003611779758250766, 'epoch': 1.7727272727272727}\n{'loss': 0.2175, 'grad_norm': 0.9126935601234436, 'learning_rate': 0.0003598646013675308, 'epoch': 1.7954545454545454}\n{'loss': 0.2807, 'grad_norm': 0.7243829369544983, 'learning_rate': 0.00035855122690998514, 'epoch': 1.8181818181818183}\n{'loss': 0.2735, 'grad_norm': 0.7600544691085815, 'learning_rate': 0.00035723785245243936, 'epoch': 1.8409090909090908}\n{'loss': 0.2768, 'grad_norm': 1.003293514251709, 'learning_rate': 0.00035592447799489364, 'epoch': 1.8636363636363638}\n{'loss': 0.2476, 'grad_norm': 0.841203510761261, 'learning_rate': 0.0003546111035373479, 'epoch': 1.8863636363636362}\n{'loss': 0.287, 'grad_norm': 0.745930016040802, 'learning_rate': 0.00035329772907980215, 'epoch': 1.9090909090909092}\n{'loss': 0.2462, 'grad_norm': 1.0922507047653198, 'learning_rate': 0.00035198435462225643, 'epoch': 1.9318181818181817}\n{'loss': 0.2867, 'grad_norm': 0.8983189463615417, 'learning_rate': 0.0003506709801647107, 'epoch': 1.9545454545454546}\n{'loss': 0.2825, 'grad_norm': 0.9317876696586609, 'learning_rate': 0.000349357605707165, 'epoch': 1.9772727272727273}\n{'loss': 0.1187, 'grad_norm': 1.8946744203567505, 'learning_rate': 0.00034804423124961927, 'epoch': 2.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8422    0.8864    0.8637      3221\n     PERCENT     0.8247    0.9195    0.8696        87\n        TYPE     0.9459    0.9645    0.9551     11501\n      VOLUME     0.8000    0.7458    0.7719        59\n\n   micro avg     0.9215    0.9465    0.9338     14868\n   macro avg     0.8532    0.8790    0.8651     14868\nweighted avg     0.9221    0.9465    0.9341     14868\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.23636268079280853, 'eval_f1_macro': 0.8650765666309503, 'eval_precision': 0.9215455140798953, 'eval_recall': 0.9464622006994888, 'eval_f1': 0.9338376800053089, 'eval_accuracy': 0.9287574606619642, 'eval_runtime': 1.5659, 'eval_samples_per_second': 3518.727, 'eval_steps_per_second': 7.025, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.2448, 'grad_norm': 0.6445263028144836, 'learning_rate': 0.0003467308567920735, 'epoch': 2.022727272727273}\n{'loss': 0.2067, 'grad_norm': 1.5586999654769897, 'learning_rate': 0.0003454174823345278, 'epoch': 2.0454545454545454}\n{'loss': 0.2066, 'grad_norm': 1.044153094291687, 'learning_rate': 0.000344104107876982, 'epoch': 2.0681818181818183}\n{'loss': 0.1826, 'grad_norm': 0.768120288848877, 'learning_rate': 0.0003427907334194363, 'epoch': 2.090909090909091}\n{'loss': 0.2005, 'grad_norm': 1.2298537492752075, 'learning_rate': 0.0003414773589618906, 'epoch': 2.1136363636363638}\n{'loss': 0.1786, 'grad_norm': 1.1636874675750732, 'learning_rate': 0.00034016398450434484, 'epoch': 2.1363636363636362}\n{'loss': 0.1752, 'grad_norm': 1.8220125436782837, 'learning_rate': 0.0003388506100467991, 'epoch': 2.159090909090909}\n{'loss': 0.1752, 'grad_norm': 1.007872223854065, 'learning_rate': 0.00033753723558925335, 'epoch': 2.1818181818181817}\n{'loss': 0.2383, 'grad_norm': 1.3055540323257446, 'learning_rate': 0.00033622386113170763, 'epoch': 2.2045454545454546}\n{'loss': 0.1453, 'grad_norm': 2.552206039428711, 'learning_rate': 0.0003349104866741619, 'epoch': 2.227272727272727}\n{'loss': 0.1912, 'grad_norm': 1.7481751441955566, 'learning_rate': 0.00033359711221661614, 'epoch': 2.25}\n{'loss': 0.1673, 'grad_norm': 1.6433866024017334, 'learning_rate': 0.00033228373775907047, 'epoch': 2.2727272727272725}\n{'loss': 0.148, 'grad_norm': 0.748187780380249, 'learning_rate': 0.0003309703633015247, 'epoch': 2.2954545454545454}\n{'loss': 0.1615, 'grad_norm': 1.8348896503448486, 'learning_rate': 0.000329656988843979, 'epoch': 2.3181818181818183}\n{'loss': 0.1732, 'grad_norm': 2.166715383529663, 'learning_rate': 0.00032834361438643326, 'epoch': 2.340909090909091}\n{'loss': 0.2181, 'grad_norm': 2.5057098865509033, 'learning_rate': 0.0003270302399288875, 'epoch': 2.3636363636363638}\n{'loss': 0.2246, 'grad_norm': 2.8007166385650635, 'learning_rate': 0.00032571686547134177, 'epoch': 2.3863636363636362}\n{'loss': 0.1768, 'grad_norm': 0.6801794767379761, 'learning_rate': 0.00032440349101379605, 'epoch': 2.409090909090909}\n{'loss': 0.1572, 'grad_norm': 1.3835821151733398, 'learning_rate': 0.0003230901165562503, 'epoch': 2.4318181818181817}\n{'loss': 0.1819, 'grad_norm': 1.6308423280715942, 'learning_rate': 0.0003217767420987046, 'epoch': 2.4545454545454546}\n{'loss': 0.1757, 'grad_norm': 2.3518459796905518, 'learning_rate': 0.00032046336764115883, 'epoch': 2.4772727272727275}\n{'loss': 0.182, 'grad_norm': 1.6773613691329956, 'learning_rate': 0.0003191499931836131, 'epoch': 2.5}\n{'loss': 0.1971, 'grad_norm': 0.8233537077903748, 'learning_rate': 0.0003178366187260674, 'epoch': 2.5227272727272725}\n{'loss': 0.1484, 'grad_norm': 1.06449556350708, 'learning_rate': 0.0003165232442685216, 'epoch': 2.5454545454545454}\n{'loss': 0.1443, 'grad_norm': 0.7334436774253845, 'learning_rate': 0.00031520986981097595, 'epoch': 2.5681818181818183}\n{'loss': 0.1766, 'grad_norm': 0.680156946182251, 'learning_rate': 0.0003138964953534302, 'epoch': 2.590909090909091}\n{'loss': 0.1678, 'grad_norm': 0.65314120054245, 'learning_rate': 0.00031258312089588446, 'epoch': 2.6136363636363638}\n{'loss': 0.2056, 'grad_norm': 0.9489151835441589, 'learning_rate': 0.00031126974643833874, 'epoch': 2.6363636363636362}\n{'loss': 0.1681, 'grad_norm': 0.9721243977546692, 'learning_rate': 0.00030995637198079297, 'epoch': 2.659090909090909}\n{'loss': 0.2379, 'grad_norm': 1.0687724351882935, 'learning_rate': 0.00030864299752324725, 'epoch': 2.6818181818181817}\n{'loss': 0.1523, 'grad_norm': 0.7588598132133484, 'learning_rate': 0.0003073296230657015, 'epoch': 2.7045454545454546}\n{'loss': 0.1721, 'grad_norm': 0.8181570768356323, 'learning_rate': 0.0003060162486081558, 'epoch': 2.7272727272727275}\n{'loss': 0.2041, 'grad_norm': 1.5148664712905884, 'learning_rate': 0.00030470287415061004, 'epoch': 2.75}\n{'loss': 0.1512, 'grad_norm': 1.1054009199142456, 'learning_rate': 0.0003033894996930643, 'epoch': 2.7727272727272725}\n{'loss': 0.1945, 'grad_norm': 1.0299885272979736, 'learning_rate': 0.0003020761252355186, 'epoch': 2.7954545454545454}\n{'loss': 0.182, 'grad_norm': 1.246712565422058, 'learning_rate': 0.0003007627507779728, 'epoch': 2.8181818181818183}\n{'loss': 0.1935, 'grad_norm': 1.2115252017974854, 'learning_rate': 0.0002994493763204271, 'epoch': 2.840909090909091}\n{'loss': 0.139, 'grad_norm': 1.0703752040863037, 'learning_rate': 0.0002981360018628814, 'epoch': 2.8636363636363638}\n{'loss': 0.1481, 'grad_norm': 1.3306515216827393, 'learning_rate': 0.00029682262740533566, 'epoch': 2.8863636363636362}\n{'loss': 0.1579, 'grad_norm': 0.9556136131286621, 'learning_rate': 0.00029550925294778994, 'epoch': 2.909090909090909}\n{'loss': 0.1622, 'grad_norm': 1.5882021188735962, 'learning_rate': 0.00029419587849024417, 'epoch': 2.9318181818181817}\n{'loss': 0.1749, 'grad_norm': 1.1420435905456543, 'learning_rate': 0.00029288250403269845, 'epoch': 2.9545454545454546}\n{'loss': 0.1644, 'grad_norm': 0.9710808396339417, 'learning_rate': 0.00029156912957515273, 'epoch': 2.9772727272727275}\n{'loss': 0.0641, 'grad_norm': 1.1107617616653442, 'learning_rate': 0.00029025575511760696, 'epoch': 3.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8719    0.8957    0.8836      3221\n     PERCENT     0.8247    0.9195    0.8696        87\n        TYPE     0.9511    0.9646    0.9578     11501\n      VOLUME     0.8772    0.8475    0.8621        59\n\n   micro avg     0.9327    0.9490    0.9408     14868\n   macro avg     0.8812    0.9068    0.8933     14868\nweighted avg     0.9329    0.9490    0.9409     14868\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.2156921774148941, 'eval_f1_macro': 0.8932681439090322, 'eval_precision': 0.9327031136378661, 'eval_recall': 0.9489507667473769, 'eval_f1': 0.9407567927987999, 'eval_accuracy': 0.9356483993488877, 'eval_runtime': 1.5704, 'eval_samples_per_second': 3508.558, 'eval_steps_per_second': 7.004, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.1388, 'grad_norm': 0.7167983055114746, 'learning_rate': 0.0002889423806600613, 'epoch': 3.022727272727273}\n{'loss': 0.1287, 'grad_norm': 0.5668429732322693, 'learning_rate': 0.0002876290062025155, 'epoch': 3.0454545454545454}\n{'loss': 0.1495, 'grad_norm': 1.854488730430603, 'learning_rate': 0.0002863156317449698, 'epoch': 3.0681818181818183}\n{'loss': 0.1365, 'grad_norm': 1.2913674116134644, 'learning_rate': 0.0002850022572874241, 'epoch': 3.090909090909091}\n{'loss': 0.1161, 'grad_norm': 1.0549582242965698, 'learning_rate': 0.0002836888828298783, 'epoch': 3.1136363636363638}\n{'loss': 0.106, 'grad_norm': 0.7807808518409729, 'learning_rate': 0.0002823755083723326, 'epoch': 3.1363636363636362}\n{'loss': 0.1347, 'grad_norm': 1.6280131340026855, 'learning_rate': 0.0002810621339147868, 'epoch': 3.159090909090909}\n{'loss': 0.1453, 'grad_norm': 1.6034945249557495, 'learning_rate': 0.00027974875945724115, 'epoch': 3.1818181818181817}\n{'loss': 0.1308, 'grad_norm': 1.4622191190719604, 'learning_rate': 0.0002784353849996954, 'epoch': 3.2045454545454546}\n{'loss': 0.1317, 'grad_norm': 0.9568418264389038, 'learning_rate': 0.00027712201054214965, 'epoch': 3.227272727272727}\n{'loss': 0.0892, 'grad_norm': 0.7178364396095276, 'learning_rate': 0.00027580863608460393, 'epoch': 3.25}\n{'loss': 0.0972, 'grad_norm': 1.2948225736618042, 'learning_rate': 0.00027449526162705816, 'epoch': 3.2727272727272725}\n{'loss': 0.1211, 'grad_norm': 1.8437572717666626, 'learning_rate': 0.00027318188716951244, 'epoch': 3.2954545454545454}\n{'loss': 0.114, 'grad_norm': 1.4959275722503662, 'learning_rate': 0.0002718685127119667, 'epoch': 3.3181818181818183}\n{'loss': 0.0921, 'grad_norm': 0.6830422282218933, 'learning_rate': 0.000270555138254421, 'epoch': 3.340909090909091}\n{'loss': 0.1083, 'grad_norm': 1.037666916847229, 'learning_rate': 0.0002692417637968753, 'epoch': 3.3636363636363638}\n{'loss': 0.1118, 'grad_norm': 1.135189414024353, 'learning_rate': 0.0002679283893393295, 'epoch': 3.3863636363636362}\n{'loss': 0.0838, 'grad_norm': 0.5561438202857971, 'learning_rate': 0.0002666150148817838, 'epoch': 3.409090909090909}\n{'loss': 0.1037, 'grad_norm': 1.2137738466262817, 'learning_rate': 0.00026530164042423807, 'epoch': 3.4318181818181817}\n{'loss': 0.1301, 'grad_norm': 1.2424646615982056, 'learning_rate': 0.0002639882659666923, 'epoch': 3.4545454545454546}\n{'loss': 0.1123, 'grad_norm': 0.9391562938690186, 'learning_rate': 0.00026267489150914663, 'epoch': 3.4772727272727275}\n{'loss': 0.096, 'grad_norm': 1.1363941431045532, 'learning_rate': 0.00026136151705160086, 'epoch': 3.5}\n{'loss': 0.135, 'grad_norm': 1.359729528427124, 'learning_rate': 0.00026004814259405514, 'epoch': 3.5227272727272725}\n{'loss': 0.1392, 'grad_norm': 1.2725943326950073, 'learning_rate': 0.0002587347681365094, 'epoch': 3.5454545454545454}\n{'loss': 0.0895, 'grad_norm': 1.2554731369018555, 'learning_rate': 0.00025742139367896364, 'epoch': 3.5681818181818183}\n{'loss': 0.1147, 'grad_norm': 1.2531412839889526, 'learning_rate': 0.0002561080192214179, 'epoch': 3.590909090909091}\n{'loss': 0.151, 'grad_norm': 0.9935856461524963, 'learning_rate': 0.0002547946447638722, 'epoch': 3.6136363636363638}\n{'loss': 0.103, 'grad_norm': 0.7866517305374146, 'learning_rate': 0.0002534812703063265, 'epoch': 3.6363636363636362}\n{'loss': 0.1132, 'grad_norm': 1.536983609199524, 'learning_rate': 0.00025216789584878076, 'epoch': 3.659090909090909}\n{'loss': 0.114, 'grad_norm': 1.3095976114273071, 'learning_rate': 0.000250854521391235, 'epoch': 3.6818181818181817}\n{'loss': 0.1005, 'grad_norm': 1.1121325492858887, 'learning_rate': 0.00024954114693368927, 'epoch': 3.7045454545454546}\n{'loss': 0.1241, 'grad_norm': 0.7810441255569458, 'learning_rate': 0.0002482277724761435, 'epoch': 3.7272727272727275}\n{'loss': 0.1256, 'grad_norm': 0.788444459438324, 'learning_rate': 0.0002469143980185978, 'epoch': 3.75}\n{'loss': 0.0778, 'grad_norm': 0.6034301519393921, 'learning_rate': 0.0002456010235610521, 'epoch': 3.7727272727272725}\n{'loss': 0.1105, 'grad_norm': 0.9760582447052002, 'learning_rate': 0.00024428764910350634, 'epoch': 3.7954545454545454}\n{'loss': 0.1327, 'grad_norm': 0.847750723361969, 'learning_rate': 0.00024297427464596062, 'epoch': 3.8181818181818183}\n{'loss': 0.1062, 'grad_norm': 1.5396461486816406, 'learning_rate': 0.00024166090018841487, 'epoch': 3.840909090909091}\n{'loss': 0.1103, 'grad_norm': 1.1452425718307495, 'learning_rate': 0.00024034752573086912, 'epoch': 3.8636363636363638}\n{'loss': 0.1284, 'grad_norm': 1.621277928352356, 'learning_rate': 0.00023903415127332338, 'epoch': 3.8863636363636362}\n{'loss': 0.13, 'grad_norm': 1.9004520177841187, 'learning_rate': 0.00023772077681577769, 'epoch': 3.909090909090909}\n{'loss': 0.1529, 'grad_norm': 1.356526255607605, 'learning_rate': 0.00023640740235823194, 'epoch': 3.9318181818181817}\n{'loss': 0.11, 'grad_norm': 1.3156626224517822, 'learning_rate': 0.00023509402790068622, 'epoch': 3.9545454545454546}\n{'loss': 0.1264, 'grad_norm': 0.7203791737556458, 'learning_rate': 0.00023378065344314047, 'epoch': 3.9772727272727275}\n{'loss': 0.1135, 'grad_norm': 2.501718282699585, 'learning_rate': 0.00023246727898559473, 'epoch': 4.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8875    0.8916    0.8896      3221\n     PERCENT     0.8889    0.9195    0.9040        87\n        TYPE     0.9464    0.9695    0.9578     11501\n      VOLUME     0.8871    0.9322    0.9091        59\n\n   micro avg     0.9333    0.9522    0.9426     14868\n   macro avg     0.9025    0.9282    0.9151     14868\nweighted avg     0.9331    0.9522    0.9425     14868\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.22561074793338776, 'eval_f1_macro': 0.9151111018462863, 'eval_precision': 0.9332849891225525, 'eval_recall': 0.9521791767554479, 'eval_f1': 0.942637413856244, 'eval_accuracy': 0.9372219207813348, 'eval_runtime': 1.7234, 'eval_samples_per_second': 3197.209, 'eval_steps_per_second': 6.383, 'epoch': 4.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.096, 'grad_norm': 1.711827278137207, 'learning_rate': 0.00023115390452804898, 'epoch': 4.0227272727272725}\n{'loss': 0.086, 'grad_norm': 0.9113327860832214, 'learning_rate': 0.00022984053007050326, 'epoch': 4.045454545454546}\n{'loss': 0.0686, 'grad_norm': 0.8646655082702637, 'learning_rate': 0.00022852715561295757, 'epoch': 4.068181818181818}\n{'loss': 0.0931, 'grad_norm': 1.1438002586364746, 'learning_rate': 0.00022721378115541182, 'epoch': 4.090909090909091}\n{'loss': 0.0873, 'grad_norm': 0.8697620034217834, 'learning_rate': 0.00022590040669786607, 'epoch': 4.113636363636363}\n{'loss': 0.0723, 'grad_norm': 0.6338090896606445, 'learning_rate': 0.00022458703224032033, 'epoch': 4.136363636363637}\n{'loss': 0.0607, 'grad_norm': 0.6997402906417847, 'learning_rate': 0.0002232736577827746, 'epoch': 4.159090909090909}\n{'loss': 0.0938, 'grad_norm': 1.27155339717865, 'learning_rate': 0.00022196028332522886, 'epoch': 4.181818181818182}\n{'loss': 0.0999, 'grad_norm': 1.457753300666809, 'learning_rate': 0.00022064690886768311, 'epoch': 4.204545454545454}\n{'loss': 0.0763, 'grad_norm': 0.9705806374549866, 'learning_rate': 0.00021933353441013742, 'epoch': 4.2272727272727275}\n{'loss': 0.0527, 'grad_norm': 0.5841372609138489, 'learning_rate': 0.00021802015995259168, 'epoch': 4.25}\n{'loss': 0.069, 'grad_norm': 0.7819960117340088, 'learning_rate': 0.00021670678549504596, 'epoch': 4.2727272727272725}\n{'loss': 0.0853, 'grad_norm': 0.8937573432922363, 'learning_rate': 0.0002153934110375002, 'epoch': 4.295454545454546}\n{'loss': 0.07, 'grad_norm': 0.9133487343788147, 'learning_rate': 0.00021408003657995446, 'epoch': 4.318181818181818}\n{'loss': 0.0765, 'grad_norm': 0.8607776165008545, 'learning_rate': 0.00021276666212240872, 'epoch': 4.340909090909091}\n{'loss': 0.0843, 'grad_norm': 1.1412663459777832, 'learning_rate': 0.00021145328766486302, 'epoch': 4.363636363636363}\n{'loss': 0.0722, 'grad_norm': 0.994027853012085, 'learning_rate': 0.0002101399132073173, 'epoch': 4.386363636363637}\n{'loss': 0.0619, 'grad_norm': 0.9791041612625122, 'learning_rate': 0.00020882653874977156, 'epoch': 4.409090909090909}\n{'loss': 0.0935, 'grad_norm': 0.7946260571479797, 'learning_rate': 0.0002075131642922258, 'epoch': 4.431818181818182}\n{'loss': 0.0921, 'grad_norm': 1.046582579612732, 'learning_rate': 0.00020619978983468006, 'epoch': 4.454545454545454}\n{'loss': 0.0934, 'grad_norm': 0.7342361807823181, 'learning_rate': 0.00020488641537713434, 'epoch': 4.4772727272727275}\n{'loss': 0.0633, 'grad_norm': 0.7058762311935425, 'learning_rate': 0.00020357304091958862, 'epoch': 4.5}\n{'loss': 0.0994, 'grad_norm': 1.4705700874328613, 'learning_rate': 0.00020225966646204288, 'epoch': 4.5227272727272725}\n{'loss': 0.1189, 'grad_norm': 1.079734444618225, 'learning_rate': 0.00020094629200449716, 'epoch': 4.545454545454545}\n{'loss': 0.0931, 'grad_norm': 0.7744104266166687, 'learning_rate': 0.0001996329175469514, 'epoch': 4.568181818181818}\n{'loss': 0.0875, 'grad_norm': 0.6675589680671692, 'learning_rate': 0.00019831954308940566, 'epoch': 4.590909090909091}\n{'loss': 0.0688, 'grad_norm': 0.6455979943275452, 'learning_rate': 0.00019700616863185994, 'epoch': 4.613636363636363}\n{'loss': 0.0767, 'grad_norm': 0.954918622970581, 'learning_rate': 0.00019569279417431423, 'epoch': 4.636363636363637}\n{'loss': 0.066, 'grad_norm': 0.9037192463874817, 'learning_rate': 0.00019437941971676848, 'epoch': 4.659090909090909}\n{'loss': 0.0982, 'grad_norm': 1.0387035608291626, 'learning_rate': 0.00019306604525922273, 'epoch': 4.681818181818182}\n{'loss': 0.0898, 'grad_norm': 0.7051665186882019, 'learning_rate': 0.000191752670801677, 'epoch': 4.704545454545455}\n{'loss': 0.0768, 'grad_norm': 1.288284182548523, 'learning_rate': 0.0001904392963441313, 'epoch': 4.7272727272727275}\n{'loss': 0.0865, 'grad_norm': 1.1400870084762573, 'learning_rate': 0.00018912592188658555, 'epoch': 4.75}\n{'loss': 0.0682, 'grad_norm': 0.8124681711196899, 'learning_rate': 0.00018781254742903983, 'epoch': 4.7727272727272725}\n{'loss': 0.093, 'grad_norm': 0.9346151947975159, 'learning_rate': 0.00018649917297149408, 'epoch': 4.795454545454545}\n{'loss': 0.0713, 'grad_norm': 0.698043942451477, 'learning_rate': 0.00018518579851394833, 'epoch': 4.818181818181818}\n{'loss': 0.0857, 'grad_norm': 0.8584858775138855, 'learning_rate': 0.0001838724240564026, 'epoch': 4.840909090909091}\n{'loss': 0.0611, 'grad_norm': 0.7931349277496338, 'learning_rate': 0.0001825590495988569, 'epoch': 4.863636363636363}\n{'loss': 0.096, 'grad_norm': 0.5681190490722656, 'learning_rate': 0.00018124567514131115, 'epoch': 4.886363636363637}\n{'loss': 0.0858, 'grad_norm': 0.8185907006263733, 'learning_rate': 0.0001799323006837654, 'epoch': 4.909090909090909}\n{'loss': 0.1097, 'grad_norm': 1.022411823272705, 'learning_rate': 0.00017861892622621968, 'epoch': 4.931818181818182}\n{'loss': 0.0842, 'grad_norm': 0.6701251268386841, 'learning_rate': 0.00017730555176867396, 'epoch': 4.954545454545455}\n{'loss': 0.1079, 'grad_norm': 1.1776487827301025, 'learning_rate': 0.00017599217731112821, 'epoch': 4.9772727272727275}\n{'loss': 0.0369, 'grad_norm': 1.9027787446975708, 'learning_rate': 0.0001746788028535825, 'epoch': 5.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8703    0.9128    0.8910      3221\n     PERCENT     0.8791    0.9195    0.8989        87\n        TYPE     0.9547    0.9588    0.9567     11501\n      VOLUME     0.9167    0.9322    0.9244        59\n\n   micro avg     0.9352    0.9485    0.9418     14868\n   macro avg     0.9052    0.9308    0.9178     14868\nweighted avg     0.9358    0.9485    0.9420     14868\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.22960156202316284, 'eval_f1_macro': 0.9177595825819179, 'eval_precision': 0.9352079050334903, 'eval_recall': 0.9484799569545332, 'eval_f1': 0.9417971750091828, 'eval_accuracy': 0.9370591427021161, 'eval_runtime': 1.6618, 'eval_samples_per_second': 3315.666, 'eval_steps_per_second': 6.619, 'epoch': 5.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.0473, 'grad_norm': 0.9502466320991516, 'learning_rate': 0.00017336542839603675, 'epoch': 5.0227272727272725}\n{'loss': 0.0682, 'grad_norm': 0.9227553009986877, 'learning_rate': 0.000172052053938491, 'epoch': 5.045454545454546}\n{'loss': 0.0711, 'grad_norm': 1.335221767425537, 'learning_rate': 0.0001707386794809453, 'epoch': 5.068181818181818}\n{'loss': 0.0893, 'grad_norm': 0.744440495967865, 'learning_rate': 0.00016942530502339956, 'epoch': 5.090909090909091}\n{'loss': 0.0755, 'grad_norm': 0.8420519232749939, 'learning_rate': 0.00016811193056585382, 'epoch': 5.113636363636363}\n{'loss': 0.0427, 'grad_norm': 0.45108529925346375, 'learning_rate': 0.00016679855610830807, 'epoch': 5.136363636363637}\n{'loss': 0.0464, 'grad_norm': 0.8307374119758606, 'learning_rate': 0.00016548518165076235, 'epoch': 5.159090909090909}\n{'loss': 0.0507, 'grad_norm': 1.1879574060440063, 'learning_rate': 0.00016417180719321663, 'epoch': 5.181818181818182}\n{'loss': 0.0512, 'grad_norm': 0.6451807618141174, 'learning_rate': 0.00016285843273567088, 'epoch': 5.204545454545454}\n{'loss': 0.0548, 'grad_norm': 1.4791632890701294, 'learning_rate': 0.00016154505827812516, 'epoch': 5.2272727272727275}\n{'loss': 0.0856, 'grad_norm': 1.1857279539108276, 'learning_rate': 0.00016023168382057942, 'epoch': 5.25}\n{'loss': 0.0428, 'grad_norm': 0.655566930770874, 'learning_rate': 0.0001589183093630337, 'epoch': 5.2727272727272725}\n{'loss': 0.0542, 'grad_norm': 0.8599584102630615, 'learning_rate': 0.00015760493490548798, 'epoch': 5.295454545454546}\n{'loss': 0.054, 'grad_norm': 0.911569356918335, 'learning_rate': 0.00015629156044794223, 'epoch': 5.318181818181818}\n{'loss': 0.055, 'grad_norm': 1.0472835302352905, 'learning_rate': 0.00015497818599039648, 'epoch': 5.340909090909091}\n{'loss': 0.0726, 'grad_norm': 1.6194219589233398, 'learning_rate': 0.00015366481153285074, 'epoch': 5.363636363636363}\n{'loss': 0.0603, 'grad_norm': 0.9085729122161865, 'learning_rate': 0.00015235143707530502, 'epoch': 5.386363636363637}\n{'loss': 0.0447, 'grad_norm': 0.5646746754646301, 'learning_rate': 0.0001510380626177593, 'epoch': 5.409090909090909}\n{'loss': 0.0453, 'grad_norm': 0.5477271676063538, 'learning_rate': 0.00014972468816021355, 'epoch': 5.431818181818182}\n{'loss': 0.0571, 'grad_norm': 0.6357768774032593, 'learning_rate': 0.00014841131370266783, 'epoch': 5.454545454545454}\n{'loss': 0.0738, 'grad_norm': 1.13865327835083, 'learning_rate': 0.00014709793924512209, 'epoch': 5.4772727272727275}\n{'loss': 0.07, 'grad_norm': 0.7676650285720825, 'learning_rate': 0.00014578456478757637, 'epoch': 5.5}\n{'loss': 0.0546, 'grad_norm': 0.550474226474762, 'learning_rate': 0.00014447119033003065, 'epoch': 5.5227272727272725}\n{'loss': 0.04, 'grad_norm': 0.7571367025375366, 'learning_rate': 0.0001431578158724849, 'epoch': 5.545454545454545}\n{'loss': 0.084, 'grad_norm': 1.1502975225448608, 'learning_rate': 0.00014184444141493915, 'epoch': 5.568181818181818}\n{'loss': 0.055, 'grad_norm': 0.6262357831001282, 'learning_rate': 0.0001405310669573934, 'epoch': 5.590909090909091}\n{'loss': 0.0512, 'grad_norm': 0.8833009600639343, 'learning_rate': 0.0001392176924998477, 'epoch': 5.613636363636363}\n{'loss': 0.0529, 'grad_norm': 0.8594263195991516, 'learning_rate': 0.00013790431804230197, 'epoch': 5.636363636363637}\n{'loss': 0.0493, 'grad_norm': 0.6122735738754272, 'learning_rate': 0.00013659094358475622, 'epoch': 5.659090909090909}\n{'loss': 0.0488, 'grad_norm': 0.9080120325088501, 'learning_rate': 0.0001352775691272105, 'epoch': 5.681818181818182}\n{'loss': 0.0531, 'grad_norm': 0.5999760627746582, 'learning_rate': 0.00013396419466966475, 'epoch': 5.704545454545455}\n{'loss': 0.0876, 'grad_norm': 1.2618886232376099, 'learning_rate': 0.00013265082021211903, 'epoch': 5.7272727272727275}\n{'loss': 0.0622, 'grad_norm': 1.0733404159545898, 'learning_rate': 0.00013133744575457331, 'epoch': 5.75}\n{'loss': 0.0534, 'grad_norm': 1.3124957084655762, 'learning_rate': 0.00013002407129702757, 'epoch': 5.7727272727272725}\n{'loss': 0.0697, 'grad_norm': 1.2442511320114136, 'learning_rate': 0.00012871069683948182, 'epoch': 5.795454545454545}\n{'loss': 0.0506, 'grad_norm': 0.7325964570045471, 'learning_rate': 0.0001273973223819361, 'epoch': 5.818181818181818}\n{'loss': 0.0531, 'grad_norm': 1.2431448698043823, 'learning_rate': 0.00012608394792439038, 'epoch': 5.840909090909091}\n{'loss': 0.0645, 'grad_norm': 0.7656183838844299, 'learning_rate': 0.00012477057346684464, 'epoch': 5.863636363636363}\n{'loss': 0.0883, 'grad_norm': 0.8611248135566711, 'learning_rate': 0.0001234571990092989, 'epoch': 5.886363636363637}\n{'loss': 0.064, 'grad_norm': 0.7742128968238831, 'learning_rate': 0.00012214382455175317, 'epoch': 5.909090909090909}\n{'loss': 0.0585, 'grad_norm': 0.8439883589744568, 'learning_rate': 0.00012083045009420744, 'epoch': 5.931818181818182}\n{'loss': 0.0518, 'grad_norm': 0.6024110317230225, 'learning_rate': 0.00011951707563666169, 'epoch': 5.954545454545455}\n{'loss': 0.061, 'grad_norm': 0.8921729326248169, 'learning_rate': 0.00011820370117911597, 'epoch': 5.9772727272727275}\n{'loss': 0.0608, 'grad_norm': 2.1223742961883545, 'learning_rate': 0.00011689032672157024, 'epoch': 6.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.9003    0.8910    0.8956      3221\n     PERCENT     0.8602    0.9195    0.8889        87\n        TYPE     0.9548    0.9617    0.9582     11501\n      VOLUME     0.9153    0.9153    0.9153        59\n\n   micro avg     0.9424    0.9460    0.9442     14868\n   macro avg     0.9076    0.9219    0.9145     14868\nweighted avg     0.9422    0.9460    0.9441     14868\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.25648027658462524, 'eval_f1_macro': 0.914500439468461, 'eval_precision': 0.9423785594639866, 'eval_recall': 0.9459913909066452, 'eval_f1': 0.9441815191487933, 'eval_accuracy': 0.9371676614215952, 'eval_runtime': 1.5723, 'eval_samples_per_second': 3504.52, 'eval_steps_per_second': 6.996, 'epoch': 6.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.0438, 'grad_norm': 0.8664804697036743, 'learning_rate': 0.00011557695226402449, 'epoch': 6.0227272727272725}\n{'loss': 0.0514, 'grad_norm': 0.612532913684845, 'learning_rate': 0.00011426357780647878, 'epoch': 6.045454545454546}\n{'loss': 0.0528, 'grad_norm': 0.6036643385887146, 'learning_rate': 0.00011295020334893304, 'epoch': 6.068181818181818}\n{'loss': 0.0328, 'grad_norm': 0.5523033142089844, 'learning_rate': 0.0001116368288913873, 'epoch': 6.090909090909091}\n{'loss': 0.0413, 'grad_norm': 0.7398952841758728, 'learning_rate': 0.00011032345443384156, 'epoch': 6.113636363636363}\n{'loss': 0.0325, 'grad_norm': 0.681394100189209, 'learning_rate': 0.00010901007997629584, 'epoch': 6.136363636363637}\n{'loss': 0.0539, 'grad_norm': 0.6751067042350769, 'learning_rate': 0.0001076967055187501, 'epoch': 6.159090909090909}\n{'loss': 0.0417, 'grad_norm': 0.432176411151886, 'learning_rate': 0.00010638333106120436, 'epoch': 6.181818181818182}\n{'loss': 0.0405, 'grad_norm': 0.613387942314148, 'learning_rate': 0.00010506995660365865, 'epoch': 6.204545454545454}\n{'loss': 0.0529, 'grad_norm': 0.7049111723899841, 'learning_rate': 0.0001037565821461129, 'epoch': 6.2272727272727275}\n{'loss': 0.0647, 'grad_norm': 0.986233651638031, 'learning_rate': 0.00010244320768856717, 'epoch': 6.25}\n{'loss': 0.0432, 'grad_norm': 0.9658567905426025, 'learning_rate': 0.00010112983323102144, 'epoch': 6.2727272727272725}\n{'loss': 0.0494, 'grad_norm': 0.7986789345741272, 'learning_rate': 9.98164587734757e-05, 'epoch': 6.295454545454546}\n{'loss': 0.0403, 'grad_norm': 0.5713303685188293, 'learning_rate': 9.850308431592997e-05, 'epoch': 6.318181818181818}\n{'loss': 0.0542, 'grad_norm': 0.6037042140960693, 'learning_rate': 9.718970985838424e-05, 'epoch': 6.340909090909091}\n{'loss': 0.0506, 'grad_norm': 0.667629599571228, 'learning_rate': 9.58763354008385e-05, 'epoch': 6.363636363636363}\n{'loss': 0.0619, 'grad_norm': 1.0283427238464355, 'learning_rate': 9.456296094329277e-05, 'epoch': 6.386363636363637}\n{'loss': 0.0414, 'grad_norm': 0.83488929271698, 'learning_rate': 9.324958648574704e-05, 'epoch': 6.409090909090909}\n{'loss': 0.0485, 'grad_norm': 0.9450563192367554, 'learning_rate': 9.19362120282013e-05, 'epoch': 6.431818181818182}\n{'loss': 0.0459, 'grad_norm': 0.6334641575813293, 'learning_rate': 9.062283757065557e-05, 'epoch': 6.454545454545454}\n{'loss': 0.0311, 'grad_norm': 0.6062713265419006, 'learning_rate': 8.930946311310984e-05, 'epoch': 6.4772727272727275}\n{'loss': 0.0571, 'grad_norm': 0.6337947845458984, 'learning_rate': 8.799608865556411e-05, 'epoch': 6.5}\n{'loss': 0.0516, 'grad_norm': 0.7619985342025757, 'learning_rate': 8.668271419801837e-05, 'epoch': 6.5227272727272725}\n{'loss': 0.0575, 'grad_norm': 0.6360939145088196, 'learning_rate': 8.536933974047265e-05, 'epoch': 6.545454545454545}\n{'loss': 0.0421, 'grad_norm': 0.469834566116333, 'learning_rate': 8.405596528292691e-05, 'epoch': 6.568181818181818}\n{'loss': 0.046, 'grad_norm': 0.868217945098877, 'learning_rate': 8.274259082538117e-05, 'epoch': 6.590909090909091}\n{'loss': 0.0833, 'grad_norm': 1.7877198457717896, 'learning_rate': 8.142921636783544e-05, 'epoch': 6.613636363636363}\n{'loss': 0.0447, 'grad_norm': 0.8267338871955872, 'learning_rate': 8.011584191028971e-05, 'epoch': 6.636363636363637}\n{'loss': 0.03, 'grad_norm': 1.1146906614303589, 'learning_rate': 7.880246745274399e-05, 'epoch': 6.659090909090909}\n{'loss': 0.0374, 'grad_norm': 1.0420050621032715, 'learning_rate': 7.748909299519824e-05, 'epoch': 6.681818181818182}\n{'loss': 0.0467, 'grad_norm': 0.6736876964569092, 'learning_rate': 7.617571853765251e-05, 'epoch': 6.704545454545455}\n{'loss': 0.0507, 'grad_norm': 1.2710959911346436, 'learning_rate': 7.486234408010678e-05, 'epoch': 6.7272727272727275}\n{'loss': 0.0325, 'grad_norm': 0.5660592913627625, 'learning_rate': 7.354896962256104e-05, 'epoch': 6.75}\n{'loss': 0.0403, 'grad_norm': 0.47866466641426086, 'learning_rate': 7.223559516501532e-05, 'epoch': 6.7727272727272725}\n{'loss': 0.0353, 'grad_norm': 0.8404648303985596, 'learning_rate': 7.092222070746958e-05, 'epoch': 6.795454545454545}\n{'loss': 0.0449, 'grad_norm': 0.944616436958313, 'learning_rate': 6.960884624992386e-05, 'epoch': 6.818181818181818}\n{'loss': 0.0309, 'grad_norm': 0.7847121357917786, 'learning_rate': 6.829547179237811e-05, 'epoch': 6.840909090909091}\n{'loss': 0.0465, 'grad_norm': 0.6741322875022888, 'learning_rate': 6.698209733483238e-05, 'epoch': 6.863636363636363}\n{'loss': 0.0618, 'grad_norm': 0.7181219458580017, 'learning_rate': 6.566872287728666e-05, 'epoch': 6.886363636363637}\n{'loss': 0.0429, 'grad_norm': 0.7201294302940369, 'learning_rate': 6.435534841974091e-05, 'epoch': 6.909090909090909}\n{'loss': 0.0752, 'grad_norm': 1.0086787939071655, 'learning_rate': 6.304197396219519e-05, 'epoch': 6.931818181818182}\n{'loss': 0.0451, 'grad_norm': 0.654242217540741, 'learning_rate': 6.172859950464944e-05, 'epoch': 6.954545454545455}\n{'loss': 0.0708, 'grad_norm': 0.684556782245636, 'learning_rate': 6.041522504710372e-05, 'epoch': 6.9772727272727275}\n{'loss': 0.0179, 'grad_norm': 1.9673137664794922, 'learning_rate': 5.9101850589557985e-05, 'epoch': 7.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8884    0.8997    0.8940      3221\n     PERCENT     0.8791    0.9195    0.8989        87\n        TYPE     0.9567    0.9610    0.9588     11501\n      VOLUME     0.9180    0.9492    0.9333        59\n\n   micro avg     0.9412    0.9474    0.9443     14868\n   macro avg     0.9106    0.9323    0.9213     14868\nweighted avg     0.9413    0.9474    0.9443     14868\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.26022201776504517, 'eval_f1_macro': 0.9212685676203274, 'eval_precision': 0.9412000534544969, 'eval_recall': 0.9474038202851762, 'eval_f1': 0.9442917476704431, 'eval_accuracy': 0.9376559956592512, 'eval_runtime': 1.6985, 'eval_samples_per_second': 3244.015, 'eval_steps_per_second': 6.476, 'epoch': 7.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.0574, 'grad_norm': 0.7184925675392151, 'learning_rate': 5.7788476132012245e-05, 'epoch': 7.0227272727272725}\n{'loss': 0.0383, 'grad_norm': 0.47522610425949097, 'learning_rate': 5.647510167446652e-05, 'epoch': 7.045454545454546}\n{'loss': 0.0355, 'grad_norm': 0.6021645069122314, 'learning_rate': 5.516172721692078e-05, 'epoch': 7.068181818181818}\n{'loss': 0.0293, 'grad_norm': 0.6432549953460693, 'learning_rate': 5.384835275937505e-05, 'epoch': 7.090909090909091}\n{'loss': 0.0425, 'grad_norm': 0.60732501745224, 'learning_rate': 5.2534978301829326e-05, 'epoch': 7.113636363636363}\n{'loss': 0.0838, 'grad_norm': 0.8957090377807617, 'learning_rate': 5.1221603844283586e-05, 'epoch': 7.136363636363637}\n{'loss': 0.0352, 'grad_norm': 0.641931414604187, 'learning_rate': 4.990822938673785e-05, 'epoch': 7.159090909090909}\n{'loss': 0.0491, 'grad_norm': 0.8945494890213013, 'learning_rate': 4.859485492919212e-05, 'epoch': 7.181818181818182}\n{'loss': 0.0346, 'grad_norm': 0.6686043739318848, 'learning_rate': 4.7281480471646386e-05, 'epoch': 7.204545454545454}\n{'loss': 0.0371, 'grad_norm': 0.6011815667152405, 'learning_rate': 4.596810601410065e-05, 'epoch': 7.2272727272727275}\n{'loss': 0.0558, 'grad_norm': 1.0982673168182373, 'learning_rate': 4.465473155655492e-05, 'epoch': 7.25}\n{'loss': 0.0309, 'grad_norm': 0.4326767325401306, 'learning_rate': 4.334135709900919e-05, 'epoch': 7.2727272727272725}\n{'loss': 0.0323, 'grad_norm': 0.711567759513855, 'learning_rate': 4.2027982641463454e-05, 'epoch': 7.295454545454546}\n{'loss': 0.0372, 'grad_norm': 0.8177815675735474, 'learning_rate': 4.071460818391772e-05, 'epoch': 7.318181818181818}\n{'loss': 0.0482, 'grad_norm': 0.8645042777061462, 'learning_rate': 3.9401233726371994e-05, 'epoch': 7.340909090909091}\n{'loss': 0.041, 'grad_norm': 0.6373488306999207, 'learning_rate': 3.8087859268826254e-05, 'epoch': 7.363636363636363}\n{'loss': 0.0322, 'grad_norm': 0.39402368664741516, 'learning_rate': 3.677448481128052e-05, 'epoch': 7.386363636363637}\n{'loss': 0.0351, 'grad_norm': 0.6873959302902222, 'learning_rate': 3.546111035373479e-05, 'epoch': 7.409090909090909}\n{'loss': 0.0459, 'grad_norm': 0.7691783905029297, 'learning_rate': 3.4147735896189055e-05, 'epoch': 7.431818181818182}\n{'loss': 0.035, 'grad_norm': 0.44906318187713623, 'learning_rate': 3.283436143864333e-05, 'epoch': 7.454545454545454}\n{'loss': 0.0367, 'grad_norm': 0.8184590339660645, 'learning_rate': 3.1520986981097595e-05, 'epoch': 7.4772727272727275}\n{'loss': 0.0556, 'grad_norm': 0.7188471555709839, 'learning_rate': 3.020761252355186e-05, 'epoch': 7.5}\n{'loss': 0.0295, 'grad_norm': 0.5069176554679871, 'learning_rate': 2.8894238066006122e-05, 'epoch': 7.5227272727272725}\n{'loss': 0.0479, 'grad_norm': 1.6532865762710571, 'learning_rate': 2.758086360846039e-05, 'epoch': 7.545454545454545}\n{'loss': 0.0321, 'grad_norm': 0.6454977989196777, 'learning_rate': 2.6267489150914663e-05, 'epoch': 7.568181818181818}\n{'loss': 0.0286, 'grad_norm': 0.388631135225296, 'learning_rate': 2.4954114693368926e-05, 'epoch': 7.590909090909091}\n{'loss': 0.0348, 'grad_norm': 0.5035532116889954, 'learning_rate': 2.3640740235823193e-05, 'epoch': 7.613636363636363}\n{'loss': 0.0565, 'grad_norm': 0.7882127165794373, 'learning_rate': 2.232736577827746e-05, 'epoch': 7.636363636363637}\n{'loss': 0.0463, 'grad_norm': 0.45999565720558167, 'learning_rate': 2.1013991320731727e-05, 'epoch': 7.659090909090909}\n{'loss': 0.0282, 'grad_norm': 0.48593470454216003, 'learning_rate': 1.9700616863185997e-05, 'epoch': 7.681818181818182}\n{'loss': 0.0405, 'grad_norm': 0.7824229598045349, 'learning_rate': 1.838724240564026e-05, 'epoch': 7.704545454545455}\n{'loss': 0.0245, 'grad_norm': 0.583104133605957, 'learning_rate': 1.7073867948094528e-05, 'epoch': 7.7272727272727275}\n{'loss': 0.0307, 'grad_norm': 0.5715476870536804, 'learning_rate': 1.5760493490548798e-05, 'epoch': 7.75}\n{'loss': 0.0511, 'grad_norm': 0.9481172561645508, 'learning_rate': 1.4447119033003061e-05, 'epoch': 7.7727272727272725}\n{'loss': 0.0429, 'grad_norm': 0.6106754541397095, 'learning_rate': 1.3133744575457331e-05, 'epoch': 7.795454545454545}\n{'loss': 0.047, 'grad_norm': 0.7140730023384094, 'learning_rate': 1.1820370117911597e-05, 'epoch': 7.818181818181818}\n{'loss': 0.0292, 'grad_norm': 0.5244653224945068, 'learning_rate': 1.0506995660365863e-05, 'epoch': 7.840909090909091}\n{'loss': 0.0301, 'grad_norm': 0.6967772245407104, 'learning_rate': 9.19362120282013e-06, 'epoch': 7.863636363636363}\n{'loss': 0.0391, 'grad_norm': 0.9819577932357788, 'learning_rate': 7.880246745274399e-06, 'epoch': 7.886363636363637}\n{'loss': 0.0407, 'grad_norm': 0.6684539318084717, 'learning_rate': 6.566872287728666e-06, 'epoch': 7.909090909090909}\n{'loss': 0.0297, 'grad_norm': 0.4960956573486328, 'learning_rate': 5.253497830182932e-06, 'epoch': 7.931818181818182}\n{'loss': 0.0361, 'grad_norm': 0.5856000185012817, 'learning_rate': 3.9401233726371994e-06, 'epoch': 7.954545454545455}\n{'loss': 0.0326, 'grad_norm': 0.32128071784973145, 'learning_rate': 2.626748915091466e-06, 'epoch': 7.9772727272727275}\n{'loss': 0.0283, 'grad_norm': 1.5899518728256226, 'learning_rate': 1.313374457545733e-06, 'epoch': 8.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8974    0.8957    0.8965      3221\n     PERCENT     0.8696    0.9195    0.8939        87\n        TYPE     0.9556    0.9639    0.9597     11501\n      VOLUME     0.9167    0.9322    0.9244        59\n\n   micro avg     0.9424    0.9487    0.9456     14868\n   macro avg     0.9098    0.9278    0.9186     14868\nweighted avg     0.9423    0.9487    0.9455     14868\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.2633667588233948, 'eval_f1_macro': 0.918621954752514, 'eval_precision': 0.9424104756814538, 'eval_recall': 0.9487489911218725, 'eval_f1': 0.9455691111409036, 'eval_accuracy': 0.9389582202930006, 'eval_runtime': 1.9279, 'eval_samples_per_second': 2858.068, 'eval_steps_per_second': 5.706, 'epoch': 8.0}\n{'train_runtime': 52.2555, 'train_samples_per_second': 3374.496, 'train_steps_per_second': 6.736, 'train_loss': 0.22975164623296057, 'epoch': 8.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n[I 2025-09-26 18:17:54,227] Trial 7 finished with value: 0.9145934975619096 and parameters: {'learning_rate': 0.0004150263285844516, 'weight_decay': 0.00017639279556852245, 'num_train_epochs': 8}. Best is trial 0 with value: 0.9174994186455976.\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8884    0.8997    0.8940      3221\n     PERCENT     0.8791    0.9195    0.8989        87\n        TYPE     0.9567    0.9610    0.9588     11501\n      VOLUME     0.9180    0.9492    0.9333        59\n\n   micro avg     0.9412    0.9474    0.9443     14868\n   macro avg     0.9106    0.9323    0.9213     14868\nweighted avg     0.9413    0.9474    0.9443     14868\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.26022201776504517, 'eval_f1_macro': 0.9212685676203274, 'eval_precision': 0.9412000534544969, 'eval_recall': 0.9474038202851762, 'eval_f1': 0.9442917476704431, 'eval_accuracy': 0.9376559956592512, 'eval_runtime': 1.5648, 'eval_samples_per_second': 3521.13, 'eval_steps_per_second': 7.029, 'epoch': 8.0}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/27552 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef5f4eefde16455191c99533fb7b379d"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nThe speedups for torchdynamo mostly come with GPU Ampere or higher and which is not detected here.\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/tmp/ipykernel_36/3364797558.py:66: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 2.413, 'grad_norm': 7.627295017242432, 'learning_rate': 0.0, 'epoch': 0.022727272727272728}\n{'loss': 2.3999, 'grad_norm': 7.770687580108643, 'learning_rate': 3.9677528605473815e-06, 'epoch': 0.045454545454545456}\n{'loss': 2.3873, 'grad_norm': 7.678284645080566, 'learning_rate': 7.935505721094763e-06, 'epoch': 0.06818181818181818}\n{'loss': 2.3872, 'grad_norm': 7.902544975280762, 'learning_rate': 1.1903258581642145e-05, 'epoch': 0.09090909090909091}\n{'loss': 2.3427, 'grad_norm': 7.494531631469727, 'learning_rate': 1.5871011442189526e-05, 'epoch': 0.11363636363636363}\n{'loss': 2.2999, 'grad_norm': 7.4794111251831055, 'learning_rate': 1.983876430273691e-05, 'epoch': 0.13636363636363635}\n{'loss': 2.258, 'grad_norm': 7.174574375152588, 'learning_rate': 2.380651716328429e-05, 'epoch': 0.1590909090909091}\n{'loss': 2.2095, 'grad_norm': 7.369345188140869, 'learning_rate': 2.7774270023831672e-05, 'epoch': 0.18181818181818182}\n{'loss': 2.149, 'grad_norm': 6.949069499969482, 'learning_rate': 3.174202288437905e-05, 'epoch': 0.20454545454545456}\n{'loss': 2.076, 'grad_norm': 6.953426837921143, 'learning_rate': 3.570977574492644e-05, 'epoch': 0.22727272727272727}\n{'loss': 1.9773, 'grad_norm': 6.940650939941406, 'learning_rate': 3.967752860547382e-05, 'epoch': 0.25}\n{'loss': 1.8997, 'grad_norm': 6.610400676727295, 'learning_rate': 4.36452814660212e-05, 'epoch': 0.2727272727272727}\n{'loss': 1.8257, 'grad_norm': 6.265495300292969, 'learning_rate': 4.761303432656858e-05, 'epoch': 0.29545454545454547}\n{'loss': 1.7547, 'grad_norm': 5.765068531036377, 'learning_rate': 5.1580787187115965e-05, 'epoch': 0.3181818181818182}\n{'loss': 1.6575, 'grad_norm': 5.352912425994873, 'learning_rate': 5.5548540047663345e-05, 'epoch': 0.3409090909090909}\n{'loss': 1.5819, 'grad_norm': 4.760874271392822, 'learning_rate': 5.951629290821072e-05, 'epoch': 0.36363636363636365}\n{'loss': 1.5053, 'grad_norm': 4.3892951011657715, 'learning_rate': 6.34840457687581e-05, 'epoch': 0.38636363636363635}\n{'loss': 1.4018, 'grad_norm': 3.7632124423980713, 'learning_rate': 6.745179862930548e-05, 'epoch': 0.4090909090909091}\n{'loss': 1.3568, 'grad_norm': 3.051640748977661, 'learning_rate': 7.141955148985288e-05, 'epoch': 0.4318181818181818}\n{'loss': 1.2635, 'grad_norm': 2.5239510536193848, 'learning_rate': 7.538730435040026e-05, 'epoch': 0.45454545454545453}\n{'loss': 1.256, 'grad_norm': 2.0092902183532715, 'learning_rate': 7.935505721094764e-05, 'epoch': 0.4772727272727273}\n{'loss': 1.1933, 'grad_norm': 1.8563389778137207, 'learning_rate': 8.332281007149502e-05, 'epoch': 0.5}\n{'loss': 1.2085, 'grad_norm': 2.0516350269317627, 'learning_rate': 8.72905629320424e-05, 'epoch': 0.5227272727272727}\n{'loss': 1.1379, 'grad_norm': 1.767463207244873, 'learning_rate': 9.125831579258978e-05, 'epoch': 0.5454545454545454}\n{'loss': 1.1609, 'grad_norm': 1.9485230445861816, 'learning_rate': 9.522606865313716e-05, 'epoch': 0.5681818181818182}\n{'loss': 1.1057, 'grad_norm': 1.768404245376587, 'learning_rate': 9.919382151368455e-05, 'epoch': 0.5909090909090909}\n{'loss': 1.0267, 'grad_norm': 1.254723310470581, 'learning_rate': 0.00010316157437423193, 'epoch': 0.6136363636363636}\n{'loss': 1.0038, 'grad_norm': 1.2472234964370728, 'learning_rate': 0.00010712932723477931, 'epoch': 0.6363636363636364}\n{'loss': 0.9916, 'grad_norm': 1.3877674341201782, 'learning_rate': 0.00011109708009532669, 'epoch': 0.6590909090909091}\n{'loss': 0.9408, 'grad_norm': 1.9055677652359009, 'learning_rate': 0.00011506483295587407, 'epoch': 0.6818181818181818}\n{'loss': 0.9062, 'grad_norm': 1.958486795425415, 'learning_rate': 0.00011903258581642144, 'epoch': 0.7045454545454546}\n{'loss': 0.8816, 'grad_norm': 1.6946661472320557, 'learning_rate': 0.00012300033867696883, 'epoch': 0.7272727272727273}\n{'loss': 0.9, 'grad_norm': 1.2656079530715942, 'learning_rate': 0.0001269680915375162, 'epoch': 0.75}\n{'loss': 0.8968, 'grad_norm': 1.0940872430801392, 'learning_rate': 0.0001309358443980636, 'epoch': 0.7727272727272727}\n{'loss': 0.7888, 'grad_norm': 0.9862346649169922, 'learning_rate': 0.00013490359725861097, 'epoch': 0.7954545454545454}\n{'loss': 0.7478, 'grad_norm': 0.9509586095809937, 'learning_rate': 0.00013887135011915835, 'epoch': 0.8181818181818182}\n{'loss': 0.7587, 'grad_norm': 0.9035843014717102, 'learning_rate': 0.00014283910297970576, 'epoch': 0.8409090909090909}\n{'loss': 0.7049, 'grad_norm': 0.8746815919876099, 'learning_rate': 0.00014680685584025314, 'epoch': 0.8636363636363636}\n{'loss': 0.6863, 'grad_norm': 0.7143409252166748, 'learning_rate': 0.00015077460870080052, 'epoch': 0.8863636363636364}\n{'loss': 0.6658, 'grad_norm': 0.7632235288619995, 'learning_rate': 0.0001547423615613479, 'epoch': 0.9090909090909091}\n{'loss': 0.7144, 'grad_norm': 0.7998123168945312, 'learning_rate': 0.00015871011442189527, 'epoch': 0.9318181818181818}\n{'loss': 0.63, 'grad_norm': 0.9921042323112488, 'learning_rate': 0.00016267786728244263, 'epoch': 0.9545454545454546}\n{'loss': 0.6303, 'grad_norm': 0.9288567900657654, 'learning_rate': 0.00016664562014299003, 'epoch': 0.9772727272727273}\n{'loss': 0.6162, 'grad_norm': 2.8661701679229736, 'learning_rate': 0.00017061337300353741, 'epoch': 1.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.7115    0.6225    0.6641      3142\n     PERCENT     0.7826    0.2727    0.4045        66\n        TYPE     0.8248    0.9072    0.8640     11415\n      VOLUME     0.0000    0.0000    0.0000        70\n\n   micro avg     0.8044    0.8392    0.8214     14693\n   macro avg     0.5797    0.4506    0.4832     14693\nweighted avg     0.7964    0.8392    0.8151     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.5721564888954163, 'eval_f1_macro': 0.4831505653572453, 'eval_precision': 0.8043577532781003, 'eval_recall': 0.8391751174028449, 'eval_f1': 0.8213976417293984, 'eval_accuracy': 0.8257155294373708, 'eval_runtime': 1.5214, 'eval_samples_per_second': 3622.395, 'eval_steps_per_second': 7.23, 'epoch': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.5463, 'grad_norm': 1.0274627208709717, 'learning_rate': 0.0001745811258640848, 'epoch': 1.0227272727272727}\n{'loss': 0.5728, 'grad_norm': 1.3190362453460693, 'learning_rate': 0.0001741402644351351, 'epoch': 1.0454545454545454}\n{'loss': 0.5633, 'grad_norm': 1.0315748453140259, 'learning_rate': 0.00017369940300618537, 'epoch': 1.0681818181818181}\n{'loss': 0.4944, 'grad_norm': 0.8564332127571106, 'learning_rate': 0.00017325854157723567, 'epoch': 1.0909090909090908}\n{'loss': 0.4964, 'grad_norm': 1.3410365581512451, 'learning_rate': 0.00017281768014828597, 'epoch': 1.1136363636363635}\n{'loss': 0.5331, 'grad_norm': 0.7596327066421509, 'learning_rate': 0.00017237681871933624, 'epoch': 1.1363636363636362}\n{'loss': 0.5285, 'grad_norm': 1.1494237184524536, 'learning_rate': 0.00017193595729038654, 'epoch': 1.1590909090909092}\n{'loss': 0.5074, 'grad_norm': 0.9375959038734436, 'learning_rate': 0.00017149509586143684, 'epoch': 1.1818181818181819}\n{'loss': 0.5093, 'grad_norm': 1.0784814357757568, 'learning_rate': 0.00017105423443248711, 'epoch': 1.2045454545454546}\n{'loss': 0.4798, 'grad_norm': 0.9006866812705994, 'learning_rate': 0.00017061337300353741, 'epoch': 1.2272727272727273}\n{'loss': 0.4773, 'grad_norm': 1.0476629734039307, 'learning_rate': 0.0001701725115745877, 'epoch': 1.25}\n{'loss': 0.4786, 'grad_norm': 0.6936603784561157, 'learning_rate': 0.000169731650145638, 'epoch': 1.2727272727272727}\n{'loss': 0.4461, 'grad_norm': 1.1715621948242188, 'learning_rate': 0.0001692907887166883, 'epoch': 1.2954545454545454}\n{'loss': 0.4289, 'grad_norm': 1.1716115474700928, 'learning_rate': 0.00016884992728773856, 'epoch': 1.3181818181818181}\n{'loss': 0.4442, 'grad_norm': 0.8720683455467224, 'learning_rate': 0.00016840906585878886, 'epoch': 1.3409090909090908}\n{'loss': 0.4577, 'grad_norm': 1.1125240325927734, 'learning_rate': 0.00016796820442983916, 'epoch': 1.3636363636363638}\n{'loss': 0.4693, 'grad_norm': 1.0256751775741577, 'learning_rate': 0.00016752734300088943, 'epoch': 1.3863636363636362}\n{'loss': 0.3839, 'grad_norm': 0.7368114590644836, 'learning_rate': 0.00016708648157193973, 'epoch': 1.4090909090909092}\n{'loss': 0.4479, 'grad_norm': 0.9449424743652344, 'learning_rate': 0.00016664562014299003, 'epoch': 1.4318181818181819}\n{'loss': 0.4574, 'grad_norm': 1.224023699760437, 'learning_rate': 0.0001662047587140403, 'epoch': 1.4545454545454546}\n{'loss': 0.3898, 'grad_norm': 1.1346770524978638, 'learning_rate': 0.0001657638972850906, 'epoch': 1.4772727272727273}\n{'loss': 0.4216, 'grad_norm': 0.938909113407135, 'learning_rate': 0.0001653230358561409, 'epoch': 1.5}\n{'loss': 0.4432, 'grad_norm': 0.9588455557823181, 'learning_rate': 0.00016488217442719118, 'epoch': 1.5227272727272727}\n{'loss': 0.3669, 'grad_norm': 0.7867111563682556, 'learning_rate': 0.00016444131299824148, 'epoch': 1.5454545454545454}\n{'loss': 0.3541, 'grad_norm': 0.9631944298744202, 'learning_rate': 0.00016400045156929178, 'epoch': 1.5681818181818183}\n{'loss': 0.3751, 'grad_norm': 1.0296381711959839, 'learning_rate': 0.00016355959014034205, 'epoch': 1.5909090909090908}\n{'loss': 0.3878, 'grad_norm': 1.3680437803268433, 'learning_rate': 0.00016311872871139235, 'epoch': 1.6136363636363638}\n{'loss': 0.4156, 'grad_norm': 0.8454881310462952, 'learning_rate': 0.00016267786728244263, 'epoch': 1.6363636363636362}\n{'loss': 0.3382, 'grad_norm': 0.7793837189674377, 'learning_rate': 0.00016223700585349293, 'epoch': 1.6590909090909092}\n{'loss': 0.4156, 'grad_norm': 1.7319368124008179, 'learning_rate': 0.00016179614442454323, 'epoch': 1.6818181818181817}\n{'loss': 0.3365, 'grad_norm': 1.2889125347137451, 'learning_rate': 0.0001613552829955935, 'epoch': 1.7045454545454546}\n{'loss': 0.3461, 'grad_norm': 0.9060981273651123, 'learning_rate': 0.0001609144215666438, 'epoch': 1.7272727272727273}\n{'loss': 0.4313, 'grad_norm': 0.9166753888130188, 'learning_rate': 0.0001604735601376941, 'epoch': 1.75}\n{'loss': 0.3968, 'grad_norm': 0.846457839012146, 'learning_rate': 0.0001600326987087444, 'epoch': 1.7727272727272727}\n{'loss': 0.3574, 'grad_norm': 0.7560869455337524, 'learning_rate': 0.00015959183727979467, 'epoch': 1.7954545454545454}\n{'loss': 0.3701, 'grad_norm': 1.2973909378051758, 'learning_rate': 0.00015915097585084497, 'epoch': 1.8181818181818183}\n{'loss': 0.3475, 'grad_norm': 1.0237213373184204, 'learning_rate': 0.00015871011442189527, 'epoch': 1.8409090909090908}\n{'loss': 0.3951, 'grad_norm': 1.1397240161895752, 'learning_rate': 0.00015826925299294555, 'epoch': 1.8636363636363638}\n{'loss': 0.3384, 'grad_norm': 1.0988062620162964, 'learning_rate': 0.00015782839156399585, 'epoch': 1.8863636363636362}\n{'loss': 0.4195, 'grad_norm': 1.126678228378296, 'learning_rate': 0.00015738753013504615, 'epoch': 1.9090909090909092}\n{'loss': 0.2867, 'grad_norm': 1.0766103267669678, 'learning_rate': 0.00015694666870609642, 'epoch': 1.9318181818181817}\n{'loss': 0.3833, 'grad_norm': 1.4436527490615845, 'learning_rate': 0.00015650580727714672, 'epoch': 1.9545454545454546}\n{'loss': 0.3352, 'grad_norm': 1.023998737335205, 'learning_rate': 0.00015606494584819702, 'epoch': 1.9772727272727273}\n{'loss': 0.4057, 'grad_norm': 3.623675584793091, 'learning_rate': 0.0001556240844192473, 'epoch': 2.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8407    0.7661    0.8017      3142\n     PERCENT     0.5385    0.9545    0.6885        66\n        TYPE     0.9144    0.9606    0.9369     11415\n      VOLUME     0.4186    0.2571    0.3186        70\n\n   micro avg     0.8960    0.9156    0.9057     14693\n   macro avg     0.6780    0.7346    0.6864     14693\nweighted avg     0.8946    0.9156    0.9039     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.3419543504714966, 'eval_f1_macro': 0.6864182895193028, 'eval_precision': 0.895970695970696, 'eval_recall': 0.9156060709181243, 'eval_f1': 0.9056819711862125, 'eval_accuracy': 0.8991729241484383, 'eval_runtime': 1.512, 'eval_samples_per_second': 3644.784, 'eval_steps_per_second': 7.275, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.3057, 'grad_norm': 1.7005919218063354, 'learning_rate': 0.0001551832229902976, 'epoch': 2.022727272727273}\n{'loss': 0.3343, 'grad_norm': 1.6574970483779907, 'learning_rate': 0.0001547423615613479, 'epoch': 2.0454545454545454}\n{'loss': 0.3581, 'grad_norm': 1.2423170804977417, 'learning_rate': 0.0001543015001323982, 'epoch': 2.0681818181818183}\n{'loss': 0.2632, 'grad_norm': 0.9308585524559021, 'learning_rate': 0.00015386063870344847, 'epoch': 2.090909090909091}\n{'loss': 0.2613, 'grad_norm': 0.9167390465736389, 'learning_rate': 0.00015341977727449877, 'epoch': 2.1136363636363638}\n{'loss': 0.2276, 'grad_norm': 0.7508106827735901, 'learning_rate': 0.00015297891584554907, 'epoch': 2.1363636363636362}\n{'loss': 0.2644, 'grad_norm': 1.0978790521621704, 'learning_rate': 0.00015253805441659934, 'epoch': 2.159090909090909}\n{'loss': 0.3006, 'grad_norm': 0.8728858828544617, 'learning_rate': 0.00015209719298764964, 'epoch': 2.1818181818181817}\n{'loss': 0.3112, 'grad_norm': 0.8403377532958984, 'learning_rate': 0.00015165633155869994, 'epoch': 2.2045454545454546}\n{'loss': 0.3114, 'grad_norm': 1.3831732273101807, 'learning_rate': 0.00015121547012975021, 'epoch': 2.227272727272727}\n{'loss': 0.2903, 'grad_norm': 0.853142499923706, 'learning_rate': 0.00015077460870080052, 'epoch': 2.25}\n{'loss': 0.2457, 'grad_norm': 0.9608758091926575, 'learning_rate': 0.00015033374727185082, 'epoch': 2.2727272727272725}\n{'loss': 0.2858, 'grad_norm': 1.1224902868270874, 'learning_rate': 0.0001498928858429011, 'epoch': 2.2954545454545454}\n{'loss': 0.2817, 'grad_norm': 1.1033234596252441, 'learning_rate': 0.0001494520244139514, 'epoch': 2.3181818181818183}\n{'loss': 0.2562, 'grad_norm': 1.2872889041900635, 'learning_rate': 0.00014901116298500166, 'epoch': 2.340909090909091}\n{'loss': 0.2907, 'grad_norm': 0.9711636304855347, 'learning_rate': 0.00014857030155605196, 'epoch': 2.3636363636363638}\n{'loss': 0.2683, 'grad_norm': 0.8338463306427002, 'learning_rate': 0.00014812944012710226, 'epoch': 2.3863636363636362}\n{'loss': 0.3213, 'grad_norm': 1.2255090475082397, 'learning_rate': 0.00014768857869815253, 'epoch': 2.409090909090909}\n{'loss': 0.263, 'grad_norm': 1.227308988571167, 'learning_rate': 0.00014724771726920284, 'epoch': 2.4318181818181817}\n{'loss': 0.2822, 'grad_norm': 0.9448779821395874, 'learning_rate': 0.00014680685584025314, 'epoch': 2.4545454545454546}\n{'loss': 0.289, 'grad_norm': 0.8475878834724426, 'learning_rate': 0.0001463659944113034, 'epoch': 2.4772727272727275}\n{'loss': 0.2545, 'grad_norm': 1.211220622062683, 'learning_rate': 0.0001459251329823537, 'epoch': 2.5}\n{'loss': 0.3162, 'grad_norm': 1.7262349128723145, 'learning_rate': 0.000145484271553404, 'epoch': 2.5227272727272725}\n{'loss': 0.2999, 'grad_norm': 0.9005089998245239, 'learning_rate': 0.00014504341012445428, 'epoch': 2.5454545454545454}\n{'loss': 0.2983, 'grad_norm': 1.0932213068008423, 'learning_rate': 0.00014460254869550458, 'epoch': 2.5681818181818183}\n{'loss': 0.3593, 'grad_norm': 2.017103672027588, 'learning_rate': 0.00014416168726655488, 'epoch': 2.590909090909091}\n{'loss': 0.2346, 'grad_norm': 0.8348813056945801, 'learning_rate': 0.00014372082583760516, 'epoch': 2.6136363636363638}\n{'loss': 0.2587, 'grad_norm': 1.8215506076812744, 'learning_rate': 0.00014327996440865546, 'epoch': 2.6363636363636362}\n{'loss': 0.269, 'grad_norm': 1.4295438528060913, 'learning_rate': 0.00014283910297970576, 'epoch': 2.659090909090909}\n{'loss': 0.2058, 'grad_norm': 1.0707414150238037, 'learning_rate': 0.00014239824155075603, 'epoch': 2.6818181818181817}\n{'loss': 0.2673, 'grad_norm': 0.8121997117996216, 'learning_rate': 0.00014195738012180633, 'epoch': 2.7045454545454546}\n{'loss': 0.2607, 'grad_norm': 0.9660631418228149, 'learning_rate': 0.0001415165186928566, 'epoch': 2.7272727272727275}\n{'loss': 0.2594, 'grad_norm': 0.9520236849784851, 'learning_rate': 0.0001410756572639069, 'epoch': 2.75}\n{'loss': 0.2258, 'grad_norm': 0.8094555139541626, 'learning_rate': 0.0001406347958349572, 'epoch': 2.7727272727272725}\n{'loss': 0.3041, 'grad_norm': 1.1432671546936035, 'learning_rate': 0.00014019393440600748, 'epoch': 2.7954545454545454}\n{'loss': 0.2865, 'grad_norm': 0.882325291633606, 'learning_rate': 0.00013975307297705778, 'epoch': 2.8181818181818183}\n{'loss': 0.2733, 'grad_norm': 1.0298879146575928, 'learning_rate': 0.00013931221154810808, 'epoch': 2.840909090909091}\n{'loss': 0.2386, 'grad_norm': 1.1911184787750244, 'learning_rate': 0.00013887135011915835, 'epoch': 2.8636363636363638}\n{'loss': 0.2317, 'grad_norm': 1.3957337141036987, 'learning_rate': 0.00013843048869020865, 'epoch': 2.8863636363636362}\n{'loss': 0.2694, 'grad_norm': 1.9091516733169556, 'learning_rate': 0.00013798962726125895, 'epoch': 2.909090909090909}\n{'loss': 0.2694, 'grad_norm': 1.0510716438293457, 'learning_rate': 0.00013754876583230922, 'epoch': 2.9318181818181817}\n{'loss': 0.2953, 'grad_norm': 1.3799902200698853, 'learning_rate': 0.00013710790440335952, 'epoch': 2.9545454545454546}\n{'loss': 0.2803, 'grad_norm': 0.8455246090888977, 'learning_rate': 0.00013666704297440982, 'epoch': 2.9772727272727275}\n{'loss': 0.1084, 'grad_norm': 3.2751283645629883, 'learning_rate': 0.0001362261815454601, 'epoch': 3.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8438    0.8440    0.8439      3142\n     PERCENT     0.6923    0.9545    0.8025        66\n        TYPE     0.9343    0.9695    0.9516     11415\n      VOLUME     0.5484    0.4857    0.5152        70\n\n   micro avg     0.9125    0.9403    0.9262     14693\n   macro avg     0.7547    0.8135    0.7783     14693\nweighted avg     0.9120    0.9403    0.9258     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.27587035298347473, 'eval_f1_macro': 0.778301020167319, 'eval_precision': 0.9124892675516809, 'eval_recall': 0.9403117130606411, 'eval_f1': 0.9261915934839445, 'eval_accuracy': 0.9178365436935466, 'eval_runtime': 1.9123, 'eval_samples_per_second': 2881.89, 'eval_steps_per_second': 5.752, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.2058, 'grad_norm': 0.8581865429878235, 'learning_rate': 0.0001357853201165104, 'epoch': 3.022727272727273}\n{'loss': 0.2286, 'grad_norm': 0.6203919053077698, 'learning_rate': 0.0001353444586875607, 'epoch': 3.0454545454545454}\n{'loss': 0.2084, 'grad_norm': 0.789428174495697, 'learning_rate': 0.00013490359725861097, 'epoch': 3.0681818181818183}\n{'loss': 0.1897, 'grad_norm': 0.8961066007614136, 'learning_rate': 0.00013446273582966127, 'epoch': 3.090909090909091}\n{'loss': 0.2494, 'grad_norm': 1.4479008913040161, 'learning_rate': 0.00013402187440071154, 'epoch': 3.1136363636363638}\n{'loss': 0.1978, 'grad_norm': 1.1253002882003784, 'learning_rate': 0.00013358101297176184, 'epoch': 3.1363636363636362}\n{'loss': 0.2014, 'grad_norm': 1.354690432548523, 'learning_rate': 0.00013314015154281214, 'epoch': 3.159090909090909}\n{'loss': 0.1954, 'grad_norm': 0.9502537846565247, 'learning_rate': 0.00013269929011386242, 'epoch': 3.1818181818181817}\n{'loss': 0.2514, 'grad_norm': 0.8995673060417175, 'learning_rate': 0.00013225842868491272, 'epoch': 3.2045454545454546}\n{'loss': 0.1828, 'grad_norm': 1.1968473196029663, 'learning_rate': 0.00013181756725596302, 'epoch': 3.227272727272727}\n{'loss': 0.2284, 'grad_norm': 0.8117972612380981, 'learning_rate': 0.0001313767058270133, 'epoch': 3.25}\n{'loss': 0.1987, 'grad_norm': 0.9547527432441711, 'learning_rate': 0.0001309358443980636, 'epoch': 3.2727272727272725}\n{'loss': 0.2181, 'grad_norm': 1.0689104795455933, 'learning_rate': 0.0001304949829691139, 'epoch': 3.2954545454545454}\n{'loss': 0.2398, 'grad_norm': 1.3334366083145142, 'learning_rate': 0.00013005412154016416, 'epoch': 3.3181818181818183}\n{'loss': 0.2164, 'grad_norm': 1.1389087438583374, 'learning_rate': 0.00012961326011121446, 'epoch': 3.340909090909091}\n{'loss': 0.2004, 'grad_norm': 0.8319056034088135, 'learning_rate': 0.00012917239868226476, 'epoch': 3.3636363636363638}\n{'loss': 0.1821, 'grad_norm': 1.0310529470443726, 'learning_rate': 0.00012873153725331504, 'epoch': 3.3863636363636362}\n{'loss': 0.2017, 'grad_norm': 1.5042524337768555, 'learning_rate': 0.00012829067582436534, 'epoch': 3.409090909090909}\n{'loss': 0.1832, 'grad_norm': 0.778157651424408, 'learning_rate': 0.00012784981439541564, 'epoch': 3.4318181818181817}\n{'loss': 0.2731, 'grad_norm': 1.2396512031555176, 'learning_rate': 0.0001274089529664659, 'epoch': 3.4545454545454546}\n{'loss': 0.1853, 'grad_norm': 1.0180414915084839, 'learning_rate': 0.0001269680915375162, 'epoch': 3.4772727272727275}\n{'loss': 0.2282, 'grad_norm': 1.3691638708114624, 'learning_rate': 0.0001265272301085665, 'epoch': 3.5}\n{'loss': 0.226, 'grad_norm': 0.7850629091262817, 'learning_rate': 0.00012608636867961678, 'epoch': 3.5227272727272725}\n{'loss': 0.1657, 'grad_norm': 0.7957167625427246, 'learning_rate': 0.00012564550725066708, 'epoch': 3.5454545454545454}\n{'loss': 0.1813, 'grad_norm': 0.9105655550956726, 'learning_rate': 0.00012520464582171738, 'epoch': 3.5681818181818183}\n{'loss': 0.2326, 'grad_norm': 0.8588090538978577, 'learning_rate': 0.00012476378439276766, 'epoch': 3.590909090909091}\n{'loss': 0.2258, 'grad_norm': 1.0915509462356567, 'learning_rate': 0.00012432292296381796, 'epoch': 3.6136363636363638}\n{'loss': 0.1785, 'grad_norm': 0.8857620358467102, 'learning_rate': 0.00012388206153486826, 'epoch': 3.6363636363636362}\n{'loss': 0.176, 'grad_norm': 1.2565867900848389, 'learning_rate': 0.00012344120010591853, 'epoch': 3.659090909090909}\n{'loss': 0.1849, 'grad_norm': 1.2013403177261353, 'learning_rate': 0.00012300033867696883, 'epoch': 3.6818181818181817}\n{'loss': 0.1749, 'grad_norm': 0.8034819960594177, 'learning_rate': 0.00012255947724801913, 'epoch': 3.7045454545454546}\n{'loss': 0.1941, 'grad_norm': 1.1337329149246216, 'learning_rate': 0.0001221186158190694, 'epoch': 3.7272727272727275}\n{'loss': 0.1479, 'grad_norm': 1.3003756999969482, 'learning_rate': 0.00012167775439011972, 'epoch': 3.75}\n{'loss': 0.2235, 'grad_norm': 0.8356970548629761, 'learning_rate': 0.00012123689296116999, 'epoch': 3.7727272727272725}\n{'loss': 0.2082, 'grad_norm': 1.3237271308898926, 'learning_rate': 0.00012079603153222029, 'epoch': 3.7954545454545454}\n{'loss': 0.1922, 'grad_norm': 0.6982022523880005, 'learning_rate': 0.00012035517010327059, 'epoch': 3.8181818181818183}\n{'loss': 0.1755, 'grad_norm': 1.0762003660202026, 'learning_rate': 0.00011991430867432086, 'epoch': 3.840909090909091}\n{'loss': 0.2689, 'grad_norm': 1.4446611404418945, 'learning_rate': 0.00011947344724537116, 'epoch': 3.8636363636363638}\n{'loss': 0.197, 'grad_norm': 1.429642677307129, 'learning_rate': 0.00011903258581642144, 'epoch': 3.8863636363636362}\n{'loss': 0.1885, 'grad_norm': 1.3124881982803345, 'learning_rate': 0.00011859172438747174, 'epoch': 3.909090909090909}\n{'loss': 0.1846, 'grad_norm': 1.3689857721328735, 'learning_rate': 0.00011815086295852204, 'epoch': 3.9318181818181817}\n{'loss': 0.2197, 'grad_norm': 1.5337095260620117, 'learning_rate': 0.00011771000152957231, 'epoch': 3.9545454545454546}\n{'loss': 0.2224, 'grad_norm': 0.8425184488296509, 'learning_rate': 0.00011726914010062261, 'epoch': 3.9772727272727275}\n{'loss': 0.3647, 'grad_norm': 4.271055221557617, 'learning_rate': 0.00011682827867167291, 'epoch': 4.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8531    0.8759    0.8643      3142\n     PERCENT     0.8611    0.9394    0.8986        66\n        TYPE     0.9413    0.9686    0.9547     11415\n      VOLUME     0.8125    0.7429    0.7761        70\n\n   micro avg     0.9216    0.9475    0.9344     14693\n   macro avg     0.8670    0.8817    0.8734     14693\nweighted avg     0.9215    0.9475    0.9343     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.25467535853385925, 'eval_f1_macro': 0.8734353259710069, 'eval_precision': 0.9215595419342026, 'eval_recall': 0.9475260328047369, 'eval_f1': 0.9343624161073825, 'eval_accuracy': 0.9258896506692785, 'eval_runtime': 1.7857, 'eval_samples_per_second': 3086.21, 'eval_steps_per_second': 6.16, 'epoch': 4.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.1704, 'grad_norm': 0.7505261898040771, 'learning_rate': 0.0001163874172427232, 'epoch': 4.0227272727272725}\n{'loss': 0.1732, 'grad_norm': 0.8590767979621887, 'learning_rate': 0.00011594655581377348, 'epoch': 4.045454545454546}\n{'loss': 0.1928, 'grad_norm': 1.0914217233657837, 'learning_rate': 0.00011550569438482378, 'epoch': 4.068181818181818}\n{'loss': 0.1768, 'grad_norm': 0.8479724526405334, 'learning_rate': 0.00011506483295587407, 'epoch': 4.090909090909091}\n{'loss': 0.1593, 'grad_norm': 1.0843772888183594, 'learning_rate': 0.00011462397152692436, 'epoch': 4.113636363636363}\n{'loss': 0.161, 'grad_norm': 1.3940389156341553, 'learning_rate': 0.00011418311009797466, 'epoch': 4.136363636363637}\n{'loss': 0.1418, 'grad_norm': 1.642610788345337, 'learning_rate': 0.00011374224866902494, 'epoch': 4.159090909090909}\n{'loss': 0.1695, 'grad_norm': 1.4419889450073242, 'learning_rate': 0.00011330138724007523, 'epoch': 4.181818181818182}\n{'loss': 0.1583, 'grad_norm': 0.9553548097610474, 'learning_rate': 0.00011286052581112553, 'epoch': 4.204545454545454}\n{'loss': 0.1641, 'grad_norm': 1.2704905271530151, 'learning_rate': 0.00011241966438217582, 'epoch': 4.2272727272727275}\n{'loss': 0.207, 'grad_norm': 3.356158494949341, 'learning_rate': 0.0001119788029532261, 'epoch': 4.25}\n{'loss': 0.1753, 'grad_norm': 2.561624526977539, 'learning_rate': 0.00011153794152427639, 'epoch': 4.2727272727272725}\n{'loss': 0.1475, 'grad_norm': 2.4241435527801514, 'learning_rate': 0.00011109708009532669, 'epoch': 4.295454545454546}\n{'loss': 0.1655, 'grad_norm': 1.315045714378357, 'learning_rate': 0.00011065621866637699, 'epoch': 4.318181818181818}\n{'loss': 0.1587, 'grad_norm': 1.0880646705627441, 'learning_rate': 0.00011021535723742726, 'epoch': 4.340909090909091}\n{'loss': 0.176, 'grad_norm': 1.6499351263046265, 'learning_rate': 0.00010977449580847756, 'epoch': 4.363636363636363}\n{'loss': 0.1857, 'grad_norm': 1.2102693319320679, 'learning_rate': 0.00010933363437952786, 'epoch': 4.386363636363637}\n{'loss': 0.14, 'grad_norm': 1.4146515130996704, 'learning_rate': 0.00010889277295057814, 'epoch': 4.409090909090909}\n{'loss': 0.1572, 'grad_norm': 1.6343433856964111, 'learning_rate': 0.00010845191152162844, 'epoch': 4.431818181818182}\n{'loss': 0.1541, 'grad_norm': 1.1055188179016113, 'learning_rate': 0.00010801105009267874, 'epoch': 4.454545454545454}\n{'loss': 0.1944, 'grad_norm': 1.6363433599472046, 'learning_rate': 0.00010757018866372901, 'epoch': 4.4772727272727275}\n{'loss': 0.1639, 'grad_norm': 1.1009494066238403, 'learning_rate': 0.00010712932723477931, 'epoch': 4.5}\n{'loss': 0.1625, 'grad_norm': 1.417869210243225, 'learning_rate': 0.00010668846580582961, 'epoch': 4.5227272727272725}\n{'loss': 0.1235, 'grad_norm': 1.1087573766708374, 'learning_rate': 0.00010624760437687988, 'epoch': 4.545454545454545}\n{'loss': 0.1759, 'grad_norm': 1.4002524614334106, 'learning_rate': 0.00010580674294793018, 'epoch': 4.568181818181818}\n{'loss': 0.1605, 'grad_norm': 0.9559157490730286, 'learning_rate': 0.00010536588151898046, 'epoch': 4.590909090909091}\n{'loss': 0.1342, 'grad_norm': 1.2595237493515015, 'learning_rate': 0.00010492502009003076, 'epoch': 4.613636363636363}\n{'loss': 0.1855, 'grad_norm': 1.3582764863967896, 'learning_rate': 0.00010448415866108106, 'epoch': 4.636363636363637}\n{'loss': 0.1543, 'grad_norm': 2.3613219261169434, 'learning_rate': 0.00010404329723213133, 'epoch': 4.659090909090909}\n{'loss': 0.116, 'grad_norm': 0.8925005197525024, 'learning_rate': 0.00010360243580318163, 'epoch': 4.681818181818182}\n{'loss': 0.1408, 'grad_norm': 1.302125334739685, 'learning_rate': 0.00010316157437423193, 'epoch': 4.704545454545455}\n{'loss': 0.1731, 'grad_norm': 0.976652204990387, 'learning_rate': 0.0001027207129452822, 'epoch': 4.7272727272727275}\n{'loss': 0.124, 'grad_norm': 0.8500556349754333, 'learning_rate': 0.0001022798515163325, 'epoch': 4.75}\n{'loss': 0.1449, 'grad_norm': 1.0775004625320435, 'learning_rate': 0.0001018389900873828, 'epoch': 4.7727272727272725}\n{'loss': 0.1524, 'grad_norm': 1.049318552017212, 'learning_rate': 0.00010139812865843308, 'epoch': 4.795454545454545}\n{'loss': 0.137, 'grad_norm': 1.5727527141571045, 'learning_rate': 0.00010095726722948338, 'epoch': 4.818181818181818}\n{'loss': 0.1724, 'grad_norm': 1.118159532546997, 'learning_rate': 0.00010051640580053368, 'epoch': 4.840909090909091}\n{'loss': 0.1728, 'grad_norm': 1.248038649559021, 'learning_rate': 0.00010007554437158395, 'epoch': 4.863636363636363}\n{'loss': 0.1939, 'grad_norm': 1.2961349487304688, 'learning_rate': 9.963468294263425e-05, 'epoch': 4.886363636363637}\n{'loss': 0.1108, 'grad_norm': 1.0057259798049927, 'learning_rate': 9.919382151368455e-05, 'epoch': 4.909090909090909}\n{'loss': 0.1453, 'grad_norm': 1.065111517906189, 'learning_rate': 9.875296008473482e-05, 'epoch': 4.931818181818182}\n{'loss': 0.1316, 'grad_norm': 0.7784425020217896, 'learning_rate': 9.831209865578512e-05, 'epoch': 4.954545454545455}\n{'loss': 0.1593, 'grad_norm': 1.9299583435058594, 'learning_rate': 9.787123722683541e-05, 'epoch': 4.9772727272727275}\n{'loss': 0.2177, 'grad_norm': 4.751372814178467, 'learning_rate': 9.74303757978857e-05, 'epoch': 5.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8716    0.8835    0.8775      3142\n     PERCENT     0.9118    0.9394    0.9254        66\n        TYPE     0.9471    0.9663    0.9566     11415\n      VOLUME     0.8169    0.8286    0.8227        70\n\n   micro avg     0.9303    0.9478    0.9389     14693\n   macro avg     0.8868    0.9044    0.8955     14693\nweighted avg     0.9302    0.9478    0.9389     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.26043927669525146, 'eval_f1_macro': 0.8955426601480494, 'eval_precision': 0.9302605210420841, 'eval_recall': 0.9477982712856462, 'eval_f1': 0.9389475103664497, 'eval_accuracy': 0.9289911851126347, 'eval_runtime': 1.5268, 'eval_samples_per_second': 3609.606, 'eval_steps_per_second': 7.205, 'epoch': 5.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.1861, 'grad_norm': 2.4291343688964844, 'learning_rate': 9.6989514368936e-05, 'epoch': 5.0227272727272725}\n{'loss': 0.1487, 'grad_norm': 0.7735767364501953, 'learning_rate': 9.654865293998628e-05, 'epoch': 5.045454545454546}\n{'loss': 0.161, 'grad_norm': 0.8676130175590515, 'learning_rate': 9.610779151103658e-05, 'epoch': 5.068181818181818}\n{'loss': 0.1249, 'grad_norm': 1.7648714780807495, 'learning_rate': 9.566693008208687e-05, 'epoch': 5.090909090909091}\n{'loss': 0.1555, 'grad_norm': 1.9221147298812866, 'learning_rate': 9.522606865313716e-05, 'epoch': 5.113636363636363}\n{'loss': 0.1412, 'grad_norm': 1.7699964046478271, 'learning_rate': 9.478520722418746e-05, 'epoch': 5.136363636363637}\n{'loss': 0.1064, 'grad_norm': 0.6839941740036011, 'learning_rate': 9.434434579523774e-05, 'epoch': 5.159090909090909}\n{'loss': 0.1306, 'grad_norm': 0.9987122416496277, 'learning_rate': 9.390348436628803e-05, 'epoch': 5.181818181818182}\n{'loss': 0.097, 'grad_norm': 0.7843207716941833, 'learning_rate': 9.346262293733833e-05, 'epoch': 5.204545454545454}\n{'loss': 0.135, 'grad_norm': 1.9662576913833618, 'learning_rate': 9.302176150838862e-05, 'epoch': 5.2272727272727275}\n{'loss': 0.1059, 'grad_norm': 1.2104694843292236, 'learning_rate': 9.25809000794389e-05, 'epoch': 5.25}\n{'loss': 0.1476, 'grad_norm': 0.7757352590560913, 'learning_rate': 9.21400386504892e-05, 'epoch': 5.2727272727272725}\n{'loss': 0.1468, 'grad_norm': 1.2637708187103271, 'learning_rate': 9.169917722153949e-05, 'epoch': 5.295454545454546}\n{'loss': 0.1134, 'grad_norm': 1.8794203996658325, 'learning_rate': 9.125831579258978e-05, 'epoch': 5.318181818181818}\n{'loss': 0.1488, 'grad_norm': 1.0641191005706787, 'learning_rate': 9.081745436364008e-05, 'epoch': 5.340909090909091}\n{'loss': 0.1258, 'grad_norm': 2.1485297679901123, 'learning_rate': 9.037659293469035e-05, 'epoch': 5.363636363636363}\n{'loss': 0.1545, 'grad_norm': 1.2480007410049438, 'learning_rate': 8.993573150574065e-05, 'epoch': 5.386363636363637}\n{'loss': 0.1567, 'grad_norm': 1.0683749914169312, 'learning_rate': 8.949487007679095e-05, 'epoch': 5.409090909090909}\n{'loss': 0.1275, 'grad_norm': 1.1325525045394897, 'learning_rate': 8.905400864784122e-05, 'epoch': 5.431818181818182}\n{'loss': 0.121, 'grad_norm': 1.0449843406677246, 'learning_rate': 8.861314721889152e-05, 'epoch': 5.454545454545454}\n{'loss': 0.1344, 'grad_norm': 1.1550722122192383, 'learning_rate': 8.817228578994182e-05, 'epoch': 5.4772727272727275}\n{'loss': 0.0977, 'grad_norm': 0.907715916633606, 'learning_rate': 8.77314243609921e-05, 'epoch': 5.5}\n{'loss': 0.1462, 'grad_norm': 1.4399242401123047, 'learning_rate': 8.72905629320424e-05, 'epoch': 5.5227272727272725}\n{'loss': 0.0898, 'grad_norm': 1.1646692752838135, 'learning_rate': 8.684970150309268e-05, 'epoch': 5.545454545454545}\n{'loss': 0.1222, 'grad_norm': 0.7574482560157776, 'learning_rate': 8.640884007414298e-05, 'epoch': 5.568181818181818}\n{'loss': 0.1197, 'grad_norm': 0.9826157689094543, 'learning_rate': 8.596797864519327e-05, 'epoch': 5.590909090909091}\n{'loss': 0.1381, 'grad_norm': 1.0722239017486572, 'learning_rate': 8.552711721624356e-05, 'epoch': 5.613636363636363}\n{'loss': 0.1388, 'grad_norm': 0.9551531076431274, 'learning_rate': 8.508625578729384e-05, 'epoch': 5.636363636363637}\n{'loss': 0.1179, 'grad_norm': 0.903331458568573, 'learning_rate': 8.464539435834414e-05, 'epoch': 5.659090909090909}\n{'loss': 0.1367, 'grad_norm': 1.4069430828094482, 'learning_rate': 8.420453292939443e-05, 'epoch': 5.681818181818182}\n{'loss': 0.1125, 'grad_norm': 1.1562471389770508, 'learning_rate': 8.376367150044472e-05, 'epoch': 5.704545454545455}\n{'loss': 0.117, 'grad_norm': 0.8890665173530579, 'learning_rate': 8.332281007149502e-05, 'epoch': 5.7272727272727275}\n{'loss': 0.1119, 'grad_norm': 0.7235842347145081, 'learning_rate': 8.28819486425453e-05, 'epoch': 5.75}\n{'loss': 0.1208, 'grad_norm': 0.9784567952156067, 'learning_rate': 8.244108721359559e-05, 'epoch': 5.7727272727272725}\n{'loss': 0.1348, 'grad_norm': 1.9982612133026123, 'learning_rate': 8.200022578464589e-05, 'epoch': 5.795454545454545}\n{'loss': 0.1385, 'grad_norm': 1.6518750190734863, 'learning_rate': 8.155936435569618e-05, 'epoch': 5.818181818181818}\n{'loss': 0.1243, 'grad_norm': 1.7441798448562622, 'learning_rate': 8.111850292674646e-05, 'epoch': 5.840909090909091}\n{'loss': 0.1302, 'grad_norm': 1.4575926065444946, 'learning_rate': 8.067764149779675e-05, 'epoch': 5.863636363636363}\n{'loss': 0.1126, 'grad_norm': 0.6775601506233215, 'learning_rate': 8.023678006884705e-05, 'epoch': 5.886363636363637}\n{'loss': 0.1503, 'grad_norm': 1.7167552709579468, 'learning_rate': 7.979591863989734e-05, 'epoch': 5.909090909090909}\n{'loss': 0.0991, 'grad_norm': 0.8850265145301819, 'learning_rate': 7.935505721094764e-05, 'epoch': 5.931818181818182}\n{'loss': 0.1269, 'grad_norm': 0.7900460362434387, 'learning_rate': 7.891419578199792e-05, 'epoch': 5.954545454545455}\n{'loss': 0.1242, 'grad_norm': 0.8153190612792969, 'learning_rate': 7.847333435304821e-05, 'epoch': 5.9772727272727275}\n{'loss': 0.0436, 'grad_norm': 1.7534750699996948, 'learning_rate': 7.803247292409851e-05, 'epoch': 6.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8836    0.8797    0.8817      3142\n     PERCENT     0.8857    0.9394    0.9118        66\n        TYPE     0.9490    0.9644    0.9566     11415\n      VOLUME     0.8182    0.7714    0.7941        70\n\n   micro avg     0.9343    0.9453    0.9398     14693\n   macro avg     0.8841    0.8887    0.8860     14693\nweighted avg     0.9341    0.9453    0.9396     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.25745484232902527, 'eval_f1_macro': 0.886044976262429, 'eval_precision': 0.9343424150689539, 'eval_recall': 0.9452800653372354, 'eval_f1': 0.9397794167399688, 'eval_accuracy': 0.929752965502231, 'eval_runtime': 1.6307, 'eval_samples_per_second': 3379.548, 'eval_steps_per_second': 6.746, 'epoch': 6.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.098, 'grad_norm': 0.9929349422454834, 'learning_rate': 7.75916114951488e-05, 'epoch': 6.0227272727272725}\n{'loss': 0.0963, 'grad_norm': 0.6450056433677673, 'learning_rate': 7.71507500661991e-05, 'epoch': 6.045454545454546}\n{'loss': 0.1174, 'grad_norm': 0.9540995359420776, 'learning_rate': 7.670988863724938e-05, 'epoch': 6.068181818181818}\n{'loss': 0.1178, 'grad_norm': 1.395686149597168, 'learning_rate': 7.626902720829967e-05, 'epoch': 6.090909090909091}\n{'loss': 0.1091, 'grad_norm': 0.8300400376319885, 'learning_rate': 7.582816577934997e-05, 'epoch': 6.113636363636363}\n{'loss': 0.1173, 'grad_norm': 0.8160837888717651, 'learning_rate': 7.538730435040026e-05, 'epoch': 6.136363636363637}\n{'loss': 0.1001, 'grad_norm': 0.9738109707832336, 'learning_rate': 7.494644292145054e-05, 'epoch': 6.159090909090909}\n{'loss': 0.0856, 'grad_norm': 1.0550706386566162, 'learning_rate': 7.450558149250083e-05, 'epoch': 6.181818181818182}\n{'loss': 0.0883, 'grad_norm': 1.0309600830078125, 'learning_rate': 7.406472006355113e-05, 'epoch': 6.204545454545454}\n{'loss': 0.0875, 'grad_norm': 0.7387747168540955, 'learning_rate': 7.362385863460142e-05, 'epoch': 6.2272727272727275}\n{'loss': 0.093, 'grad_norm': 0.8154158592224121, 'learning_rate': 7.31829972056517e-05, 'epoch': 6.25}\n{'loss': 0.1551, 'grad_norm': 1.7791566848754883, 'learning_rate': 7.2742135776702e-05, 'epoch': 6.2727272727272725}\n{'loss': 0.0985, 'grad_norm': 1.1854302883148193, 'learning_rate': 7.230127434775229e-05, 'epoch': 6.295454545454546}\n{'loss': 0.1277, 'grad_norm': 1.381801962852478, 'learning_rate': 7.186041291880258e-05, 'epoch': 6.318181818181818}\n{'loss': 0.1311, 'grad_norm': 1.6237715482711792, 'learning_rate': 7.141955148985288e-05, 'epoch': 6.340909090909091}\n{'loss': 0.121, 'grad_norm': 0.9416300058364868, 'learning_rate': 7.097869006090316e-05, 'epoch': 6.363636363636363}\n{'loss': 0.1268, 'grad_norm': 1.5840057134628296, 'learning_rate': 7.053782863195345e-05, 'epoch': 6.386363636363637}\n{'loss': 0.1061, 'grad_norm': 1.9085084199905396, 'learning_rate': 7.009696720300374e-05, 'epoch': 6.409090909090909}\n{'loss': 0.1031, 'grad_norm': 1.84988272190094, 'learning_rate': 6.965610577405404e-05, 'epoch': 6.431818181818182}\n{'loss': 0.0998, 'grad_norm': 1.1154906749725342, 'learning_rate': 6.921524434510432e-05, 'epoch': 6.454545454545454}\n{'loss': 0.1118, 'grad_norm': 1.008547306060791, 'learning_rate': 6.877438291615461e-05, 'epoch': 6.4772727272727275}\n{'loss': 0.0763, 'grad_norm': 0.9340552091598511, 'learning_rate': 6.833352148720491e-05, 'epoch': 6.5}\n{'loss': 0.1184, 'grad_norm': 1.2933902740478516, 'learning_rate': 6.78926600582552e-05, 'epoch': 6.5227272727272725}\n{'loss': 0.0917, 'grad_norm': 0.992876410484314, 'learning_rate': 6.745179862930548e-05, 'epoch': 6.545454545454545}\n{'loss': 0.1353, 'grad_norm': 1.0268932580947876, 'learning_rate': 6.701093720035577e-05, 'epoch': 6.568181818181818}\n{'loss': 0.1329, 'grad_norm': 1.1169039011001587, 'learning_rate': 6.657007577140607e-05, 'epoch': 6.590909090909091}\n{'loss': 0.1024, 'grad_norm': 1.3967890739440918, 'learning_rate': 6.612921434245636e-05, 'epoch': 6.613636363636363}\n{'loss': 0.1133, 'grad_norm': 1.0536015033721924, 'learning_rate': 6.568835291350664e-05, 'epoch': 6.636363636363637}\n{'loss': 0.0884, 'grad_norm': 0.8742184042930603, 'learning_rate': 6.524749148455694e-05, 'epoch': 6.659090909090909}\n{'loss': 0.1178, 'grad_norm': 1.0093579292297363, 'learning_rate': 6.480663005560723e-05, 'epoch': 6.681818181818182}\n{'loss': 0.0971, 'grad_norm': 0.6869179010391235, 'learning_rate': 6.436576862665752e-05, 'epoch': 6.704545454545455}\n{'loss': 0.099, 'grad_norm': 0.834008514881134, 'learning_rate': 6.392490719770782e-05, 'epoch': 6.7272727272727275}\n{'loss': 0.1158, 'grad_norm': 1.2103214263916016, 'learning_rate': 6.34840457687581e-05, 'epoch': 6.75}\n{'loss': 0.103, 'grad_norm': 0.8157756328582764, 'learning_rate': 6.304318433980839e-05, 'epoch': 6.7727272727272725}\n{'loss': 0.1295, 'grad_norm': 1.4483689069747925, 'learning_rate': 6.260232291085869e-05, 'epoch': 6.795454545454545}\n{'loss': 0.1299, 'grad_norm': 1.099937081336975, 'learning_rate': 6.216146148190898e-05, 'epoch': 6.818181818181818}\n{'loss': 0.1099, 'grad_norm': 0.7665405869483948, 'learning_rate': 6.172060005295926e-05, 'epoch': 6.840909090909091}\n{'loss': 0.1133, 'grad_norm': 1.5880385637283325, 'learning_rate': 6.127973862400956e-05, 'epoch': 6.863636363636363}\n{'loss': 0.115, 'grad_norm': 1.27183997631073, 'learning_rate': 6.083887719505986e-05, 'epoch': 6.886363636363637}\n{'loss': 0.0898, 'grad_norm': 0.680787205696106, 'learning_rate': 6.0398015766110145e-05, 'epoch': 6.909090909090909}\n{'loss': 0.1052, 'grad_norm': 0.8165555596351624, 'learning_rate': 5.995715433716043e-05, 'epoch': 6.931818181818182}\n{'loss': 0.1152, 'grad_norm': 0.9318119287490845, 'learning_rate': 5.951629290821072e-05, 'epoch': 6.954545454545455}\n{'loss': 0.1057, 'grad_norm': 1.0791964530944824, 'learning_rate': 5.907543147926102e-05, 'epoch': 6.9772727272727275}\n{'loss': 0.069, 'grad_norm': 4.402195930480957, 'learning_rate': 5.8634570050311305e-05, 'epoch': 7.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8594    0.9045    0.8814      3142\n     PERCENT     0.8986    0.9394    0.9185        66\n        TYPE     0.9532    0.9574    0.9553     11415\n      VOLUME     0.8551    0.8429    0.8489        70\n\n   micro avg     0.9317    0.9455    0.9385     14693\n   macro avg     0.8915    0.9110    0.9010     14693\nweighted avg     0.9324    0.9455    0.9388     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.2638802230358124, 'eval_f1_macro': 0.9010266887811035, 'eval_precision': 0.9316611897257059, 'eval_recall': 0.9454842441979173, 'eval_f1': 0.9385218213754897, 'eval_accuracy': 0.9286647078028077, 'eval_runtime': 1.5636, 'eval_samples_per_second': 3524.6, 'eval_steps_per_second': 7.035, 'epoch': 7.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.0939, 'grad_norm': 1.0459561347961426, 'learning_rate': 5.81937086213616e-05, 'epoch': 7.0227272727272725}\n{'loss': 0.0833, 'grad_norm': 1.0977627038955688, 'learning_rate': 5.775284719241189e-05, 'epoch': 7.045454545454546}\n{'loss': 0.1064, 'grad_norm': 1.2824440002441406, 'learning_rate': 5.731198576346218e-05, 'epoch': 7.068181818181818}\n{'loss': 0.094, 'grad_norm': 1.2087652683258057, 'learning_rate': 5.687112433451247e-05, 'epoch': 7.090909090909091}\n{'loss': 0.1015, 'grad_norm': 1.0778148174285889, 'learning_rate': 5.6430262905562765e-05, 'epoch': 7.113636363636363}\n{'loss': 0.0697, 'grad_norm': 0.6277651786804199, 'learning_rate': 5.598940147661305e-05, 'epoch': 7.136363636363637}\n{'loss': 0.0704, 'grad_norm': 0.8595600128173828, 'learning_rate': 5.5548540047663345e-05, 'epoch': 7.159090909090909}\n{'loss': 0.0677, 'grad_norm': 0.7471295595169067, 'learning_rate': 5.510767861871363e-05, 'epoch': 7.181818181818182}\n{'loss': 0.0826, 'grad_norm': 1.0908892154693604, 'learning_rate': 5.466681718976393e-05, 'epoch': 7.204545454545454}\n{'loss': 0.1015, 'grad_norm': 1.5932894945144653, 'learning_rate': 5.422595576081422e-05, 'epoch': 7.2272727272727275}\n{'loss': 0.1086, 'grad_norm': 2.0563924312591553, 'learning_rate': 5.3785094331864505e-05, 'epoch': 7.25}\n{'loss': 0.088, 'grad_norm': 1.413643717765808, 'learning_rate': 5.3344232902914805e-05, 'epoch': 7.2727272727272725}\n{'loss': 0.1049, 'grad_norm': 1.2273098230361938, 'learning_rate': 5.290337147396509e-05, 'epoch': 7.295454545454546}\n{'loss': 0.1075, 'grad_norm': 0.8674771785736084, 'learning_rate': 5.246251004501538e-05, 'epoch': 7.318181818181818}\n{'loss': 0.0959, 'grad_norm': 1.0093766450881958, 'learning_rate': 5.2021648616065665e-05, 'epoch': 7.340909090909091}\n{'loss': 0.1242, 'grad_norm': 1.082824468612671, 'learning_rate': 5.1580787187115965e-05, 'epoch': 7.363636363636363}\n{'loss': 0.0902, 'grad_norm': 1.8758653402328491, 'learning_rate': 5.113992575816625e-05, 'epoch': 7.386363636363637}\n{'loss': 0.1299, 'grad_norm': 1.7387107610702515, 'learning_rate': 5.069906432921654e-05, 'epoch': 7.409090909090909}\n{'loss': 0.0832, 'grad_norm': 1.5432052612304688, 'learning_rate': 5.025820290026684e-05, 'epoch': 7.431818181818182}\n{'loss': 0.0916, 'grad_norm': 1.3971766233444214, 'learning_rate': 4.9817341471317125e-05, 'epoch': 7.454545454545454}\n{'loss': 0.0749, 'grad_norm': 0.7619962692260742, 'learning_rate': 4.937648004236741e-05, 'epoch': 7.4772727272727275}\n{'loss': 0.0792, 'grad_norm': 0.9608882069587708, 'learning_rate': 4.8935618613417705e-05, 'epoch': 7.5}\n{'loss': 0.0995, 'grad_norm': 1.155133843421936, 'learning_rate': 4.8494757184468e-05, 'epoch': 7.5227272727272725}\n{'loss': 0.1215, 'grad_norm': 1.6379255056381226, 'learning_rate': 4.805389575551829e-05, 'epoch': 7.545454545454545}\n{'loss': 0.1137, 'grad_norm': 1.3096674680709839, 'learning_rate': 4.761303432656858e-05, 'epoch': 7.568181818181818}\n{'loss': 0.0704, 'grad_norm': 0.8692843317985535, 'learning_rate': 4.717217289761887e-05, 'epoch': 7.590909090909091}\n{'loss': 0.1005, 'grad_norm': 1.262515664100647, 'learning_rate': 4.6731311468669165e-05, 'epoch': 7.613636363636363}\n{'loss': 0.0635, 'grad_norm': 0.7039009928703308, 'learning_rate': 4.629045003971945e-05, 'epoch': 7.636363636363637}\n{'loss': 0.1166, 'grad_norm': 1.461982250213623, 'learning_rate': 4.5849588610769745e-05, 'epoch': 7.659090909090909}\n{'loss': 0.0789, 'grad_norm': 0.7935266494750977, 'learning_rate': 4.540872718182004e-05, 'epoch': 7.681818181818182}\n{'loss': 0.0836, 'grad_norm': 0.6510710120201111, 'learning_rate': 4.4967865752870325e-05, 'epoch': 7.704545454545455}\n{'loss': 0.0686, 'grad_norm': 0.9518667459487915, 'learning_rate': 4.452700432392061e-05, 'epoch': 7.7272727272727275}\n{'loss': 0.0683, 'grad_norm': 0.7654851078987122, 'learning_rate': 4.408614289497091e-05, 'epoch': 7.75}\n{'loss': 0.1103, 'grad_norm': 0.8738296031951904, 'learning_rate': 4.36452814660212e-05, 'epoch': 7.7727272727272725}\n{'loss': 0.0825, 'grad_norm': 0.935350239276886, 'learning_rate': 4.320442003707149e-05, 'epoch': 7.795454545454545}\n{'loss': 0.0831, 'grad_norm': 0.9979333877563477, 'learning_rate': 4.276355860812178e-05, 'epoch': 7.818181818181818}\n{'loss': 0.09, 'grad_norm': 0.734885036945343, 'learning_rate': 4.232269717917207e-05, 'epoch': 7.840909090909091}\n{'loss': 0.1296, 'grad_norm': 0.8401724696159363, 'learning_rate': 4.188183575022236e-05, 'epoch': 7.863636363636363}\n{'loss': 0.0723, 'grad_norm': 0.7079057693481445, 'learning_rate': 4.144097432127265e-05, 'epoch': 7.886363636363637}\n{'loss': 0.0723, 'grad_norm': 0.7679221034049988, 'learning_rate': 4.1000112892322945e-05, 'epoch': 7.909090909090909}\n{'loss': 0.1026, 'grad_norm': 1.0157798528671265, 'learning_rate': 4.055925146337323e-05, 'epoch': 7.931818181818182}\n{'loss': 0.0931, 'grad_norm': 0.9477999806404114, 'learning_rate': 4.0118390034423525e-05, 'epoch': 7.954545454545455}\n{'loss': 0.1066, 'grad_norm': 0.8246821165084839, 'learning_rate': 3.967752860547382e-05, 'epoch': 7.9772727272727275}\n{'loss': 0.0315, 'grad_norm': 2.1448936462402344, 'learning_rate': 3.9236667176524105e-05, 'epoch': 8.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8839    0.8889    0.8864      3142\n     PERCENT     0.8986    0.9394    0.9185        66\n        TYPE     0.9516    0.9636    0.9576     11415\n      VOLUME     0.8889    0.9143    0.9014        70\n\n   micro avg     0.9367    0.9473    0.9419     14693\n   macro avg     0.9057    0.9265    0.9160     14693\nweighted avg     0.9366    0.9473    0.9419     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.26629602909088135, 'eval_f1_macro': 0.9159677792997298, 'eval_precision': 0.9366713776162595, 'eval_recall': 0.9472537943238277, 'eval_f1': 0.9419328641039524, 'eval_accuracy': 0.9315485907062793, 'eval_runtime': 1.9232, 'eval_samples_per_second': 2865.517, 'eval_steps_per_second': 5.72, 'epoch': 8.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.0817, 'grad_norm': 0.8695593476295471, 'learning_rate': 3.87958057475744e-05, 'epoch': 8.022727272727273}\n{'loss': 0.0808, 'grad_norm': 0.9812735319137573, 'learning_rate': 3.835494431862469e-05, 'epoch': 8.045454545454545}\n{'loss': 0.0841, 'grad_norm': 0.9402951598167419, 'learning_rate': 3.7914082889674985e-05, 'epoch': 8.068181818181818}\n{'loss': 0.119, 'grad_norm': 1.182245135307312, 'learning_rate': 3.747322146072527e-05, 'epoch': 8.090909090909092}\n{'loss': 0.068, 'grad_norm': 0.8106023669242859, 'learning_rate': 3.7032360031775565e-05, 'epoch': 8.113636363636363}\n{'loss': 0.1022, 'grad_norm': 1.0714023113250732, 'learning_rate': 3.659149860282585e-05, 'epoch': 8.136363636363637}\n{'loss': 0.0604, 'grad_norm': 1.3530869483947754, 'learning_rate': 3.6150637173876145e-05, 'epoch': 8.159090909090908}\n{'loss': 0.0944, 'grad_norm': 1.5437755584716797, 'learning_rate': 3.570977574492644e-05, 'epoch': 8.181818181818182}\n{'loss': 0.0989, 'grad_norm': 1.1527422666549683, 'learning_rate': 3.5268914315976725e-05, 'epoch': 8.204545454545455}\n{'loss': 0.0776, 'grad_norm': 1.2547823190689087, 'learning_rate': 3.482805288702702e-05, 'epoch': 8.227272727272727}\n{'loss': 0.0831, 'grad_norm': 1.2182012796401978, 'learning_rate': 3.4387191458077305e-05, 'epoch': 8.25}\n{'loss': 0.0721, 'grad_norm': 1.0029335021972656, 'learning_rate': 3.39463300291276e-05, 'epoch': 8.272727272727273}\n{'loss': 0.1004, 'grad_norm': 1.0434224605560303, 'learning_rate': 3.3505468600177885e-05, 'epoch': 8.295454545454545}\n{'loss': 0.0891, 'grad_norm': 0.8866637945175171, 'learning_rate': 3.306460717122818e-05, 'epoch': 8.318181818181818}\n{'loss': 0.0779, 'grad_norm': 0.983306884765625, 'learning_rate': 3.262374574227847e-05, 'epoch': 8.340909090909092}\n{'loss': 0.085, 'grad_norm': 1.0469459295272827, 'learning_rate': 3.218288431332876e-05, 'epoch': 8.363636363636363}\n{'loss': 0.0465, 'grad_norm': 0.7506548762321472, 'learning_rate': 3.174202288437905e-05, 'epoch': 8.386363636363637}\n{'loss': 0.0703, 'grad_norm': 1.3000524044036865, 'learning_rate': 3.1301161455429346e-05, 'epoch': 8.409090909090908}\n{'loss': 0.1087, 'grad_norm': 1.3175147771835327, 'learning_rate': 3.086030002647963e-05, 'epoch': 8.431818181818182}\n{'loss': 0.0599, 'grad_norm': 0.7744356989860535, 'learning_rate': 3.041943859752993e-05, 'epoch': 8.454545454545455}\n{'loss': 0.0857, 'grad_norm': 1.2708425521850586, 'learning_rate': 2.9978577168580216e-05, 'epoch': 8.477272727272727}\n{'loss': 0.0659, 'grad_norm': 1.4474139213562012, 'learning_rate': 2.953771573963051e-05, 'epoch': 8.5}\n{'loss': 0.0694, 'grad_norm': 0.9850687980651855, 'learning_rate': 2.90968543106808e-05, 'epoch': 8.522727272727273}\n{'loss': 0.0834, 'grad_norm': 0.7667854428291321, 'learning_rate': 2.865599288173109e-05, 'epoch': 8.545454545454545}\n{'loss': 0.1004, 'grad_norm': 1.1783047914505005, 'learning_rate': 2.8215131452781382e-05, 'epoch': 8.568181818181818}\n{'loss': 0.0877, 'grad_norm': 0.7258185148239136, 'learning_rate': 2.7774270023831672e-05, 'epoch': 8.590909090909092}\n{'loss': 0.0674, 'grad_norm': 0.813702404499054, 'learning_rate': 2.7333408594881966e-05, 'epoch': 8.613636363636363}\n{'loss': 0.0802, 'grad_norm': 0.8235552906990051, 'learning_rate': 2.6892547165932252e-05, 'epoch': 8.636363636363637}\n{'loss': 0.0838, 'grad_norm': 1.2420680522918701, 'learning_rate': 2.6451685736982546e-05, 'epoch': 8.659090909090908}\n{'loss': 0.0869, 'grad_norm': 0.9859963059425354, 'learning_rate': 2.6010824308032832e-05, 'epoch': 8.681818181818182}\n{'loss': 0.0692, 'grad_norm': 0.8761109113693237, 'learning_rate': 2.5569962879083126e-05, 'epoch': 8.704545454545455}\n{'loss': 0.0944, 'grad_norm': 1.0164021253585815, 'learning_rate': 2.512910145013342e-05, 'epoch': 8.727272727272727}\n{'loss': 0.0853, 'grad_norm': 0.7041755318641663, 'learning_rate': 2.4688240021183706e-05, 'epoch': 8.75}\n{'loss': 0.0835, 'grad_norm': 0.9608291387557983, 'learning_rate': 2.4247378592234e-05, 'epoch': 8.772727272727273}\n{'loss': 0.1109, 'grad_norm': 0.8087114691734314, 'learning_rate': 2.380651716328429e-05, 'epoch': 8.795454545454545}\n{'loss': 0.0657, 'grad_norm': 0.8702490329742432, 'learning_rate': 2.3365655734334583e-05, 'epoch': 8.818181818181818}\n{'loss': 0.1158, 'grad_norm': 0.8916336894035339, 'learning_rate': 2.2924794305384873e-05, 'epoch': 8.840909090909092}\n{'loss': 0.0633, 'grad_norm': 0.8917508125305176, 'learning_rate': 2.2483932876435163e-05, 'epoch': 8.863636363636363}\n{'loss': 0.0905, 'grad_norm': 1.1164828538894653, 'learning_rate': 2.2043071447485456e-05, 'epoch': 8.886363636363637}\n{'loss': 0.0723, 'grad_norm': 1.3653773069381714, 'learning_rate': 2.1602210018535746e-05, 'epoch': 8.909090909090908}\n{'loss': 0.1047, 'grad_norm': 1.770398497581482, 'learning_rate': 2.1161348589586036e-05, 'epoch': 8.931818181818182}\n{'loss': 0.0959, 'grad_norm': 0.837526261806488, 'learning_rate': 2.0720487160636326e-05, 'epoch': 8.954545454545455}\n{'loss': 0.0607, 'grad_norm': 0.6875342726707458, 'learning_rate': 2.0279625731686616e-05, 'epoch': 8.977272727272727}\n{'loss': 0.0328, 'grad_norm': 2.392923355102539, 'learning_rate': 1.983876430273691e-05, 'epoch': 9.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8630    0.9023    0.8822      3142\n     PERCENT     0.9118    0.9394    0.9254        66\n        TYPE     0.9520    0.9598    0.9559     11415\n      VOLUME     0.8750    0.9000    0.8873        70\n\n   micro avg     0.9319    0.9471    0.9394     14693\n   macro avg     0.9005    0.9254    0.9127     14693\nweighted avg     0.9324    0.9471    0.9397     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.2764631509780884, 'eval_f1_macro': 0.9127021389648036, 'eval_precision': 0.9318958012455635, 'eval_recall': 0.9471176750833731, 'eval_f1': 0.9394450820225478, 'eval_accuracy': 0.9293176624224616, 'eval_runtime': 1.6206, 'eval_samples_per_second': 3400.656, 'eval_steps_per_second': 6.788, 'epoch': 9.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.0849, 'grad_norm': 1.222611904144287, 'learning_rate': 1.93979028737872e-05, 'epoch': 9.022727272727273}\n{'loss': 0.0741, 'grad_norm': 1.0928950309753418, 'learning_rate': 1.8957041444837493e-05, 'epoch': 9.045454545454545}\n{'loss': 0.0751, 'grad_norm': 0.9462174773216248, 'learning_rate': 1.8516180015887783e-05, 'epoch': 9.068181818181818}\n{'loss': 0.0917, 'grad_norm': 1.4663020372390747, 'learning_rate': 1.8075318586938073e-05, 'epoch': 9.090909090909092}\n{'loss': 0.0701, 'grad_norm': 0.9924014210700989, 'learning_rate': 1.7634457157988363e-05, 'epoch': 9.113636363636363}\n{'loss': 0.057, 'grad_norm': 0.9327152371406555, 'learning_rate': 1.7193595729038653e-05, 'epoch': 9.136363636363637}\n{'loss': 0.0883, 'grad_norm': 0.9810450077056885, 'learning_rate': 1.6752734300088943e-05, 'epoch': 9.159090909090908}\n{'loss': 0.0774, 'grad_norm': 0.7604455351829529, 'learning_rate': 1.6311872871139236e-05, 'epoch': 9.181818181818182}\n{'loss': 0.0828, 'grad_norm': 0.9757314324378967, 'learning_rate': 1.5871011442189526e-05, 'epoch': 9.204545454545455}\n{'loss': 0.0792, 'grad_norm': 0.8745925426483154, 'learning_rate': 1.5430150013239816e-05, 'epoch': 9.227272727272727}\n{'loss': 0.0616, 'grad_norm': 1.227110743522644, 'learning_rate': 1.4989288584290108e-05, 'epoch': 9.25}\n{'loss': 0.0833, 'grad_norm': 0.9624513983726501, 'learning_rate': 1.45484271553404e-05, 'epoch': 9.272727272727273}\n{'loss': 0.1112, 'grad_norm': 1.1468571424484253, 'learning_rate': 1.4107565726390691e-05, 'epoch': 9.295454545454545}\n{'loss': 0.0867, 'grad_norm': 1.131516456604004, 'learning_rate': 1.3666704297440983e-05, 'epoch': 9.318181818181818}\n{'loss': 0.1023, 'grad_norm': 0.9511717557907104, 'learning_rate': 1.3225842868491273e-05, 'epoch': 9.340909090909092}\n{'loss': 0.0894, 'grad_norm': 1.1319385766983032, 'learning_rate': 1.2784981439541563e-05, 'epoch': 9.363636363636363}\n{'loss': 0.0795, 'grad_norm': 1.117592692375183, 'learning_rate': 1.2344120010591853e-05, 'epoch': 9.386363636363637}\n{'loss': 0.0782, 'grad_norm': 1.2040228843688965, 'learning_rate': 1.1903258581642145e-05, 'epoch': 9.409090909090908}\n{'loss': 0.0801, 'grad_norm': 1.1342034339904785, 'learning_rate': 1.1462397152692436e-05, 'epoch': 9.431818181818182}\n{'loss': 0.0563, 'grad_norm': 1.0051089525222778, 'learning_rate': 1.1021535723742728e-05, 'epoch': 9.454545454545455}\n{'loss': 0.0735, 'grad_norm': 0.9701492190361023, 'learning_rate': 1.0580674294793018e-05, 'epoch': 9.477272727272727}\n{'loss': 0.0926, 'grad_norm': 1.1476151943206787, 'learning_rate': 1.0139812865843308e-05, 'epoch': 9.5}\n{'loss': 0.0926, 'grad_norm': 0.948008120059967, 'learning_rate': 9.6989514368936e-06, 'epoch': 9.522727272727273}\n{'loss': 0.0679, 'grad_norm': 1.566570520401001, 'learning_rate': 9.258090007943891e-06, 'epoch': 9.545454545454545}\n{'loss': 0.0722, 'grad_norm': 0.671203076839447, 'learning_rate': 8.817228578994181e-06, 'epoch': 9.568181818181818}\n{'loss': 0.0971, 'grad_norm': 1.5259817838668823, 'learning_rate': 8.376367150044471e-06, 'epoch': 9.590909090909092}\n{'loss': 0.0912, 'grad_norm': 0.9589458107948303, 'learning_rate': 7.935505721094763e-06, 'epoch': 9.613636363636363}\n{'loss': 0.0985, 'grad_norm': 0.7442756295204163, 'learning_rate': 7.494644292145054e-06, 'epoch': 9.636363636363637}\n{'loss': 0.0606, 'grad_norm': 0.8102892637252808, 'learning_rate': 7.053782863195346e-06, 'epoch': 9.659090909090908}\n{'loss': 0.0896, 'grad_norm': 0.7573778033256531, 'learning_rate': 6.6129214342456364e-06, 'epoch': 9.681818181818182}\n{'loss': 0.0877, 'grad_norm': 1.006000280380249, 'learning_rate': 6.1720600052959264e-06, 'epoch': 9.704545454545455}\n{'loss': 0.094, 'grad_norm': 0.8990130424499512, 'learning_rate': 5.731198576346218e-06, 'epoch': 9.727272727272727}\n{'loss': 0.1001, 'grad_norm': 1.0642400979995728, 'learning_rate': 5.290337147396509e-06, 'epoch': 9.75}\n{'loss': 0.0599, 'grad_norm': 1.051310420036316, 'learning_rate': 4.8494757184468e-06, 'epoch': 9.772727272727273}\n{'loss': 0.0844, 'grad_norm': 0.8984681963920593, 'learning_rate': 4.408614289497091e-06, 'epoch': 9.795454545454545}\n{'loss': 0.1086, 'grad_norm': 1.076962947845459, 'learning_rate': 3.9677528605473815e-06, 'epoch': 9.818181818181818}\n{'loss': 0.0729, 'grad_norm': 0.7065216302871704, 'learning_rate': 3.526891431597673e-06, 'epoch': 9.840909090909092}\n{'loss': 0.0574, 'grad_norm': 0.6501045823097229, 'learning_rate': 3.0860300026479632e-06, 'epoch': 9.863636363636363}\n{'loss': 0.083, 'grad_norm': 0.7103601694107056, 'learning_rate': 2.6451685736982545e-06, 'epoch': 9.886363636363637}\n{'loss': 0.0719, 'grad_norm': 0.8272677659988403, 'learning_rate': 2.2043071447485453e-06, 'epoch': 9.909090909090908}\n{'loss': 0.0871, 'grad_norm': 0.7775821089744568, 'learning_rate': 1.7634457157988364e-06, 'epoch': 9.931818181818182}\n{'loss': 0.0804, 'grad_norm': 1.0284744501113892, 'learning_rate': 1.3225842868491272e-06, 'epoch': 9.954545454545455}\n{'loss': 0.0597, 'grad_norm': 0.8648906350135803, 'learning_rate': 8.817228578994182e-07, 'epoch': 9.977272727272727}\n{'loss': 0.0261, 'grad_norm': 2.6100354194641113, 'learning_rate': 4.408614289497091e-07, 'epoch': 10.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8691    0.9004    0.8845      3142\n     PERCENT     0.9118    0.9394    0.9254        66\n        TYPE     0.9518    0.9621    0.9569     11415\n      VOLUME     0.8750    0.9000    0.8873        70\n\n   micro avg     0.9332    0.9485    0.9408     14693\n   macro avg     0.9019    0.9255    0.9135     14693\nweighted avg     0.9336    0.9485    0.9409     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.27754920721054077, 'eval_f1_macro': 0.9135215317844692, 'eval_precision': 0.9332351168552869, 'eval_recall': 0.9484788674879194, 'eval_f1': 0.9407952474178086, 'eval_accuracy': 0.9301882685820002, 'eval_runtime': 1.6148, 'eval_samples_per_second': 3412.75, 'eval_steps_per_second': 6.812, 'epoch': 10.0}\n{'train_runtime': 64.6401, 'train_samples_per_second': 3409.804, 'train_steps_per_second': 6.807, 'train_loss': 0.29401881010694936, 'epoch': 10.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nThe speedups for torchdynamo mostly come with GPU Ampere or higher and which is not detected here.\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8839    0.8889    0.8864      3142\n     PERCENT     0.8986    0.9394    0.9185        66\n        TYPE     0.9516    0.9636    0.9576     11415\n      VOLUME     0.8889    0.9143    0.9014        70\n\n   micro avg     0.9367    0.9473    0.9419     14693\n   macro avg     0.9057    0.9265    0.9160     14693\nweighted avg     0.9366    0.9473    0.9419     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.26629602909088135, 'eval_f1_macro': 0.9159677792997298, 'eval_precision': 0.9366713776162595, 'eval_recall': 0.9472537943238277, 'eval_f1': 0.9419328641039524, 'eval_accuracy': 0.9315485907062793, 'eval_runtime': 1.6403, 'eval_samples_per_second': 3359.743, 'eval_steps_per_second': 6.706, 'epoch': 10.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/3364797558.py:66: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 2.2318, 'grad_norm': 7.58394193649292, 'learning_rate': 0.0, 'epoch': 0.022727272727272728}\n{'loss': 2.2298, 'grad_norm': 7.743690490722656, 'learning_rate': 3.9677528605473815e-06, 'epoch': 0.045454545454545456}\n{'loss': 2.2211, 'grad_norm': 7.7692084312438965, 'learning_rate': 7.935505721094763e-06, 'epoch': 0.06818181818181818}\n{'loss': 2.1808, 'grad_norm': 7.4710259437561035, 'learning_rate': 1.1903258581642145e-05, 'epoch': 0.09090909090909091}\n{'loss': 2.1763, 'grad_norm': 7.419295310974121, 'learning_rate': 1.5871011442189526e-05, 'epoch': 0.11363636363636363}\n{'loss': 2.1265, 'grad_norm': 7.558272361755371, 'learning_rate': 1.983876430273691e-05, 'epoch': 0.13636363636363635}\n{'loss': 2.0923, 'grad_norm': 7.25667667388916, 'learning_rate': 2.380651716328429e-05, 'epoch': 0.1590909090909091}\n{'loss': 2.049, 'grad_norm': 6.725028991699219, 'learning_rate': 2.7774270023831672e-05, 'epoch': 0.18181818181818182}\n{'loss': 1.9688, 'grad_norm': 7.19038724899292, 'learning_rate': 3.174202288437905e-05, 'epoch': 0.20454545454545456}\n{'loss': 1.8992, 'grad_norm': 6.572154998779297, 'learning_rate': 3.570977574492644e-05, 'epoch': 0.22727272727272727}\n{'loss': 1.7874, 'grad_norm': 6.643913269042969, 'learning_rate': 3.967752860547382e-05, 'epoch': 0.25}\n{'loss': 1.7423, 'grad_norm': 5.909463882446289, 'learning_rate': 4.36452814660212e-05, 'epoch': 0.2727272727272727}\n{'loss': 1.6592, 'grad_norm': 5.843990802764893, 'learning_rate': 4.761303432656858e-05, 'epoch': 0.29545454545454547}\n{'loss': 1.537, 'grad_norm': 5.694952964782715, 'learning_rate': 5.1580787187115965e-05, 'epoch': 0.3181818181818182}\n{'loss': 1.5312, 'grad_norm': 4.6713104248046875, 'learning_rate': 5.5548540047663345e-05, 'epoch': 0.3409090909090909}\n{'loss': 1.4265, 'grad_norm': 4.254234313964844, 'learning_rate': 5.951629290821072e-05, 'epoch': 0.36363636363636365}\n{'loss': 1.3348, 'grad_norm': 3.6298439502716064, 'learning_rate': 6.34840457687581e-05, 'epoch': 0.38636363636363635}\n{'loss': 1.3096, 'grad_norm': 2.869939088821411, 'learning_rate': 6.745179862930548e-05, 'epoch': 0.4090909090909091}\n{'loss': 1.2396, 'grad_norm': 2.221461057662964, 'learning_rate': 7.141955148985288e-05, 'epoch': 0.4318181818181818}\n{'loss': 1.2314, 'grad_norm': 1.9811939001083374, 'learning_rate': 7.538730435040026e-05, 'epoch': 0.45454545454545453}\n{'loss': 1.1884, 'grad_norm': 2.0727741718292236, 'learning_rate': 7.935505721094764e-05, 'epoch': 0.4772727272727273}\n{'loss': 1.1147, 'grad_norm': 2.047630548477173, 'learning_rate': 8.332281007149502e-05, 'epoch': 0.5}\n{'loss': 1.1144, 'grad_norm': 2.0214428901672363, 'learning_rate': 8.72905629320424e-05, 'epoch': 0.5227272727272727}\n{'loss': 1.1607, 'grad_norm': 2.3146800994873047, 'learning_rate': 9.125831579258978e-05, 'epoch': 0.5454545454545454}\n{'loss': 1.0852, 'grad_norm': 1.8956362009048462, 'learning_rate': 9.522606865313716e-05, 'epoch': 0.5681818181818182}\n{'loss': 1.0009, 'grad_norm': 1.4093693494796753, 'learning_rate': 9.919382151368455e-05, 'epoch': 0.5909090909090909}\n{'loss': 1.0502, 'grad_norm': 1.1971702575683594, 'learning_rate': 0.00010316157437423193, 'epoch': 0.6136363636363636}\n{'loss': 0.9596, 'grad_norm': 1.608527421951294, 'learning_rate': 0.00010712932723477931, 'epoch': 0.6363636363636364}\n{'loss': 0.9817, 'grad_norm': 1.6393675804138184, 'learning_rate': 0.00011109708009532669, 'epoch': 0.6590909090909091}\n{'loss': 0.9735, 'grad_norm': 1.9422956705093384, 'learning_rate': 0.00011506483295587407, 'epoch': 0.6818181818181818}\n{'loss': 0.9165, 'grad_norm': 1.7252461910247803, 'learning_rate': 0.00011903258581642144, 'epoch': 0.7045454545454546}\n{'loss': 0.8971, 'grad_norm': 1.5017601251602173, 'learning_rate': 0.00012300033867696883, 'epoch': 0.7272727272727273}\n{'loss': 0.832, 'grad_norm': 1.178163766860962, 'learning_rate': 0.0001269680915375162, 'epoch': 0.75}\n{'loss': 0.8244, 'grad_norm': 0.9306865930557251, 'learning_rate': 0.0001309358443980636, 'epoch': 0.7727272727272727}\n{'loss': 0.7602, 'grad_norm': 1.2658357620239258, 'learning_rate': 0.00013490359725861097, 'epoch': 0.7954545454545454}\n{'loss': 0.7287, 'grad_norm': 1.2375538349151611, 'learning_rate': 0.00013887135011915835, 'epoch': 0.8181818181818182}\n{'loss': 0.7128, 'grad_norm': 1.5878387689590454, 'learning_rate': 0.00014283910297970576, 'epoch': 0.8409090909090909}\n{'loss': 0.7303, 'grad_norm': 1.0327435731887817, 'learning_rate': 0.00014680685584025314, 'epoch': 0.8636363636363636}\n{'loss': 0.6551, 'grad_norm': 1.0758370161056519, 'learning_rate': 0.00015077460870080052, 'epoch': 0.8863636363636364}\n{'loss': 0.6736, 'grad_norm': 1.3633993864059448, 'learning_rate': 0.0001547423615613479, 'epoch': 0.9090909090909091}\n{'loss': 0.7247, 'grad_norm': 1.5121021270751953, 'learning_rate': 0.00015871011442189527, 'epoch': 0.9318181818181818}\n{'loss': 0.6566, 'grad_norm': 1.1316739320755005, 'learning_rate': 0.00016267786728244263, 'epoch': 0.9545454545454546}\n{'loss': 0.6521, 'grad_norm': 1.405051827430725, 'learning_rate': 0.00016664562014299003, 'epoch': 0.9772727272727273}\n{'loss': 0.4812, 'grad_norm': 2.652346611022949, 'learning_rate': 0.00017061337300353741, 'epoch': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.7141    0.6266    0.6675      3404\n     PERCENT     1.0000    0.0423    0.0811        71\n        TYPE     0.8208    0.9050    0.8609     11194\n      VOLUME     0.0000    0.0000    0.0000        56\n\n   micro avg     0.8000    0.8331    0.8162     14725\n   macro avg     0.6337    0.3935    0.4024     14725\nweighted avg     0.7939    0.8331    0.8091     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.5989574193954468, 'eval_f1_macro': 0.40235990702584956, 'eval_precision': 0.8000391312854628, 'eval_recall': 0.8330730050933786, 'eval_f1': 0.8162219708563445, 'eval_accuracy': 0.8198213013210547, 'eval_runtime': 1.5643, 'eval_samples_per_second': 3523.059, 'eval_steps_per_second': 7.032, 'epoch': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.5465, 'grad_norm': 0.8104797601699829, 'learning_rate': 0.0001745811258640848, 'epoch': 1.0227272727272727}\n{'loss': 0.5182, 'grad_norm': 0.7307463884353638, 'learning_rate': 0.0001741402644351351, 'epoch': 1.0454545454545454}\n{'loss': 0.5243, 'grad_norm': 0.631624698638916, 'learning_rate': 0.00017369940300618537, 'epoch': 1.0681818181818181}\n{'loss': 0.4847, 'grad_norm': 0.7735127210617065, 'learning_rate': 0.00017325854157723567, 'epoch': 1.0909090909090908}\n{'loss': 0.5388, 'grad_norm': 1.0210670232772827, 'learning_rate': 0.00017281768014828597, 'epoch': 1.1136363636363635}\n{'loss': 0.4339, 'grad_norm': 0.9477896094322205, 'learning_rate': 0.00017237681871933624, 'epoch': 1.1363636363636362}\n{'loss': 0.4661, 'grad_norm': 0.9035437107086182, 'learning_rate': 0.00017193595729038654, 'epoch': 1.1590909090909092}\n{'loss': 0.5213, 'grad_norm': 1.2128273248672485, 'learning_rate': 0.00017149509586143684, 'epoch': 1.1818181818181819}\n{'loss': 0.4868, 'grad_norm': 1.045318603515625, 'learning_rate': 0.00017105423443248711, 'epoch': 1.2045454545454546}\n{'loss': 0.5182, 'grad_norm': 1.1903570890426636, 'learning_rate': 0.00017061337300353741, 'epoch': 1.2272727272727273}\n{'loss': 0.5042, 'grad_norm': 1.1487969160079956, 'learning_rate': 0.0001701725115745877, 'epoch': 1.25}\n{'loss': 0.47, 'grad_norm': 2.216416120529175, 'learning_rate': 0.000169731650145638, 'epoch': 1.2727272727272727}\n{'loss': 0.4018, 'grad_norm': 1.8269709348678589, 'learning_rate': 0.0001692907887166883, 'epoch': 1.2954545454545454}\n{'loss': 0.4346, 'grad_norm': 0.9477844834327698, 'learning_rate': 0.00016884992728773856, 'epoch': 1.3181818181818181}\n{'loss': 0.4567, 'grad_norm': 0.7016336917877197, 'learning_rate': 0.00016840906585878886, 'epoch': 1.3409090909090908}\n{'loss': 0.4812, 'grad_norm': 1.4154572486877441, 'learning_rate': 0.00016796820442983916, 'epoch': 1.3636363636363638}\n{'loss': 0.3937, 'grad_norm': 1.7534583806991577, 'learning_rate': 0.00016752734300088943, 'epoch': 1.3863636363636362}\n{'loss': 0.3956, 'grad_norm': 1.266044020652771, 'learning_rate': 0.00016708648157193973, 'epoch': 1.4090909090909092}\n{'loss': 0.4381, 'grad_norm': 0.7539897561073303, 'learning_rate': 0.00016664562014299003, 'epoch': 1.4318181818181819}\n{'loss': 0.4339, 'grad_norm': 1.2005646228790283, 'learning_rate': 0.0001662047587140403, 'epoch': 1.4545454545454546}\n{'loss': 0.4336, 'grad_norm': 2.7092831134796143, 'learning_rate': 0.0001657638972850906, 'epoch': 1.4772727272727273}\n{'loss': 0.4292, 'grad_norm': 2.493896484375, 'learning_rate': 0.0001653230358561409, 'epoch': 1.5}\n{'loss': 0.4285, 'grad_norm': 1.1668425798416138, 'learning_rate': 0.00016488217442719118, 'epoch': 1.5227272727272727}\n{'loss': 0.3463, 'grad_norm': 1.2270418405532837, 'learning_rate': 0.00016444131299824148, 'epoch': 1.5454545454545454}\n{'loss': 0.4208, 'grad_norm': 2.0680058002471924, 'learning_rate': 0.00016400045156929178, 'epoch': 1.5681818181818183}\n{'loss': 0.3654, 'grad_norm': 0.9433696269989014, 'learning_rate': 0.00016355959014034205, 'epoch': 1.5909090909090908}\n{'loss': 0.3638, 'grad_norm': 0.8042392730712891, 'learning_rate': 0.00016311872871139235, 'epoch': 1.6136363636363638}\n{'loss': 0.3929, 'grad_norm': 1.2810454368591309, 'learning_rate': 0.00016267786728244263, 'epoch': 1.6363636363636362}\n{'loss': 0.3913, 'grad_norm': 1.083777904510498, 'learning_rate': 0.00016223700585349293, 'epoch': 1.6590909090909092}\n{'loss': 0.385, 'grad_norm': 0.8235499262809753, 'learning_rate': 0.00016179614442454323, 'epoch': 1.6818181818181817}\n{'loss': 0.3495, 'grad_norm': 0.6533482670783997, 'learning_rate': 0.0001613552829955935, 'epoch': 1.7045454545454546}\n{'loss': 0.4133, 'grad_norm': 0.7585067749023438, 'learning_rate': 0.0001609144215666438, 'epoch': 1.7272727272727273}\n{'loss': 0.3392, 'grad_norm': 0.831026017665863, 'learning_rate': 0.0001604735601376941, 'epoch': 1.75}\n{'loss': 0.2837, 'grad_norm': 0.9374725818634033, 'learning_rate': 0.0001600326987087444, 'epoch': 1.7727272727272727}\n{'loss': 0.3976, 'grad_norm': 0.971225917339325, 'learning_rate': 0.00015959183727979467, 'epoch': 1.7954545454545454}\n{'loss': 0.377, 'grad_norm': 0.8610583543777466, 'learning_rate': 0.00015915097585084497, 'epoch': 1.8181818181818183}\n{'loss': 0.3954, 'grad_norm': 0.965591549873352, 'learning_rate': 0.00015871011442189527, 'epoch': 1.8409090909090908}\n{'loss': 0.3447, 'grad_norm': 0.7954413294792175, 'learning_rate': 0.00015826925299294555, 'epoch': 1.8636363636363638}\n{'loss': 0.3668, 'grad_norm': 0.8155924677848816, 'learning_rate': 0.00015782839156399585, 'epoch': 1.8863636363636362}\n{'loss': 0.3705, 'grad_norm': 1.885145664215088, 'learning_rate': 0.00015738753013504615, 'epoch': 1.9090909090909092}\n{'loss': 0.3038, 'grad_norm': 0.6802398562431335, 'learning_rate': 0.00015694666870609642, 'epoch': 1.9318181818181817}\n{'loss': 0.3393, 'grad_norm': 0.9118834733963013, 'learning_rate': 0.00015650580727714672, 'epoch': 1.9545454545454546}\n{'loss': 0.3145, 'grad_norm': 0.7818120718002319, 'learning_rate': 0.00015606494584819702, 'epoch': 1.9772727272727273}\n{'loss': 0.1907, 'grad_norm': 3.828641414642334, 'learning_rate': 0.0001556240844192473, 'epoch': 2.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8034    0.8152    0.8093      3404\n     PERCENT     0.7234    0.9577    0.8242        71\n        TYPE     0.9124    0.9629    0.9370     11194\n      VOLUME     0.3393    0.3393    0.3393        56\n\n   micro avg     0.8847    0.9264    0.9051     14725\n   macro avg     0.6946    0.7688    0.7274     14725\nweighted avg     0.8841    0.9264    0.9046     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.3270575702190399, 'eval_f1_macro': 0.727445105394747, 'eval_precision': 0.8847451031262161, 'eval_recall': 0.926383701188455, 'eval_f1': 0.9050857578874034, 'eval_accuracy': 0.9013868333059255, 'eval_runtime': 1.5562, 'eval_samples_per_second': 3541.268, 'eval_steps_per_second': 7.068, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.2669, 'grad_norm': 1.63884699344635, 'learning_rate': 0.0001551832229902976, 'epoch': 2.022727272727273}\n{'loss': 0.3131, 'grad_norm': 0.8083773255348206, 'learning_rate': 0.0001547423615613479, 'epoch': 2.0454545454545454}\n{'loss': 0.2731, 'grad_norm': 1.1475266218185425, 'learning_rate': 0.0001543015001323982, 'epoch': 2.0681818181818183}\n{'loss': 0.3088, 'grad_norm': 0.9863203167915344, 'learning_rate': 0.00015386063870344847, 'epoch': 2.090909090909091}\n{'loss': 0.3059, 'grad_norm': 1.0095183849334717, 'learning_rate': 0.00015341977727449877, 'epoch': 2.1136363636363638}\n{'loss': 0.279, 'grad_norm': 0.8335973024368286, 'learning_rate': 0.00015297891584554907, 'epoch': 2.1363636363636362}\n{'loss': 0.2849, 'grad_norm': 1.472212553024292, 'learning_rate': 0.00015253805441659934, 'epoch': 2.159090909090909}\n{'loss': 0.3298, 'grad_norm': 0.8710939288139343, 'learning_rate': 0.00015209719298764964, 'epoch': 2.1818181818181817}\n{'loss': 0.3031, 'grad_norm': 1.767776370048523, 'learning_rate': 0.00015165633155869994, 'epoch': 2.2045454545454546}\n{'loss': 0.272, 'grad_norm': 1.5482248067855835, 'learning_rate': 0.00015121547012975021, 'epoch': 2.227272727272727}\n{'loss': 0.3175, 'grad_norm': 1.464310884475708, 'learning_rate': 0.00015077460870080052, 'epoch': 2.25}\n{'loss': 0.2735, 'grad_norm': 0.7127404808998108, 'learning_rate': 0.00015033374727185082, 'epoch': 2.2727272727272725}\n{'loss': 0.2467, 'grad_norm': 1.0999103784561157, 'learning_rate': 0.0001498928858429011, 'epoch': 2.2954545454545454}\n{'loss': 0.2465, 'grad_norm': 1.2885254621505737, 'learning_rate': 0.0001494520244139514, 'epoch': 2.3181818181818183}\n{'loss': 0.2494, 'grad_norm': 1.2419803142547607, 'learning_rate': 0.00014901116298500166, 'epoch': 2.340909090909091}\n{'loss': 0.3076, 'grad_norm': 1.5625635385513306, 'learning_rate': 0.00014857030155605196, 'epoch': 2.3636363636363638}\n{'loss': 0.2699, 'grad_norm': 1.058075189590454, 'learning_rate': 0.00014812944012710226, 'epoch': 2.3863636363636362}\n{'loss': 0.2673, 'grad_norm': 1.1345775127410889, 'learning_rate': 0.00014768857869815253, 'epoch': 2.409090909090909}\n{'loss': 0.2411, 'grad_norm': 1.0968677997589111, 'learning_rate': 0.00014724771726920284, 'epoch': 2.4318181818181817}\n{'loss': 0.2462, 'grad_norm': 0.7988384366035461, 'learning_rate': 0.00014680685584025314, 'epoch': 2.4545454545454546}\n{'loss': 0.2525, 'grad_norm': 1.7590489387512207, 'learning_rate': 0.0001463659944113034, 'epoch': 2.4772727272727275}\n{'loss': 0.2306, 'grad_norm': 1.2615060806274414, 'learning_rate': 0.0001459251329823537, 'epoch': 2.5}\n{'loss': 0.2624, 'grad_norm': 1.321386456489563, 'learning_rate': 0.000145484271553404, 'epoch': 2.5227272727272725}\n{'loss': 0.2296, 'grad_norm': 1.0277765989303589, 'learning_rate': 0.00014504341012445428, 'epoch': 2.5454545454545454}\n{'loss': 0.2912, 'grad_norm': 0.7634167075157166, 'learning_rate': 0.00014460254869550458, 'epoch': 2.5681818181818183}\n{'loss': 0.2329, 'grad_norm': 0.7977663278579712, 'learning_rate': 0.00014416168726655488, 'epoch': 2.590909090909091}\n{'loss': 0.2959, 'grad_norm': 1.5698578357696533, 'learning_rate': 0.00014372082583760516, 'epoch': 2.6136363636363638}\n{'loss': 0.2035, 'grad_norm': 1.395277738571167, 'learning_rate': 0.00014327996440865546, 'epoch': 2.6363636363636362}\n{'loss': 0.2118, 'grad_norm': 0.8322380185127258, 'learning_rate': 0.00014283910297970576, 'epoch': 2.659090909090909}\n{'loss': 0.2242, 'grad_norm': 0.946341335773468, 'learning_rate': 0.00014239824155075603, 'epoch': 2.6818181818181817}\n{'loss': 0.2654, 'grad_norm': 0.9510214924812317, 'learning_rate': 0.00014195738012180633, 'epoch': 2.7045454545454546}\n{'loss': 0.2021, 'grad_norm': 0.6416085362434387, 'learning_rate': 0.0001415165186928566, 'epoch': 2.7272727272727275}\n{'loss': 0.2689, 'grad_norm': 1.0059000253677368, 'learning_rate': 0.0001410756572639069, 'epoch': 2.75}\n{'loss': 0.2648, 'grad_norm': 0.695398211479187, 'learning_rate': 0.0001406347958349572, 'epoch': 2.7727272727272725}\n{'loss': 0.2401, 'grad_norm': 0.8892563581466675, 'learning_rate': 0.00014019393440600748, 'epoch': 2.7954545454545454}\n{'loss': 0.2426, 'grad_norm': 0.7739244103431702, 'learning_rate': 0.00013975307297705778, 'epoch': 2.8181818181818183}\n{'loss': 0.245, 'grad_norm': 1.0275113582611084, 'learning_rate': 0.00013931221154810808, 'epoch': 2.840909090909091}\n{'loss': 0.2863, 'grad_norm': 1.0226227045059204, 'learning_rate': 0.00013887135011915835, 'epoch': 2.8636363636363638}\n{'loss': 0.2591, 'grad_norm': 0.8253098130226135, 'learning_rate': 0.00013843048869020865, 'epoch': 2.8863636363636362}\n{'loss': 0.1948, 'grad_norm': 0.8322919607162476, 'learning_rate': 0.00013798962726125895, 'epoch': 2.909090909090909}\n{'loss': 0.2111, 'grad_norm': 1.532824993133545, 'learning_rate': 0.00013754876583230922, 'epoch': 2.9318181818181817}\n{'loss': 0.2268, 'grad_norm': 1.271365761756897, 'learning_rate': 0.00013710790440335952, 'epoch': 2.9545454545454546}\n{'loss': 0.2243, 'grad_norm': 1.1177570819854736, 'learning_rate': 0.00013666704297440982, 'epoch': 2.9772727272727275}\n{'loss': 0.1125, 'grad_norm': 2.017449378967285, 'learning_rate': 0.0001362261815454601, 'epoch': 3.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8724    0.8252    0.8481      3404\n     PERCENT     0.9429    0.9296    0.9362        71\n        TYPE     0.9226    0.9720    0.9467     11194\n      VOLUME     0.7455    0.7321    0.7387        56\n\n   micro avg     0.9114    0.9370    0.9240     14725\n   macro avg     0.8708    0.8647    0.8674     14725\nweighted avg     0.9104    0.9370    0.9230     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.2924882769584656, 'eval_f1_macro': 0.8674261993801388, 'eval_precision': 0.9113547790474932, 'eval_recall': 0.9369779286926995, 'eval_f1': 0.9239887489954459, 'eval_accuracy': 0.9162418461875788, 'eval_runtime': 1.6684, 'eval_samples_per_second': 3303.175, 'eval_steps_per_second': 6.593, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.1916, 'grad_norm': 1.0844835042953491, 'learning_rate': 0.0001357853201165104, 'epoch': 3.022727272727273}\n{'loss': 0.2277, 'grad_norm': 1.3430769443511963, 'learning_rate': 0.0001353444586875607, 'epoch': 3.0454545454545454}\n{'loss': 0.2107, 'grad_norm': 0.8183798789978027, 'learning_rate': 0.00013490359725861097, 'epoch': 3.0681818181818183}\n{'loss': 0.1724, 'grad_norm': 1.1707226037979126, 'learning_rate': 0.00013446273582966127, 'epoch': 3.090909090909091}\n{'loss': 0.165, 'grad_norm': 0.8904667496681213, 'learning_rate': 0.00013402187440071154, 'epoch': 3.1136363636363638}\n{'loss': 0.2228, 'grad_norm': 1.0884641408920288, 'learning_rate': 0.00013358101297176184, 'epoch': 3.1363636363636362}\n{'loss': 0.228, 'grad_norm': 0.8639401793479919, 'learning_rate': 0.00013314015154281214, 'epoch': 3.159090909090909}\n{'loss': 0.1917, 'grad_norm': 0.7669054865837097, 'learning_rate': 0.00013269929011386242, 'epoch': 3.1818181818181817}\n{'loss': 0.1928, 'grad_norm': 0.7355019450187683, 'learning_rate': 0.00013225842868491272, 'epoch': 3.2045454545454546}\n{'loss': 0.1904, 'grad_norm': 0.9116995930671692, 'learning_rate': 0.00013181756725596302, 'epoch': 3.227272727272727}\n{'loss': 0.1607, 'grad_norm': 1.7909427881240845, 'learning_rate': 0.0001313767058270133, 'epoch': 3.25}\n{'loss': 0.1831, 'grad_norm': 1.132886528968811, 'learning_rate': 0.0001309358443980636, 'epoch': 3.2727272727272725}\n{'loss': 0.1982, 'grad_norm': 1.0843968391418457, 'learning_rate': 0.0001304949829691139, 'epoch': 3.2954545454545454}\n{'loss': 0.2142, 'grad_norm': 1.1916579008102417, 'learning_rate': 0.00013005412154016416, 'epoch': 3.3181818181818183}\n{'loss': 0.1684, 'grad_norm': 0.7834630608558655, 'learning_rate': 0.00012961326011121446, 'epoch': 3.340909090909091}\n{'loss': 0.1992, 'grad_norm': 1.6050060987472534, 'learning_rate': 0.00012917239868226476, 'epoch': 3.3636363636363638}\n{'loss': 0.2266, 'grad_norm': 2.46036958694458, 'learning_rate': 0.00012873153725331504, 'epoch': 3.3863636363636362}\n{'loss': 0.1663, 'grad_norm': 1.102681279182434, 'learning_rate': 0.00012829067582436534, 'epoch': 3.409090909090909}\n{'loss': 0.1999, 'grad_norm': 0.948380708694458, 'learning_rate': 0.00012784981439541564, 'epoch': 3.4318181818181817}\n{'loss': 0.1875, 'grad_norm': 2.30298113822937, 'learning_rate': 0.0001274089529664659, 'epoch': 3.4545454545454546}\n{'loss': 0.214, 'grad_norm': 1.665887475013733, 'learning_rate': 0.0001269680915375162, 'epoch': 3.4772727272727275}\n{'loss': 0.1532, 'grad_norm': 1.7283241748809814, 'learning_rate': 0.0001265272301085665, 'epoch': 3.5}\n{'loss': 0.1808, 'grad_norm': 1.5052576065063477, 'learning_rate': 0.00012608636867961678, 'epoch': 3.5227272727272725}\n{'loss': 0.2048, 'grad_norm': 1.2307610511779785, 'learning_rate': 0.00012564550725066708, 'epoch': 3.5454545454545454}\n{'loss': 0.2205, 'grad_norm': 1.388319969177246, 'learning_rate': 0.00012520464582171738, 'epoch': 3.5681818181818183}\n{'loss': 0.2223, 'grad_norm': 1.2542232275009155, 'learning_rate': 0.00012476378439276766, 'epoch': 3.590909090909091}\n{'loss': 0.1773, 'grad_norm': 1.2475166320800781, 'learning_rate': 0.00012432292296381796, 'epoch': 3.6136363636363638}\n{'loss': 0.1737, 'grad_norm': 0.819585382938385, 'learning_rate': 0.00012388206153486826, 'epoch': 3.6363636363636362}\n{'loss': 0.168, 'grad_norm': 1.1282892227172852, 'learning_rate': 0.00012344120010591853, 'epoch': 3.659090909090909}\n{'loss': 0.1837, 'grad_norm': 0.7499490976333618, 'learning_rate': 0.00012300033867696883, 'epoch': 3.6818181818181817}\n{'loss': 0.1873, 'grad_norm': 0.7788422703742981, 'learning_rate': 0.00012255947724801913, 'epoch': 3.7045454545454546}\n{'loss': 0.1837, 'grad_norm': 1.0450880527496338, 'learning_rate': 0.0001221186158190694, 'epoch': 3.7272727272727275}\n{'loss': 0.1642, 'grad_norm': 1.363577127456665, 'learning_rate': 0.00012167775439011972, 'epoch': 3.75}\n{'loss': 0.1712, 'grad_norm': 0.783661961555481, 'learning_rate': 0.00012123689296116999, 'epoch': 3.7727272727272725}\n{'loss': 0.1589, 'grad_norm': 1.2070304155349731, 'learning_rate': 0.00012079603153222029, 'epoch': 3.7954545454545454}\n{'loss': 0.1801, 'grad_norm': 1.4559481143951416, 'learning_rate': 0.00012035517010327059, 'epoch': 3.8181818181818183}\n{'loss': 0.1762, 'grad_norm': 3.137995719909668, 'learning_rate': 0.00011991430867432086, 'epoch': 3.840909090909091}\n{'loss': 0.2053, 'grad_norm': 1.4936391115188599, 'learning_rate': 0.00011947344724537116, 'epoch': 3.8636363636363638}\n{'loss': 0.1644, 'grad_norm': 1.3072643280029297, 'learning_rate': 0.00011903258581642144, 'epoch': 3.8863636363636362}\n{'loss': 0.1782, 'grad_norm': 1.6152983903884888, 'learning_rate': 0.00011859172438747174, 'epoch': 3.909090909090909}\n{'loss': 0.1611, 'grad_norm': 1.9640387296676636, 'learning_rate': 0.00011815086295852204, 'epoch': 3.9318181818181817}\n{'loss': 0.1909, 'grad_norm': 0.845783531665802, 'learning_rate': 0.00011771000152957231, 'epoch': 3.9545454545454546}\n{'loss': 0.1633, 'grad_norm': 0.877307116985321, 'learning_rate': 0.00011726914010062261, 'epoch': 3.9772727272727275}\n{'loss': 0.1452, 'grad_norm': 4.132698059082031, 'learning_rate': 0.00011682827867167291, 'epoch': 4.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8943    0.8628    0.8783      3404\n     PERCENT     0.9420    0.9155    0.9286        71\n        TYPE     0.9331    0.9756    0.9539     11194\n      VOLUME     0.7321    0.7321    0.7321        56\n\n   micro avg     0.9240    0.9483    0.9360     14725\n   macro avg     0.8754    0.8715    0.8732     14725\nweighted avg     0.9234    0.9483    0.9354     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.2687162160873413, 'eval_f1_macro': 0.8732215486364177, 'eval_precision': 0.9239727387017799, 'eval_recall': 0.9483191850594227, 'eval_f1': 0.9359876667336952, 'eval_accuracy': 0.9271501397796416, 'eval_runtime': 1.5273, 'eval_samples_per_second': 3608.424, 'eval_steps_per_second': 7.202, 'epoch': 4.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.1862, 'grad_norm': 1.7404893636703491, 'learning_rate': 0.0001163874172427232, 'epoch': 4.0227272727272725}\n{'loss': 0.157, 'grad_norm': 1.4648867845535278, 'learning_rate': 0.00011594655581377348, 'epoch': 4.045454545454546}\n{'loss': 0.1656, 'grad_norm': 1.3927866220474243, 'learning_rate': 0.00011550569438482378, 'epoch': 4.068181818181818}\n{'loss': 0.1482, 'grad_norm': 1.3335705995559692, 'learning_rate': 0.00011506483295587407, 'epoch': 4.090909090909091}\n{'loss': 0.121, 'grad_norm': 1.0319464206695557, 'learning_rate': 0.00011462397152692436, 'epoch': 4.113636363636363}\n{'loss': 0.1341, 'grad_norm': 0.820747971534729, 'learning_rate': 0.00011418311009797466, 'epoch': 4.136363636363637}\n{'loss': 0.1746, 'grad_norm': 0.8004069924354553, 'learning_rate': 0.00011374224866902494, 'epoch': 4.159090909090909}\n{'loss': 0.1462, 'grad_norm': 0.9986468553543091, 'learning_rate': 0.00011330138724007523, 'epoch': 4.181818181818182}\n{'loss': 0.1529, 'grad_norm': 1.1350743770599365, 'learning_rate': 0.00011286052581112553, 'epoch': 4.204545454545454}\n{'loss': 0.1763, 'grad_norm': 1.988640308380127, 'learning_rate': 0.00011241966438217582, 'epoch': 4.2272727272727275}\n{'loss': 0.1498, 'grad_norm': 1.6531729698181152, 'learning_rate': 0.0001119788029532261, 'epoch': 4.25}\n{'loss': 0.1175, 'grad_norm': 0.8677962422370911, 'learning_rate': 0.00011153794152427639, 'epoch': 4.2727272727272725}\n{'loss': 0.1243, 'grad_norm': 0.7696288228034973, 'learning_rate': 0.00011109708009532669, 'epoch': 4.295454545454546}\n{'loss': 0.1301, 'grad_norm': 0.89505934715271, 'learning_rate': 0.00011065621866637699, 'epoch': 4.318181818181818}\n{'loss': 0.1245, 'grad_norm': 1.0576210021972656, 'learning_rate': 0.00011021535723742726, 'epoch': 4.340909090909091}\n{'loss': 0.1683, 'grad_norm': 1.8843157291412354, 'learning_rate': 0.00010977449580847756, 'epoch': 4.363636363636363}\n{'loss': 0.1412, 'grad_norm': 0.8938461542129517, 'learning_rate': 0.00010933363437952786, 'epoch': 4.386363636363637}\n{'loss': 0.1134, 'grad_norm': 0.7684447169303894, 'learning_rate': 0.00010889277295057814, 'epoch': 4.409090909090909}\n{'loss': 0.1332, 'grad_norm': 1.2554214000701904, 'learning_rate': 0.00010845191152162844, 'epoch': 4.431818181818182}\n{'loss': 0.1385, 'grad_norm': 0.824694037437439, 'learning_rate': 0.00010801105009267874, 'epoch': 4.454545454545454}\n{'loss': 0.1344, 'grad_norm': 0.8340809345245361, 'learning_rate': 0.00010757018866372901, 'epoch': 4.4772727272727275}\n{'loss': 0.1334, 'grad_norm': 1.1049591302871704, 'learning_rate': 0.00010712932723477931, 'epoch': 4.5}\n{'loss': 0.1306, 'grad_norm': 1.2071350812911987, 'learning_rate': 0.00010668846580582961, 'epoch': 4.5227272727272725}\n{'loss': 0.1719, 'grad_norm': 1.6970093250274658, 'learning_rate': 0.00010624760437687988, 'epoch': 4.545454545454545}\n{'loss': 0.1472, 'grad_norm': 1.2438201904296875, 'learning_rate': 0.00010580674294793018, 'epoch': 4.568181818181818}\n{'loss': 0.1866, 'grad_norm': 1.435699462890625, 'learning_rate': 0.00010536588151898046, 'epoch': 4.590909090909091}\n{'loss': 0.1315, 'grad_norm': 0.8431324362754822, 'learning_rate': 0.00010492502009003076, 'epoch': 4.613636363636363}\n{'loss': 0.1408, 'grad_norm': 0.9458472728729248, 'learning_rate': 0.00010448415866108106, 'epoch': 4.636363636363637}\n{'loss': 0.1289, 'grad_norm': 1.5981061458587646, 'learning_rate': 0.00010404329723213133, 'epoch': 4.659090909090909}\n{'loss': 0.1669, 'grad_norm': 1.811692237854004, 'learning_rate': 0.00010360243580318163, 'epoch': 4.681818181818182}\n{'loss': 0.1327, 'grad_norm': 2.025378942489624, 'learning_rate': 0.00010316157437423193, 'epoch': 4.704545454545455}\n{'loss': 0.1625, 'grad_norm': 1.5384849309921265, 'learning_rate': 0.0001027207129452822, 'epoch': 4.7272727272727275}\n{'loss': 0.1815, 'grad_norm': 1.7866421937942505, 'learning_rate': 0.0001022798515163325, 'epoch': 4.75}\n{'loss': 0.1345, 'grad_norm': 1.5976430177688599, 'learning_rate': 0.0001018389900873828, 'epoch': 4.7727272727272725}\n{'loss': 0.1351, 'grad_norm': 1.0339235067367554, 'learning_rate': 0.00010139812865843308, 'epoch': 4.795454545454545}\n{'loss': 0.1923, 'grad_norm': 0.9520466923713684, 'learning_rate': 0.00010095726722948338, 'epoch': 4.818181818181818}\n{'loss': 0.1451, 'grad_norm': 0.8424298167228699, 'learning_rate': 0.00010051640580053368, 'epoch': 4.840909090909091}\n{'loss': 0.147, 'grad_norm': 1.061014175415039, 'learning_rate': 0.00010007554437158395, 'epoch': 4.863636363636363}\n{'loss': 0.1226, 'grad_norm': 1.2042863368988037, 'learning_rate': 9.963468294263425e-05, 'epoch': 4.886363636363637}\n{'loss': 0.1582, 'grad_norm': 0.9404639601707458, 'learning_rate': 9.919382151368455e-05, 'epoch': 4.909090909090909}\n{'loss': 0.1171, 'grad_norm': 1.0650907754898071, 'learning_rate': 9.875296008473482e-05, 'epoch': 4.931818181818182}\n{'loss': 0.1144, 'grad_norm': 0.8858184218406677, 'learning_rate': 9.831209865578512e-05, 'epoch': 4.954545454545455}\n{'loss': 0.1726, 'grad_norm': 0.7237654328346252, 'learning_rate': 9.787123722683541e-05, 'epoch': 4.9772727272727275}\n{'loss': 0.0899, 'grad_norm': 6.157350540161133, 'learning_rate': 9.74303757978857e-05, 'epoch': 5.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8978    0.8649    0.8810      3404\n     PERCENT     0.9565    0.9296    0.9429        71\n        TYPE     0.9348    0.9746    0.9543     11194\n      VOLUME     0.7143    0.7143    0.7143        56\n\n   micro avg     0.9260    0.9480    0.9369     14725\n   macro avg     0.8759    0.8708    0.8731     14725\nweighted avg     0.9255    0.9480    0.9364     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.2603954076766968, 'eval_f1_macro': 0.8731203165035017, 'eval_precision': 0.9260364842454395, 'eval_recall': 0.9480475382003396, 'eval_f1': 0.9369127516778524, 'eval_accuracy': 0.9289590527873705, 'eval_runtime': 1.5591, 'eval_samples_per_second': 3534.633, 'eval_steps_per_second': 7.055, 'epoch': 5.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.1367, 'grad_norm': 0.9241293668746948, 'learning_rate': 9.6989514368936e-05, 'epoch': 5.0227272727272725}\n{'loss': 0.1312, 'grad_norm': 0.8323089480400085, 'learning_rate': 9.654865293998628e-05, 'epoch': 5.045454545454546}\n{'loss': 0.1202, 'grad_norm': 0.8582413792610168, 'learning_rate': 9.610779151103658e-05, 'epoch': 5.068181818181818}\n{'loss': 0.1255, 'grad_norm': 0.8370614647865295, 'learning_rate': 9.566693008208687e-05, 'epoch': 5.090909090909091}\n{'loss': 0.1535, 'grad_norm': 0.7922120094299316, 'learning_rate': 9.522606865313716e-05, 'epoch': 5.113636363636363}\n{'loss': 0.1388, 'grad_norm': 1.1129951477050781, 'learning_rate': 9.478520722418746e-05, 'epoch': 5.136363636363637}\n{'loss': 0.1393, 'grad_norm': 0.7887548208236694, 'learning_rate': 9.434434579523774e-05, 'epoch': 5.159090909090909}\n{'loss': 0.1075, 'grad_norm': 2.053981065750122, 'learning_rate': 9.390348436628803e-05, 'epoch': 5.181818181818182}\n{'loss': 0.1168, 'grad_norm': 0.8991648554801941, 'learning_rate': 9.346262293733833e-05, 'epoch': 5.204545454545454}\n{'loss': 0.0981, 'grad_norm': 2.1096737384796143, 'learning_rate': 9.302176150838862e-05, 'epoch': 5.2272727272727275}\n{'loss': 0.1097, 'grad_norm': 1.074818730354309, 'learning_rate': 9.25809000794389e-05, 'epoch': 5.25}\n{'loss': 0.1325, 'grad_norm': 0.9356196522712708, 'learning_rate': 9.21400386504892e-05, 'epoch': 5.2727272727272725}\n{'loss': 0.1182, 'grad_norm': 1.2287801504135132, 'learning_rate': 9.169917722153949e-05, 'epoch': 5.295454545454546}\n{'loss': 0.1083, 'grad_norm': 1.7850781679153442, 'learning_rate': 9.125831579258978e-05, 'epoch': 5.318181818181818}\n{'loss': 0.1378, 'grad_norm': 1.8206020593643188, 'learning_rate': 9.081745436364008e-05, 'epoch': 5.340909090909091}\n{'loss': 0.0892, 'grad_norm': 1.2095552682876587, 'learning_rate': 9.037659293469035e-05, 'epoch': 5.363636363636363}\n{'loss': 0.1124, 'grad_norm': 1.0451850891113281, 'learning_rate': 8.993573150574065e-05, 'epoch': 5.386363636363637}\n{'loss': 0.1098, 'grad_norm': 0.9727427363395691, 'learning_rate': 8.949487007679095e-05, 'epoch': 5.409090909090909}\n{'loss': 0.1569, 'grad_norm': 1.0504286289215088, 'learning_rate': 8.905400864784122e-05, 'epoch': 5.431818181818182}\n{'loss': 0.1047, 'grad_norm': 1.4901329278945923, 'learning_rate': 8.861314721889152e-05, 'epoch': 5.454545454545454}\n{'loss': 0.1401, 'grad_norm': 2.714448928833008, 'learning_rate': 8.817228578994182e-05, 'epoch': 5.4772727272727275}\n{'loss': 0.0984, 'grad_norm': 2.234494209289551, 'learning_rate': 8.77314243609921e-05, 'epoch': 5.5}\n{'loss': 0.1261, 'grad_norm': 2.330976963043213, 'learning_rate': 8.72905629320424e-05, 'epoch': 5.5227272727272725}\n{'loss': 0.128, 'grad_norm': 1.2427210807800293, 'learning_rate': 8.684970150309268e-05, 'epoch': 5.545454545454545}\n{'loss': 0.1015, 'grad_norm': 0.8765807747840881, 'learning_rate': 8.640884007414298e-05, 'epoch': 5.568181818181818}\n{'loss': 0.1026, 'grad_norm': 1.8431798219680786, 'learning_rate': 8.596797864519327e-05, 'epoch': 5.590909090909091}\n{'loss': 0.1079, 'grad_norm': 1.6708000898361206, 'learning_rate': 8.552711721624356e-05, 'epoch': 5.613636363636363}\n{'loss': 0.1275, 'grad_norm': 0.9021346569061279, 'learning_rate': 8.508625578729384e-05, 'epoch': 5.636363636363637}\n{'loss': 0.0992, 'grad_norm': 1.6173826456069946, 'learning_rate': 8.464539435834414e-05, 'epoch': 5.659090909090909}\n{'loss': 0.1284, 'grad_norm': 1.6611640453338623, 'learning_rate': 8.420453292939443e-05, 'epoch': 5.681818181818182}\n{'loss': 0.1108, 'grad_norm': 1.1047111749649048, 'learning_rate': 8.376367150044472e-05, 'epoch': 5.704545454545455}\n{'loss': 0.155, 'grad_norm': 1.5797266960144043, 'learning_rate': 8.332281007149502e-05, 'epoch': 5.7272727272727275}\n{'loss': 0.1082, 'grad_norm': 1.9200352430343628, 'learning_rate': 8.28819486425453e-05, 'epoch': 5.75}\n{'loss': 0.0942, 'grad_norm': 1.0030022859573364, 'learning_rate': 8.244108721359559e-05, 'epoch': 5.7727272727272725}\n{'loss': 0.1104, 'grad_norm': 1.4814568758010864, 'learning_rate': 8.200022578464589e-05, 'epoch': 5.795454545454545}\n{'loss': 0.0963, 'grad_norm': 1.0793383121490479, 'learning_rate': 8.155936435569618e-05, 'epoch': 5.818181818181818}\n{'loss': 0.0951, 'grad_norm': 1.016297698020935, 'learning_rate': 8.111850292674646e-05, 'epoch': 5.840909090909091}\n{'loss': 0.1243, 'grad_norm': 1.1115226745605469, 'learning_rate': 8.067764149779675e-05, 'epoch': 5.863636363636363}\n{'loss': 0.1217, 'grad_norm': 1.5993660688400269, 'learning_rate': 8.023678006884705e-05, 'epoch': 5.886363636363637}\n{'loss': 0.1152, 'grad_norm': 1.1569032669067383, 'learning_rate': 7.979591863989734e-05, 'epoch': 5.909090909090909}\n{'loss': 0.1679, 'grad_norm': 1.934550166130066, 'learning_rate': 7.935505721094764e-05, 'epoch': 5.931818181818182}\n{'loss': 0.1342, 'grad_norm': 2.0701489448547363, 'learning_rate': 7.891419578199792e-05, 'epoch': 5.954545454545455}\n{'loss': 0.1022, 'grad_norm': 1.128735065460205, 'learning_rate': 7.847333435304821e-05, 'epoch': 5.9772727272727275}\n{'loss': 0.0149, 'grad_norm': 0.556718111038208, 'learning_rate': 7.803247292409851e-05, 'epoch': 6.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8933    0.8781    0.8856      3404\n     PERCENT     0.9565    0.9296    0.9429        71\n        TYPE     0.9411    0.9698    0.9553     11194\n      VOLUME     0.7407    0.7143    0.7273        56\n\n   micro avg     0.9298    0.9474    0.9385     14725\n   macro avg     0.8829    0.8729    0.8778     14725\nweighted avg     0.9294    0.9474    0.9382     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.26619622111320496, 'eval_f1_macro': 0.8777537250872638, 'eval_precision': 0.9298187150093309, 'eval_recall': 0.9474363327674024, 'eval_f1': 0.9385448551919002, 'eval_accuracy': 0.9303294414295894, 'eval_runtime': 1.6276, 'eval_samples_per_second': 3385.984, 'eval_steps_per_second': 6.758, 'epoch': 6.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.1005, 'grad_norm': 0.7334058284759521, 'learning_rate': 7.75916114951488e-05, 'epoch': 6.0227272727272725}\n{'loss': 0.1142, 'grad_norm': 1.1916148662567139, 'learning_rate': 7.71507500661991e-05, 'epoch': 6.045454545454546}\n{'loss': 0.0995, 'grad_norm': 1.3643677234649658, 'learning_rate': 7.670988863724938e-05, 'epoch': 6.068181818181818}\n{'loss': 0.0891, 'grad_norm': 1.9061766862869263, 'learning_rate': 7.626902720829967e-05, 'epoch': 6.090909090909091}\n{'loss': 0.1008, 'grad_norm': 1.0669560432434082, 'learning_rate': 7.582816577934997e-05, 'epoch': 6.113636363636363}\n{'loss': 0.0842, 'grad_norm': 2.323202610015869, 'learning_rate': 7.538730435040026e-05, 'epoch': 6.136363636363637}\n{'loss': 0.0905, 'grad_norm': 0.7040804624557495, 'learning_rate': 7.494644292145054e-05, 'epoch': 6.159090909090909}\n{'loss': 0.1119, 'grad_norm': 1.0069499015808105, 'learning_rate': 7.450558149250083e-05, 'epoch': 6.181818181818182}\n{'loss': 0.1133, 'grad_norm': 0.7598270773887634, 'learning_rate': 7.406472006355113e-05, 'epoch': 6.204545454545454}\n{'loss': 0.0772, 'grad_norm': 1.393079400062561, 'learning_rate': 7.362385863460142e-05, 'epoch': 6.2272727272727275}\n{'loss': 0.1104, 'grad_norm': 1.0756641626358032, 'learning_rate': 7.31829972056517e-05, 'epoch': 6.25}\n{'loss': 0.08, 'grad_norm': 1.292837142944336, 'learning_rate': 7.2742135776702e-05, 'epoch': 6.2727272727272725}\n{'loss': 0.0811, 'grad_norm': 1.1841057538986206, 'learning_rate': 7.230127434775229e-05, 'epoch': 6.295454545454546}\n{'loss': 0.0803, 'grad_norm': 0.923024594783783, 'learning_rate': 7.186041291880258e-05, 'epoch': 6.318181818181818}\n{'loss': 0.1026, 'grad_norm': 0.9453439116477966, 'learning_rate': 7.141955148985288e-05, 'epoch': 6.340909090909091}\n{'loss': 0.0826, 'grad_norm': 0.8810562491416931, 'learning_rate': 7.097869006090316e-05, 'epoch': 6.363636363636363}\n{'loss': 0.0997, 'grad_norm': 1.5920130014419556, 'learning_rate': 7.053782863195345e-05, 'epoch': 6.386363636363637}\n{'loss': 0.087, 'grad_norm': 1.2693148851394653, 'learning_rate': 7.009696720300374e-05, 'epoch': 6.409090909090909}\n{'loss': 0.0787, 'grad_norm': 0.8225700259208679, 'learning_rate': 6.965610577405404e-05, 'epoch': 6.431818181818182}\n{'loss': 0.1301, 'grad_norm': 1.684326410293579, 'learning_rate': 6.921524434510432e-05, 'epoch': 6.454545454545454}\n{'loss': 0.0967, 'grad_norm': 1.058605432510376, 'learning_rate': 6.877438291615461e-05, 'epoch': 6.4772727272727275}\n{'loss': 0.0918, 'grad_norm': 0.9744290709495544, 'learning_rate': 6.833352148720491e-05, 'epoch': 6.5}\n{'loss': 0.0749, 'grad_norm': 0.5821765661239624, 'learning_rate': 6.78926600582552e-05, 'epoch': 6.5227272727272725}\n{'loss': 0.092, 'grad_norm': 0.8018211126327515, 'learning_rate': 6.745179862930548e-05, 'epoch': 6.545454545454545}\n{'loss': 0.0807, 'grad_norm': 0.9219156503677368, 'learning_rate': 6.701093720035577e-05, 'epoch': 6.568181818181818}\n{'loss': 0.099, 'grad_norm': 1.4114223718643188, 'learning_rate': 6.657007577140607e-05, 'epoch': 6.590909090909091}\n{'loss': 0.0867, 'grad_norm': 0.7881126403808594, 'learning_rate': 6.612921434245636e-05, 'epoch': 6.613636363636363}\n{'loss': 0.116, 'grad_norm': 1.6108356714248657, 'learning_rate': 6.568835291350664e-05, 'epoch': 6.636363636363637}\n{'loss': 0.1281, 'grad_norm': 1.0716363191604614, 'learning_rate': 6.524749148455694e-05, 'epoch': 6.659090909090909}\n{'loss': 0.076, 'grad_norm': 1.059860348701477, 'learning_rate': 6.480663005560723e-05, 'epoch': 6.681818181818182}\n{'loss': 0.1002, 'grad_norm': 0.9406479001045227, 'learning_rate': 6.436576862665752e-05, 'epoch': 6.704545454545455}\n{'loss': 0.0933, 'grad_norm': 0.7111914753913879, 'learning_rate': 6.392490719770782e-05, 'epoch': 6.7272727272727275}\n{'loss': 0.1396, 'grad_norm': 1.0176501274108887, 'learning_rate': 6.34840457687581e-05, 'epoch': 6.75}\n{'loss': 0.0993, 'grad_norm': 0.929148256778717, 'learning_rate': 6.304318433980839e-05, 'epoch': 6.7727272727272725}\n{'loss': 0.1076, 'grad_norm': 1.2155681848526, 'learning_rate': 6.260232291085869e-05, 'epoch': 6.795454545454545}\n{'loss': 0.0755, 'grad_norm': 0.9314447641372681, 'learning_rate': 6.216146148190898e-05, 'epoch': 6.818181818181818}\n{'loss': 0.1047, 'grad_norm': 1.1850991249084473, 'learning_rate': 6.172060005295926e-05, 'epoch': 6.840909090909091}\n{'loss': 0.0917, 'grad_norm': 1.1904373168945312, 'learning_rate': 6.127973862400956e-05, 'epoch': 6.863636363636363}\n{'loss': 0.1071, 'grad_norm': 0.9748358130455017, 'learning_rate': 6.083887719505986e-05, 'epoch': 6.886363636363637}\n{'loss': 0.0967, 'grad_norm': 1.0958456993103027, 'learning_rate': 6.0398015766110145e-05, 'epoch': 6.909090909090909}\n{'loss': 0.0989, 'grad_norm': 1.3555591106414795, 'learning_rate': 5.995715433716043e-05, 'epoch': 6.931818181818182}\n{'loss': 0.0696, 'grad_norm': 0.8006227612495422, 'learning_rate': 5.951629290821072e-05, 'epoch': 6.954545454545455}\n{'loss': 0.1086, 'grad_norm': 0.7504251599311829, 'learning_rate': 5.907543147926102e-05, 'epoch': 6.9772727272727275}\n{'loss': 0.0543, 'grad_norm': 4.504785060882568, 'learning_rate': 5.8634570050311305e-05, 'epoch': 7.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8897    0.8843    0.8870      3404\n     PERCENT     0.9296    0.9296    0.9296        71\n        TYPE     0.9418    0.9680    0.9547     11194\n      VOLUME     0.7636    0.7500    0.7568        56\n\n   micro avg     0.9293    0.9476    0.9384     14725\n   macro avg     0.8812    0.8830    0.8820     14725\nweighted avg     0.9290    0.9476    0.9382     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.2707781493663788, 'eval_f1_macro': 0.8820094278595962, 'eval_precision': 0.9293373293373294, 'eval_recall': 0.9476400679117147, 'eval_f1': 0.938399462004035, 'eval_accuracy': 0.9293975771528805, 'eval_runtime': 1.5182, 'eval_samples_per_second': 3630.011, 'eval_steps_per_second': 7.246, 'epoch': 7.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.0567, 'grad_norm': 1.1298136711120605, 'learning_rate': 5.81937086213616e-05, 'epoch': 7.0227272727272725}\n{'loss': 0.0977, 'grad_norm': 1.3897910118103027, 'learning_rate': 5.775284719241189e-05, 'epoch': 7.045454545454546}\n{'loss': 0.0765, 'grad_norm': 0.8817950487136841, 'learning_rate': 5.731198576346218e-05, 'epoch': 7.068181818181818}\n{'loss': 0.0826, 'grad_norm': 1.0023353099822998, 'learning_rate': 5.687112433451247e-05, 'epoch': 7.090909090909091}\n{'loss': 0.0806, 'grad_norm': 0.9614874124526978, 'learning_rate': 5.6430262905562765e-05, 'epoch': 7.113636363636363}\n{'loss': 0.0816, 'grad_norm': 1.3684452772140503, 'learning_rate': 5.598940147661305e-05, 'epoch': 7.136363636363637}\n{'loss': 0.0981, 'grad_norm': 1.6991748809814453, 'learning_rate': 5.5548540047663345e-05, 'epoch': 7.159090909090909}\n{'loss': 0.0951, 'grad_norm': 1.547755241394043, 'learning_rate': 5.510767861871363e-05, 'epoch': 7.181818181818182}\n{'loss': 0.0722, 'grad_norm': 0.7764605283737183, 'learning_rate': 5.466681718976393e-05, 'epoch': 7.204545454545454}\n{'loss': 0.0824, 'grad_norm': 0.9731490612030029, 'learning_rate': 5.422595576081422e-05, 'epoch': 7.2272727272727275}\n{'loss': 0.0607, 'grad_norm': 0.9959878325462341, 'learning_rate': 5.3785094331864505e-05, 'epoch': 7.25}\n{'loss': 0.0642, 'grad_norm': 1.0784801244735718, 'learning_rate': 5.3344232902914805e-05, 'epoch': 7.2727272727272725}\n{'loss': 0.0971, 'grad_norm': 0.8850465416908264, 'learning_rate': 5.290337147396509e-05, 'epoch': 7.295454545454546}\n{'loss': 0.0874, 'grad_norm': 0.8142581582069397, 'learning_rate': 5.246251004501538e-05, 'epoch': 7.318181818181818}\n{'loss': 0.0803, 'grad_norm': 0.8524852395057678, 'learning_rate': 5.2021648616065665e-05, 'epoch': 7.340909090909091}\n{'loss': 0.1189, 'grad_norm': 1.3560742139816284, 'learning_rate': 5.1580787187115965e-05, 'epoch': 7.363636363636363}\n{'loss': 0.069, 'grad_norm': 0.8418480753898621, 'learning_rate': 5.113992575816625e-05, 'epoch': 7.386363636363637}\n{'loss': 0.0574, 'grad_norm': 0.537621796131134, 'learning_rate': 5.069906432921654e-05, 'epoch': 7.409090909090909}\n{'loss': 0.0913, 'grad_norm': 0.7214228510856628, 'learning_rate': 5.025820290026684e-05, 'epoch': 7.431818181818182}\n{'loss': 0.065, 'grad_norm': 1.2675930261611938, 'learning_rate': 4.9817341471317125e-05, 'epoch': 7.454545454545454}\n{'loss': 0.1106, 'grad_norm': 0.8225818872451782, 'learning_rate': 4.937648004236741e-05, 'epoch': 7.4772727272727275}\n{'loss': 0.0971, 'grad_norm': 0.7784956693649292, 'learning_rate': 4.8935618613417705e-05, 'epoch': 7.5}\n{'loss': 0.0838, 'grad_norm': 0.9196822643280029, 'learning_rate': 4.8494757184468e-05, 'epoch': 7.5227272727272725}\n{'loss': 0.0933, 'grad_norm': 1.0188889503479004, 'learning_rate': 4.805389575551829e-05, 'epoch': 7.545454545454545}\n{'loss': 0.079, 'grad_norm': 0.9087003469467163, 'learning_rate': 4.761303432656858e-05, 'epoch': 7.568181818181818}\n{'loss': 0.1003, 'grad_norm': 0.8744686841964722, 'learning_rate': 4.717217289761887e-05, 'epoch': 7.590909090909091}\n{'loss': 0.0951, 'grad_norm': 1.4557937383651733, 'learning_rate': 4.6731311468669165e-05, 'epoch': 7.613636363636363}\n{'loss': 0.071, 'grad_norm': 1.297027587890625, 'learning_rate': 4.629045003971945e-05, 'epoch': 7.636363636363637}\n{'loss': 0.1118, 'grad_norm': 0.916342556476593, 'learning_rate': 4.5849588610769745e-05, 'epoch': 7.659090909090909}\n{'loss': 0.0898, 'grad_norm': 0.910199761390686, 'learning_rate': 4.540872718182004e-05, 'epoch': 7.681818181818182}\n{'loss': 0.112, 'grad_norm': 1.1801916360855103, 'learning_rate': 4.4967865752870325e-05, 'epoch': 7.704545454545455}\n{'loss': 0.0844, 'grad_norm': 1.956157922744751, 'learning_rate': 4.452700432392061e-05, 'epoch': 7.7272727272727275}\n{'loss': 0.0873, 'grad_norm': 1.057007908821106, 'learning_rate': 4.408614289497091e-05, 'epoch': 7.75}\n{'loss': 0.1069, 'grad_norm': 0.837360680103302, 'learning_rate': 4.36452814660212e-05, 'epoch': 7.7727272727272725}\n{'loss': 0.09, 'grad_norm': 0.8501303791999817, 'learning_rate': 4.320442003707149e-05, 'epoch': 7.795454545454545}\n{'loss': 0.0727, 'grad_norm': 0.8602796196937561, 'learning_rate': 4.276355860812178e-05, 'epoch': 7.818181818181818}\n{'loss': 0.0687, 'grad_norm': 0.7476837038993835, 'learning_rate': 4.232269717917207e-05, 'epoch': 7.840909090909091}\n{'loss': 0.0847, 'grad_norm': 0.9248089790344238, 'learning_rate': 4.188183575022236e-05, 'epoch': 7.863636363636363}\n{'loss': 0.107, 'grad_norm': 0.7753884792327881, 'learning_rate': 4.144097432127265e-05, 'epoch': 7.886363636363637}\n{'loss': 0.0539, 'grad_norm': 0.8312426209449768, 'learning_rate': 4.1000112892322945e-05, 'epoch': 7.909090909090909}\n{'loss': 0.086, 'grad_norm': 1.2474254369735718, 'learning_rate': 4.055925146337323e-05, 'epoch': 7.931818181818182}\n{'loss': 0.0719, 'grad_norm': 1.1881840229034424, 'learning_rate': 4.0118390034423525e-05, 'epoch': 7.954545454545455}\n{'loss': 0.1175, 'grad_norm': 1.2653566598892212, 'learning_rate': 3.967752860547382e-05, 'epoch': 7.9772727272727275}\n{'loss': 0.0991, 'grad_norm': 4.32488489151001, 'learning_rate': 3.9236667176524105e-05, 'epoch': 8.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.9010    0.8769    0.8888      3404\n     PERCENT     0.9296    0.9296    0.9296        71\n        TYPE     0.9383    0.9733    0.9555     11194\n      VOLUME     0.7778    0.7500    0.7636        56\n\n   micro avg     0.9294    0.9499    0.9396     14725\n   macro avg     0.8867    0.8824    0.8844     14725\nweighted avg     0.9290    0.9499    0.9392     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.28688639402389526, 'eval_f1_macro': 0.8843634466869134, 'eval_precision': 0.9294352159468439, 'eval_recall': 0.9499490662139219, 'eval_f1': 0.9395801847187238, 'eval_accuracy': 0.930877596886477, 'eval_runtime': 1.9659, 'eval_samples_per_second': 2803.275, 'eval_steps_per_second': 5.595, 'epoch': 8.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.0916, 'grad_norm': 1.4637221097946167, 'learning_rate': 3.87958057475744e-05, 'epoch': 8.022727272727273}\n{'loss': 0.1052, 'grad_norm': 1.661926507949829, 'learning_rate': 3.835494431862469e-05, 'epoch': 8.045454545454545}\n{'loss': 0.1015, 'grad_norm': 1.1819264888763428, 'learning_rate': 3.7914082889674985e-05, 'epoch': 8.068181818181818}\n{'loss': 0.0648, 'grad_norm': 0.6066725850105286, 'learning_rate': 3.747322146072527e-05, 'epoch': 8.090909090909092}\n{'loss': 0.0804, 'grad_norm': 0.9338101148605347, 'learning_rate': 3.7032360031775565e-05, 'epoch': 8.113636363636363}\n{'loss': 0.0731, 'grad_norm': 1.0685466527938843, 'learning_rate': 3.659149860282585e-05, 'epoch': 8.136363636363637}\n{'loss': 0.0886, 'grad_norm': 0.8886119723320007, 'learning_rate': 3.6150637173876145e-05, 'epoch': 8.159090909090908}\n{'loss': 0.0543, 'grad_norm': 0.816906750202179, 'learning_rate': 3.570977574492644e-05, 'epoch': 8.181818181818182}\n{'loss': 0.0848, 'grad_norm': 0.8567427396774292, 'learning_rate': 3.5268914315976725e-05, 'epoch': 8.204545454545455}\n{'loss': 0.0955, 'grad_norm': 1.6809147596359253, 'learning_rate': 3.482805288702702e-05, 'epoch': 8.227272727272727}\n{'loss': 0.0746, 'grad_norm': 0.8876953721046448, 'learning_rate': 3.4387191458077305e-05, 'epoch': 8.25}\n{'loss': 0.067, 'grad_norm': 0.9227590560913086, 'learning_rate': 3.39463300291276e-05, 'epoch': 8.272727272727273}\n{'loss': 0.0994, 'grad_norm': 1.2886312007904053, 'learning_rate': 3.3505468600177885e-05, 'epoch': 8.295454545454545}\n{'loss': 0.0799, 'grad_norm': 0.7181760668754578, 'learning_rate': 3.306460717122818e-05, 'epoch': 8.318181818181818}\n{'loss': 0.0772, 'grad_norm': 1.3778783082962036, 'learning_rate': 3.262374574227847e-05, 'epoch': 8.340909090909092}\n{'loss': 0.0658, 'grad_norm': 0.7613217830657959, 'learning_rate': 3.218288431332876e-05, 'epoch': 8.363636363636363}\n{'loss': 0.0545, 'grad_norm': 0.9553396701812744, 'learning_rate': 3.174202288437905e-05, 'epoch': 8.386363636363637}\n{'loss': 0.0828, 'grad_norm': 0.9076446890830994, 'learning_rate': 3.1301161455429346e-05, 'epoch': 8.409090909090908}\n{'loss': 0.0691, 'grad_norm': 1.023998737335205, 'learning_rate': 3.086030002647963e-05, 'epoch': 8.431818181818182}\n{'loss': 0.0611, 'grad_norm': 0.918677568435669, 'learning_rate': 3.041943859752993e-05, 'epoch': 8.454545454545455}\n{'loss': 0.0772, 'grad_norm': 0.974860429763794, 'learning_rate': 2.9978577168580216e-05, 'epoch': 8.477272727272727}\n{'loss': 0.0694, 'grad_norm': 0.8904029130935669, 'learning_rate': 2.953771573963051e-05, 'epoch': 8.5}\n{'loss': 0.0872, 'grad_norm': 0.754734218120575, 'learning_rate': 2.90968543106808e-05, 'epoch': 8.522727272727273}\n{'loss': 0.0596, 'grad_norm': 1.4582030773162842, 'learning_rate': 2.865599288173109e-05, 'epoch': 8.545454545454545}\n{'loss': 0.0852, 'grad_norm': 0.9408702850341797, 'learning_rate': 2.8215131452781382e-05, 'epoch': 8.568181818181818}\n{'loss': 0.0648, 'grad_norm': 0.6806356906890869, 'learning_rate': 2.7774270023831672e-05, 'epoch': 8.590909090909092}\n{'loss': 0.0683, 'grad_norm': 1.0111407041549683, 'learning_rate': 2.7333408594881966e-05, 'epoch': 8.613636363636363}\n{'loss': 0.0766, 'grad_norm': 1.0686736106872559, 'learning_rate': 2.6892547165932252e-05, 'epoch': 8.636363636363637}\n{'loss': 0.086, 'grad_norm': 0.906183660030365, 'learning_rate': 2.6451685736982546e-05, 'epoch': 8.659090909090908}\n{'loss': 0.0674, 'grad_norm': 0.9138258099555969, 'learning_rate': 2.6010824308032832e-05, 'epoch': 8.681818181818182}\n{'loss': 0.0812, 'grad_norm': 0.8893517255783081, 'learning_rate': 2.5569962879083126e-05, 'epoch': 8.704545454545455}\n{'loss': 0.086, 'grad_norm': 0.8493722081184387, 'learning_rate': 2.512910145013342e-05, 'epoch': 8.727272727272727}\n{'loss': 0.0859, 'grad_norm': 0.7147332429885864, 'learning_rate': 2.4688240021183706e-05, 'epoch': 8.75}\n{'loss': 0.0619, 'grad_norm': 0.6756185293197632, 'learning_rate': 2.4247378592234e-05, 'epoch': 8.772727272727273}\n{'loss': 0.0974, 'grad_norm': 1.305736780166626, 'learning_rate': 2.380651716328429e-05, 'epoch': 8.795454545454545}\n{'loss': 0.0897, 'grad_norm': 0.7939624190330505, 'learning_rate': 2.3365655734334583e-05, 'epoch': 8.818181818181818}\n{'loss': 0.0738, 'grad_norm': 0.9957733750343323, 'learning_rate': 2.2924794305384873e-05, 'epoch': 8.840909090909092}\n{'loss': 0.1051, 'grad_norm': 1.108059048652649, 'learning_rate': 2.2483932876435163e-05, 'epoch': 8.863636363636363}\n{'loss': 0.0592, 'grad_norm': 0.7462638020515442, 'learning_rate': 2.2043071447485456e-05, 'epoch': 8.886363636363637}\n{'loss': 0.0752, 'grad_norm': 0.9772814512252808, 'learning_rate': 2.1602210018535746e-05, 'epoch': 8.909090909090908}\n{'loss': 0.0561, 'grad_norm': 1.0941344499588013, 'learning_rate': 2.1161348589586036e-05, 'epoch': 8.931818181818182}\n{'loss': 0.0815, 'grad_norm': 1.6225043535232544, 'learning_rate': 2.0720487160636326e-05, 'epoch': 8.954545454545455}\n{'loss': 0.0775, 'grad_norm': 0.9368364810943604, 'learning_rate': 2.0279625731686616e-05, 'epoch': 8.977272727272727}\n{'loss': 0.1929, 'grad_norm': 6.030949115753174, 'learning_rate': 1.983876430273691e-05, 'epoch': 9.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8893    0.8872    0.8882      3404\n     PERCENT     0.9296    0.9296    0.9296        71\n        TYPE     0.9423    0.9683    0.9551     11194\n      VOLUME     0.7857    0.7857    0.7857        56\n\n   micro avg     0.9297    0.9487    0.9391     14725\n   macro avg     0.8867    0.8927    0.8897     14725\nweighted avg     0.9294    0.9487    0.9389     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.2863727807998657, 'eval_f1_macro': 0.8896578108533643, 'eval_precision': 0.9296552642087049, 'eval_recall': 0.9486587436332767, 'eval_f1': 0.9390608719034654, 'eval_accuracy': 0.9303294414295894, 'eval_runtime': 1.4892, 'eval_samples_per_second': 3700.578, 'eval_steps_per_second': 7.386, 'epoch': 9.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.0752, 'grad_norm': 0.960841953754425, 'learning_rate': 1.93979028737872e-05, 'epoch': 9.022727272727273}\n{'loss': 0.0816, 'grad_norm': 1.272974967956543, 'learning_rate': 1.8957041444837493e-05, 'epoch': 9.045454545454545}\n{'loss': 0.0762, 'grad_norm': 1.1277644634246826, 'learning_rate': 1.8516180015887783e-05, 'epoch': 9.068181818181818}\n{'loss': 0.0812, 'grad_norm': 1.828538417816162, 'learning_rate': 1.8075318586938073e-05, 'epoch': 9.090909090909092}\n{'loss': 0.0655, 'grad_norm': 0.6508905291557312, 'learning_rate': 1.7634457157988363e-05, 'epoch': 9.113636363636363}\n{'loss': 0.0743, 'grad_norm': 0.8379848599433899, 'learning_rate': 1.7193595729038653e-05, 'epoch': 9.136363636363637}\n{'loss': 0.0626, 'grad_norm': 1.0249990224838257, 'learning_rate': 1.6752734300088943e-05, 'epoch': 9.159090909090908}\n{'loss': 0.0764, 'grad_norm': 1.1859325170516968, 'learning_rate': 1.6311872871139236e-05, 'epoch': 9.181818181818182}\n{'loss': 0.0912, 'grad_norm': 1.2698733806610107, 'learning_rate': 1.5871011442189526e-05, 'epoch': 9.204545454545455}\n{'loss': 0.0806, 'grad_norm': 1.2768057584762573, 'learning_rate': 1.5430150013239816e-05, 'epoch': 9.227272727272727}\n{'loss': 0.0756, 'grad_norm': 0.9423442482948303, 'learning_rate': 1.4989288584290108e-05, 'epoch': 9.25}\n{'loss': 0.0644, 'grad_norm': 0.6116526126861572, 'learning_rate': 1.45484271553404e-05, 'epoch': 9.272727272727273}\n{'loss': 0.0833, 'grad_norm': 0.7673684358596802, 'learning_rate': 1.4107565726390691e-05, 'epoch': 9.295454545454545}\n{'loss': 0.0848, 'grad_norm': 1.2963179349899292, 'learning_rate': 1.3666704297440983e-05, 'epoch': 9.318181818181818}\n{'loss': 0.0641, 'grad_norm': 0.8317763805389404, 'learning_rate': 1.3225842868491273e-05, 'epoch': 9.340909090909092}\n{'loss': 0.0473, 'grad_norm': 0.6571768522262573, 'learning_rate': 1.2784981439541563e-05, 'epoch': 9.363636363636363}\n{'loss': 0.0817, 'grad_norm': 1.2748092412948608, 'learning_rate': 1.2344120010591853e-05, 'epoch': 9.386363636363637}\n{'loss': 0.0668, 'grad_norm': 1.200518012046814, 'learning_rate': 1.1903258581642145e-05, 'epoch': 9.409090909090908}\n{'loss': 0.0802, 'grad_norm': 0.7267153859138489, 'learning_rate': 1.1462397152692436e-05, 'epoch': 9.431818181818182}\n{'loss': 0.0725, 'grad_norm': 0.9656310081481934, 'learning_rate': 1.1021535723742728e-05, 'epoch': 9.454545454545455}\n{'loss': 0.0777, 'grad_norm': 0.8776610493659973, 'learning_rate': 1.0580674294793018e-05, 'epoch': 9.477272727272727}\n{'loss': 0.0612, 'grad_norm': 0.7578968405723572, 'learning_rate': 1.0139812865843308e-05, 'epoch': 9.5}\n{'loss': 0.0637, 'grad_norm': 0.9192720055580139, 'learning_rate': 9.6989514368936e-06, 'epoch': 9.522727272727273}\n{'loss': 0.0503, 'grad_norm': 0.5242213606834412, 'learning_rate': 9.258090007943891e-06, 'epoch': 9.545454545454545}\n{'loss': 0.1202, 'grad_norm': 1.289110541343689, 'learning_rate': 8.817228578994181e-06, 'epoch': 9.568181818181818}\n{'loss': 0.0514, 'grad_norm': 0.7065591812133789, 'learning_rate': 8.376367150044471e-06, 'epoch': 9.590909090909092}\n{'loss': 0.0613, 'grad_norm': 0.942466676235199, 'learning_rate': 7.935505721094763e-06, 'epoch': 9.613636363636363}\n{'loss': 0.0718, 'grad_norm': 0.7340012788772583, 'learning_rate': 7.494644292145054e-06, 'epoch': 9.636363636363637}\n{'loss': 0.0999, 'grad_norm': 0.9587664008140564, 'learning_rate': 7.053782863195346e-06, 'epoch': 9.659090909090908}\n{'loss': 0.0763, 'grad_norm': 0.9682205319404602, 'learning_rate': 6.6129214342456364e-06, 'epoch': 9.681818181818182}\n{'loss': 0.0821, 'grad_norm': 0.9599878191947937, 'learning_rate': 6.1720600052959264e-06, 'epoch': 9.704545454545455}\n{'loss': 0.0711, 'grad_norm': 0.9732595682144165, 'learning_rate': 5.731198576346218e-06, 'epoch': 9.727272727272727}\n{'loss': 0.0751, 'grad_norm': 0.7043247818946838, 'learning_rate': 5.290337147396509e-06, 'epoch': 9.75}\n{'loss': 0.0603, 'grad_norm': 1.543851613998413, 'learning_rate': 4.8494757184468e-06, 'epoch': 9.772727272727273}\n{'loss': 0.0953, 'grad_norm': 1.1050570011138916, 'learning_rate': 4.408614289497091e-06, 'epoch': 9.795454545454545}\n{'loss': 0.0812, 'grad_norm': 0.9606068134307861, 'learning_rate': 3.9677528605473815e-06, 'epoch': 9.818181818181818}\n{'loss': 0.053, 'grad_norm': 0.7020065784454346, 'learning_rate': 3.526891431597673e-06, 'epoch': 9.840909090909092}\n{'loss': 0.059, 'grad_norm': 0.6381027102470398, 'learning_rate': 3.0860300026479632e-06, 'epoch': 9.863636363636363}\n{'loss': 0.0728, 'grad_norm': 0.776154637336731, 'learning_rate': 2.6451685736982545e-06, 'epoch': 9.886363636363637}\n{'loss': 0.0734, 'grad_norm': 1.0353246927261353, 'learning_rate': 2.2043071447485453e-06, 'epoch': 9.909090909090908}\n{'loss': 0.0595, 'grad_norm': 0.7830576300621033, 'learning_rate': 1.7634457157988364e-06, 'epoch': 9.931818181818182}\n{'loss': 0.0723, 'grad_norm': 0.8580960631370544, 'learning_rate': 1.3225842868491272e-06, 'epoch': 9.954545454545455}\n{'loss': 0.0681, 'grad_norm': 0.7973189353942871, 'learning_rate': 8.817228578994182e-07, 'epoch': 9.977272727272727}\n{'loss': 0.0296, 'grad_norm': 3.192445755004883, 'learning_rate': 4.408614289497091e-07, 'epoch': 10.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8891    0.8875    0.8883      3404\n     PERCENT     0.9296    0.9296    0.9296        71\n        TYPE     0.9419    0.9683    0.9549     11194\n      VOLUME     0.7818    0.7679    0.7748        56\n\n   micro avg     0.9293    0.9487    0.9389     14725\n   macro avg     0.8856    0.8883    0.8869     14725\nweighted avg     0.9291    0.9487    0.9387     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.288786917924881, 'eval_f1_macro': 0.8868890754738292, 'eval_precision': 0.9293460182289934, 'eval_recall': 0.9486587436332767, 'eval_f1': 0.9389030783707487, 'eval_accuracy': 0.9301649947925231, 'eval_runtime': 1.4823, 'eval_samples_per_second': 3717.916, 'eval_steps_per_second': 7.421, 'epoch': 10.0}\n{'train_runtime': 64.2255, 'train_samples_per_second': 3431.816, 'train_steps_per_second': 6.851, 'train_loss': 0.2745108462531458, 'epoch': 10.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nThe speedups for torchdynamo mostly come with GPU Ampere or higher and which is not detected here.\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8893    0.8872    0.8882      3404\n     PERCENT     0.9296    0.9296    0.9296        71\n        TYPE     0.9423    0.9683    0.9551     11194\n      VOLUME     0.7857    0.7857    0.7857        56\n\n   micro avg     0.9297    0.9487    0.9391     14725\n   macro avg     0.8867    0.8927    0.8897     14725\nweighted avg     0.9294    0.9487    0.9389     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.2863727807998657, 'eval_f1_macro': 0.8896578108533643, 'eval_precision': 0.9296552642087049, 'eval_recall': 0.9486587436332767, 'eval_f1': 0.9390608719034654, 'eval_accuracy': 0.9303294414295894, 'eval_runtime': 1.5703, 'eval_samples_per_second': 3509.627, 'eval_steps_per_second': 7.005, 'epoch': 10.0}\n","output_type":"stream"},{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/tmp/ipykernel_36/3364797558.py:66: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 2.2572, 'grad_norm': 7.357028961181641, 'learning_rate': 0.0, 'epoch': 0.022727272727272728}\n{'loss': 2.2706, 'grad_norm': 7.434906005859375, 'learning_rate': 3.9677528605473815e-06, 'epoch': 0.045454545454545456}\n{'loss': 2.257, 'grad_norm': 7.399474620819092, 'learning_rate': 7.935505721094763e-06, 'epoch': 0.06818181818181818}\n{'loss': 2.2453, 'grad_norm': 7.214163303375244, 'learning_rate': 1.1903258581642145e-05, 'epoch': 0.09090909090909091}\n{'loss': 2.2131, 'grad_norm': 7.225630283355713, 'learning_rate': 1.5871011442189526e-05, 'epoch': 0.11363636363636363}\n{'loss': 2.1912, 'grad_norm': 7.596639633178711, 'learning_rate': 1.983876430273691e-05, 'epoch': 0.13636363636363635}\n{'loss': 2.1352, 'grad_norm': 6.7994818687438965, 'learning_rate': 2.380651716328429e-05, 'epoch': 0.1590909090909091}\n{'loss': 2.0892, 'grad_norm': 7.031325340270996, 'learning_rate': 2.7774270023831672e-05, 'epoch': 0.18181818181818182}\n{'loss': 2.0127, 'grad_norm': 6.885680198669434, 'learning_rate': 3.174202288437905e-05, 'epoch': 0.20454545454545456}\n{'loss': 1.9455, 'grad_norm': 6.753686904907227, 'learning_rate': 3.570977574492644e-05, 'epoch': 0.22727272727272727}\n{'loss': 1.8812, 'grad_norm': 6.218811511993408, 'learning_rate': 3.967752860547382e-05, 'epoch': 0.25}\n{'loss': 1.8033, 'grad_norm': 6.103575229644775, 'learning_rate': 4.36452814660212e-05, 'epoch': 0.2727272727272727}\n{'loss': 1.6989, 'grad_norm': 5.924468994140625, 'learning_rate': 4.761303432656858e-05, 'epoch': 0.29545454545454547}\n{'loss': 1.6487, 'grad_norm': 5.409346103668213, 'learning_rate': 5.1580787187115965e-05, 'epoch': 0.3181818181818182}\n{'loss': 1.5721, 'grad_norm': 4.713029861450195, 'learning_rate': 5.5548540047663345e-05, 'epoch': 0.3409090909090909}\n{'loss': 1.4428, 'grad_norm': 4.536482810974121, 'learning_rate': 5.951629290821072e-05, 'epoch': 0.36363636363636365}\n{'loss': 1.3971, 'grad_norm': 3.741960287094116, 'learning_rate': 6.34840457687581e-05, 'epoch': 0.38636363636363635}\n{'loss': 1.2789, 'grad_norm': 3.1317269802093506, 'learning_rate': 6.745179862930548e-05, 'epoch': 0.4090909090909091}\n{'loss': 1.2268, 'grad_norm': 2.3824801445007324, 'learning_rate': 7.141955148985288e-05, 'epoch': 0.4318181818181818}\n{'loss': 1.2344, 'grad_norm': 1.8188073635101318, 'learning_rate': 7.538730435040026e-05, 'epoch': 0.45454545454545453}\n{'loss': 1.1539, 'grad_norm': 1.7051790952682495, 'learning_rate': 7.935505721094764e-05, 'epoch': 0.4772727272727273}\n{'loss': 1.1455, 'grad_norm': 2.068497657775879, 'learning_rate': 8.332281007149502e-05, 'epoch': 0.5}\n{'loss': 1.1146, 'grad_norm': 2.107595205307007, 'learning_rate': 8.72905629320424e-05, 'epoch': 0.5227272727272727}\n{'loss': 1.0415, 'grad_norm': 1.7205822467803955, 'learning_rate': 9.125831579258978e-05, 'epoch': 0.5454545454545454}\n{'loss': 1.0693, 'grad_norm': 1.7919833660125732, 'learning_rate': 9.522606865313716e-05, 'epoch': 0.5681818181818182}\n{'loss': 1.0575, 'grad_norm': 1.1893694400787354, 'learning_rate': 9.919382151368455e-05, 'epoch': 0.5909090909090909}\n{'loss': 1.0534, 'grad_norm': 1.077870488166809, 'learning_rate': 0.00010316157437423193, 'epoch': 0.6136363636363636}\n{'loss': 0.993, 'grad_norm': 1.3339753150939941, 'learning_rate': 0.00010712932723477931, 'epoch': 0.6363636363636364}\n{'loss': 0.9606, 'grad_norm': 2.012131452560425, 'learning_rate': 0.00011109708009532669, 'epoch': 0.6590909090909091}\n{'loss': 0.9313, 'grad_norm': 1.798116683959961, 'learning_rate': 0.00011506483295587407, 'epoch': 0.6818181818181818}\n{'loss': 0.9292, 'grad_norm': 1.6703346967697144, 'learning_rate': 0.00011903258581642144, 'epoch': 0.7045454545454546}\n{'loss': 0.9679, 'grad_norm': 1.2660784721374512, 'learning_rate': 0.00012300033867696883, 'epoch': 0.7272727272727273}\n{'loss': 0.8469, 'grad_norm': 1.156336784362793, 'learning_rate': 0.0001269680915375162, 'epoch': 0.75}\n{'loss': 0.8358, 'grad_norm': 1.0503755807876587, 'learning_rate': 0.0001309358443980636, 'epoch': 0.7727272727272727}\n{'loss': 0.8934, 'grad_norm': 0.9859971404075623, 'learning_rate': 0.00013490359725861097, 'epoch': 0.7954545454545454}\n{'loss': 0.8038, 'grad_norm': 0.9419578313827515, 'learning_rate': 0.00013887135011915835, 'epoch': 0.8181818181818182}\n{'loss': 0.7157, 'grad_norm': 0.9353764653205872, 'learning_rate': 0.00014283910297970576, 'epoch': 0.8409090909090909}\n{'loss': 0.7342, 'grad_norm': 0.9800861477851868, 'learning_rate': 0.00014680685584025314, 'epoch': 0.8636363636363636}\n{'loss': 0.7069, 'grad_norm': 0.8148962259292603, 'learning_rate': 0.00015077460870080052, 'epoch': 0.8863636363636364}\n{'loss': 0.6655, 'grad_norm': 0.8451642990112305, 'learning_rate': 0.0001547423615613479, 'epoch': 0.9090909090909091}\n{'loss': 0.6312, 'grad_norm': 0.9912528395652771, 'learning_rate': 0.00015871011442189527, 'epoch': 0.9318181818181818}\n{'loss': 0.6082, 'grad_norm': 0.7083732485771179, 'learning_rate': 0.00016267786728244263, 'epoch': 0.9545454545454546}\n{'loss': 0.6388, 'grad_norm': 1.066401481628418, 'learning_rate': 0.00016664562014299003, 'epoch': 0.9772727272727273}\n{'loss': 0.6947, 'grad_norm': 2.521566152572632, 'learning_rate': 0.00017061337300353741, 'epoch': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.6815    0.7049    0.6930      3311\n     PERCENT     0.8571    0.4186    0.5625        86\n        TYPE     0.8459    0.8918    0.8682     11299\n      VOLUME     0.0000    0.0000    0.0000        42\n\n   micro avg     0.8093    0.8445    0.8265     14738\n   macro avg     0.5961    0.5038    0.5309     14738\nweighted avg     0.8066    0.8445    0.8246     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.5636242628097534, 'eval_f1_macro': 0.5309348299017236, 'eval_precision': 0.809338015346599, 'eval_recall': 0.8444836477133939, 'eval_f1': 0.826537388763448, 'eval_accuracy': 0.8284246762471996, 'eval_runtime': 1.51, 'eval_samples_per_second': 3649.12, 'eval_steps_per_second': 7.285, 'epoch': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.5592, 'grad_norm': 1.7229164838790894, 'learning_rate': 0.0001745811258640848, 'epoch': 1.0227272727272727}\n{'loss': 0.5905, 'grad_norm': 1.8860876560211182, 'learning_rate': 0.0001741402644351351, 'epoch': 1.0454545454545454}\n{'loss': 0.5602, 'grad_norm': 0.9789963960647583, 'learning_rate': 0.00017369940300618537, 'epoch': 1.0681818181818181}\n{'loss': 0.5442, 'grad_norm': 0.7965663075447083, 'learning_rate': 0.00017325854157723567, 'epoch': 1.0909090909090908}\n{'loss': 0.5183, 'grad_norm': 0.969120979309082, 'learning_rate': 0.00017281768014828597, 'epoch': 1.1136363636363635}\n{'loss': 0.5777, 'grad_norm': 1.191943883895874, 'learning_rate': 0.00017237681871933624, 'epoch': 1.1363636363636362}\n{'loss': 0.4288, 'grad_norm': 0.9816702008247375, 'learning_rate': 0.00017193595729038654, 'epoch': 1.1590909090909092}\n{'loss': 0.5094, 'grad_norm': 1.13300359249115, 'learning_rate': 0.00017149509586143684, 'epoch': 1.1818181818181819}\n{'loss': 0.4836, 'grad_norm': 1.00528883934021, 'learning_rate': 0.00017105423443248711, 'epoch': 1.2045454545454546}\n{'loss': 0.5354, 'grad_norm': 0.7879138588905334, 'learning_rate': 0.00017061337300353741, 'epoch': 1.2272727272727273}\n{'loss': 0.5099, 'grad_norm': 1.1502115726470947, 'learning_rate': 0.0001701725115745877, 'epoch': 1.25}\n{'loss': 0.4511, 'grad_norm': 0.8705875277519226, 'learning_rate': 0.000169731650145638, 'epoch': 1.2727272727272727}\n{'loss': 0.4364, 'grad_norm': 0.8532982468605042, 'learning_rate': 0.0001692907887166883, 'epoch': 1.2954545454545454}\n{'loss': 0.5004, 'grad_norm': 1.268633484840393, 'learning_rate': 0.00016884992728773856, 'epoch': 1.3181818181818181}\n{'loss': 0.3931, 'grad_norm': 1.2455776929855347, 'learning_rate': 0.00016840906585878886, 'epoch': 1.3409090909090908}\n{'loss': 0.4499, 'grad_norm': 1.2395029067993164, 'learning_rate': 0.00016796820442983916, 'epoch': 1.3636363636363638}\n{'loss': 0.4463, 'grad_norm': 1.8611823320388794, 'learning_rate': 0.00016752734300088943, 'epoch': 1.3863636363636362}\n{'loss': 0.446, 'grad_norm': 1.0957728624343872, 'learning_rate': 0.00016708648157193973, 'epoch': 1.4090909090909092}\n{'loss': 0.4197, 'grad_norm': 1.0111339092254639, 'learning_rate': 0.00016664562014299003, 'epoch': 1.4318181818181819}\n{'loss': 0.4058, 'grad_norm': 0.981468677520752, 'learning_rate': 0.0001662047587140403, 'epoch': 1.4545454545454546}\n{'loss': 0.3752, 'grad_norm': 1.448225498199463, 'learning_rate': 0.0001657638972850906, 'epoch': 1.4772727272727273}\n{'loss': 0.4184, 'grad_norm': 0.7917822003364563, 'learning_rate': 0.0001653230358561409, 'epoch': 1.5}\n{'loss': 0.3917, 'grad_norm': 0.6859632134437561, 'learning_rate': 0.00016488217442719118, 'epoch': 1.5227272727272727}\n{'loss': 0.3747, 'grad_norm': 1.154096245765686, 'learning_rate': 0.00016444131299824148, 'epoch': 1.5454545454545454}\n{'loss': 0.3715, 'grad_norm': 0.9470091462135315, 'learning_rate': 0.00016400045156929178, 'epoch': 1.5681818181818183}\n{'loss': 0.374, 'grad_norm': 0.9324954152107239, 'learning_rate': 0.00016355959014034205, 'epoch': 1.5909090909090908}\n{'loss': 0.3685, 'grad_norm': 1.3058114051818848, 'learning_rate': 0.00016311872871139235, 'epoch': 1.6136363636363638}\n{'loss': 0.4025, 'grad_norm': 1.4698199033737183, 'learning_rate': 0.00016267786728244263, 'epoch': 1.6363636363636362}\n{'loss': 0.3639, 'grad_norm': 0.7662143111228943, 'learning_rate': 0.00016223700585349293, 'epoch': 1.6590909090909092}\n{'loss': 0.392, 'grad_norm': 0.9610000848770142, 'learning_rate': 0.00016179614442454323, 'epoch': 1.6818181818181817}\n{'loss': 0.3599, 'grad_norm': 0.9083689451217651, 'learning_rate': 0.0001613552829955935, 'epoch': 1.7045454545454546}\n{'loss': 0.3557, 'grad_norm': 0.7587834596633911, 'learning_rate': 0.0001609144215666438, 'epoch': 1.7272727272727273}\n{'loss': 0.3749, 'grad_norm': 1.4903260469436646, 'learning_rate': 0.0001604735601376941, 'epoch': 1.75}\n{'loss': 0.3765, 'grad_norm': 0.8673728704452515, 'learning_rate': 0.0001600326987087444, 'epoch': 1.7727272727272727}\n{'loss': 0.3166, 'grad_norm': 0.811112642288208, 'learning_rate': 0.00015959183727979467, 'epoch': 1.7954545454545454}\n{'loss': 0.3542, 'grad_norm': 0.8200892806053162, 'learning_rate': 0.00015915097585084497, 'epoch': 1.8181818181818183}\n{'loss': 0.2947, 'grad_norm': 1.2557123899459839, 'learning_rate': 0.00015871011442189527, 'epoch': 1.8409090909090908}\n{'loss': 0.3701, 'grad_norm': 0.8825678825378418, 'learning_rate': 0.00015826925299294555, 'epoch': 1.8636363636363638}\n{'loss': 0.3832, 'grad_norm': 1.2664644718170166, 'learning_rate': 0.00015782839156399585, 'epoch': 1.8863636363636362}\n{'loss': 0.3833, 'grad_norm': 1.0714304447174072, 'learning_rate': 0.00015738753013504615, 'epoch': 1.9090909090909092}\n{'loss': 0.3928, 'grad_norm': 0.8188475370407104, 'learning_rate': 0.00015694666870609642, 'epoch': 1.9318181818181817}\n{'loss': 0.3799, 'grad_norm': 1.0131943225860596, 'learning_rate': 0.00015650580727714672, 'epoch': 1.9545454545454546}\n{'loss': 0.2946, 'grad_norm': 1.0043715238571167, 'learning_rate': 0.00015606494584819702, 'epoch': 1.9772727272727273}\n{'loss': 0.2564, 'grad_norm': 3.303468704223633, 'learning_rate': 0.0001556240844192473, 'epoch': 2.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8407    0.8052    0.8226      3311\n     PERCENT     0.6423    0.9186    0.7560        86\n        TYPE     0.9215    0.9634    0.9420     11299\n      VOLUME     0.5769    0.3571    0.4412        42\n\n   micro avg     0.9017    0.9259    0.9137     14738\n   macro avg     0.7454    0.7611    0.7404     14738\nweighted avg     0.9008    0.9259    0.9127     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.3168402314186096, 'eval_f1_macro': 0.7404411035667486, 'eval_precision': 0.9017379237428137, 'eval_recall': 0.925905821685439, 'eval_f1': 0.913662080278531, 'eval_accuracy': 0.9074913939129009, 'eval_runtime': 1.5368, 'eval_samples_per_second': 3585.34, 'eval_steps_per_second': 7.158, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.2777, 'grad_norm': 0.9000925421714783, 'learning_rate': 0.0001551832229902976, 'epoch': 2.022727272727273}\n{'loss': 0.2894, 'grad_norm': 0.9737458825111389, 'learning_rate': 0.0001547423615613479, 'epoch': 2.0454545454545454}\n{'loss': 0.3196, 'grad_norm': 0.88975590467453, 'learning_rate': 0.0001543015001323982, 'epoch': 2.0681818181818183}\n{'loss': 0.3357, 'grad_norm': 1.1511645317077637, 'learning_rate': 0.00015386063870344847, 'epoch': 2.090909090909091}\n{'loss': 0.2963, 'grad_norm': 1.0833038091659546, 'learning_rate': 0.00015341977727449877, 'epoch': 2.1136363636363638}\n{'loss': 0.2247, 'grad_norm': 1.0697153806686401, 'learning_rate': 0.00015297891584554907, 'epoch': 2.1363636363636362}\n{'loss': 0.2817, 'grad_norm': 0.8396615386009216, 'learning_rate': 0.00015253805441659934, 'epoch': 2.159090909090909}\n{'loss': 0.3081, 'grad_norm': 1.2884925603866577, 'learning_rate': 0.00015209719298764964, 'epoch': 2.1818181818181817}\n{'loss': 0.3074, 'grad_norm': 0.9973503947257996, 'learning_rate': 0.00015165633155869994, 'epoch': 2.2045454545454546}\n{'loss': 0.337, 'grad_norm': 1.0377424955368042, 'learning_rate': 0.00015121547012975021, 'epoch': 2.227272727272727}\n{'loss': 0.3064, 'grad_norm': 0.9628331661224365, 'learning_rate': 0.00015077460870080052, 'epoch': 2.25}\n{'loss': 0.2586, 'grad_norm': 1.415488839149475, 'learning_rate': 0.00015033374727185082, 'epoch': 2.2727272727272725}\n{'loss': 0.3395, 'grad_norm': 0.9059603810310364, 'learning_rate': 0.0001498928858429011, 'epoch': 2.2954545454545454}\n{'loss': 0.3069, 'grad_norm': 1.140849232673645, 'learning_rate': 0.0001494520244139514, 'epoch': 2.3181818181818183}\n{'loss': 0.2044, 'grad_norm': 1.4627426862716675, 'learning_rate': 0.00014901116298500166, 'epoch': 2.340909090909091}\n{'loss': 0.2709, 'grad_norm': 1.752537488937378, 'learning_rate': 0.00014857030155605196, 'epoch': 2.3636363636363638}\n{'loss': 0.2386, 'grad_norm': 1.2164009809494019, 'learning_rate': 0.00014812944012710226, 'epoch': 2.3863636363636362}\n{'loss': 0.3192, 'grad_norm': 1.0253782272338867, 'learning_rate': 0.00014768857869815253, 'epoch': 2.409090909090909}\n{'loss': 0.2801, 'grad_norm': 1.019444227218628, 'learning_rate': 0.00014724771726920284, 'epoch': 2.4318181818181817}\n{'loss': 0.2209, 'grad_norm': 2.091597318649292, 'learning_rate': 0.00014680685584025314, 'epoch': 2.4545454545454546}\n{'loss': 0.2693, 'grad_norm': 1.4319742918014526, 'learning_rate': 0.0001463659944113034, 'epoch': 2.4772727272727275}\n{'loss': 0.2456, 'grad_norm': 1.5549495220184326, 'learning_rate': 0.0001459251329823537, 'epoch': 2.5}\n{'loss': 0.2529, 'grad_norm': 1.4979444742202759, 'learning_rate': 0.000145484271553404, 'epoch': 2.5227272727272725}\n{'loss': 0.2519, 'grad_norm': 1.2714053392410278, 'learning_rate': 0.00014504341012445428, 'epoch': 2.5454545454545454}\n{'loss': 0.2508, 'grad_norm': 1.7992370128631592, 'learning_rate': 0.00014460254869550458, 'epoch': 2.5681818181818183}\n{'loss': 0.251, 'grad_norm': 1.3589109182357788, 'learning_rate': 0.00014416168726655488, 'epoch': 2.590909090909091}\n{'loss': 0.2457, 'grad_norm': 0.8675611615180969, 'learning_rate': 0.00014372082583760516, 'epoch': 2.6136363636363638}\n{'loss': 0.2937, 'grad_norm': 1.141860842704773, 'learning_rate': 0.00014327996440865546, 'epoch': 2.6363636363636362}\n{'loss': 0.224, 'grad_norm': 0.6847466230392456, 'learning_rate': 0.00014283910297970576, 'epoch': 2.659090909090909}\n{'loss': 0.1996, 'grad_norm': 0.9868929386138916, 'learning_rate': 0.00014239824155075603, 'epoch': 2.6818181818181817}\n{'loss': 0.2178, 'grad_norm': 1.0845071077346802, 'learning_rate': 0.00014195738012180633, 'epoch': 2.7045454545454546}\n{'loss': 0.3157, 'grad_norm': 1.2773891687393188, 'learning_rate': 0.0001415165186928566, 'epoch': 2.7272727272727275}\n{'loss': 0.2637, 'grad_norm': 1.1161773204803467, 'learning_rate': 0.0001410756572639069, 'epoch': 2.75}\n{'loss': 0.2694, 'grad_norm': 1.2729238271713257, 'learning_rate': 0.0001406347958349572, 'epoch': 2.7727272727272725}\n{'loss': 0.2395, 'grad_norm': 0.8279178142547607, 'learning_rate': 0.00014019393440600748, 'epoch': 2.7954545454545454}\n{'loss': 0.2675, 'grad_norm': 0.994873583316803, 'learning_rate': 0.00013975307297705778, 'epoch': 2.8181818181818183}\n{'loss': 0.2879, 'grad_norm': 1.1579164266586304, 'learning_rate': 0.00013931221154810808, 'epoch': 2.840909090909091}\n{'loss': 0.2292, 'grad_norm': 1.2112805843353271, 'learning_rate': 0.00013887135011915835, 'epoch': 2.8636363636363638}\n{'loss': 0.2678, 'grad_norm': 1.1312259435653687, 'learning_rate': 0.00013843048869020865, 'epoch': 2.8863636363636362}\n{'loss': 0.2283, 'grad_norm': 0.9079023003578186, 'learning_rate': 0.00013798962726125895, 'epoch': 2.909090909090909}\n{'loss': 0.2677, 'grad_norm': 1.4103009700775146, 'learning_rate': 0.00013754876583230922, 'epoch': 2.9318181818181817}\n{'loss': 0.2602, 'grad_norm': 0.8971174955368042, 'learning_rate': 0.00013710790440335952, 'epoch': 2.9545454545454546}\n{'loss': 0.2736, 'grad_norm': 1.4198102951049805, 'learning_rate': 0.00013666704297440982, 'epoch': 2.9772727272727275}\n{'loss': 0.133, 'grad_norm': 2.1104578971862793, 'learning_rate': 0.0001362261815454601, 'epoch': 3.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8142    0.8883    0.8496      3311\n     PERCENT     0.7619    0.9302    0.8377        86\n        TYPE     0.9418    0.9589    0.9503     11299\n      VOLUME     0.6667    0.5714    0.6154        42\n\n   micro avg     0.9097    0.9418    0.9255     14738\n   macro avg     0.7962    0.8372    0.8133     14738\nweighted avg     0.9113    0.9418    0.9261     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.27317240834236145, 'eval_f1_macro': 0.8132565420677846, 'eval_precision': 0.9097463459395687, 'eval_recall': 0.9417831456099878, 'eval_f1': 0.9254875812635439, 'eval_accuracy': 0.9187476094202502, 'eval_runtime': 1.8679, 'eval_samples_per_second': 2949.802, 'eval_steps_per_second': 5.889, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.1736, 'grad_norm': 1.3963277339935303, 'learning_rate': 0.0001357853201165104, 'epoch': 3.022727272727273}\n{'loss': 0.2192, 'grad_norm': 1.147758960723877, 'learning_rate': 0.0001353444586875607, 'epoch': 3.0454545454545454}\n{'loss': 0.2029, 'grad_norm': 0.8200308084487915, 'learning_rate': 0.00013490359725861097, 'epoch': 3.0681818181818183}\n{'loss': 0.2087, 'grad_norm': 1.5655293464660645, 'learning_rate': 0.00013446273582966127, 'epoch': 3.090909090909091}\n{'loss': 0.2156, 'grad_norm': 2.1853275299072266, 'learning_rate': 0.00013402187440071154, 'epoch': 3.1136363636363638}\n{'loss': 0.1896, 'grad_norm': 2.2111082077026367, 'learning_rate': 0.00013358101297176184, 'epoch': 3.1363636363636362}\n{'loss': 0.2627, 'grad_norm': 2.553485870361328, 'learning_rate': 0.00013314015154281214, 'epoch': 3.159090909090909}\n{'loss': 0.1695, 'grad_norm': 0.9409915208816528, 'learning_rate': 0.00013269929011386242, 'epoch': 3.1818181818181817}\n{'loss': 0.1976, 'grad_norm': 0.9608873128890991, 'learning_rate': 0.00013225842868491272, 'epoch': 3.2045454545454546}\n{'loss': 0.2204, 'grad_norm': 1.0112993717193604, 'learning_rate': 0.00013181756725596302, 'epoch': 3.227272727272727}\n{'loss': 0.2212, 'grad_norm': 2.0517680644989014, 'learning_rate': 0.0001313767058270133, 'epoch': 3.25}\n{'loss': 0.2022, 'grad_norm': 1.792686104774475, 'learning_rate': 0.0001309358443980636, 'epoch': 3.2727272727272725}\n{'loss': 0.1872, 'grad_norm': 1.2455973625183105, 'learning_rate': 0.0001304949829691139, 'epoch': 3.2954545454545454}\n{'loss': 0.2232, 'grad_norm': 0.8591943979263306, 'learning_rate': 0.00013005412154016416, 'epoch': 3.3181818181818183}\n{'loss': 0.1812, 'grad_norm': 0.8317391872406006, 'learning_rate': 0.00012961326011121446, 'epoch': 3.340909090909091}\n{'loss': 0.1752, 'grad_norm': 1.8522365093231201, 'learning_rate': 0.00012917239868226476, 'epoch': 3.3636363636363638}\n{'loss': 0.1512, 'grad_norm': 1.0833332538604736, 'learning_rate': 0.00012873153725331504, 'epoch': 3.3863636363636362}\n{'loss': 0.1844, 'grad_norm': 0.7964786291122437, 'learning_rate': 0.00012829067582436534, 'epoch': 3.409090909090909}\n{'loss': 0.173, 'grad_norm': 1.3652169704437256, 'learning_rate': 0.00012784981439541564, 'epoch': 3.4318181818181817}\n{'loss': 0.2094, 'grad_norm': 0.8752851486206055, 'learning_rate': 0.0001274089529664659, 'epoch': 3.4545454545454546}\n{'loss': 0.174, 'grad_norm': 1.690371036529541, 'learning_rate': 0.0001269680915375162, 'epoch': 3.4772727272727275}\n{'loss': 0.2003, 'grad_norm': 1.4123293161392212, 'learning_rate': 0.0001265272301085665, 'epoch': 3.5}\n{'loss': 0.1972, 'grad_norm': 0.9385006427764893, 'learning_rate': 0.00012608636867961678, 'epoch': 3.5227272727272725}\n{'loss': 0.1534, 'grad_norm': 0.8098877668380737, 'learning_rate': 0.00012564550725066708, 'epoch': 3.5454545454545454}\n{'loss': 0.2079, 'grad_norm': 0.9768909215927124, 'learning_rate': 0.00012520464582171738, 'epoch': 3.5681818181818183}\n{'loss': 0.2187, 'grad_norm': 2.2797892093658447, 'learning_rate': 0.00012476378439276766, 'epoch': 3.590909090909091}\n{'loss': 0.2182, 'grad_norm': 1.6913444995880127, 'learning_rate': 0.00012432292296381796, 'epoch': 3.6136363636363638}\n{'loss': 0.2025, 'grad_norm': 1.6395525932312012, 'learning_rate': 0.00012388206153486826, 'epoch': 3.6363636363636362}\n{'loss': 0.1912, 'grad_norm': 1.7228111028671265, 'learning_rate': 0.00012344120010591853, 'epoch': 3.659090909090909}\n{'loss': 0.1899, 'grad_norm': 0.943952202796936, 'learning_rate': 0.00012300033867696883, 'epoch': 3.6818181818181817}\n{'loss': 0.1827, 'grad_norm': 1.5374633073806763, 'learning_rate': 0.00012255947724801913, 'epoch': 3.7045454545454546}\n{'loss': 0.1985, 'grad_norm': 0.8938290476799011, 'learning_rate': 0.0001221186158190694, 'epoch': 3.7272727272727275}\n{'loss': 0.2161, 'grad_norm': 1.0924124717712402, 'learning_rate': 0.00012167775439011972, 'epoch': 3.75}\n{'loss': 0.167, 'grad_norm': 0.9471977353096008, 'learning_rate': 0.00012123689296116999, 'epoch': 3.7727272727272725}\n{'loss': 0.2161, 'grad_norm': 1.389725923538208, 'learning_rate': 0.00012079603153222029, 'epoch': 3.7954545454545454}\n{'loss': 0.1907, 'grad_norm': 1.9181653261184692, 'learning_rate': 0.00012035517010327059, 'epoch': 3.8181818181818183}\n{'loss': 0.2537, 'grad_norm': 1.6910769939422607, 'learning_rate': 0.00011991430867432086, 'epoch': 3.840909090909091}\n{'loss': 0.24, 'grad_norm': 1.8771308660507202, 'learning_rate': 0.00011947344724537116, 'epoch': 3.8636363636363638}\n{'loss': 0.1451, 'grad_norm': 1.0402463674545288, 'learning_rate': 0.00011903258581642144, 'epoch': 3.8863636363636362}\n{'loss': 0.1898, 'grad_norm': 1.494332194328308, 'learning_rate': 0.00011859172438747174, 'epoch': 3.909090909090909}\n{'loss': 0.2293, 'grad_norm': 1.6004464626312256, 'learning_rate': 0.00011815086295852204, 'epoch': 3.9318181818181817}\n{'loss': 0.2111, 'grad_norm': 1.3079817295074463, 'learning_rate': 0.00011771000152957231, 'epoch': 3.9545454545454546}\n{'loss': 0.2014, 'grad_norm': 1.1280577182769775, 'learning_rate': 0.00011726914010062261, 'epoch': 3.9772727272727275}\n{'loss': 0.0202, 'grad_norm': 0.6521297097206116, 'learning_rate': 0.00011682827867167291, 'epoch': 4.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8856    0.8559    0.8705      3311\n     PERCENT     0.8444    0.8837    0.8636        86\n        TYPE     0.9366    0.9709    0.9534     11299\n      VOLUME     0.6279    0.6429    0.6353        42\n\n   micro avg     0.9243    0.9436    0.9339     14738\n   macro avg     0.8236    0.8383    0.8307     14738\nweighted avg     0.9237    0.9436    0.9334     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.2521900534629822, 'eval_f1_macro': 0.830718222832355, 'eval_precision': 0.9242988169613187, 'eval_recall': 0.9436151445243588, 'eval_f1': 0.9338571044856299, 'eval_accuracy': 0.9253046281623954, 'eval_runtime': 1.5262, 'eval_samples_per_second': 3610.308, 'eval_steps_per_second': 7.208, 'epoch': 4.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.1627, 'grad_norm': 1.297086477279663, 'learning_rate': 0.0001163874172427232, 'epoch': 4.0227272727272725}\n{'loss': 0.1242, 'grad_norm': 1.9874159097671509, 'learning_rate': 0.00011594655581377348, 'epoch': 4.045454545454546}\n{'loss': 0.2013, 'grad_norm': 1.4730324745178223, 'learning_rate': 0.00011550569438482378, 'epoch': 4.068181818181818}\n{'loss': 0.1692, 'grad_norm': 1.6836880445480347, 'learning_rate': 0.00011506483295587407, 'epoch': 4.090909090909091}\n{'loss': 0.1801, 'grad_norm': 1.143133521080017, 'learning_rate': 0.00011462397152692436, 'epoch': 4.113636363636363}\n{'loss': 0.1538, 'grad_norm': 1.6253126859664917, 'learning_rate': 0.00011418311009797466, 'epoch': 4.136363636363637}\n{'loss': 0.1609, 'grad_norm': 2.5562944412231445, 'learning_rate': 0.00011374224866902494, 'epoch': 4.159090909090909}\n{'loss': 0.1503, 'grad_norm': 1.6095399856567383, 'learning_rate': 0.00011330138724007523, 'epoch': 4.181818181818182}\n{'loss': 0.1255, 'grad_norm': 1.4909895658493042, 'learning_rate': 0.00011286052581112553, 'epoch': 4.204545454545454}\n{'loss': 0.1794, 'grad_norm': 1.0586273670196533, 'learning_rate': 0.00011241966438217582, 'epoch': 4.2272727272727275}\n{'loss': 0.1605, 'grad_norm': 0.9970167875289917, 'learning_rate': 0.0001119788029532261, 'epoch': 4.25}\n{'loss': 0.1761, 'grad_norm': 1.1198742389678955, 'learning_rate': 0.00011153794152427639, 'epoch': 4.2727272727272725}\n{'loss': 0.1714, 'grad_norm': 1.5711396932601929, 'learning_rate': 0.00011109708009532669, 'epoch': 4.295454545454546}\n{'loss': 0.1899, 'grad_norm': 1.9584009647369385, 'learning_rate': 0.00011065621866637699, 'epoch': 4.318181818181818}\n{'loss': 0.172, 'grad_norm': 1.4268251657485962, 'learning_rate': 0.00011021535723742726, 'epoch': 4.340909090909091}\n{'loss': 0.138, 'grad_norm': 1.1355761289596558, 'learning_rate': 0.00010977449580847756, 'epoch': 4.363636363636363}\n{'loss': 0.1687, 'grad_norm': 1.315122127532959, 'learning_rate': 0.00010933363437952786, 'epoch': 4.386363636363637}\n{'loss': 0.1836, 'grad_norm': 1.6794763803482056, 'learning_rate': 0.00010889277295057814, 'epoch': 4.409090909090909}\n{'loss': 0.1444, 'grad_norm': 0.9355993270874023, 'learning_rate': 0.00010845191152162844, 'epoch': 4.431818181818182}\n{'loss': 0.1408, 'grad_norm': 1.668926477432251, 'learning_rate': 0.00010801105009267874, 'epoch': 4.454545454545454}\n{'loss': 0.1571, 'grad_norm': 0.908604085445404, 'learning_rate': 0.00010757018866372901, 'epoch': 4.4772727272727275}\n{'loss': 0.1854, 'grad_norm': 0.8372241258621216, 'learning_rate': 0.00010712932723477931, 'epoch': 4.5}\n{'loss': 0.1463, 'grad_norm': 1.3352574110031128, 'learning_rate': 0.00010668846580582961, 'epoch': 4.5227272727272725}\n{'loss': 0.1542, 'grad_norm': 0.9288533329963684, 'learning_rate': 0.00010624760437687988, 'epoch': 4.545454545454545}\n{'loss': 0.1572, 'grad_norm': 1.6898051500320435, 'learning_rate': 0.00010580674294793018, 'epoch': 4.568181818181818}\n{'loss': 0.154, 'grad_norm': 0.8210580348968506, 'learning_rate': 0.00010536588151898046, 'epoch': 4.590909090909091}\n{'loss': 0.1835, 'grad_norm': 0.8700230121612549, 'learning_rate': 0.00010492502009003076, 'epoch': 4.613636363636363}\n{'loss': 0.1766, 'grad_norm': 1.4031962156295776, 'learning_rate': 0.00010448415866108106, 'epoch': 4.636363636363637}\n{'loss': 0.2, 'grad_norm': 1.8151183128356934, 'learning_rate': 0.00010404329723213133, 'epoch': 4.659090909090909}\n{'loss': 0.1438, 'grad_norm': 2.0095152854919434, 'learning_rate': 0.00010360243580318163, 'epoch': 4.681818181818182}\n{'loss': 0.1538, 'grad_norm': 1.4903761148452759, 'learning_rate': 0.00010316157437423193, 'epoch': 4.704545454545455}\n{'loss': 0.1373, 'grad_norm': 0.9410684704780579, 'learning_rate': 0.0001027207129452822, 'epoch': 4.7272727272727275}\n{'loss': 0.164, 'grad_norm': 1.3293064832687378, 'learning_rate': 0.0001022798515163325, 'epoch': 4.75}\n{'loss': 0.1501, 'grad_norm': 2.797520875930786, 'learning_rate': 0.0001018389900873828, 'epoch': 4.7727272727272725}\n{'loss': 0.1592, 'grad_norm': 3.2777085304260254, 'learning_rate': 0.00010139812865843308, 'epoch': 4.795454545454545}\n{'loss': 0.1568, 'grad_norm': 2.282667398452759, 'learning_rate': 0.00010095726722948338, 'epoch': 4.818181818181818}\n{'loss': 0.1194, 'grad_norm': 1.2271265983581543, 'learning_rate': 0.00010051640580053368, 'epoch': 4.840909090909091}\n{'loss': 0.1679, 'grad_norm': 1.3383235931396484, 'learning_rate': 0.00010007554437158395, 'epoch': 4.863636363636363}\n{'loss': 0.1747, 'grad_norm': 1.2583597898483276, 'learning_rate': 9.963468294263425e-05, 'epoch': 4.886363636363637}\n{'loss': 0.1572, 'grad_norm': 2.4125537872314453, 'learning_rate': 9.919382151368455e-05, 'epoch': 4.909090909090909}\n{'loss': 0.1553, 'grad_norm': 2.8654932975769043, 'learning_rate': 9.875296008473482e-05, 'epoch': 4.931818181818182}\n{'loss': 0.1336, 'grad_norm': 1.7536215782165527, 'learning_rate': 9.831209865578512e-05, 'epoch': 4.954545454545455}\n{'loss': 0.129, 'grad_norm': 1.391972303390503, 'learning_rate': 9.787123722683541e-05, 'epoch': 4.9772727272727275}\n{'loss': 0.2047, 'grad_norm': 10.00942325592041, 'learning_rate': 9.74303757978857e-05, 'epoch': 5.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8731    0.8810    0.8770      3311\n     PERCENT     0.8750    0.8953    0.8851        86\n        TYPE     0.9459    0.9632    0.9544     11299\n      VOLUME     0.7500    0.7857    0.7674        42\n\n   micro avg     0.9286    0.9438    0.9362     14738\n   macro avg     0.8610    0.8813    0.8710     14738\nweighted avg     0.9285    0.9438    0.9361     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.24482043087482452, 'eval_f1_macro': 0.8709921530954889, 'eval_precision': 0.9286334201215034, 'eval_recall': 0.9438186999592889, 'eval_f1': 0.9361644849749301, 'eval_accuracy': 0.9275995847221463, 'eval_runtime': 1.4978, 'eval_samples_per_second': 3678.825, 'eval_steps_per_second': 7.344, 'epoch': 5.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.1203, 'grad_norm': 1.3507200479507446, 'learning_rate': 9.6989514368936e-05, 'epoch': 5.0227272727272725}\n{'loss': 0.1399, 'grad_norm': 1.2769532203674316, 'learning_rate': 9.654865293998628e-05, 'epoch': 5.045454545454546}\n{'loss': 0.1169, 'grad_norm': 0.6843737959861755, 'learning_rate': 9.610779151103658e-05, 'epoch': 5.068181818181818}\n{'loss': 0.1613, 'grad_norm': 1.2576650381088257, 'learning_rate': 9.566693008208687e-05, 'epoch': 5.090909090909091}\n{'loss': 0.1278, 'grad_norm': 0.8916916251182556, 'learning_rate': 9.522606865313716e-05, 'epoch': 5.113636363636363}\n{'loss': 0.1349, 'grad_norm': 0.8709037899971008, 'learning_rate': 9.478520722418746e-05, 'epoch': 5.136363636363637}\n{'loss': 0.132, 'grad_norm': 0.9301983714103699, 'learning_rate': 9.434434579523774e-05, 'epoch': 5.159090909090909}\n{'loss': 0.1075, 'grad_norm': 0.8596133589744568, 'learning_rate': 9.390348436628803e-05, 'epoch': 5.181818181818182}\n{'loss': 0.1449, 'grad_norm': 0.8686955571174622, 'learning_rate': 9.346262293733833e-05, 'epoch': 5.204545454545454}\n{'loss': 0.1359, 'grad_norm': 1.0877549648284912, 'learning_rate': 9.302176150838862e-05, 'epoch': 5.2272727272727275}\n{'loss': 0.1106, 'grad_norm': 0.7927473783493042, 'learning_rate': 9.25809000794389e-05, 'epoch': 5.25}\n{'loss': 0.1328, 'grad_norm': 1.5026915073394775, 'learning_rate': 9.21400386504892e-05, 'epoch': 5.2727272727272725}\n{'loss': 0.1347, 'grad_norm': 0.8531867861747742, 'learning_rate': 9.169917722153949e-05, 'epoch': 5.295454545454546}\n{'loss': 0.1293, 'grad_norm': 0.9839953184127808, 'learning_rate': 9.125831579258978e-05, 'epoch': 5.318181818181818}\n{'loss': 0.1192, 'grad_norm': 0.9090065956115723, 'learning_rate': 9.081745436364008e-05, 'epoch': 5.340909090909091}\n{'loss': 0.1019, 'grad_norm': 0.6050375699996948, 'learning_rate': 9.037659293469035e-05, 'epoch': 5.363636363636363}\n{'loss': 0.1391, 'grad_norm': 1.1266695261001587, 'learning_rate': 8.993573150574065e-05, 'epoch': 5.386363636363637}\n{'loss': 0.1116, 'grad_norm': 0.983672559261322, 'learning_rate': 8.949487007679095e-05, 'epoch': 5.409090909090909}\n{'loss': 0.1116, 'grad_norm': 2.0592896938323975, 'learning_rate': 8.905400864784122e-05, 'epoch': 5.431818181818182}\n{'loss': 0.1341, 'grad_norm': 0.8154606223106384, 'learning_rate': 8.861314721889152e-05, 'epoch': 5.454545454545454}\n{'loss': 0.1729, 'grad_norm': 0.9636591672897339, 'learning_rate': 8.817228578994182e-05, 'epoch': 5.4772727272727275}\n{'loss': 0.1282, 'grad_norm': 0.8479440808296204, 'learning_rate': 8.77314243609921e-05, 'epoch': 5.5}\n{'loss': 0.1291, 'grad_norm': 0.9215748310089111, 'learning_rate': 8.72905629320424e-05, 'epoch': 5.5227272727272725}\n{'loss': 0.1487, 'grad_norm': 0.9991750121116638, 'learning_rate': 8.684970150309268e-05, 'epoch': 5.545454545454545}\n{'loss': 0.1171, 'grad_norm': 0.8039140105247498, 'learning_rate': 8.640884007414298e-05, 'epoch': 5.568181818181818}\n{'loss': 0.1104, 'grad_norm': 1.264324426651001, 'learning_rate': 8.596797864519327e-05, 'epoch': 5.590909090909091}\n{'loss': 0.1706, 'grad_norm': 1.234818935394287, 'learning_rate': 8.552711721624356e-05, 'epoch': 5.613636363636363}\n{'loss': 0.1315, 'grad_norm': 1.0495179891586304, 'learning_rate': 8.508625578729384e-05, 'epoch': 5.636363636363637}\n{'loss': 0.1367, 'grad_norm': 1.4329335689544678, 'learning_rate': 8.464539435834414e-05, 'epoch': 5.659090909090909}\n{'loss': 0.1079, 'grad_norm': 0.9256868958473206, 'learning_rate': 8.420453292939443e-05, 'epoch': 5.681818181818182}\n{'loss': 0.113, 'grad_norm': 1.6276726722717285, 'learning_rate': 8.376367150044472e-05, 'epoch': 5.704545454545455}\n{'loss': 0.1367, 'grad_norm': 0.9443091154098511, 'learning_rate': 8.332281007149502e-05, 'epoch': 5.7272727272727275}\n{'loss': 0.1191, 'grad_norm': 0.9896749258041382, 'learning_rate': 8.28819486425453e-05, 'epoch': 5.75}\n{'loss': 0.1154, 'grad_norm': 1.4939091205596924, 'learning_rate': 8.244108721359559e-05, 'epoch': 5.7727272727272725}\n{'loss': 0.1254, 'grad_norm': 2.0939252376556396, 'learning_rate': 8.200022578464589e-05, 'epoch': 5.795454545454545}\n{'loss': 0.1617, 'grad_norm': 1.5946099758148193, 'learning_rate': 8.155936435569618e-05, 'epoch': 5.818181818181818}\n{'loss': 0.1445, 'grad_norm': 0.9491708874702454, 'learning_rate': 8.111850292674646e-05, 'epoch': 5.840909090909091}\n{'loss': 0.0989, 'grad_norm': 0.6947988271713257, 'learning_rate': 8.067764149779675e-05, 'epoch': 5.863636363636363}\n{'loss': 0.0852, 'grad_norm': 0.6467974185943604, 'learning_rate': 8.023678006884705e-05, 'epoch': 5.886363636363637}\n{'loss': 0.1261, 'grad_norm': 1.6608351469039917, 'learning_rate': 7.979591863989734e-05, 'epoch': 5.909090909090909}\n{'loss': 0.1592, 'grad_norm': 1.3304097652435303, 'learning_rate': 7.935505721094764e-05, 'epoch': 5.931818181818182}\n{'loss': 0.1144, 'grad_norm': 0.978026270866394, 'learning_rate': 7.891419578199792e-05, 'epoch': 5.954545454545455}\n{'loss': 0.1522, 'grad_norm': 1.188287377357483, 'learning_rate': 7.847333435304821e-05, 'epoch': 5.9772727272727275}\n{'loss': 0.199, 'grad_norm': 5.298262596130371, 'learning_rate': 7.803247292409851e-05, 'epoch': 6.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8713    0.9000    0.8855      3311\n     PERCENT     0.8681    0.9186    0.8927        86\n        TYPE     0.9442    0.9665    0.9552     11299\n      VOLUME     0.7727    0.8095    0.7907        42\n\n   micro avg     0.9268    0.9508    0.9386     14738\n   macro avg     0.8641    0.8987    0.8810     14738\nweighted avg     0.9269    0.9508    0.9387     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.254630982875824, 'eval_f1_macro': 0.8810054583589562, 'eval_precision': 0.9267857142857143, 'eval_recall': 0.9508074365585562, 'eval_f1': 0.9386429097729252, 'eval_accuracy': 0.9306048849789629, 'eval_runtime': 1.4975, 'eval_samples_per_second': 3679.491, 'eval_steps_per_second': 7.346, 'epoch': 6.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.1315, 'grad_norm': 0.8704747557640076, 'learning_rate': 7.75916114951488e-05, 'epoch': 6.0227272727272725}\n{'loss': 0.0933, 'grad_norm': 0.953201949596405, 'learning_rate': 7.71507500661991e-05, 'epoch': 6.045454545454546}\n{'loss': 0.1098, 'grad_norm': 1.2669742107391357, 'learning_rate': 7.670988863724938e-05, 'epoch': 6.068181818181818}\n{'loss': 0.0949, 'grad_norm': 0.7399654388427734, 'learning_rate': 7.626902720829967e-05, 'epoch': 6.090909090909091}\n{'loss': 0.1109, 'grad_norm': 0.7895733714103699, 'learning_rate': 7.582816577934997e-05, 'epoch': 6.113636363636363}\n{'loss': 0.0913, 'grad_norm': 1.191772699356079, 'learning_rate': 7.538730435040026e-05, 'epoch': 6.136363636363637}\n{'loss': 0.1051, 'grad_norm': 1.4657269716262817, 'learning_rate': 7.494644292145054e-05, 'epoch': 6.159090909090909}\n{'loss': 0.0903, 'grad_norm': 1.354084849357605, 'learning_rate': 7.450558149250083e-05, 'epoch': 6.181818181818182}\n{'loss': 0.0889, 'grad_norm': 1.2358189821243286, 'learning_rate': 7.406472006355113e-05, 'epoch': 6.204545454545454}\n{'loss': 0.1148, 'grad_norm': 1.307898998260498, 'learning_rate': 7.362385863460142e-05, 'epoch': 6.2272727272727275}\n{'loss': 0.1119, 'grad_norm': 1.039119839668274, 'learning_rate': 7.31829972056517e-05, 'epoch': 6.25}\n{'loss': 0.086, 'grad_norm': 1.38201105594635, 'learning_rate': 7.2742135776702e-05, 'epoch': 6.2727272727272725}\n{'loss': 0.0952, 'grad_norm': 0.802055835723877, 'learning_rate': 7.230127434775229e-05, 'epoch': 6.295454545454546}\n{'loss': 0.1079, 'grad_norm': 1.018131136894226, 'learning_rate': 7.186041291880258e-05, 'epoch': 6.318181818181818}\n{'loss': 0.0899, 'grad_norm': 0.7945685982704163, 'learning_rate': 7.141955148985288e-05, 'epoch': 6.340909090909091}\n{'loss': 0.1027, 'grad_norm': 0.7835283279418945, 'learning_rate': 7.097869006090316e-05, 'epoch': 6.363636363636363}\n{'loss': 0.1259, 'grad_norm': 0.8142184019088745, 'learning_rate': 7.053782863195345e-05, 'epoch': 6.386363636363637}\n{'loss': 0.1218, 'grad_norm': 1.5392284393310547, 'learning_rate': 7.009696720300374e-05, 'epoch': 6.409090909090909}\n{'loss': 0.1242, 'grad_norm': 1.175116777420044, 'learning_rate': 6.965610577405404e-05, 'epoch': 6.431818181818182}\n{'loss': 0.1038, 'grad_norm': 1.980981469154358, 'learning_rate': 6.921524434510432e-05, 'epoch': 6.454545454545454}\n{'loss': 0.1343, 'grad_norm': 1.3366100788116455, 'learning_rate': 6.877438291615461e-05, 'epoch': 6.4772727272727275}\n{'loss': 0.0938, 'grad_norm': 1.4835171699523926, 'learning_rate': 6.833352148720491e-05, 'epoch': 6.5}\n{'loss': 0.0949, 'grad_norm': 0.9818854928016663, 'learning_rate': 6.78926600582552e-05, 'epoch': 6.5227272727272725}\n{'loss': 0.1087, 'grad_norm': 1.174842357635498, 'learning_rate': 6.745179862930548e-05, 'epoch': 6.545454545454545}\n{'loss': 0.1443, 'grad_norm': 1.287541151046753, 'learning_rate': 6.701093720035577e-05, 'epoch': 6.568181818181818}\n{'loss': 0.1063, 'grad_norm': 1.088827133178711, 'learning_rate': 6.657007577140607e-05, 'epoch': 6.590909090909091}\n{'loss': 0.0866, 'grad_norm': 1.2731865644454956, 'learning_rate': 6.612921434245636e-05, 'epoch': 6.613636363636363}\n{'loss': 0.1121, 'grad_norm': 0.9319213032722473, 'learning_rate': 6.568835291350664e-05, 'epoch': 6.636363636363637}\n{'loss': 0.1139, 'grad_norm': 1.2548991441726685, 'learning_rate': 6.524749148455694e-05, 'epoch': 6.659090909090909}\n{'loss': 0.1186, 'grad_norm': 1.6606992483139038, 'learning_rate': 6.480663005560723e-05, 'epoch': 6.681818181818182}\n{'loss': 0.1123, 'grad_norm': 1.0073506832122803, 'learning_rate': 6.436576862665752e-05, 'epoch': 6.704545454545455}\n{'loss': 0.0983, 'grad_norm': 0.8416516184806824, 'learning_rate': 6.392490719770782e-05, 'epoch': 6.7272727272727275}\n{'loss': 0.1224, 'grad_norm': 0.9605698585510254, 'learning_rate': 6.34840457687581e-05, 'epoch': 6.75}\n{'loss': 0.111, 'grad_norm': 1.2948200702667236, 'learning_rate': 6.304318433980839e-05, 'epoch': 6.7727272727272725}\n{'loss': 0.0942, 'grad_norm': 0.6958549618721008, 'learning_rate': 6.260232291085869e-05, 'epoch': 6.795454545454545}\n{'loss': 0.0952, 'grad_norm': 1.6017793416976929, 'learning_rate': 6.216146148190898e-05, 'epoch': 6.818181818181818}\n{'loss': 0.1176, 'grad_norm': 1.0383812189102173, 'learning_rate': 6.172060005295926e-05, 'epoch': 6.840909090909091}\n{'loss': 0.1051, 'grad_norm': 1.0116205215454102, 'learning_rate': 6.127973862400956e-05, 'epoch': 6.863636363636363}\n{'loss': 0.0852, 'grad_norm': 0.8939090371131897, 'learning_rate': 6.083887719505986e-05, 'epoch': 6.886363636363637}\n{'loss': 0.1277, 'grad_norm': 0.9873297810554504, 'learning_rate': 6.0398015766110145e-05, 'epoch': 6.909090909090909}\n{'loss': 0.1166, 'grad_norm': 1.8645379543304443, 'learning_rate': 5.995715433716043e-05, 'epoch': 6.931818181818182}\n{'loss': 0.0983, 'grad_norm': 1.156537413597107, 'learning_rate': 5.951629290821072e-05, 'epoch': 6.954545454545455}\n{'loss': 0.1293, 'grad_norm': 1.4599742889404297, 'learning_rate': 5.907543147926102e-05, 'epoch': 6.9772727272727275}\n{'loss': 0.1556, 'grad_norm': 2.7864818572998047, 'learning_rate': 5.8634570050311305e-05, 'epoch': 7.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8896    0.8858    0.8877      3311\n     PERCENT     0.8602    0.9302    0.8939        86\n        TYPE     0.9433    0.9688    0.9559     11299\n      VOLUME     0.8500    0.8095    0.8293        42\n\n   micro avg     0.9308    0.9495    0.9400     14738\n   macro avg     0.8858    0.8986    0.8917     14738\nweighted avg     0.9305    0.9495    0.9398     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.25629210472106934, 'eval_f1_macro': 0.8916730542405811, 'eval_precision': 0.9307569509112678, 'eval_recall': 0.9494504003256887, 'eval_f1': 0.9400107483541583, 'eval_accuracy': 0.9305502431561117, 'eval_runtime': 1.4732, 'eval_samples_per_second': 3740.194, 'eval_steps_per_second': 7.467, 'epoch': 7.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.0847, 'grad_norm': 0.7539553642272949, 'learning_rate': 5.81937086213616e-05, 'epoch': 7.0227272727272725}\n{'loss': 0.1356, 'grad_norm': 0.7934336066246033, 'learning_rate': 5.775284719241189e-05, 'epoch': 7.045454545454546}\n{'loss': 0.0921, 'grad_norm': 0.6868593096733093, 'learning_rate': 5.731198576346218e-05, 'epoch': 7.068181818181818}\n{'loss': 0.0832, 'grad_norm': 0.7776491045951843, 'learning_rate': 5.687112433451247e-05, 'epoch': 7.090909090909091}\n{'loss': 0.1007, 'grad_norm': 1.1498578786849976, 'learning_rate': 5.6430262905562765e-05, 'epoch': 7.113636363636363}\n{'loss': 0.1106, 'grad_norm': 1.234869122505188, 'learning_rate': 5.598940147661305e-05, 'epoch': 7.136363636363637}\n{'loss': 0.0939, 'grad_norm': 0.8702472448348999, 'learning_rate': 5.5548540047663345e-05, 'epoch': 7.159090909090909}\n{'loss': 0.0694, 'grad_norm': 0.9862681031227112, 'learning_rate': 5.510767861871363e-05, 'epoch': 7.181818181818182}\n{'loss': 0.0974, 'grad_norm': 1.3841084241867065, 'learning_rate': 5.466681718976393e-05, 'epoch': 7.204545454545454}\n{'loss': 0.0957, 'grad_norm': 1.5954378843307495, 'learning_rate': 5.422595576081422e-05, 'epoch': 7.2272727272727275}\n{'loss': 0.0811, 'grad_norm': 1.3770544528961182, 'learning_rate': 5.3785094331864505e-05, 'epoch': 7.25}\n{'loss': 0.0639, 'grad_norm': 0.7518095970153809, 'learning_rate': 5.3344232902914805e-05, 'epoch': 7.2727272727272725}\n{'loss': 0.1428, 'grad_norm': 1.1411243677139282, 'learning_rate': 5.290337147396509e-05, 'epoch': 7.295454545454546}\n{'loss': 0.0787, 'grad_norm': 0.7250552177429199, 'learning_rate': 5.246251004501538e-05, 'epoch': 7.318181818181818}\n{'loss': 0.1124, 'grad_norm': 1.1486541032791138, 'learning_rate': 5.2021648616065665e-05, 'epoch': 7.340909090909091}\n{'loss': 0.1438, 'grad_norm': 0.9357736706733704, 'learning_rate': 5.1580787187115965e-05, 'epoch': 7.363636363636363}\n{'loss': 0.0893, 'grad_norm': 0.9065622091293335, 'learning_rate': 5.113992575816625e-05, 'epoch': 7.386363636363637}\n{'loss': 0.0735, 'grad_norm': 1.4904667139053345, 'learning_rate': 5.069906432921654e-05, 'epoch': 7.409090909090909}\n{'loss': 0.0854, 'grad_norm': 1.071683406829834, 'learning_rate': 5.025820290026684e-05, 'epoch': 7.431818181818182}\n{'loss': 0.087, 'grad_norm': 1.3090752363204956, 'learning_rate': 4.9817341471317125e-05, 'epoch': 7.454545454545454}\n{'loss': 0.0721, 'grad_norm': 0.933722734451294, 'learning_rate': 4.937648004236741e-05, 'epoch': 7.4772727272727275}\n{'loss': 0.0883, 'grad_norm': 0.7355350255966187, 'learning_rate': 4.8935618613417705e-05, 'epoch': 7.5}\n{'loss': 0.0871, 'grad_norm': 1.0734577178955078, 'learning_rate': 4.8494757184468e-05, 'epoch': 7.5227272727272725}\n{'loss': 0.0926, 'grad_norm': 1.5097585916519165, 'learning_rate': 4.805389575551829e-05, 'epoch': 7.545454545454545}\n{'loss': 0.062, 'grad_norm': 1.1797511577606201, 'learning_rate': 4.761303432656858e-05, 'epoch': 7.568181818181818}\n{'loss': 0.0892, 'grad_norm': 1.5931787490844727, 'learning_rate': 4.717217289761887e-05, 'epoch': 7.590909090909091}\n{'loss': 0.1205, 'grad_norm': 1.728403925895691, 'learning_rate': 4.6731311468669165e-05, 'epoch': 7.613636363636363}\n{'loss': 0.0957, 'grad_norm': 1.2304145097732544, 'learning_rate': 4.629045003971945e-05, 'epoch': 7.636363636363637}\n{'loss': 0.099, 'grad_norm': 1.5998644828796387, 'learning_rate': 4.5849588610769745e-05, 'epoch': 7.659090909090909}\n{'loss': 0.0776, 'grad_norm': 0.9598388671875, 'learning_rate': 4.540872718182004e-05, 'epoch': 7.681818181818182}\n{'loss': 0.1044, 'grad_norm': 0.856610119342804, 'learning_rate': 4.4967865752870325e-05, 'epoch': 7.704545454545455}\n{'loss': 0.1244, 'grad_norm': 1.3092925548553467, 'learning_rate': 4.452700432392061e-05, 'epoch': 7.7272727272727275}\n{'loss': 0.09, 'grad_norm': 2.128847122192383, 'learning_rate': 4.408614289497091e-05, 'epoch': 7.75}\n{'loss': 0.0777, 'grad_norm': 1.397750973701477, 'learning_rate': 4.36452814660212e-05, 'epoch': 7.7727272727272725}\n{'loss': 0.0876, 'grad_norm': 1.6388307809829712, 'learning_rate': 4.320442003707149e-05, 'epoch': 7.795454545454545}\n{'loss': 0.0985, 'grad_norm': 1.9527002573013306, 'learning_rate': 4.276355860812178e-05, 'epoch': 7.818181818181818}\n{'loss': 0.0925, 'grad_norm': 1.0671756267547607, 'learning_rate': 4.232269717917207e-05, 'epoch': 7.840909090909091}\n{'loss': 0.0767, 'grad_norm': 0.9300941824913025, 'learning_rate': 4.188183575022236e-05, 'epoch': 7.863636363636363}\n{'loss': 0.1377, 'grad_norm': 1.0924545526504517, 'learning_rate': 4.144097432127265e-05, 'epoch': 7.886363636363637}\n{'loss': 0.0958, 'grad_norm': 1.0770835876464844, 'learning_rate': 4.1000112892322945e-05, 'epoch': 7.909090909090909}\n{'loss': 0.1, 'grad_norm': 1.2128750085830688, 'learning_rate': 4.055925146337323e-05, 'epoch': 7.931818181818182}\n{'loss': 0.0944, 'grad_norm': 1.1965430974960327, 'learning_rate': 4.0118390034423525e-05, 'epoch': 7.954545454545455}\n{'loss': 0.0656, 'grad_norm': 0.7805339097976685, 'learning_rate': 3.967752860547382e-05, 'epoch': 7.9772727272727275}\n{'loss': 0.1525, 'grad_norm': 10.549078941345215, 'learning_rate': 3.9236667176524105e-05, 'epoch': 8.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8985    0.8792    0.8887      3311\n     PERCENT     0.8667    0.9070    0.8864        86\n        TYPE     0.9421    0.9696    0.9557     11299\n      VOLUME     0.8000    0.8571    0.8276        42\n\n   micro avg     0.9318    0.9486    0.9402     14738\n   macro avg     0.8768    0.9032    0.8896     14738\nweighted avg     0.9315    0.9486    0.9399     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.266826331615448, 'eval_f1_macro': 0.8895891229787796, 'eval_precision': 0.9318181818181818, 'eval_recall': 0.9486361785859683, 'eval_f1': 0.9401519736399704, 'eval_accuracy': 0.9312605868531775, 'eval_runtime': 1.8656, 'eval_samples_per_second': 2953.493, 'eval_steps_per_second': 5.896, 'epoch': 8.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.0807, 'grad_norm': 0.9228354096412659, 'learning_rate': 3.87958057475744e-05, 'epoch': 8.022727272727273}\n{'loss': 0.0945, 'grad_norm': 1.6753089427947998, 'learning_rate': 3.835494431862469e-05, 'epoch': 8.045454545454545}\n{'loss': 0.0778, 'grad_norm': 2.09249210357666, 'learning_rate': 3.7914082889674985e-05, 'epoch': 8.068181818181818}\n{'loss': 0.0782, 'grad_norm': 0.8834493160247803, 'learning_rate': 3.747322146072527e-05, 'epoch': 8.090909090909092}\n{'loss': 0.0912, 'grad_norm': 1.0842925310134888, 'learning_rate': 3.7032360031775565e-05, 'epoch': 8.113636363636363}\n{'loss': 0.0846, 'grad_norm': 1.0782389640808105, 'learning_rate': 3.659149860282585e-05, 'epoch': 8.136363636363637}\n{'loss': 0.0807, 'grad_norm': 0.8794413805007935, 'learning_rate': 3.6150637173876145e-05, 'epoch': 8.159090909090908}\n{'loss': 0.0932, 'grad_norm': 1.0722215175628662, 'learning_rate': 3.570977574492644e-05, 'epoch': 8.181818181818182}\n{'loss': 0.0909, 'grad_norm': 1.5161563158035278, 'learning_rate': 3.5268914315976725e-05, 'epoch': 8.204545454545455}\n{'loss': 0.0855, 'grad_norm': 0.8321882486343384, 'learning_rate': 3.482805288702702e-05, 'epoch': 8.227272727272727}\n{'loss': 0.0955, 'grad_norm': 0.8344486355781555, 'learning_rate': 3.4387191458077305e-05, 'epoch': 8.25}\n{'loss': 0.0926, 'grad_norm': 1.0785857439041138, 'learning_rate': 3.39463300291276e-05, 'epoch': 8.272727272727273}\n{'loss': 0.0935, 'grad_norm': 0.9667579531669617, 'learning_rate': 3.3505468600177885e-05, 'epoch': 8.295454545454545}\n{'loss': 0.0982, 'grad_norm': 1.051778793334961, 'learning_rate': 3.306460717122818e-05, 'epoch': 8.318181818181818}\n{'loss': 0.0801, 'grad_norm': 1.005358099937439, 'learning_rate': 3.262374574227847e-05, 'epoch': 8.340909090909092}\n{'loss': 0.0911, 'grad_norm': 1.0148371458053589, 'learning_rate': 3.218288431332876e-05, 'epoch': 8.363636363636363}\n{'loss': 0.0894, 'grad_norm': 1.05399751663208, 'learning_rate': 3.174202288437905e-05, 'epoch': 8.386363636363637}\n{'loss': 0.1003, 'grad_norm': 0.8487049341201782, 'learning_rate': 3.1301161455429346e-05, 'epoch': 8.409090909090908}\n{'loss': 0.1016, 'grad_norm': 0.9431093335151672, 'learning_rate': 3.086030002647963e-05, 'epoch': 8.431818181818182}\n{'loss': 0.0859, 'grad_norm': 0.8054438829421997, 'learning_rate': 3.041943859752993e-05, 'epoch': 8.454545454545455}\n{'loss': 0.0806, 'grad_norm': 0.6960603594779968, 'learning_rate': 2.9978577168580216e-05, 'epoch': 8.477272727272727}\n{'loss': 0.0726, 'grad_norm': 0.831491231918335, 'learning_rate': 2.953771573963051e-05, 'epoch': 8.5}\n{'loss': 0.0777, 'grad_norm': 0.6869164705276489, 'learning_rate': 2.90968543106808e-05, 'epoch': 8.522727272727273}\n{'loss': 0.0952, 'grad_norm': 1.0879992246627808, 'learning_rate': 2.865599288173109e-05, 'epoch': 8.545454545454545}\n{'loss': 0.0708, 'grad_norm': 1.074020504951477, 'learning_rate': 2.8215131452781382e-05, 'epoch': 8.568181818181818}\n{'loss': 0.0833, 'grad_norm': 0.7981438636779785, 'learning_rate': 2.7774270023831672e-05, 'epoch': 8.590909090909092}\n{'loss': 0.117, 'grad_norm': 1.044632077217102, 'learning_rate': 2.7333408594881966e-05, 'epoch': 8.613636363636363}\n{'loss': 0.0648, 'grad_norm': 1.2090027332305908, 'learning_rate': 2.6892547165932252e-05, 'epoch': 8.636363636363637}\n{'loss': 0.0813, 'grad_norm': 0.7879970073699951, 'learning_rate': 2.6451685736982546e-05, 'epoch': 8.659090909090908}\n{'loss': 0.059, 'grad_norm': 0.573371410369873, 'learning_rate': 2.6010824308032832e-05, 'epoch': 8.681818181818182}\n{'loss': 0.0963, 'grad_norm': 0.9521189332008362, 'learning_rate': 2.5569962879083126e-05, 'epoch': 8.704545454545455}\n{'loss': 0.0985, 'grad_norm': 1.8186215162277222, 'learning_rate': 2.512910145013342e-05, 'epoch': 8.727272727272727}\n{'loss': 0.0579, 'grad_norm': 1.2442740201950073, 'learning_rate': 2.4688240021183706e-05, 'epoch': 8.75}\n{'loss': 0.0608, 'grad_norm': 0.6890102624893188, 'learning_rate': 2.4247378592234e-05, 'epoch': 8.772727272727273}\n{'loss': 0.0983, 'grad_norm': 1.1856411695480347, 'learning_rate': 2.380651716328429e-05, 'epoch': 8.795454545454545}\n{'loss': 0.0583, 'grad_norm': 0.6974120736122131, 'learning_rate': 2.3365655734334583e-05, 'epoch': 8.818181818181818}\n{'loss': 0.0866, 'grad_norm': 0.859197199344635, 'learning_rate': 2.2924794305384873e-05, 'epoch': 8.840909090909092}\n{'loss': 0.0542, 'grad_norm': 0.7049630880355835, 'learning_rate': 2.2483932876435163e-05, 'epoch': 8.863636363636363}\n{'loss': 0.0776, 'grad_norm': 0.8788519501686096, 'learning_rate': 2.2043071447485456e-05, 'epoch': 8.886363636363637}\n{'loss': 0.1032, 'grad_norm': 0.7207794785499573, 'learning_rate': 2.1602210018535746e-05, 'epoch': 8.909090909090908}\n{'loss': 0.077, 'grad_norm': 0.9116388559341431, 'learning_rate': 2.1161348589586036e-05, 'epoch': 8.931818181818182}\n{'loss': 0.0767, 'grad_norm': 1.2346138954162598, 'learning_rate': 2.0720487160636326e-05, 'epoch': 8.954545454545455}\n{'loss': 0.0995, 'grad_norm': 1.0233403444290161, 'learning_rate': 2.0279625731686616e-05, 'epoch': 8.977272727272727}\n{'loss': 0.2072, 'grad_norm': 3.3961164951324463, 'learning_rate': 1.983876430273691e-05, 'epoch': 9.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8828    0.8967    0.8897      3311\n     PERCENT     0.8667    0.9070    0.8864        86\n        TYPE     0.9468    0.9645    0.9556     11299\n      VOLUME     0.8000    0.8571    0.8276        42\n\n   micro avg     0.9316    0.9486    0.9400     14738\n   macro avg     0.8741    0.9063    0.8898     14738\nweighted avg     0.9316    0.9486    0.9400     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.26577550172805786, 'eval_f1_macro': 0.8898147141075026, 'eval_precision': 0.931569829424307, 'eval_recall': 0.9486361785859683, 'eval_f1': 0.940025549653735, 'eval_accuracy': 0.9304956013332605, 'eval_runtime': 1.5597, 'eval_samples_per_second': 3532.746, 'eval_steps_per_second': 7.053, 'epoch': 9.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.0831, 'grad_norm': 0.9894503951072693, 'learning_rate': 1.93979028737872e-05, 'epoch': 9.022727272727273}\n{'loss': 0.0517, 'grad_norm': 0.6740157008171082, 'learning_rate': 1.8957041444837493e-05, 'epoch': 9.045454545454545}\n{'loss': 0.128, 'grad_norm': 0.899177610874176, 'learning_rate': 1.8516180015887783e-05, 'epoch': 9.068181818181818}\n{'loss': 0.0848, 'grad_norm': 0.8427467942237854, 'learning_rate': 1.8075318586938073e-05, 'epoch': 9.090909090909092}\n{'loss': 0.0789, 'grad_norm': 0.7836734652519226, 'learning_rate': 1.7634457157988363e-05, 'epoch': 9.113636363636363}\n{'loss': 0.0723, 'grad_norm': 0.8508447408676147, 'learning_rate': 1.7193595729038653e-05, 'epoch': 9.136363636363637}\n{'loss': 0.0597, 'grad_norm': 0.6490413546562195, 'learning_rate': 1.6752734300088943e-05, 'epoch': 9.159090909090908}\n{'loss': 0.0792, 'grad_norm': 1.2583513259887695, 'learning_rate': 1.6311872871139236e-05, 'epoch': 9.181818181818182}\n{'loss': 0.0544, 'grad_norm': 1.3202223777770996, 'learning_rate': 1.5871011442189526e-05, 'epoch': 9.204545454545455}\n{'loss': 0.0631, 'grad_norm': 0.7787116765975952, 'learning_rate': 1.5430150013239816e-05, 'epoch': 9.227272727272727}\n{'loss': 0.0795, 'grad_norm': 0.952043890953064, 'learning_rate': 1.4989288584290108e-05, 'epoch': 9.25}\n{'loss': 0.0998, 'grad_norm': 0.8334242105484009, 'learning_rate': 1.45484271553404e-05, 'epoch': 9.272727272727273}\n{'loss': 0.0708, 'grad_norm': 1.2976890802383423, 'learning_rate': 1.4107565726390691e-05, 'epoch': 9.295454545454545}\n{'loss': 0.0843, 'grad_norm': 0.9576095938682556, 'learning_rate': 1.3666704297440983e-05, 'epoch': 9.318181818181818}\n{'loss': 0.0691, 'grad_norm': 0.7817862629890442, 'learning_rate': 1.3225842868491273e-05, 'epoch': 9.340909090909092}\n{'loss': 0.081, 'grad_norm': 1.3691617250442505, 'learning_rate': 1.2784981439541563e-05, 'epoch': 9.363636363636363}\n{'loss': 0.0543, 'grad_norm': 0.8055858016014099, 'learning_rate': 1.2344120010591853e-05, 'epoch': 9.386363636363637}\n{'loss': 0.0847, 'grad_norm': 0.9345225691795349, 'learning_rate': 1.1903258581642145e-05, 'epoch': 9.409090909090908}\n{'loss': 0.0809, 'grad_norm': 0.9625394940376282, 'learning_rate': 1.1462397152692436e-05, 'epoch': 9.431818181818182}\n{'loss': 0.0776, 'grad_norm': 0.9225452542304993, 'learning_rate': 1.1021535723742728e-05, 'epoch': 9.454545454545455}\n{'loss': 0.0675, 'grad_norm': 1.0971400737762451, 'learning_rate': 1.0580674294793018e-05, 'epoch': 9.477272727272727}\n{'loss': 0.1059, 'grad_norm': 1.1084518432617188, 'learning_rate': 1.0139812865843308e-05, 'epoch': 9.5}\n{'loss': 0.09, 'grad_norm': 0.8199707865715027, 'learning_rate': 9.6989514368936e-06, 'epoch': 9.522727272727273}\n{'loss': 0.0633, 'grad_norm': 0.8973585963249207, 'learning_rate': 9.258090007943891e-06, 'epoch': 9.545454545454545}\n{'loss': 0.0809, 'grad_norm': 0.518463671207428, 'learning_rate': 8.817228578994181e-06, 'epoch': 9.568181818181818}\n{'loss': 0.0646, 'grad_norm': 1.046121597290039, 'learning_rate': 8.376367150044471e-06, 'epoch': 9.590909090909092}\n{'loss': 0.082, 'grad_norm': 0.6469110250473022, 'learning_rate': 7.935505721094763e-06, 'epoch': 9.613636363636363}\n{'loss': 0.0715, 'grad_norm': 1.3705631494522095, 'learning_rate': 7.494644292145054e-06, 'epoch': 9.636363636363637}\n{'loss': 0.0769, 'grad_norm': 1.170174241065979, 'learning_rate': 7.053782863195346e-06, 'epoch': 9.659090909090908}\n{'loss': 0.0732, 'grad_norm': 0.8291968107223511, 'learning_rate': 6.6129214342456364e-06, 'epoch': 9.681818181818182}\n{'loss': 0.0808, 'grad_norm': 1.0205976963043213, 'learning_rate': 6.1720600052959264e-06, 'epoch': 9.704545454545455}\n{'loss': 0.0912, 'grad_norm': 1.136732816696167, 'learning_rate': 5.731198576346218e-06, 'epoch': 9.727272727272727}\n{'loss': 0.1014, 'grad_norm': 1.5928986072540283, 'learning_rate': 5.290337147396509e-06, 'epoch': 9.75}\n{'loss': 0.0591, 'grad_norm': 0.7603626251220703, 'learning_rate': 4.8494757184468e-06, 'epoch': 9.772727272727273}\n{'loss': 0.0743, 'grad_norm': 0.9701035022735596, 'learning_rate': 4.408614289497091e-06, 'epoch': 9.795454545454545}\n{'loss': 0.0545, 'grad_norm': 0.7078324556350708, 'learning_rate': 3.9677528605473815e-06, 'epoch': 9.818181818181818}\n{'loss': 0.1164, 'grad_norm': 1.6259068250656128, 'learning_rate': 3.526891431597673e-06, 'epoch': 9.840909090909092}\n{'loss': 0.0835, 'grad_norm': 1.0297064781188965, 'learning_rate': 3.0860300026479632e-06, 'epoch': 9.863636363636363}\n{'loss': 0.0758, 'grad_norm': 0.7337667942047119, 'learning_rate': 2.6451685736982545e-06, 'epoch': 9.886363636363637}\n{'loss': 0.068, 'grad_norm': 0.772596001625061, 'learning_rate': 2.2043071447485453e-06, 'epoch': 9.909090909090908}\n{'loss': 0.078, 'grad_norm': 0.6589494943618774, 'learning_rate': 1.7634457157988364e-06, 'epoch': 9.931818181818182}\n{'loss': 0.0643, 'grad_norm': 1.0780998468399048, 'learning_rate': 1.3225842868491272e-06, 'epoch': 9.954545454545455}\n{'loss': 0.0592, 'grad_norm': 0.7357819080352783, 'learning_rate': 8.817228578994182e-07, 'epoch': 9.977272727272727}\n{'loss': 0.1141, 'grad_norm': 6.23450231552124, 'learning_rate': 4.408614289497091e-07, 'epoch': 10.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8817    0.8955    0.8885      3311\n     PERCENT     0.8667    0.9070    0.8864        86\n        TYPE     0.9466    0.9641    0.9552     11299\n      VOLUME     0.8182    0.8571    0.8372        42\n\n   micro avg     0.9312    0.9480    0.9395     14738\n   macro avg     0.8783    0.9059    0.8918     14738\nweighted avg     0.9311    0.9480    0.9395     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.26712408661842346, 'eval_f1_macro': 0.8918321515721376, 'eval_precision': 0.9311562812395868, 'eval_recall': 0.9480255122811779, 'eval_f1': 0.939515180042363, 'eval_accuracy': 0.9299491831047484, 'eval_runtime': 1.4753, 'eval_samples_per_second': 3734.712, 'eval_steps_per_second': 7.456, 'epoch': 10.0}\n{'train_runtime': 62.6854, 'train_samples_per_second': 3516.29, 'train_steps_per_second': 7.019, 'train_loss': 0.2860900304661217, 'epoch': 10.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nThe speedups for torchdynamo mostly come with GPU Ampere or higher and which is not detected here.\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8817    0.8955    0.8885      3311\n     PERCENT     0.8667    0.9070    0.8864        86\n        TYPE     0.9466    0.9641    0.9552     11299\n      VOLUME     0.8182    0.8571    0.8372        42\n\n   micro avg     0.9312    0.9480    0.9395     14738\n   macro avg     0.8783    0.9059    0.8918     14738\nweighted avg     0.9311    0.9480    0.9395     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.26712408661842346, 'eval_f1_macro': 0.8918321515721376, 'eval_precision': 0.9311562812395868, 'eval_recall': 0.9480255122811779, 'eval_f1': 0.939515180042363, 'eval_accuracy': 0.9299491831047484, 'eval_runtime': 1.5642, 'eval_samples_per_second': 3522.587, 'eval_steps_per_second': 7.032, 'epoch': 10.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/3364797558.py:66: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 2.2736, 'grad_norm': 6.993783473968506, 'learning_rate': 0.0, 'epoch': 0.022727272727272728}\n{'loss': 2.2792, 'grad_norm': 7.328367710113525, 'learning_rate': 3.9677528605473815e-06, 'epoch': 0.045454545454545456}\n{'loss': 2.2636, 'grad_norm': 6.9862494468688965, 'learning_rate': 7.935505721094763e-06, 'epoch': 0.06818181818181818}\n{'loss': 2.2658, 'grad_norm': 7.491092205047607, 'learning_rate': 1.1903258581642145e-05, 'epoch': 0.09090909090909091}\n{'loss': 2.2258, 'grad_norm': 7.090653896331787, 'learning_rate': 1.5871011442189526e-05, 'epoch': 0.11363636363636363}\n{'loss': 2.1878, 'grad_norm': 6.765668869018555, 'learning_rate': 1.983876430273691e-05, 'epoch': 0.13636363636363635}\n{'loss': 2.1471, 'grad_norm': 6.919877529144287, 'learning_rate': 2.380651716328429e-05, 'epoch': 0.1590909090909091}\n{'loss': 2.1027, 'grad_norm': 6.633739948272705, 'learning_rate': 2.7774270023831672e-05, 'epoch': 0.18181818181818182}\n{'loss': 2.0331, 'grad_norm': 6.6024980545043945, 'learning_rate': 3.174202288437905e-05, 'epoch': 0.20454545454545456}\n{'loss': 1.97, 'grad_norm': 6.70845890045166, 'learning_rate': 3.570977574492644e-05, 'epoch': 0.22727272727272727}\n{'loss': 1.8968, 'grad_norm': 6.598848819732666, 'learning_rate': 3.967752860547382e-05, 'epoch': 0.25}\n{'loss': 1.8119, 'grad_norm': 6.1258225440979, 'learning_rate': 4.36452814660212e-05, 'epoch': 0.2727272727272727}\n{'loss': 1.7617, 'grad_norm': 5.504838943481445, 'learning_rate': 4.761303432656858e-05, 'epoch': 0.29545454545454547}\n{'loss': 1.6823, 'grad_norm': 5.232119083404541, 'learning_rate': 5.1580787187115965e-05, 'epoch': 0.3181818181818182}\n{'loss': 1.5998, 'grad_norm': 4.8609185218811035, 'learning_rate': 5.5548540047663345e-05, 'epoch': 0.3409090909090909}\n{'loss': 1.4942, 'grad_norm': 4.6349711418151855, 'learning_rate': 5.951629290821072e-05, 'epoch': 0.36363636363636365}\n{'loss': 1.4116, 'grad_norm': 4.072139739990234, 'learning_rate': 6.34840457687581e-05, 'epoch': 0.38636363636363635}\n{'loss': 1.3166, 'grad_norm': 3.617349147796631, 'learning_rate': 6.745179862930548e-05, 'epoch': 0.4090909090909091}\n{'loss': 1.2619, 'grad_norm': 2.877768039703369, 'learning_rate': 7.141955148985288e-05, 'epoch': 0.4318181818181818}\n{'loss': 1.2407, 'grad_norm': 2.1948859691619873, 'learning_rate': 7.538730435040026e-05, 'epoch': 0.45454545454545453}\n{'loss': 1.1977, 'grad_norm': 1.941003441810608, 'learning_rate': 7.935505721094764e-05, 'epoch': 0.4772727272727273}\n{'loss': 1.1269, 'grad_norm': 1.5913913249969482, 'learning_rate': 8.332281007149502e-05, 'epoch': 0.5}\n{'loss': 1.1029, 'grad_norm': 1.695925235748291, 'learning_rate': 8.72905629320424e-05, 'epoch': 0.5227272727272727}\n{'loss': 1.0941, 'grad_norm': 1.8645869493484497, 'learning_rate': 9.125831579258978e-05, 'epoch': 0.5454545454545454}\n{'loss': 1.0985, 'grad_norm': 1.8220341205596924, 'learning_rate': 9.522606865313716e-05, 'epoch': 0.5681818181818182}\n{'loss': 0.9971, 'grad_norm': 1.388515591621399, 'learning_rate': 9.919382151368455e-05, 'epoch': 0.5909090909090909}\n{'loss': 1.1045, 'grad_norm': 1.4458088874816895, 'learning_rate': 0.00010316157437423193, 'epoch': 0.6136363636363636}\n{'loss': 1.0026, 'grad_norm': 1.512498140335083, 'learning_rate': 0.00010712932723477931, 'epoch': 0.6363636363636364}\n{'loss': 1.0158, 'grad_norm': 1.6944893598556519, 'learning_rate': 0.00011109708009532669, 'epoch': 0.6590909090909091}\n{'loss': 1.0071, 'grad_norm': 1.5330299139022827, 'learning_rate': 0.00011506483295587407, 'epoch': 0.6818181818181818}\n{'loss': 0.9176, 'grad_norm': 2.009267568588257, 'learning_rate': 0.00011903258581642144, 'epoch': 0.7045454545454546}\n{'loss': 0.9117, 'grad_norm': 2.055218458175659, 'learning_rate': 0.00012300033867696883, 'epoch': 0.7272727272727273}\n{'loss': 0.8723, 'grad_norm': 1.5354676246643066, 'learning_rate': 0.0001269680915375162, 'epoch': 0.75}\n{'loss': 0.8635, 'grad_norm': 1.4454469680786133, 'learning_rate': 0.0001309358443980636, 'epoch': 0.7727272727272727}\n{'loss': 0.8092, 'grad_norm': 1.4356387853622437, 'learning_rate': 0.00013490359725861097, 'epoch': 0.7954545454545454}\n{'loss': 0.7744, 'grad_norm': 1.3203972578048706, 'learning_rate': 0.00013887135011915835, 'epoch': 0.8181818181818182}\n{'loss': 0.7522, 'grad_norm': 0.8956283926963806, 'learning_rate': 0.00014283910297970576, 'epoch': 0.8409090909090909}\n{'loss': 0.709, 'grad_norm': 1.2503172159194946, 'learning_rate': 0.00014680685584025314, 'epoch': 0.8636363636363636}\n{'loss': 0.679, 'grad_norm': 1.4533944129943848, 'learning_rate': 0.00015077460870080052, 'epoch': 0.8863636363636364}\n{'loss': 0.6097, 'grad_norm': 0.8312057256698608, 'learning_rate': 0.0001547423615613479, 'epoch': 0.9090909090909091}\n{'loss': 0.6377, 'grad_norm': 0.7021205425262451, 'learning_rate': 0.00015871011442189527, 'epoch': 0.9318181818181818}\n{'loss': 0.639, 'grad_norm': 1.0334566831588745, 'learning_rate': 0.00016267786728244263, 'epoch': 0.9545454545454546}\n{'loss': 0.6401, 'grad_norm': 1.1953269243240356, 'learning_rate': 0.00016664562014299003, 'epoch': 0.9772727272727273}\n{'loss': 0.5779, 'grad_norm': 3.980684280395508, 'learning_rate': 0.00017061337300353741, 'epoch': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.7698    0.6204    0.6871      3456\n     PERCENT     0.0000    0.0000    0.0000        77\n        TYPE     0.8280    0.8983    0.8617     11282\n      VOLUME     0.0000    0.0000    0.0000        41\n\n   micro avg     0.8172    0.8265    0.8218     14856\n   macro avg     0.3994    0.3797    0.3872     14856\nweighted avg     0.8079    0.8265    0.8142     14856\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.5892638564109802, 'eval_f1_macro': 0.38719479895797637, 'eval_precision': 0.8171835485159058, 'eval_recall': 0.8265347334410339, 'eval_f1': 0.8218325413292283, 'eval_accuracy': 0.82631636205396, 'eval_runtime': 1.6526, 'eval_samples_per_second': 3334.182, 'eval_steps_per_second': 6.656, 'epoch': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.5822, 'grad_norm': 1.389628529548645, 'learning_rate': 0.0001745811258640848, 'epoch': 1.0227272727272727}\n{'loss': 0.6206, 'grad_norm': 0.9565359354019165, 'learning_rate': 0.0001741402644351351, 'epoch': 1.0454545454545454}\n{'loss': 0.5899, 'grad_norm': 1.3693315982818604, 'learning_rate': 0.00017369940300618537, 'epoch': 1.0681818181818181}\n{'loss': 0.5829, 'grad_norm': 3.3636348247528076, 'learning_rate': 0.00017325854157723567, 'epoch': 1.0909090909090908}\n{'loss': 0.5467, 'grad_norm': 1.6609456539154053, 'learning_rate': 0.00017281768014828597, 'epoch': 1.1136363636363635}\n{'loss': 0.5473, 'grad_norm': 0.8878585696220398, 'learning_rate': 0.00017237681871933624, 'epoch': 1.1363636363636362}\n{'loss': 0.5495, 'grad_norm': 1.409271478652954, 'learning_rate': 0.00017193595729038654, 'epoch': 1.1590909090909092}\n{'loss': 0.4515, 'grad_norm': 1.0313471555709839, 'learning_rate': 0.00017149509586143684, 'epoch': 1.1818181818181819}\n{'loss': 0.5236, 'grad_norm': 1.2302550077438354, 'learning_rate': 0.00017105423443248711, 'epoch': 1.2045454545454546}\n{'loss': 0.4645, 'grad_norm': 1.1218924522399902, 'learning_rate': 0.00017061337300353741, 'epoch': 1.2272727272727273}\n{'loss': 0.5264, 'grad_norm': 0.7788670063018799, 'learning_rate': 0.0001701725115745877, 'epoch': 1.25}\n{'loss': 0.434, 'grad_norm': 1.4552351236343384, 'learning_rate': 0.000169731650145638, 'epoch': 1.2727272727272727}\n{'loss': 0.5295, 'grad_norm': 1.436409592628479, 'learning_rate': 0.0001692907887166883, 'epoch': 1.2954545454545454}\n{'loss': 0.5152, 'grad_norm': 0.7533582448959351, 'learning_rate': 0.00016884992728773856, 'epoch': 1.3181818181818181}\n{'loss': 0.4637, 'grad_norm': 0.9353976845741272, 'learning_rate': 0.00016840906585878886, 'epoch': 1.3409090909090908}\n{'loss': 0.4378, 'grad_norm': 0.6767938733100891, 'learning_rate': 0.00016796820442983916, 'epoch': 1.3636363636363638}\n{'loss': 0.4286, 'grad_norm': 0.6391398906707764, 'learning_rate': 0.00016752734300088943, 'epoch': 1.3863636363636362}\n{'loss': 0.4971, 'grad_norm': 1.2957507371902466, 'learning_rate': 0.00016708648157193973, 'epoch': 1.4090909090909092}\n{'loss': 0.4637, 'grad_norm': 0.9622489809989929, 'learning_rate': 0.00016664562014299003, 'epoch': 1.4318181818181819}\n{'loss': 0.3783, 'grad_norm': 0.7883932590484619, 'learning_rate': 0.0001662047587140403, 'epoch': 1.4545454545454546}\n{'loss': 0.4037, 'grad_norm': 2.12245774269104, 'learning_rate': 0.0001657638972850906, 'epoch': 1.4772727272727273}\n{'loss': 0.4287, 'grad_norm': 1.0090712308883667, 'learning_rate': 0.0001653230358561409, 'epoch': 1.5}\n{'loss': 0.4254, 'grad_norm': 0.8403788208961487, 'learning_rate': 0.00016488217442719118, 'epoch': 1.5227272727272727}\n{'loss': 0.3764, 'grad_norm': 0.7511422038078308, 'learning_rate': 0.00016444131299824148, 'epoch': 1.5454545454545454}\n{'loss': 0.4313, 'grad_norm': 1.185132622718811, 'learning_rate': 0.00016400045156929178, 'epoch': 1.5681818181818183}\n{'loss': 0.4633, 'grad_norm': 1.4672796726226807, 'learning_rate': 0.00016355959014034205, 'epoch': 1.5909090909090908}\n{'loss': 0.4168, 'grad_norm': 1.1025407314300537, 'learning_rate': 0.00016311872871139235, 'epoch': 1.6136363636363638}\n{'loss': 0.4421, 'grad_norm': 1.3445026874542236, 'learning_rate': 0.00016267786728244263, 'epoch': 1.6363636363636362}\n{'loss': 0.3495, 'grad_norm': 1.193382978439331, 'learning_rate': 0.00016223700585349293, 'epoch': 1.6590909090909092}\n{'loss': 0.3839, 'grad_norm': 1.362410306930542, 'learning_rate': 0.00016179614442454323, 'epoch': 1.6818181818181817}\n{'loss': 0.3997, 'grad_norm': 0.9495178461074829, 'learning_rate': 0.0001613552829955935, 'epoch': 1.7045454545454546}\n{'loss': 0.4096, 'grad_norm': 0.9895797967910767, 'learning_rate': 0.0001609144215666438, 'epoch': 1.7272727272727273}\n{'loss': 0.4285, 'grad_norm': 1.1673352718353271, 'learning_rate': 0.0001604735601376941, 'epoch': 1.75}\n{'loss': 0.3374, 'grad_norm': 0.8029536008834839, 'learning_rate': 0.0001600326987087444, 'epoch': 1.7727272727272727}\n{'loss': 0.3424, 'grad_norm': 0.9832237362861633, 'learning_rate': 0.00015959183727979467, 'epoch': 1.7954545454545454}\n{'loss': 0.3642, 'grad_norm': 1.6911253929138184, 'learning_rate': 0.00015915097585084497, 'epoch': 1.8181818181818183}\n{'loss': 0.4049, 'grad_norm': 1.3111603260040283, 'learning_rate': 0.00015871011442189527, 'epoch': 1.8409090909090908}\n{'loss': 0.3648, 'grad_norm': 1.0153805017471313, 'learning_rate': 0.00015826925299294555, 'epoch': 1.8636363636363638}\n{'loss': 0.3659, 'grad_norm': 0.7341501116752625, 'learning_rate': 0.00015782839156399585, 'epoch': 1.8863636363636362}\n{'loss': 0.3619, 'grad_norm': 0.8165955543518066, 'learning_rate': 0.00015738753013504615, 'epoch': 1.9090909090909092}\n{'loss': 0.3682, 'grad_norm': 1.155724048614502, 'learning_rate': 0.00015694666870609642, 'epoch': 1.9318181818181817}\n{'loss': 0.3172, 'grad_norm': 0.8005161285400391, 'learning_rate': 0.00015650580727714672, 'epoch': 1.9545454545454546}\n{'loss': 0.3577, 'grad_norm': 0.832456648349762, 'learning_rate': 0.00015606494584819702, 'epoch': 1.9772727272727273}\n{'loss': 0.3635, 'grad_norm': 3.8249659538269043, 'learning_rate': 0.0001556240844192473, 'epoch': 2.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8496    0.7781    0.8123      3456\n     PERCENT     0.6116    0.9610    0.7475        77\n        TYPE     0.9103    0.9610    0.9350     11282\n      VOLUME     0.4762    0.2439    0.3226        41\n\n   micro avg     0.8947    0.9165    0.9055     14856\n   macro avg     0.7119    0.7360    0.7043     14856\nweighted avg     0.8935    0.9165    0.9038     14856\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.3279409110546112, 'eval_f1_macro': 0.7043242448923278, 'eval_precision': 0.8947230071630413, 'eval_recall': 0.9164647280560043, 'eval_f1': 0.9054633724603466, 'eval_accuracy': 0.9024695387293299, 'eval_runtime': 1.4884, 'eval_samples_per_second': 3701.897, 'eval_steps_per_second': 7.39, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.2839, 'grad_norm': 0.7901809811592102, 'learning_rate': 0.0001551832229902976, 'epoch': 2.022727272727273}\n{'loss': 0.3062, 'grad_norm': 0.7755041718482971, 'learning_rate': 0.0001547423615613479, 'epoch': 2.0454545454545454}\n{'loss': 0.3322, 'grad_norm': 0.9766760468482971, 'learning_rate': 0.0001543015001323982, 'epoch': 2.0681818181818183}\n{'loss': 0.292, 'grad_norm': 0.7103386521339417, 'learning_rate': 0.00015386063870344847, 'epoch': 2.090909090909091}\n{'loss': 0.2639, 'grad_norm': 0.7511613368988037, 'learning_rate': 0.00015341977727449877, 'epoch': 2.1136363636363638}\n{'loss': 0.2997, 'grad_norm': 0.9375583529472351, 'learning_rate': 0.00015297891584554907, 'epoch': 2.1363636363636362}\n{'loss': 0.2309, 'grad_norm': 1.2507083415985107, 'learning_rate': 0.00015253805441659934, 'epoch': 2.159090909090909}\n{'loss': 0.242, 'grad_norm': 0.7630171775817871, 'learning_rate': 0.00015209719298764964, 'epoch': 2.1818181818181817}\n{'loss': 0.2846, 'grad_norm': 1.3333165645599365, 'learning_rate': 0.00015165633155869994, 'epoch': 2.2045454545454546}\n{'loss': 0.2483, 'grad_norm': 0.7269123792648315, 'learning_rate': 0.00015121547012975021, 'epoch': 2.227272727272727}\n{'loss': 0.3389, 'grad_norm': 0.9607845544815063, 'learning_rate': 0.00015077460870080052, 'epoch': 2.25}\n{'loss': 0.2404, 'grad_norm': 0.9092433452606201, 'learning_rate': 0.00015033374727185082, 'epoch': 2.2727272727272725}\n{'loss': 0.2336, 'grad_norm': 0.8872933387756348, 'learning_rate': 0.0001498928858429011, 'epoch': 2.2954545454545454}\n{'loss': 0.2767, 'grad_norm': 0.9402121305465698, 'learning_rate': 0.0001494520244139514, 'epoch': 2.3181818181818183}\n{'loss': 0.2769, 'grad_norm': 1.1451183557510376, 'learning_rate': 0.00014901116298500166, 'epoch': 2.340909090909091}\n{'loss': 0.2626, 'grad_norm': 1.2395418882369995, 'learning_rate': 0.00014857030155605196, 'epoch': 2.3636363636363638}\n{'loss': 0.2348, 'grad_norm': 0.7628980875015259, 'learning_rate': 0.00014812944012710226, 'epoch': 2.3863636363636362}\n{'loss': 0.2274, 'grad_norm': 1.2045831680297852, 'learning_rate': 0.00014768857869815253, 'epoch': 2.409090909090909}\n{'loss': 0.2696, 'grad_norm': 1.2930891513824463, 'learning_rate': 0.00014724771726920284, 'epoch': 2.4318181818181817}\n{'loss': 0.2595, 'grad_norm': 1.5384353399276733, 'learning_rate': 0.00014680685584025314, 'epoch': 2.4545454545454546}\n{'loss': 0.331, 'grad_norm': 1.3935078382492065, 'learning_rate': 0.0001463659944113034, 'epoch': 2.4772727272727275}\n{'loss': 0.2346, 'grad_norm': 1.1546460390090942, 'learning_rate': 0.0001459251329823537, 'epoch': 2.5}\n{'loss': 0.2095, 'grad_norm': 0.8738290071487427, 'learning_rate': 0.000145484271553404, 'epoch': 2.5227272727272725}\n{'loss': 0.2894, 'grad_norm': 0.9040208458900452, 'learning_rate': 0.00014504341012445428, 'epoch': 2.5454545454545454}\n{'loss': 0.2472, 'grad_norm': 1.6234774589538574, 'learning_rate': 0.00014460254869550458, 'epoch': 2.5681818181818183}\n{'loss': 0.2863, 'grad_norm': 1.444975733757019, 'learning_rate': 0.00014416168726655488, 'epoch': 2.590909090909091}\n{'loss': 0.2312, 'grad_norm': 1.51628577709198, 'learning_rate': 0.00014372082583760516, 'epoch': 2.6136363636363638}\n{'loss': 0.3033, 'grad_norm': 1.5261890888214111, 'learning_rate': 0.00014327996440865546, 'epoch': 2.6363636363636362}\n{'loss': 0.2895, 'grad_norm': 1.1520588397979736, 'learning_rate': 0.00014283910297970576, 'epoch': 2.659090909090909}\n{'loss': 0.2342, 'grad_norm': 0.7943413853645325, 'learning_rate': 0.00014239824155075603, 'epoch': 2.6818181818181817}\n{'loss': 0.2527, 'grad_norm': 1.0420585870742798, 'learning_rate': 0.00014195738012180633, 'epoch': 2.7045454545454546}\n{'loss': 0.2091, 'grad_norm': 0.889988124370575, 'learning_rate': 0.0001415165186928566, 'epoch': 2.7272727272727275}\n{'loss': 0.2448, 'grad_norm': 1.062660813331604, 'learning_rate': 0.0001410756572639069, 'epoch': 2.75}\n{'loss': 0.3306, 'grad_norm': 1.2292263507843018, 'learning_rate': 0.0001406347958349572, 'epoch': 2.7727272727272725}\n{'loss': 0.2366, 'grad_norm': 1.5676058530807495, 'learning_rate': 0.00014019393440600748, 'epoch': 2.7954545454545454}\n{'loss': 0.2733, 'grad_norm': 0.9961866140365601, 'learning_rate': 0.00013975307297705778, 'epoch': 2.8181818181818183}\n{'loss': 0.2791, 'grad_norm': 1.1863934993743896, 'learning_rate': 0.00013931221154810808, 'epoch': 2.840909090909091}\n{'loss': 0.2807, 'grad_norm': 1.1351590156555176, 'learning_rate': 0.00013887135011915835, 'epoch': 2.8636363636363638}\n{'loss': 0.2549, 'grad_norm': 1.9047564268112183, 'learning_rate': 0.00013843048869020865, 'epoch': 2.8863636363636362}\n{'loss': 0.2549, 'grad_norm': 1.8376836776733398, 'learning_rate': 0.00013798962726125895, 'epoch': 2.909090909090909}\n{'loss': 0.2721, 'grad_norm': 1.865696668624878, 'learning_rate': 0.00013754876583230922, 'epoch': 2.9318181818181817}\n{'loss': 0.3046, 'grad_norm': 1.193655014038086, 'learning_rate': 0.00013710790440335952, 'epoch': 2.9545454545454546}\n{'loss': 0.2708, 'grad_norm': 1.137665867805481, 'learning_rate': 0.00013666704297440982, 'epoch': 2.9772727272727275}\n{'loss': 0.1783, 'grad_norm': 4.367664813995361, 'learning_rate': 0.0001362261815454601, 'epoch': 3.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8262    0.8788    0.8517      3456\n     PERCENT     0.8090    0.9351    0.8675        77\n        TYPE     0.9404    0.9548    0.9475     11282\n      VOLUME     0.6667    0.6341    0.6500        41\n\n   micro avg     0.9114    0.9361    0.9236     14856\n   macro avg     0.8106    0.8507    0.8292     14856\nweighted avg     0.9124    0.9361    0.9240     14856\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.26754459738731384, 'eval_f1_macro': 0.8291637128362707, 'eval_precision': 0.9113965528540533, 'eval_recall': 0.9361200861604739, 'eval_f1': 0.923592893906691, 'eval_accuracy': 0.918896866840731, 'eval_runtime': 1.5241, 'eval_samples_per_second': 3615.179, 'eval_steps_per_second': 7.217, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.2224, 'grad_norm': 1.6199400424957275, 'learning_rate': 0.0001357853201165104, 'epoch': 3.022727272727273}\n{'loss': 0.1949, 'grad_norm': 2.405926465988159, 'learning_rate': 0.0001353444586875607, 'epoch': 3.0454545454545454}\n{'loss': 0.2025, 'grad_norm': 0.925764262676239, 'learning_rate': 0.00013490359725861097, 'epoch': 3.0681818181818183}\n{'loss': 0.2683, 'grad_norm': 1.245202898979187, 'learning_rate': 0.00013446273582966127, 'epoch': 3.090909090909091}\n{'loss': 0.1791, 'grad_norm': 0.9805609583854675, 'learning_rate': 0.00013402187440071154, 'epoch': 3.1136363636363638}\n{'loss': 0.2565, 'grad_norm': 2.254863739013672, 'learning_rate': 0.00013358101297176184, 'epoch': 3.1363636363636362}\n{'loss': 0.1774, 'grad_norm': 2.4452121257781982, 'learning_rate': 0.00013314015154281214, 'epoch': 3.159090909090909}\n{'loss': 0.2362, 'grad_norm': 2.0333776473999023, 'learning_rate': 0.00013269929011386242, 'epoch': 3.1818181818181817}\n{'loss': 0.1753, 'grad_norm': 1.2955267429351807, 'learning_rate': 0.00013225842868491272, 'epoch': 3.2045454545454546}\n{'loss': 0.1599, 'grad_norm': 0.8176435232162476, 'learning_rate': 0.00013181756725596302, 'epoch': 3.227272727272727}\n{'loss': 0.2246, 'grad_norm': 1.5431805849075317, 'learning_rate': 0.0001313767058270133, 'epoch': 3.25}\n{'loss': 0.178, 'grad_norm': 0.8858436346054077, 'learning_rate': 0.0001309358443980636, 'epoch': 3.2727272727272725}\n{'loss': 0.2366, 'grad_norm': 1.1707161664962769, 'learning_rate': 0.0001304949829691139, 'epoch': 3.2954545454545454}\n{'loss': 0.2065, 'grad_norm': 2.4456589221954346, 'learning_rate': 0.00013005412154016416, 'epoch': 3.3181818181818183}\n{'loss': 0.1517, 'grad_norm': 2.0955958366394043, 'learning_rate': 0.00012961326011121446, 'epoch': 3.340909090909091}\n{'loss': 0.2041, 'grad_norm': 1.0526628494262695, 'learning_rate': 0.00012917239868226476, 'epoch': 3.3636363636363638}\n{'loss': 0.1711, 'grad_norm': 1.3604731559753418, 'learning_rate': 0.00012873153725331504, 'epoch': 3.3863636363636362}\n{'loss': 0.1929, 'grad_norm': 0.8267112970352173, 'learning_rate': 0.00012829067582436534, 'epoch': 3.409090909090909}\n{'loss': 0.1587, 'grad_norm': 1.2778687477111816, 'learning_rate': 0.00012784981439541564, 'epoch': 3.4318181818181817}\n{'loss': 0.1885, 'grad_norm': 1.5338447093963623, 'learning_rate': 0.0001274089529664659, 'epoch': 3.4545454545454546}\n{'loss': 0.156, 'grad_norm': 1.630589246749878, 'learning_rate': 0.0001269680915375162, 'epoch': 3.4772727272727275}\n{'loss': 0.161, 'grad_norm': 1.5755324363708496, 'learning_rate': 0.0001265272301085665, 'epoch': 3.5}\n{'loss': 0.1933, 'grad_norm': 1.1380443572998047, 'learning_rate': 0.00012608636867961678, 'epoch': 3.5227272727272725}\n{'loss': 0.2182, 'grad_norm': 1.1655552387237549, 'learning_rate': 0.00012564550725066708, 'epoch': 3.5454545454545454}\n{'loss': 0.2295, 'grad_norm': 1.1325055360794067, 'learning_rate': 0.00012520464582171738, 'epoch': 3.5681818181818183}\n{'loss': 0.2066, 'grad_norm': 1.8395748138427734, 'learning_rate': 0.00012476378439276766, 'epoch': 3.590909090909091}\n{'loss': 0.2101, 'grad_norm': 1.6854649782180786, 'learning_rate': 0.00012432292296381796, 'epoch': 3.6136363636363638}\n{'loss': 0.1643, 'grad_norm': 1.3109380006790161, 'learning_rate': 0.00012388206153486826, 'epoch': 3.6363636363636362}\n{'loss': 0.1945, 'grad_norm': 1.1435965299606323, 'learning_rate': 0.00012344120010591853, 'epoch': 3.659090909090909}\n{'loss': 0.2002, 'grad_norm': 1.0806316137313843, 'learning_rate': 0.00012300033867696883, 'epoch': 3.6818181818181817}\n{'loss': 0.1509, 'grad_norm': 1.2192286252975464, 'learning_rate': 0.00012255947724801913, 'epoch': 3.7045454545454546}\n{'loss': 0.2025, 'grad_norm': 0.950864851474762, 'learning_rate': 0.0001221186158190694, 'epoch': 3.7272727272727275}\n{'loss': 0.1511, 'grad_norm': 0.933376133441925, 'learning_rate': 0.00012167775439011972, 'epoch': 3.75}\n{'loss': 0.1983, 'grad_norm': 1.3424100875854492, 'learning_rate': 0.00012123689296116999, 'epoch': 3.7727272727272725}\n{'loss': 0.2247, 'grad_norm': 1.3546557426452637, 'learning_rate': 0.00012079603153222029, 'epoch': 3.7954545454545454}\n{'loss': 0.2099, 'grad_norm': 1.2041758298873901, 'learning_rate': 0.00012035517010327059, 'epoch': 3.8181818181818183}\n{'loss': 0.1811, 'grad_norm': 0.7386623620986938, 'learning_rate': 0.00011991430867432086, 'epoch': 3.840909090909091}\n{'loss': 0.1471, 'grad_norm': 0.8132653832435608, 'learning_rate': 0.00011947344724537116, 'epoch': 3.8636363636363638}\n{'loss': 0.1934, 'grad_norm': 1.4652202129364014, 'learning_rate': 0.00011903258581642144, 'epoch': 3.8863636363636362}\n{'loss': 0.2176, 'grad_norm': 0.8773557543754578, 'learning_rate': 0.00011859172438747174, 'epoch': 3.909090909090909}\n{'loss': 0.2533, 'grad_norm': 1.2607812881469727, 'learning_rate': 0.00011815086295852204, 'epoch': 3.9318181818181817}\n{'loss': 0.245, 'grad_norm': 1.4593440294265747, 'learning_rate': 0.00011771000152957231, 'epoch': 3.9545454545454546}\n{'loss': 0.1662, 'grad_norm': 0.9424854516983032, 'learning_rate': 0.00011726914010062261, 'epoch': 3.9772727272727275}\n{'loss': 0.345, 'grad_norm': 10.277883529663086, 'learning_rate': 0.00011682827867167291, 'epoch': 4.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8910    0.8608    0.8756      3456\n     PERCENT     0.8276    0.9351    0.8780        77\n        TYPE     0.9373    0.9677    0.9523     11282\n      VOLUME     0.7500    0.7317    0.7407        41\n\n   micro avg     0.9260    0.9420    0.9339     14856\n   macro avg     0.8515    0.8738    0.8617     14856\nweighted avg     0.9255    0.9420    0.9335     14856\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.25163930654525757, 'eval_f1_macro': 0.8616807384945623, 'eval_precision': 0.925962683604605, 'eval_recall': 0.942043618739903, 'eval_f1': 0.933933933933934, 'eval_accuracy': 0.9274912967798086, 'eval_runtime': 1.5244, 'eval_samples_per_second': 3614.646, 'eval_steps_per_second': 7.216, 'epoch': 4.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.1568, 'grad_norm': 0.6783970594406128, 'learning_rate': 0.0001163874172427232, 'epoch': 4.0227272727272725}\n{'loss': 0.1524, 'grad_norm': 0.8835489153862, 'learning_rate': 0.00011594655581377348, 'epoch': 4.045454545454546}\n{'loss': 0.1756, 'grad_norm': 0.9180413484573364, 'learning_rate': 0.00011550569438482378, 'epoch': 4.068181818181818}\n{'loss': 0.178, 'grad_norm': 0.907825767993927, 'learning_rate': 0.00011506483295587407, 'epoch': 4.090909090909091}\n{'loss': 0.1544, 'grad_norm': 0.7346757650375366, 'learning_rate': 0.00011462397152692436, 'epoch': 4.113636363636363}\n{'loss': 0.1412, 'grad_norm': 0.8160372376441956, 'learning_rate': 0.00011418311009797466, 'epoch': 4.136363636363637}\n{'loss': 0.1762, 'grad_norm': 1.5841434001922607, 'learning_rate': 0.00011374224866902494, 'epoch': 4.159090909090909}\n{'loss': 0.138, 'grad_norm': 0.874255895614624, 'learning_rate': 0.00011330138724007523, 'epoch': 4.181818181818182}\n{'loss': 0.1865, 'grad_norm': 0.9908139705657959, 'learning_rate': 0.00011286052581112553, 'epoch': 4.204545454545454}\n{'loss': 0.1766, 'grad_norm': 0.940680742263794, 'learning_rate': 0.00011241966438217582, 'epoch': 4.2272727272727275}\n{'loss': 0.1805, 'grad_norm': 1.3902729749679565, 'learning_rate': 0.0001119788029532261, 'epoch': 4.25}\n{'loss': 0.1296, 'grad_norm': 0.7580680251121521, 'learning_rate': 0.00011153794152427639, 'epoch': 4.2727272727272725}\n{'loss': 0.153, 'grad_norm': 1.3349889516830444, 'learning_rate': 0.00011109708009532669, 'epoch': 4.295454545454546}\n{'loss': 0.1423, 'grad_norm': 0.7844172716140747, 'learning_rate': 0.00011065621866637699, 'epoch': 4.318181818181818}\n{'loss': 0.1275, 'grad_norm': 0.8630959391593933, 'learning_rate': 0.00011021535723742726, 'epoch': 4.340909090909091}\n{'loss': 0.1555, 'grad_norm': 1.303155541419983, 'learning_rate': 0.00010977449580847756, 'epoch': 4.363636363636363}\n{'loss': 0.1497, 'grad_norm': 0.8369158506393433, 'learning_rate': 0.00010933363437952786, 'epoch': 4.386363636363637}\n{'loss': 0.1502, 'grad_norm': 1.6792128086090088, 'learning_rate': 0.00010889277295057814, 'epoch': 4.409090909090909}\n{'loss': 0.1475, 'grad_norm': 1.177432656288147, 'learning_rate': 0.00010845191152162844, 'epoch': 4.431818181818182}\n{'loss': 0.1464, 'grad_norm': 1.1500709056854248, 'learning_rate': 0.00010801105009267874, 'epoch': 4.454545454545454}\n{'loss': 0.1332, 'grad_norm': 1.4785701036453247, 'learning_rate': 0.00010757018866372901, 'epoch': 4.4772727272727275}\n{'loss': 0.175, 'grad_norm': 1.17611825466156, 'learning_rate': 0.00010712932723477931, 'epoch': 4.5}\n{'loss': 0.1499, 'grad_norm': 0.985809862613678, 'learning_rate': 0.00010668846580582961, 'epoch': 4.5227272727272725}\n{'loss': 0.1245, 'grad_norm': 0.9882079362869263, 'learning_rate': 0.00010624760437687988, 'epoch': 4.545454545454545}\n{'loss': 0.1434, 'grad_norm': 1.8766989707946777, 'learning_rate': 0.00010580674294793018, 'epoch': 4.568181818181818}\n{'loss': 0.1488, 'grad_norm': 0.9594062566757202, 'learning_rate': 0.00010536588151898046, 'epoch': 4.590909090909091}\n{'loss': 0.1573, 'grad_norm': 0.977025032043457, 'learning_rate': 0.00010492502009003076, 'epoch': 4.613636363636363}\n{'loss': 0.1766, 'grad_norm': 1.3012330532073975, 'learning_rate': 0.00010448415866108106, 'epoch': 4.636363636363637}\n{'loss': 0.1315, 'grad_norm': 0.7940335273742676, 'learning_rate': 0.00010404329723213133, 'epoch': 4.659090909090909}\n{'loss': 0.172, 'grad_norm': 1.2510409355163574, 'learning_rate': 0.00010360243580318163, 'epoch': 4.681818181818182}\n{'loss': 0.1754, 'grad_norm': 0.9572973251342773, 'learning_rate': 0.00010316157437423193, 'epoch': 4.704545454545455}\n{'loss': 0.1413, 'grad_norm': 0.8899469375610352, 'learning_rate': 0.0001027207129452822, 'epoch': 4.7272727272727275}\n{'loss': 0.1292, 'grad_norm': 0.7999425530433655, 'learning_rate': 0.0001022798515163325, 'epoch': 4.75}\n{'loss': 0.1247, 'grad_norm': 1.318610668182373, 'learning_rate': 0.0001018389900873828, 'epoch': 4.7727272727272725}\n{'loss': 0.2131, 'grad_norm': 2.127350330352783, 'learning_rate': 0.00010139812865843308, 'epoch': 4.795454545454545}\n{'loss': 0.0981, 'grad_norm': 0.7108643054962158, 'learning_rate': 0.00010095726722948338, 'epoch': 4.818181818181818}\n{'loss': 0.1323, 'grad_norm': 0.8239058256149292, 'learning_rate': 0.00010051640580053368, 'epoch': 4.840909090909091}\n{'loss': 0.1429, 'grad_norm': 0.7888890504837036, 'learning_rate': 0.00010007554437158395, 'epoch': 4.863636363636363}\n{'loss': 0.1622, 'grad_norm': 1.3089287281036377, 'learning_rate': 9.963468294263425e-05, 'epoch': 4.886363636363637}\n{'loss': 0.1685, 'grad_norm': 1.0042556524276733, 'learning_rate': 9.919382151368455e-05, 'epoch': 4.909090909090909}\n{'loss': 0.1612, 'grad_norm': 1.2362885475158691, 'learning_rate': 9.875296008473482e-05, 'epoch': 4.931818181818182}\n{'loss': 0.1405, 'grad_norm': 0.8705031275749207, 'learning_rate': 9.831209865578512e-05, 'epoch': 4.954545454545455}\n{'loss': 0.1272, 'grad_norm': 0.8945357203483582, 'learning_rate': 9.787123722683541e-05, 'epoch': 4.9772727272727275}\n{'loss': 0.2004, 'grad_norm': 4.86013126373291, 'learning_rate': 9.74303757978857e-05, 'epoch': 5.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8873    0.8814    0.8843      3456\n     PERCENT     0.8625    0.8961    0.8790        77\n        TYPE     0.9429    0.9685    0.9555     11282\n      VOLUME     0.7333    0.8049    0.7674        41\n\n   micro avg     0.9292    0.9474    0.9382     14856\n   macro avg     0.8565    0.8877    0.8716     14856\nweighted avg     0.9289    0.9474    0.9380     14856\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.24999666213989258, 'eval_f1_macro': 0.8715660694463241, 'eval_precision': 0.9292269096190665, 'eval_recall': 0.947428648357566, 'eval_f1': 0.9382395093823952, 'eval_accuracy': 0.9308093994778068, 'eval_runtime': 1.5245, 'eval_samples_per_second': 3614.267, 'eval_steps_per_second': 7.215, 'epoch': 5.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.0842, 'grad_norm': 0.8278126120567322, 'learning_rate': 9.6989514368936e-05, 'epoch': 5.0227272727272725}\n{'loss': 0.1516, 'grad_norm': 0.8273427486419678, 'learning_rate': 9.654865293998628e-05, 'epoch': 5.045454545454546}\n{'loss': 0.103, 'grad_norm': 0.6460674405097961, 'learning_rate': 9.610779151103658e-05, 'epoch': 5.068181818181818}\n{'loss': 0.1315, 'grad_norm': 0.8981513977050781, 'learning_rate': 9.566693008208687e-05, 'epoch': 5.090909090909091}\n{'loss': 0.1129, 'grad_norm': 0.9561753869056702, 'learning_rate': 9.522606865313716e-05, 'epoch': 5.113636363636363}\n{'loss': 0.1116, 'grad_norm': 0.7764599919319153, 'learning_rate': 9.478520722418746e-05, 'epoch': 5.136363636363637}\n{'loss': 0.1307, 'grad_norm': 1.0908840894699097, 'learning_rate': 9.434434579523774e-05, 'epoch': 5.159090909090909}\n{'loss': 0.1173, 'grad_norm': 0.7451679110527039, 'learning_rate': 9.390348436628803e-05, 'epoch': 5.181818181818182}\n{'loss': 0.1002, 'grad_norm': 1.0711485147476196, 'learning_rate': 9.346262293733833e-05, 'epoch': 5.204545454545454}\n{'loss': 0.1424, 'grad_norm': 1.4012316465377808, 'learning_rate': 9.302176150838862e-05, 'epoch': 5.2272727272727275}\n{'loss': 0.09, 'grad_norm': 0.8891052007675171, 'learning_rate': 9.25809000794389e-05, 'epoch': 5.25}\n{'loss': 0.1174, 'grad_norm': 0.8889777064323425, 'learning_rate': 9.21400386504892e-05, 'epoch': 5.2727272727272725}\n{'loss': 0.1284, 'grad_norm': 0.9587740898132324, 'learning_rate': 9.169917722153949e-05, 'epoch': 5.295454545454546}\n{'loss': 0.1034, 'grad_norm': 1.2287886142730713, 'learning_rate': 9.125831579258978e-05, 'epoch': 5.318181818181818}\n{'loss': 0.1107, 'grad_norm': 1.025368571281433, 'learning_rate': 9.081745436364008e-05, 'epoch': 5.340909090909091}\n{'loss': 0.1345, 'grad_norm': 0.8413965702056885, 'learning_rate': 9.037659293469035e-05, 'epoch': 5.363636363636363}\n{'loss': 0.1252, 'grad_norm': 1.0786536931991577, 'learning_rate': 8.993573150574065e-05, 'epoch': 5.386363636363637}\n{'loss': 0.1198, 'grad_norm': 1.389569878578186, 'learning_rate': 8.949487007679095e-05, 'epoch': 5.409090909090909}\n{'loss': 0.1084, 'grad_norm': 0.6791384816169739, 'learning_rate': 8.905400864784122e-05, 'epoch': 5.431818181818182}\n{'loss': 0.1573, 'grad_norm': 1.8119542598724365, 'learning_rate': 8.861314721889152e-05, 'epoch': 5.454545454545454}\n{'loss': 0.1055, 'grad_norm': 1.1011290550231934, 'learning_rate': 8.817228578994182e-05, 'epoch': 5.4772727272727275}\n{'loss': 0.0869, 'grad_norm': 1.0799254179000854, 'learning_rate': 8.77314243609921e-05, 'epoch': 5.5}\n{'loss': 0.1203, 'grad_norm': 2.0668609142303467, 'learning_rate': 8.72905629320424e-05, 'epoch': 5.5227272727272725}\n{'loss': 0.162, 'grad_norm': 1.2997585535049438, 'learning_rate': 8.684970150309268e-05, 'epoch': 5.545454545454545}\n{'loss': 0.1454, 'grad_norm': 1.616346836090088, 'learning_rate': 8.640884007414298e-05, 'epoch': 5.568181818181818}\n{'loss': 0.1021, 'grad_norm': 1.2600024938583374, 'learning_rate': 8.596797864519327e-05, 'epoch': 5.590909090909091}\n{'loss': 0.116, 'grad_norm': 1.3653780221939087, 'learning_rate': 8.552711721624356e-05, 'epoch': 5.613636363636363}\n{'loss': 0.1085, 'grad_norm': 1.3133620023727417, 'learning_rate': 8.508625578729384e-05, 'epoch': 5.636363636363637}\n{'loss': 0.1446, 'grad_norm': 0.9012956619262695, 'learning_rate': 8.464539435834414e-05, 'epoch': 5.659090909090909}\n{'loss': 0.1222, 'grad_norm': 0.8294080495834351, 'learning_rate': 8.420453292939443e-05, 'epoch': 5.681818181818182}\n{'loss': 0.1024, 'grad_norm': 1.381691575050354, 'learning_rate': 8.376367150044472e-05, 'epoch': 5.704545454545455}\n{'loss': 0.136, 'grad_norm': 2.5129473209381104, 'learning_rate': 8.332281007149502e-05, 'epoch': 5.7272727272727275}\n{'loss': 0.1429, 'grad_norm': 1.5067603588104248, 'learning_rate': 8.28819486425453e-05, 'epoch': 5.75}\n{'loss': 0.1076, 'grad_norm': 0.8723167777061462, 'learning_rate': 8.244108721359559e-05, 'epoch': 5.7727272727272725}\n{'loss': 0.1554, 'grad_norm': 1.4313935041427612, 'learning_rate': 8.200022578464589e-05, 'epoch': 5.795454545454545}\n{'loss': 0.1458, 'grad_norm': 1.394749641418457, 'learning_rate': 8.155936435569618e-05, 'epoch': 5.818181818181818}\n{'loss': 0.12, 'grad_norm': 1.393097996711731, 'learning_rate': 8.111850292674646e-05, 'epoch': 5.840909090909091}\n{'loss': 0.1195, 'grad_norm': 0.7871521711349487, 'learning_rate': 8.067764149779675e-05, 'epoch': 5.863636363636363}\n{'loss': 0.0955, 'grad_norm': 1.5512100458145142, 'learning_rate': 8.023678006884705e-05, 'epoch': 5.886363636363637}\n{'loss': 0.1406, 'grad_norm': 1.1702258586883545, 'learning_rate': 7.979591863989734e-05, 'epoch': 5.909090909090909}\n{'loss': 0.1201, 'grad_norm': 2.4219791889190674, 'learning_rate': 7.935505721094764e-05, 'epoch': 5.931818181818182}\n{'loss': 0.1405, 'grad_norm': 1.4567712545394897, 'learning_rate': 7.891419578199792e-05, 'epoch': 5.954545454545455}\n{'loss': 0.1457, 'grad_norm': 0.8475208878517151, 'learning_rate': 7.847333435304821e-05, 'epoch': 5.9772727272727275}\n{'loss': 0.1183, 'grad_norm': 3.9940133094787598, 'learning_rate': 7.803247292409851e-05, 'epoch': 6.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.9033    0.8727    0.8877      3456\n     PERCENT     0.8372    0.9351    0.8834        77\n        TYPE     0.9407    0.9707    0.9555     11282\n      VOLUME     0.8537    0.8537    0.8537        41\n\n   micro avg     0.9316    0.9474    0.9394     14856\n   macro avg     0.8837    0.9080    0.8951     14856\nweighted avg     0.9312    0.9474    0.9390     14856\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.26259005069732666, 'eval_f1_macro': 0.8950663135247002, 'eval_precision': 0.9316211028000265, 'eval_recall': 0.9473613354873451, 'eval_f1': 0.9394252911924708, 'eval_accuracy': 0.9321692776327241, 'eval_runtime': 1.4597, 'eval_samples_per_second': 3774.682, 'eval_steps_per_second': 7.536, 'epoch': 6.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.0673, 'grad_norm': 0.5757128000259399, 'learning_rate': 7.75916114951488e-05, 'epoch': 6.0227272727272725}\n{'loss': 0.0997, 'grad_norm': 0.9937192797660828, 'learning_rate': 7.71507500661991e-05, 'epoch': 6.045454545454546}\n{'loss': 0.0878, 'grad_norm': 0.9522342085838318, 'learning_rate': 7.670988863724938e-05, 'epoch': 6.068181818181818}\n{'loss': 0.1147, 'grad_norm': 1.3822261095046997, 'learning_rate': 7.626902720829967e-05, 'epoch': 6.090909090909091}\n{'loss': 0.1081, 'grad_norm': 1.134173035621643, 'learning_rate': 7.582816577934997e-05, 'epoch': 6.113636363636363}\n{'loss': 0.0774, 'grad_norm': 0.9741771221160889, 'learning_rate': 7.538730435040026e-05, 'epoch': 6.136363636363637}\n{'loss': 0.0984, 'grad_norm': 0.834805428981781, 'learning_rate': 7.494644292145054e-05, 'epoch': 6.159090909090909}\n{'loss': 0.1111, 'grad_norm': 0.9172134399414062, 'learning_rate': 7.450558149250083e-05, 'epoch': 6.181818181818182}\n{'loss': 0.1431, 'grad_norm': 1.217144250869751, 'learning_rate': 7.406472006355113e-05, 'epoch': 6.204545454545454}\n{'loss': 0.1016, 'grad_norm': 0.9183747172355652, 'learning_rate': 7.362385863460142e-05, 'epoch': 6.2272727272727275}\n{'loss': 0.1027, 'grad_norm': 0.8958811163902283, 'learning_rate': 7.31829972056517e-05, 'epoch': 6.25}\n{'loss': 0.1078, 'grad_norm': 1.0105514526367188, 'learning_rate': 7.2742135776702e-05, 'epoch': 6.2727272727272725}\n{'loss': 0.0834, 'grad_norm': 0.6863836646080017, 'learning_rate': 7.230127434775229e-05, 'epoch': 6.295454545454546}\n{'loss': 0.1069, 'grad_norm': 0.7674577832221985, 'learning_rate': 7.186041291880258e-05, 'epoch': 6.318181818181818}\n{'loss': 0.1272, 'grad_norm': 0.8160955309867859, 'learning_rate': 7.141955148985288e-05, 'epoch': 6.340909090909091}\n{'loss': 0.1299, 'grad_norm': 0.8015540838241577, 'learning_rate': 7.097869006090316e-05, 'epoch': 6.363636363636363}\n{'loss': 0.0774, 'grad_norm': 1.1362626552581787, 'learning_rate': 7.053782863195345e-05, 'epoch': 6.386363636363637}\n{'loss': 0.106, 'grad_norm': 0.9699073433876038, 'learning_rate': 7.009696720300374e-05, 'epoch': 6.409090909090909}\n{'loss': 0.1179, 'grad_norm': 0.893624484539032, 'learning_rate': 6.965610577405404e-05, 'epoch': 6.431818181818182}\n{'loss': 0.0696, 'grad_norm': 0.6359043121337891, 'learning_rate': 6.921524434510432e-05, 'epoch': 6.454545454545454}\n{'loss': 0.081, 'grad_norm': 0.659927487373352, 'learning_rate': 6.877438291615461e-05, 'epoch': 6.4772727272727275}\n{'loss': 0.1066, 'grad_norm': 0.7789012789726257, 'learning_rate': 6.833352148720491e-05, 'epoch': 6.5}\n{'loss': 0.0814, 'grad_norm': 1.0313570499420166, 'learning_rate': 6.78926600582552e-05, 'epoch': 6.5227272727272725}\n{'loss': 0.1411, 'grad_norm': 1.0726851224899292, 'learning_rate': 6.745179862930548e-05, 'epoch': 6.545454545454545}\n{'loss': 0.1037, 'grad_norm': 1.353855013847351, 'learning_rate': 6.701093720035577e-05, 'epoch': 6.568181818181818}\n{'loss': 0.0896, 'grad_norm': 1.0634541511535645, 'learning_rate': 6.657007577140607e-05, 'epoch': 6.590909090909091}\n{'loss': 0.0987, 'grad_norm': 1.0325143337249756, 'learning_rate': 6.612921434245636e-05, 'epoch': 6.613636363636363}\n{'loss': 0.0843, 'grad_norm': 0.9451319575309753, 'learning_rate': 6.568835291350664e-05, 'epoch': 6.636363636363637}\n{'loss': 0.1156, 'grad_norm': 0.8029996752738953, 'learning_rate': 6.524749148455694e-05, 'epoch': 6.659090909090909}\n{'loss': 0.1185, 'grad_norm': 1.3286998271942139, 'learning_rate': 6.480663005560723e-05, 'epoch': 6.681818181818182}\n{'loss': 0.1112, 'grad_norm': 0.8926143646240234, 'learning_rate': 6.436576862665752e-05, 'epoch': 6.704545454545455}\n{'loss': 0.1131, 'grad_norm': 1.857913613319397, 'learning_rate': 6.392490719770782e-05, 'epoch': 6.7272727272727275}\n{'loss': 0.0868, 'grad_norm': 0.8478230834007263, 'learning_rate': 6.34840457687581e-05, 'epoch': 6.75}\n{'loss': 0.0911, 'grad_norm': 1.2786144018173218, 'learning_rate': 6.304318433980839e-05, 'epoch': 6.7727272727272725}\n{'loss': 0.1167, 'grad_norm': 1.0910755395889282, 'learning_rate': 6.260232291085869e-05, 'epoch': 6.795454545454545}\n{'loss': 0.1186, 'grad_norm': 1.128368616104126, 'learning_rate': 6.216146148190898e-05, 'epoch': 6.818181818181818}\n{'loss': 0.1201, 'grad_norm': 0.9815402030944824, 'learning_rate': 6.172060005295926e-05, 'epoch': 6.840909090909091}\n{'loss': 0.1164, 'grad_norm': 1.150410532951355, 'learning_rate': 6.127973862400956e-05, 'epoch': 6.863636363636363}\n{'loss': 0.0847, 'grad_norm': 0.7251130938529968, 'learning_rate': 6.083887719505986e-05, 'epoch': 6.886363636363637}\n{'loss': 0.0939, 'grad_norm': 0.7472028732299805, 'learning_rate': 6.0398015766110145e-05, 'epoch': 6.909090909090909}\n{'loss': 0.1205, 'grad_norm': 1.0249791145324707, 'learning_rate': 5.995715433716043e-05, 'epoch': 6.931818181818182}\n{'loss': 0.1211, 'grad_norm': 1.778950810432434, 'learning_rate': 5.951629290821072e-05, 'epoch': 6.954545454545455}\n{'loss': 0.0867, 'grad_norm': 1.0049468278884888, 'learning_rate': 5.907543147926102e-05, 'epoch': 6.9772727272727275}\n{'loss': 0.0226, 'grad_norm': 3.0445072650909424, 'learning_rate': 5.8634570050311305e-05, 'epoch': 7.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8882    0.8892    0.8887      3456\n     PERCENT     0.8690    0.9481    0.9068        77\n        TYPE     0.9481    0.9656    0.9568     11282\n      VOLUME     0.9048    0.9268    0.9157        41\n\n   micro avg     0.9338    0.9476    0.9407     14856\n   macro avg     0.9025    0.9324    0.9170     14856\nweighted avg     0.9336    0.9476    0.9406     14856\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.2623622417449951, 'eval_f1_macro': 0.9169869888803258, 'eval_precision': 0.9338020695144601, 'eval_recall': 0.9476305869682283, 'eval_f1': 0.9406655084859014, 'eval_accuracy': 0.9331483899042646, 'eval_runtime': 1.4582, 'eval_samples_per_second': 3778.568, 'eval_steps_per_second': 7.543, 'epoch': 7.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.0941, 'grad_norm': 1.0501466989517212, 'learning_rate': 5.81937086213616e-05, 'epoch': 7.0227272727272725}\n{'loss': 0.083, 'grad_norm': 1.0281718969345093, 'learning_rate': 5.775284719241189e-05, 'epoch': 7.045454545454546}\n{'loss': 0.0878, 'grad_norm': 0.9030085802078247, 'learning_rate': 5.731198576346218e-05, 'epoch': 7.068181818181818}\n{'loss': 0.1166, 'grad_norm': 0.7723588943481445, 'learning_rate': 5.687112433451247e-05, 'epoch': 7.090909090909091}\n{'loss': 0.0889, 'grad_norm': 1.0898897647857666, 'learning_rate': 5.6430262905562765e-05, 'epoch': 7.113636363636363}\n{'loss': 0.07, 'grad_norm': 1.3124679327011108, 'learning_rate': 5.598940147661305e-05, 'epoch': 7.136363636363637}\n{'loss': 0.1035, 'grad_norm': 0.9488077163696289, 'learning_rate': 5.5548540047663345e-05, 'epoch': 7.159090909090909}\n{'loss': 0.0781, 'grad_norm': 0.828048050403595, 'learning_rate': 5.510767861871363e-05, 'epoch': 7.181818181818182}\n{'loss': 0.0928, 'grad_norm': 0.82606440782547, 'learning_rate': 5.466681718976393e-05, 'epoch': 7.204545454545454}\n{'loss': 0.0881, 'grad_norm': 0.8631458878517151, 'learning_rate': 5.422595576081422e-05, 'epoch': 7.2272727272727275}\n{'loss': 0.1023, 'grad_norm': 1.9927524328231812, 'learning_rate': 5.3785094331864505e-05, 'epoch': 7.25}\n{'loss': 0.0636, 'grad_norm': 0.8302326202392578, 'learning_rate': 5.3344232902914805e-05, 'epoch': 7.2727272727272725}\n{'loss': 0.1044, 'grad_norm': 1.2214235067367554, 'learning_rate': 5.290337147396509e-05, 'epoch': 7.295454545454546}\n{'loss': 0.0806, 'grad_norm': 0.6581218242645264, 'learning_rate': 5.246251004501538e-05, 'epoch': 7.318181818181818}\n{'loss': 0.0958, 'grad_norm': 1.0252399444580078, 'learning_rate': 5.2021648616065665e-05, 'epoch': 7.340909090909091}\n{'loss': 0.0798, 'grad_norm': 0.6260948777198792, 'learning_rate': 5.1580787187115965e-05, 'epoch': 7.363636363636363}\n{'loss': 0.0868, 'grad_norm': 0.7398964166641235, 'learning_rate': 5.113992575816625e-05, 'epoch': 7.386363636363637}\n{'loss': 0.0811, 'grad_norm': 0.617881178855896, 'learning_rate': 5.069906432921654e-05, 'epoch': 7.409090909090909}\n{'loss': 0.0939, 'grad_norm': 1.0056416988372803, 'learning_rate': 5.025820290026684e-05, 'epoch': 7.431818181818182}\n{'loss': 0.0787, 'grad_norm': 1.1550183296203613, 'learning_rate': 4.9817341471317125e-05, 'epoch': 7.454545454545454}\n{'loss': 0.0933, 'grad_norm': 1.226154088973999, 'learning_rate': 4.937648004236741e-05, 'epoch': 7.4772727272727275}\n{'loss': 0.0881, 'grad_norm': 0.8935196399688721, 'learning_rate': 4.8935618613417705e-05, 'epoch': 7.5}\n{'loss': 0.0696, 'grad_norm': 0.9339115023612976, 'learning_rate': 4.8494757184468e-05, 'epoch': 7.5227272727272725}\n{'loss': 0.0932, 'grad_norm': 0.686229944229126, 'learning_rate': 4.805389575551829e-05, 'epoch': 7.545454545454545}\n{'loss': 0.0989, 'grad_norm': 0.8456359505653381, 'learning_rate': 4.761303432656858e-05, 'epoch': 7.568181818181818}\n{'loss': 0.1091, 'grad_norm': 2.128058433532715, 'learning_rate': 4.717217289761887e-05, 'epoch': 7.590909090909091}\n{'loss': 0.0985, 'grad_norm': 1.1066622734069824, 'learning_rate': 4.6731311468669165e-05, 'epoch': 7.613636363636363}\n{'loss': 0.0929, 'grad_norm': 1.1266192197799683, 'learning_rate': 4.629045003971945e-05, 'epoch': 7.636363636363637}\n{'loss': 0.1034, 'grad_norm': 0.9713994860649109, 'learning_rate': 4.5849588610769745e-05, 'epoch': 7.659090909090909}\n{'loss': 0.0807, 'grad_norm': 1.123968482017517, 'learning_rate': 4.540872718182004e-05, 'epoch': 7.681818181818182}\n{'loss': 0.0529, 'grad_norm': 0.9683653116226196, 'learning_rate': 4.4967865752870325e-05, 'epoch': 7.704545454545455}\n{'loss': 0.0708, 'grad_norm': 1.045401930809021, 'learning_rate': 4.452700432392061e-05, 'epoch': 7.7272727272727275}\n{'loss': 0.0876, 'grad_norm': 1.01480233669281, 'learning_rate': 4.408614289497091e-05, 'epoch': 7.75}\n{'loss': 0.0937, 'grad_norm': 1.4740545749664307, 'learning_rate': 4.36452814660212e-05, 'epoch': 7.7727272727272725}\n{'loss': 0.0888, 'grad_norm': 1.5038018226623535, 'learning_rate': 4.320442003707149e-05, 'epoch': 7.795454545454545}\n{'loss': 0.0816, 'grad_norm': 0.8935320973396301, 'learning_rate': 4.276355860812178e-05, 'epoch': 7.818181818181818}\n{'loss': 0.1033, 'grad_norm': 1.3554191589355469, 'learning_rate': 4.232269717917207e-05, 'epoch': 7.840909090909091}\n{'loss': 0.0836, 'grad_norm': 0.7683037519454956, 'learning_rate': 4.188183575022236e-05, 'epoch': 7.863636363636363}\n{'loss': 0.0894, 'grad_norm': 0.8915566802024841, 'learning_rate': 4.144097432127265e-05, 'epoch': 7.886363636363637}\n{'loss': 0.0829, 'grad_norm': 1.130570411682129, 'learning_rate': 4.1000112892322945e-05, 'epoch': 7.909090909090909}\n{'loss': 0.1008, 'grad_norm': 1.2527978420257568, 'learning_rate': 4.055925146337323e-05, 'epoch': 7.931818181818182}\n{'loss': 0.0849, 'grad_norm': 0.8500092625617981, 'learning_rate': 4.0118390034423525e-05, 'epoch': 7.954545454545455}\n{'loss': 0.0611, 'grad_norm': 1.4988890886306763, 'learning_rate': 3.967752860547382e-05, 'epoch': 7.9772727272727275}\n{'loss': 0.0928, 'grad_norm': 4.900140285491943, 'learning_rate': 3.9236667176524105e-05, 'epoch': 8.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8950    0.8883    0.8917      3456\n     PERCENT     0.8795    0.9481    0.9125        77\n        TYPE     0.9486    0.9659    0.9571     11282\n      VOLUME     0.8605    0.9024    0.8810        41\n\n   micro avg     0.9357    0.9476    0.9416     14856\n   macro avg     0.8959    0.9262    0.9106     14856\nweighted avg     0.9355    0.9476    0.9415     14856\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.2696845233440399, 'eval_f1_macro': 0.9105633026181886, 'eval_precision': 0.9357218824780643, 'eval_recall': 0.9475632740980076, 'eval_f1': 0.9416053511705684, 'eval_accuracy': 0.9336923411662315, 'eval_runtime': 1.894, 'eval_samples_per_second': 2909.133, 'eval_steps_per_second': 5.808, 'epoch': 8.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.0593, 'grad_norm': 0.7457199096679688, 'learning_rate': 3.87958057475744e-05, 'epoch': 8.022727272727273}\n{'loss': 0.0994, 'grad_norm': 0.8632977604866028, 'learning_rate': 3.835494431862469e-05, 'epoch': 8.045454545454545}\n{'loss': 0.0875, 'grad_norm': 1.175841212272644, 'learning_rate': 3.7914082889674985e-05, 'epoch': 8.068181818181818}\n{'loss': 0.076, 'grad_norm': 1.1073799133300781, 'learning_rate': 3.747322146072527e-05, 'epoch': 8.090909090909092}\n{'loss': 0.0742, 'grad_norm': 1.395436406135559, 'learning_rate': 3.7032360031775565e-05, 'epoch': 8.113636363636363}\n{'loss': 0.0874, 'grad_norm': 0.8165427446365356, 'learning_rate': 3.659149860282585e-05, 'epoch': 8.136363636363637}\n{'loss': 0.0833, 'grad_norm': 0.7205488085746765, 'learning_rate': 3.6150637173876145e-05, 'epoch': 8.159090909090908}\n{'loss': 0.0814, 'grad_norm': 0.7084531188011169, 'learning_rate': 3.570977574492644e-05, 'epoch': 8.181818181818182}\n{'loss': 0.0798, 'grad_norm': 0.8786330819129944, 'learning_rate': 3.5268914315976725e-05, 'epoch': 8.204545454545455}\n{'loss': 0.0859, 'grad_norm': 0.8042089343070984, 'learning_rate': 3.482805288702702e-05, 'epoch': 8.227272727272727}\n{'loss': 0.1034, 'grad_norm': 0.8315871357917786, 'learning_rate': 3.4387191458077305e-05, 'epoch': 8.25}\n{'loss': 0.0609, 'grad_norm': 0.71534264087677, 'learning_rate': 3.39463300291276e-05, 'epoch': 8.272727272727273}\n{'loss': 0.0742, 'grad_norm': 0.7974499464035034, 'learning_rate': 3.3505468600177885e-05, 'epoch': 8.295454545454545}\n{'loss': 0.0723, 'grad_norm': 1.0136921405792236, 'learning_rate': 3.306460717122818e-05, 'epoch': 8.318181818181818}\n{'loss': 0.1036, 'grad_norm': 1.618351936340332, 'learning_rate': 3.262374574227847e-05, 'epoch': 8.340909090909092}\n{'loss': 0.0841, 'grad_norm': 0.9896824955940247, 'learning_rate': 3.218288431332876e-05, 'epoch': 8.363636363636363}\n{'loss': 0.0862, 'grad_norm': 0.9263288378715515, 'learning_rate': 3.174202288437905e-05, 'epoch': 8.386363636363637}\n{'loss': 0.0713, 'grad_norm': 0.9552398324012756, 'learning_rate': 3.1301161455429346e-05, 'epoch': 8.409090909090908}\n{'loss': 0.0766, 'grad_norm': 0.697153627872467, 'learning_rate': 3.086030002647963e-05, 'epoch': 8.431818181818182}\n{'loss': 0.0665, 'grad_norm': 0.8693141341209412, 'learning_rate': 3.041943859752993e-05, 'epoch': 8.454545454545455}\n{'loss': 0.0862, 'grad_norm': 0.9376292824745178, 'learning_rate': 2.9978577168580216e-05, 'epoch': 8.477272727272727}\n{'loss': 0.0652, 'grad_norm': 0.8625780344009399, 'learning_rate': 2.953771573963051e-05, 'epoch': 8.5}\n{'loss': 0.0806, 'grad_norm': 0.875874400138855, 'learning_rate': 2.90968543106808e-05, 'epoch': 8.522727272727273}\n{'loss': 0.0753, 'grad_norm': 0.7856677174568176, 'learning_rate': 2.865599288173109e-05, 'epoch': 8.545454545454545}\n{'loss': 0.0681, 'grad_norm': 1.049868106842041, 'learning_rate': 2.8215131452781382e-05, 'epoch': 8.568181818181818}\n{'loss': 0.0896, 'grad_norm': 0.8964752554893494, 'learning_rate': 2.7774270023831672e-05, 'epoch': 8.590909090909092}\n{'loss': 0.0733, 'grad_norm': 0.6560808420181274, 'learning_rate': 2.7333408594881966e-05, 'epoch': 8.613636363636363}\n{'loss': 0.0631, 'grad_norm': 0.9726635813713074, 'learning_rate': 2.6892547165932252e-05, 'epoch': 8.636363636363637}\n{'loss': 0.0802, 'grad_norm': 0.7986415028572083, 'learning_rate': 2.6451685736982546e-05, 'epoch': 8.659090909090908}\n{'loss': 0.0705, 'grad_norm': 1.1873068809509277, 'learning_rate': 2.6010824308032832e-05, 'epoch': 8.681818181818182}\n{'loss': 0.07, 'grad_norm': 0.746887743473053, 'learning_rate': 2.5569962879083126e-05, 'epoch': 8.704545454545455}\n{'loss': 0.0921, 'grad_norm': 1.1226049661636353, 'learning_rate': 2.512910145013342e-05, 'epoch': 8.727272727272727}\n{'loss': 0.0808, 'grad_norm': 0.860185980796814, 'learning_rate': 2.4688240021183706e-05, 'epoch': 8.75}\n{'loss': 0.0769, 'grad_norm': 1.0373724699020386, 'learning_rate': 2.4247378592234e-05, 'epoch': 8.772727272727273}\n{'loss': 0.071, 'grad_norm': 1.102112054824829, 'learning_rate': 2.380651716328429e-05, 'epoch': 8.795454545454545}\n{'loss': 0.0951, 'grad_norm': 0.7564525604248047, 'learning_rate': 2.3365655734334583e-05, 'epoch': 8.818181818181818}\n{'loss': 0.0869, 'grad_norm': 0.8239403367042542, 'learning_rate': 2.2924794305384873e-05, 'epoch': 8.840909090909092}\n{'loss': 0.1078, 'grad_norm': 1.1376193761825562, 'learning_rate': 2.2483932876435163e-05, 'epoch': 8.863636363636363}\n{'loss': 0.0684, 'grad_norm': 0.9803221821784973, 'learning_rate': 2.2043071447485456e-05, 'epoch': 8.886363636363637}\n{'loss': 0.0963, 'grad_norm': 1.1466031074523926, 'learning_rate': 2.1602210018535746e-05, 'epoch': 8.909090909090908}\n{'loss': 0.1059, 'grad_norm': 0.9574524164199829, 'learning_rate': 2.1161348589586036e-05, 'epoch': 8.931818181818182}\n{'loss': 0.0886, 'grad_norm': 1.215103030204773, 'learning_rate': 2.0720487160636326e-05, 'epoch': 8.954545454545455}\n{'loss': 0.0582, 'grad_norm': 0.8692432045936584, 'learning_rate': 2.0279625731686616e-05, 'epoch': 8.977272727272727}\n{'loss': 0.1567, 'grad_norm': 6.6740827560424805, 'learning_rate': 1.983876430273691e-05, 'epoch': 9.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.9028    0.8840    0.8933      3456\n     PERCENT     0.8795    0.9481    0.9125        77\n        TYPE     0.9470    0.9692    0.9579     11282\n      VOLUME     0.9070    0.9512    0.9286        41\n\n   micro avg     0.9366    0.9492    0.9428     14856\n   macro avg     0.9091    0.9381    0.9231     14856\nweighted avg     0.9362    0.9492    0.9426     14856\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.2721715271472931, 'eval_f1_macro': 0.9230731660022962, 'eval_precision': 0.9365701381509033, 'eval_recall': 0.9491787829833064, 'eval_f1': 0.9428323081037709, 'eval_accuracy': 0.934671453437772, 'eval_runtime': 1.4821, 'eval_samples_per_second': 3717.809, 'eval_steps_per_second': 7.422, 'epoch': 9.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.0745, 'grad_norm': 0.6706055402755737, 'learning_rate': 1.93979028737872e-05, 'epoch': 9.022727272727273}\n{'loss': 0.0681, 'grad_norm': 0.9025905132293701, 'learning_rate': 1.8957041444837493e-05, 'epoch': 9.045454545454545}\n{'loss': 0.075, 'grad_norm': 0.7133867740631104, 'learning_rate': 1.8516180015887783e-05, 'epoch': 9.068181818181818}\n{'loss': 0.0711, 'grad_norm': 0.7440642714500427, 'learning_rate': 1.8075318586938073e-05, 'epoch': 9.090909090909092}\n{'loss': 0.0642, 'grad_norm': 0.748037576675415, 'learning_rate': 1.7634457157988363e-05, 'epoch': 9.113636363636363}\n{'loss': 0.0815, 'grad_norm': 0.9888754487037659, 'learning_rate': 1.7193595729038653e-05, 'epoch': 9.136363636363637}\n{'loss': 0.084, 'grad_norm': 0.8386178016662598, 'learning_rate': 1.6752734300088943e-05, 'epoch': 9.159090909090908}\n{'loss': 0.0745, 'grad_norm': 0.7970304489135742, 'learning_rate': 1.6311872871139236e-05, 'epoch': 9.181818181818182}\n{'loss': 0.0809, 'grad_norm': 0.8214352130889893, 'learning_rate': 1.5871011442189526e-05, 'epoch': 9.204545454545455}\n{'loss': 0.1028, 'grad_norm': 1.3502116203308105, 'learning_rate': 1.5430150013239816e-05, 'epoch': 9.227272727272727}\n{'loss': 0.069, 'grad_norm': 0.71306312084198, 'learning_rate': 1.4989288584290108e-05, 'epoch': 9.25}\n{'loss': 0.0609, 'grad_norm': 1.151458501815796, 'learning_rate': 1.45484271553404e-05, 'epoch': 9.272727272727273}\n{'loss': 0.0736, 'grad_norm': 0.8162477016448975, 'learning_rate': 1.4107565726390691e-05, 'epoch': 9.295454545454545}\n{'loss': 0.1021, 'grad_norm': 0.8231815695762634, 'learning_rate': 1.3666704297440983e-05, 'epoch': 9.318181818181818}\n{'loss': 0.1061, 'grad_norm': 1.1915656328201294, 'learning_rate': 1.3225842868491273e-05, 'epoch': 9.340909090909092}\n{'loss': 0.0668, 'grad_norm': 0.7283597588539124, 'learning_rate': 1.2784981439541563e-05, 'epoch': 9.363636363636363}\n{'loss': 0.0665, 'grad_norm': 0.769858181476593, 'learning_rate': 1.2344120010591853e-05, 'epoch': 9.386363636363637}\n{'loss': 0.0602, 'grad_norm': 1.0326086282730103, 'learning_rate': 1.1903258581642145e-05, 'epoch': 9.409090909090908}\n{'loss': 0.0967, 'grad_norm': 0.7321521043777466, 'learning_rate': 1.1462397152692436e-05, 'epoch': 9.431818181818182}\n{'loss': 0.0782, 'grad_norm': 0.8055295348167419, 'learning_rate': 1.1021535723742728e-05, 'epoch': 9.454545454545455}\n{'loss': 0.0719, 'grad_norm': 1.028490662574768, 'learning_rate': 1.0580674294793018e-05, 'epoch': 9.477272727272727}\n{'loss': 0.0717, 'grad_norm': 0.804518461227417, 'learning_rate': 1.0139812865843308e-05, 'epoch': 9.5}\n{'loss': 0.072, 'grad_norm': 0.9519805312156677, 'learning_rate': 9.6989514368936e-06, 'epoch': 9.522727272727273}\n{'loss': 0.0846, 'grad_norm': 0.9722256064414978, 'learning_rate': 9.258090007943891e-06, 'epoch': 9.545454545454545}\n{'loss': 0.0694, 'grad_norm': 0.8000583648681641, 'learning_rate': 8.817228578994181e-06, 'epoch': 9.568181818181818}\n{'loss': 0.0652, 'grad_norm': 0.7647653818130493, 'learning_rate': 8.376367150044471e-06, 'epoch': 9.590909090909092}\n{'loss': 0.0581, 'grad_norm': 0.7018690705299377, 'learning_rate': 7.935505721094763e-06, 'epoch': 9.613636363636363}\n{'loss': 0.0729, 'grad_norm': 0.8208757638931274, 'learning_rate': 7.494644292145054e-06, 'epoch': 9.636363636363637}\n{'loss': 0.0603, 'grad_norm': 0.5937938094139099, 'learning_rate': 7.053782863195346e-06, 'epoch': 9.659090909090908}\n{'loss': 0.053, 'grad_norm': 0.6782587766647339, 'learning_rate': 6.6129214342456364e-06, 'epoch': 9.681818181818182}\n{'loss': 0.0549, 'grad_norm': 1.0186471939086914, 'learning_rate': 6.1720600052959264e-06, 'epoch': 9.704545454545455}\n{'loss': 0.0862, 'grad_norm': 1.177555799484253, 'learning_rate': 5.731198576346218e-06, 'epoch': 9.727272727272727}\n{'loss': 0.0532, 'grad_norm': 0.551960825920105, 'learning_rate': 5.290337147396509e-06, 'epoch': 9.75}\n{'loss': 0.0701, 'grad_norm': 0.7577298879623413, 'learning_rate': 4.8494757184468e-06, 'epoch': 9.772727272727273}\n{'loss': 0.0722, 'grad_norm': 0.9285699725151062, 'learning_rate': 4.408614289497091e-06, 'epoch': 9.795454545454545}\n{'loss': 0.0727, 'grad_norm': 0.9843253493309021, 'learning_rate': 3.9677528605473815e-06, 'epoch': 9.818181818181818}\n{'loss': 0.0698, 'grad_norm': 0.7661665678024292, 'learning_rate': 3.526891431597673e-06, 'epoch': 9.840909090909092}\n{'loss': 0.0646, 'grad_norm': 0.7689648270606995, 'learning_rate': 3.0860300026479632e-06, 'epoch': 9.863636363636363}\n{'loss': 0.0601, 'grad_norm': 1.0961742401123047, 'learning_rate': 2.6451685736982545e-06, 'epoch': 9.886363636363637}\n{'loss': 0.093, 'grad_norm': 1.1701363325119019, 'learning_rate': 2.2043071447485453e-06, 'epoch': 9.909090909090908}\n{'loss': 0.0581, 'grad_norm': 0.8007631301879883, 'learning_rate': 1.7634457157988364e-06, 'epoch': 9.931818181818182}\n{'loss': 0.081, 'grad_norm': 1.0050851106643677, 'learning_rate': 1.3225842868491272e-06, 'epoch': 9.954545454545455}\n{'loss': 0.0752, 'grad_norm': 0.6549093723297119, 'learning_rate': 8.817228578994182e-07, 'epoch': 9.977272727272727}\n{'loss': 0.1743, 'grad_norm': 5.91567850112915, 'learning_rate': 4.408614289497091e-07, 'epoch': 10.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8961    0.8860    0.8910      3456\n     PERCENT     0.8795    0.9481    0.9125        77\n        TYPE     0.9471    0.9679    0.9574     11282\n      VOLUME     0.8864    0.9512    0.9176        41\n\n   micro avg     0.9350    0.9487    0.9418     14856\n   macro avg     0.9023    0.9383    0.9196     14856\nweighted avg     0.9347    0.9487    0.9416     14856\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.27537211775779724, 'eval_f1_macro': 0.9196401871887692, 'eval_precision': 0.9349873955154571, 'eval_recall': 0.9487075928917609, 'eval_f1': 0.9417975275643167, 'eval_accuracy': 0.9339099216710183, 'eval_runtime': 1.4946, 'eval_samples_per_second': 3686.515, 'eval_steps_per_second': 7.36, 'epoch': 10.0}\n{'train_runtime': 63.0233, 'train_samples_per_second': 3497.436, 'train_steps_per_second': 6.982, 'train_loss': 0.2857263969680802, 'epoch': 10.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nThe speedups for torchdynamo mostly come with GPU Ampere or higher and which is not detected here.\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.9028    0.8840    0.8933      3456\n     PERCENT     0.8795    0.9481    0.9125        77\n        TYPE     0.9470    0.9692    0.9579     11282\n      VOLUME     0.9070    0.9512    0.9286        41\n\n   micro avg     0.9366    0.9492    0.9428     14856\n   macro avg     0.9091    0.9381    0.9231     14856\nweighted avg     0.9362    0.9492    0.9426     14856\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.2721715271472931, 'eval_f1_macro': 0.9230731660022962, 'eval_precision': 0.9365701381509033, 'eval_recall': 0.9491787829833064, 'eval_f1': 0.9428323081037709, 'eval_accuracy': 0.934671453437772, 'eval_runtime': 1.9802, 'eval_samples_per_second': 2782.545, 'eval_steps_per_second': 5.555, 'epoch': 10.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/3364797558.py:66: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 2.1831, 'grad_norm': 7.3428192138671875, 'learning_rate': 0.0, 'epoch': 0.022727272727272728}\n{'loss': 2.1806, 'grad_norm': 7.105165004730225, 'learning_rate': 3.9677528605473815e-06, 'epoch': 0.045454545454545456}\n{'loss': 2.1791, 'grad_norm': 6.93489933013916, 'learning_rate': 7.935505721094763e-06, 'epoch': 0.06818181818181818}\n{'loss': 2.1484, 'grad_norm': 7.213156223297119, 'learning_rate': 1.1903258581642145e-05, 'epoch': 0.09090909090909091}\n{'loss': 2.1276, 'grad_norm': 7.152639389038086, 'learning_rate': 1.5871011442189526e-05, 'epoch': 0.11363636363636363}\n{'loss': 2.0901, 'grad_norm': 6.96719217300415, 'learning_rate': 1.983876430273691e-05, 'epoch': 0.13636363636363635}\n{'loss': 2.0539, 'grad_norm': 6.58499813079834, 'learning_rate': 2.380651716328429e-05, 'epoch': 0.1590909090909091}\n{'loss': 2.0005, 'grad_norm': 6.664762496948242, 'learning_rate': 2.7774270023831672e-05, 'epoch': 0.18181818181818182}\n{'loss': 1.9276, 'grad_norm': 6.6102423667907715, 'learning_rate': 3.174202288437905e-05, 'epoch': 0.20454545454545456}\n{'loss': 1.8735, 'grad_norm': 6.081855773925781, 'learning_rate': 3.570977574492644e-05, 'epoch': 0.22727272727272727}\n{'loss': 1.7881, 'grad_norm': 6.127443313598633, 'learning_rate': 3.967752860547382e-05, 'epoch': 0.25}\n{'loss': 1.7368, 'grad_norm': 5.412753105163574, 'learning_rate': 4.36452814660212e-05, 'epoch': 0.2727272727272727}\n{'loss': 1.6453, 'grad_norm': 5.3998894691467285, 'learning_rate': 4.761303432656858e-05, 'epoch': 0.29545454545454547}\n{'loss': 1.5765, 'grad_norm': 4.807955741882324, 'learning_rate': 5.1580787187115965e-05, 'epoch': 0.3181818181818182}\n{'loss': 1.558, 'grad_norm': 4.122076988220215, 'learning_rate': 5.5548540047663345e-05, 'epoch': 0.3409090909090909}\n{'loss': 1.3972, 'grad_norm': 4.1034770011901855, 'learning_rate': 5.951629290821072e-05, 'epoch': 0.36363636363636365}\n{'loss': 1.37, 'grad_norm': 3.270888090133667, 'learning_rate': 6.34840457687581e-05, 'epoch': 0.38636363636363635}\n{'loss': 1.344, 'grad_norm': 2.6943929195404053, 'learning_rate': 6.745179862930548e-05, 'epoch': 0.4090909090909091}\n{'loss': 1.2941, 'grad_norm': 2.2523751258850098, 'learning_rate': 7.141955148985288e-05, 'epoch': 0.4318181818181818}\n{'loss': 1.2296, 'grad_norm': 1.988403081893921, 'learning_rate': 7.538730435040026e-05, 'epoch': 0.45454545454545453}\n{'loss': 1.2145, 'grad_norm': 1.9972238540649414, 'learning_rate': 7.935505721094764e-05, 'epoch': 0.4772727272727273}\n{'loss': 1.2007, 'grad_norm': 2.0158538818359375, 'learning_rate': 8.332281007149502e-05, 'epoch': 0.5}\n{'loss': 1.1413, 'grad_norm': 1.8041958808898926, 'learning_rate': 8.72905629320424e-05, 'epoch': 0.5227272727272727}\n{'loss': 1.0646, 'grad_norm': 1.362870216369629, 'learning_rate': 9.125831579258978e-05, 'epoch': 0.5454545454545454}\n{'loss': 1.0912, 'grad_norm': 1.5886662006378174, 'learning_rate': 9.522606865313716e-05, 'epoch': 0.5681818181818182}\n{'loss': 1.0278, 'grad_norm': 1.2096388339996338, 'learning_rate': 9.919382151368455e-05, 'epoch': 0.5909090909090909}\n{'loss': 1.0152, 'grad_norm': 1.1391221284866333, 'learning_rate': 0.00010316157437423193, 'epoch': 0.6136363636363636}\n{'loss': 0.9697, 'grad_norm': 1.9034816026687622, 'learning_rate': 0.00010712932723477931, 'epoch': 0.6363636363636364}\n{'loss': 0.9975, 'grad_norm': 1.6544504165649414, 'learning_rate': 0.00011109708009532669, 'epoch': 0.6590909090909091}\n{'loss': 0.9267, 'grad_norm': 1.9680742025375366, 'learning_rate': 0.00011506483295587407, 'epoch': 0.6818181818181818}\n{'loss': 0.926, 'grad_norm': 1.9243143796920776, 'learning_rate': 0.00011903258581642144, 'epoch': 0.7045454545454546}\n{'loss': 0.8688, 'grad_norm': 1.4299780130386353, 'learning_rate': 0.00012300033867696883, 'epoch': 0.7272727272727273}\n{'loss': 0.8474, 'grad_norm': 0.9337648153305054, 'learning_rate': 0.0001269680915375162, 'epoch': 0.75}\n{'loss': 0.8027, 'grad_norm': 0.9655870795249939, 'learning_rate': 0.0001309358443980636, 'epoch': 0.7727272727272727}\n{'loss': 0.8365, 'grad_norm': 1.6586804389953613, 'learning_rate': 0.00013490359725861097, 'epoch': 0.7954545454545454}\n{'loss': 0.7541, 'grad_norm': 1.3158577680587769, 'learning_rate': 0.00013887135011915835, 'epoch': 0.8181818181818182}\n{'loss': 0.7889, 'grad_norm': 1.354904055595398, 'learning_rate': 0.00014283910297970576, 'epoch': 0.8409090909090909}\n{'loss': 0.6898, 'grad_norm': 1.181416630744934, 'learning_rate': 0.00014680685584025314, 'epoch': 0.8636363636363636}\n{'loss': 0.746, 'grad_norm': 0.8835164308547974, 'learning_rate': 0.00015077460870080052, 'epoch': 0.8863636363636364}\n{'loss': 0.6877, 'grad_norm': 0.9538665413856506, 'learning_rate': 0.0001547423615613479, 'epoch': 0.9090909090909091}\n{'loss': 0.5823, 'grad_norm': 1.049810767173767, 'learning_rate': 0.00015871011442189527, 'epoch': 0.9318181818181818}\n{'loss': 0.5913, 'grad_norm': 0.9946486949920654, 'learning_rate': 0.00016267786728244263, 'epoch': 0.9545454545454546}\n{'loss': 0.6452, 'grad_norm': 1.120143175125122, 'learning_rate': 0.00016664562014299003, 'epoch': 0.9772727272727273}\n{'loss': 0.5766, 'grad_norm': 2.115652322769165, 'learning_rate': 0.00017061337300353741, 'epoch': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.7466    0.6368    0.6873      3221\n     PERCENT     0.8929    0.2874    0.4348        87\n        TYPE     0.8518    0.9039    0.8771     11501\n      VOLUME     0.0000    0.0000    0.0000        59\n\n   micro avg     0.8326    0.8388    0.8357     14868\n   macro avg     0.6228    0.4570    0.4998     14868\nweighted avg     0.8259    0.8388    0.8299     14868\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.5467473864555359, 'eval_f1_macro': 0.499798145371979, 'eval_precision': 0.8325767690253671, 'eval_recall': 0.838848533763788, 'eval_f1': 0.8357008844813721, 'eval_accuracy': 0.8367878459034184, 'eval_runtime': 1.4902, 'eval_samples_per_second': 3697.608, 'eval_steps_per_second': 7.382, 'epoch': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.5768, 'grad_norm': 0.8886855840682983, 'learning_rate': 0.0001745811258640848, 'epoch': 1.0227272727272727}\n{'loss': 0.6215, 'grad_norm': 1.4438687562942505, 'learning_rate': 0.0001741402644351351, 'epoch': 1.0454545454545454}\n{'loss': 0.5113, 'grad_norm': 0.9315398931503296, 'learning_rate': 0.00017369940300618537, 'epoch': 1.0681818181818181}\n{'loss': 0.568, 'grad_norm': 0.8153699636459351, 'learning_rate': 0.00017325854157723567, 'epoch': 1.0909090909090908}\n{'loss': 0.564, 'grad_norm': 1.07937490940094, 'learning_rate': 0.00017281768014828597, 'epoch': 1.1136363636363635}\n{'loss': 0.5287, 'grad_norm': 1.7722686529159546, 'learning_rate': 0.00017237681871933624, 'epoch': 1.1363636363636362}\n{'loss': 0.5083, 'grad_norm': 1.4440687894821167, 'learning_rate': 0.00017193595729038654, 'epoch': 1.1590909090909092}\n{'loss': 0.5008, 'grad_norm': 0.8165600299835205, 'learning_rate': 0.00017149509586143684, 'epoch': 1.1818181818181819}\n{'loss': 0.5077, 'grad_norm': 0.8457169532775879, 'learning_rate': 0.00017105423443248711, 'epoch': 1.2045454545454546}\n{'loss': 0.4458, 'grad_norm': 0.971605122089386, 'learning_rate': 0.00017061337300353741, 'epoch': 1.2272727272727273}\n{'loss': 0.4686, 'grad_norm': 1.0913057327270508, 'learning_rate': 0.0001701725115745877, 'epoch': 1.25}\n{'loss': 0.4653, 'grad_norm': 1.081369161605835, 'learning_rate': 0.000169731650145638, 'epoch': 1.2727272727272727}\n{'loss': 0.3946, 'grad_norm': 0.7578063607215881, 'learning_rate': 0.0001692907887166883, 'epoch': 1.2954545454545454}\n{'loss': 0.5093, 'grad_norm': 1.993844985961914, 'learning_rate': 0.00016884992728773856, 'epoch': 1.3181818181818181}\n{'loss': 0.4521, 'grad_norm': 2.275428533554077, 'learning_rate': 0.00016840906585878886, 'epoch': 1.3409090909090908}\n{'loss': 0.4771, 'grad_norm': 0.9582122564315796, 'learning_rate': 0.00016796820442983916, 'epoch': 1.3636363636363638}\n{'loss': 0.4137, 'grad_norm': 1.1220170259475708, 'learning_rate': 0.00016752734300088943, 'epoch': 1.3863636363636362}\n{'loss': 0.3993, 'grad_norm': 1.0916829109191895, 'learning_rate': 0.00016708648157193973, 'epoch': 1.4090909090909092}\n{'loss': 0.402, 'grad_norm': 1.1150704622268677, 'learning_rate': 0.00016664562014299003, 'epoch': 1.4318181818181819}\n{'loss': 0.4166, 'grad_norm': 1.135408878326416, 'learning_rate': 0.0001662047587140403, 'epoch': 1.4545454545454546}\n{'loss': 0.3637, 'grad_norm': 1.01357901096344, 'learning_rate': 0.0001657638972850906, 'epoch': 1.4772727272727273}\n{'loss': 0.4025, 'grad_norm': 1.4453444480895996, 'learning_rate': 0.0001653230358561409, 'epoch': 1.5}\n{'loss': 0.4406, 'grad_norm': 2.089836835861206, 'learning_rate': 0.00016488217442719118, 'epoch': 1.5227272727272727}\n{'loss': 0.4275, 'grad_norm': 0.941949188709259, 'learning_rate': 0.00016444131299824148, 'epoch': 1.5454545454545454}\n{'loss': 0.3999, 'grad_norm': 1.1453884840011597, 'learning_rate': 0.00016400045156929178, 'epoch': 1.5681818181818183}\n{'loss': 0.4174, 'grad_norm': 0.9312180876731873, 'learning_rate': 0.00016355959014034205, 'epoch': 1.5909090909090908}\n{'loss': 0.4153, 'grad_norm': 1.3202545642852783, 'learning_rate': 0.00016311872871139235, 'epoch': 1.6136363636363638}\n{'loss': 0.3844, 'grad_norm': 1.0571550130844116, 'learning_rate': 0.00016267786728244263, 'epoch': 1.6363636363636362}\n{'loss': 0.4046, 'grad_norm': 1.1136162281036377, 'learning_rate': 0.00016223700585349293, 'epoch': 1.6590909090909092}\n{'loss': 0.3867, 'grad_norm': 1.1349921226501465, 'learning_rate': 0.00016179614442454323, 'epoch': 1.6818181818181817}\n{'loss': 0.3802, 'grad_norm': 0.8964623212814331, 'learning_rate': 0.0001613552829955935, 'epoch': 1.7045454545454546}\n{'loss': 0.3431, 'grad_norm': 0.789758026599884, 'learning_rate': 0.0001609144215666438, 'epoch': 1.7272727272727273}\n{'loss': 0.3911, 'grad_norm': 0.9882561564445496, 'learning_rate': 0.0001604735601376941, 'epoch': 1.75}\n{'loss': 0.3875, 'grad_norm': 1.8168874979019165, 'learning_rate': 0.0001600326987087444, 'epoch': 1.7727272727272727}\n{'loss': 0.3248, 'grad_norm': 1.2781322002410889, 'learning_rate': 0.00015959183727979467, 'epoch': 1.7954545454545454}\n{'loss': 0.3961, 'grad_norm': 1.060103178024292, 'learning_rate': 0.00015915097585084497, 'epoch': 1.8181818181818183}\n{'loss': 0.369, 'grad_norm': 0.8341938853263855, 'learning_rate': 0.00015871011442189527, 'epoch': 1.8409090909090908}\n{'loss': 0.3642, 'grad_norm': 1.0439224243164062, 'learning_rate': 0.00015826925299294555, 'epoch': 1.8636363636363638}\n{'loss': 0.3564, 'grad_norm': 0.9329031109809875, 'learning_rate': 0.00015782839156399585, 'epoch': 1.8863636363636362}\n{'loss': 0.3732, 'grad_norm': 0.8017022013664246, 'learning_rate': 0.00015738753013504615, 'epoch': 1.9090909090909092}\n{'loss': 0.334, 'grad_norm': 0.8363956212997437, 'learning_rate': 0.00015694666870609642, 'epoch': 1.9318181818181817}\n{'loss': 0.3978, 'grad_norm': 1.1388928890228271, 'learning_rate': 0.00015650580727714672, 'epoch': 1.9545454545454546}\n{'loss': 0.3581, 'grad_norm': 0.7612982392311096, 'learning_rate': 0.00015606494584819702, 'epoch': 1.9772727272727273}\n{'loss': 0.2426, 'grad_norm': 2.7128167152404785, 'learning_rate': 0.0001556240844192473, 'epoch': 2.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8107    0.8308    0.8206      3221\n     PERCENT     0.6452    0.9195    0.7583        87\n        TYPE     0.9307    0.9568    0.9436     11501\n      VOLUME     0.3889    0.3559    0.3717        59\n\n   micro avg     0.9006    0.9269    0.9136     14868\n   macro avg     0.6939    0.7658    0.7235     14868\nweighted avg     0.9009    0.9269    0.9136     14868\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.30413758754730225, 'eval_f1_macro': 0.7235399653692809, 'eval_precision': 0.900601228597569, 'eval_recall': 0.9268899650255582, 'eval_f1': 0.913556513092476, 'eval_accuracy': 0.9096581660336408, 'eval_runtime': 1.4716, 'eval_samples_per_second': 3744.186, 'eval_steps_per_second': 7.475, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.3704, 'grad_norm': 0.940277099609375, 'learning_rate': 0.0001551832229902976, 'epoch': 2.022727272727273}\n{'loss': 0.3091, 'grad_norm': 1.217978596687317, 'learning_rate': 0.0001547423615613479, 'epoch': 2.0454545454545454}\n{'loss': 0.354, 'grad_norm': 1.1208045482635498, 'learning_rate': 0.0001543015001323982, 'epoch': 2.0681818181818183}\n{'loss': 0.3122, 'grad_norm': 0.8677845001220703, 'learning_rate': 0.00015386063870344847, 'epoch': 2.090909090909091}\n{'loss': 0.3489, 'grad_norm': 1.336053490638733, 'learning_rate': 0.00015341977727449877, 'epoch': 2.1136363636363638}\n{'loss': 0.2863, 'grad_norm': 0.6692563891410828, 'learning_rate': 0.00015297891584554907, 'epoch': 2.1363636363636362}\n{'loss': 0.2605, 'grad_norm': 0.7580469846725464, 'learning_rate': 0.00015253805441659934, 'epoch': 2.159090909090909}\n{'loss': 0.2879, 'grad_norm': 1.1125942468643188, 'learning_rate': 0.00015209719298764964, 'epoch': 2.1818181818181817}\n{'loss': 0.3287, 'grad_norm': 0.8736571073532104, 'learning_rate': 0.00015165633155869994, 'epoch': 2.2045454545454546}\n{'loss': 0.2434, 'grad_norm': 1.3916244506835938, 'learning_rate': 0.00015121547012975021, 'epoch': 2.227272727272727}\n{'loss': 0.2803, 'grad_norm': 0.640315592288971, 'learning_rate': 0.00015077460870080052, 'epoch': 2.25}\n{'loss': 0.257, 'grad_norm': 1.241992473602295, 'learning_rate': 0.00015033374727185082, 'epoch': 2.2727272727272725}\n{'loss': 0.2336, 'grad_norm': 0.9533981084823608, 'learning_rate': 0.0001498928858429011, 'epoch': 2.2954545454545454}\n{'loss': 0.2544, 'grad_norm': 1.4444780349731445, 'learning_rate': 0.0001494520244139514, 'epoch': 2.3181818181818183}\n{'loss': 0.255, 'grad_norm': 0.8657602667808533, 'learning_rate': 0.00014901116298500166, 'epoch': 2.340909090909091}\n{'loss': 0.2926, 'grad_norm': 1.1249369382858276, 'learning_rate': 0.00014857030155605196, 'epoch': 2.3636363636363638}\n{'loss': 0.3316, 'grad_norm': 1.397480845451355, 'learning_rate': 0.00014812944012710226, 'epoch': 2.3863636363636362}\n{'loss': 0.2674, 'grad_norm': 1.5988775491714478, 'learning_rate': 0.00014768857869815253, 'epoch': 2.409090909090909}\n{'loss': 0.2327, 'grad_norm': 1.2024394273757935, 'learning_rate': 0.00014724771726920284, 'epoch': 2.4318181818181817}\n{'loss': 0.2703, 'grad_norm': 1.2099896669387817, 'learning_rate': 0.00014680685584025314, 'epoch': 2.4545454545454546}\n{'loss': 0.2484, 'grad_norm': 0.8919017314910889, 'learning_rate': 0.0001463659944113034, 'epoch': 2.4772727272727275}\n{'loss': 0.2757, 'grad_norm': 1.1702518463134766, 'learning_rate': 0.0001459251329823537, 'epoch': 2.5}\n{'loss': 0.3182, 'grad_norm': 1.6209986209869385, 'learning_rate': 0.000145484271553404, 'epoch': 2.5227272727272725}\n{'loss': 0.2456, 'grad_norm': 1.0714446306228638, 'learning_rate': 0.00014504341012445428, 'epoch': 2.5454545454545454}\n{'loss': 0.2465, 'grad_norm': 0.9303532838821411, 'learning_rate': 0.00014460254869550458, 'epoch': 2.5681818181818183}\n{'loss': 0.2579, 'grad_norm': 1.153049349784851, 'learning_rate': 0.00014416168726655488, 'epoch': 2.590909090909091}\n{'loss': 0.2437, 'grad_norm': 0.7401394248008728, 'learning_rate': 0.00014372082583760516, 'epoch': 2.6136363636363638}\n{'loss': 0.3123, 'grad_norm': 1.4981276988983154, 'learning_rate': 0.00014327996440865546, 'epoch': 2.6363636363636362}\n{'loss': 0.2491, 'grad_norm': 0.8103256225585938, 'learning_rate': 0.00014283910297970576, 'epoch': 2.659090909090909}\n{'loss': 0.347, 'grad_norm': 1.5720512866973877, 'learning_rate': 0.00014239824155075603, 'epoch': 2.6818181818181817}\n{'loss': 0.2481, 'grad_norm': 1.128357172012329, 'learning_rate': 0.00014195738012180633, 'epoch': 2.7045454545454546}\n{'loss': 0.2738, 'grad_norm': 1.0070604085922241, 'learning_rate': 0.0001415165186928566, 'epoch': 2.7272727272727275}\n{'loss': 0.2986, 'grad_norm': 1.4538609981536865, 'learning_rate': 0.0001410756572639069, 'epoch': 2.75}\n{'loss': 0.2507, 'grad_norm': 1.1627253293991089, 'learning_rate': 0.0001406347958349572, 'epoch': 2.7727272727272725}\n{'loss': 0.2951, 'grad_norm': 0.8817890286445618, 'learning_rate': 0.00014019393440600748, 'epoch': 2.7954545454545454}\n{'loss': 0.2646, 'grad_norm': 1.2634921073913574, 'learning_rate': 0.00013975307297705778, 'epoch': 2.8181818181818183}\n{'loss': 0.268, 'grad_norm': 0.85174560546875, 'learning_rate': 0.00013931221154810808, 'epoch': 2.840909090909091}\n{'loss': 0.2238, 'grad_norm': 1.01399564743042, 'learning_rate': 0.00013887135011915835, 'epoch': 2.8636363636363638}\n{'loss': 0.2477, 'grad_norm': 1.2867443561553955, 'learning_rate': 0.00013843048869020865, 'epoch': 2.8863636363636362}\n{'loss': 0.23, 'grad_norm': 0.8719744086265564, 'learning_rate': 0.00013798962726125895, 'epoch': 2.909090909090909}\n{'loss': 0.23, 'grad_norm': 1.6386287212371826, 'learning_rate': 0.00013754876583230922, 'epoch': 2.9318181818181817}\n{'loss': 0.2445, 'grad_norm': 0.8370323777198792, 'learning_rate': 0.00013710790440335952, 'epoch': 2.9545454545454546}\n{'loss': 0.2277, 'grad_norm': 0.7915531396865845, 'learning_rate': 0.00013666704297440982, 'epoch': 2.9772727272727275}\n{'loss': 0.1503, 'grad_norm': 3.546217918395996, 'learning_rate': 0.0001362261815454601, 'epoch': 3.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8465    0.8780    0.8619      3221\n     PERCENT     0.7547    0.9195    0.8290        87\n        TYPE     0.9446    0.9644    0.9544     11501\n      VOLUME     0.6034    0.5932    0.5983        59\n\n   micro avg     0.9205    0.9440    0.9321     14868\n   macro avg     0.7873    0.8388    0.8109     14868\nweighted avg     0.9209    0.9440    0.9323     14868\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.2453521192073822, 'eval_f1_macro': 0.8109190938285672, 'eval_precision': 0.9205089525808355, 'eval_recall': 0.9439736346516008, 'eval_f1': 0.9320936410426698, 'eval_accuracy': 0.9262072707542051, 'eval_runtime': 1.4665, 'eval_samples_per_second': 3757.12, 'eval_steps_per_second': 7.501, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.2387, 'grad_norm': 0.7976852059364319, 'learning_rate': 0.0001357853201165104, 'epoch': 3.022727272727273}\n{'loss': 0.2165, 'grad_norm': 0.8047690987586975, 'learning_rate': 0.0001353444586875607, 'epoch': 3.0454545454545454}\n{'loss': 0.2442, 'grad_norm': 0.9848095178604126, 'learning_rate': 0.00013490359725861097, 'epoch': 3.0681818181818183}\n{'loss': 0.2227, 'grad_norm': 1.030336856842041, 'learning_rate': 0.00013446273582966127, 'epoch': 3.090909090909091}\n{'loss': 0.2315, 'grad_norm': 1.1934828758239746, 'learning_rate': 0.00013402187440071154, 'epoch': 3.1136363636363638}\n{'loss': 0.1889, 'grad_norm': 0.8801076412200928, 'learning_rate': 0.00013358101297176184, 'epoch': 3.1363636363636362}\n{'loss': 0.2278, 'grad_norm': 2.0188517570495605, 'learning_rate': 0.00013314015154281214, 'epoch': 3.159090909090909}\n{'loss': 0.2476, 'grad_norm': 1.8078049421310425, 'learning_rate': 0.00013269929011386242, 'epoch': 3.1818181818181817}\n{'loss': 0.2145, 'grad_norm': 1.2278788089752197, 'learning_rate': 0.00013225842868491272, 'epoch': 3.2045454545454546}\n{'loss': 0.2056, 'grad_norm': 1.453588604927063, 'learning_rate': 0.00013181756725596302, 'epoch': 3.227272727272727}\n{'loss': 0.1734, 'grad_norm': 1.3738982677459717, 'learning_rate': 0.0001313767058270133, 'epoch': 3.25}\n{'loss': 0.1991, 'grad_norm': 1.610012412071228, 'learning_rate': 0.0001309358443980636, 'epoch': 3.2727272727272725}\n{'loss': 0.212, 'grad_norm': 2.835749626159668, 'learning_rate': 0.0001304949829691139, 'epoch': 3.2954545454545454}\n{'loss': 0.1866, 'grad_norm': 1.4190212488174438, 'learning_rate': 0.00013005412154016416, 'epoch': 3.3181818181818183}\n{'loss': 0.1806, 'grad_norm': 0.8029311299324036, 'learning_rate': 0.00012961326011121446, 'epoch': 3.340909090909091}\n{'loss': 0.2012, 'grad_norm': 1.636906385421753, 'learning_rate': 0.00012917239868226476, 'epoch': 3.3636363636363638}\n{'loss': 0.2164, 'grad_norm': 2.280111312866211, 'learning_rate': 0.00012873153725331504, 'epoch': 3.3863636363636362}\n{'loss': 0.1711, 'grad_norm': 0.9009591341018677, 'learning_rate': 0.00012829067582436534, 'epoch': 3.409090909090909}\n{'loss': 0.1958, 'grad_norm': 0.9329899549484253, 'learning_rate': 0.00012784981439541564, 'epoch': 3.4318181818181817}\n{'loss': 0.2146, 'grad_norm': 1.1082266569137573, 'learning_rate': 0.0001274089529664659, 'epoch': 3.4545454545454546}\n{'loss': 0.2036, 'grad_norm': 1.0659146308898926, 'learning_rate': 0.0001269680915375162, 'epoch': 3.4772727272727275}\n{'loss': 0.1866, 'grad_norm': 2.657823324203491, 'learning_rate': 0.0001265272301085665, 'epoch': 3.5}\n{'loss': 0.2069, 'grad_norm': 2.5769670009613037, 'learning_rate': 0.00012608636867961678, 'epoch': 3.5227272727272725}\n{'loss': 0.234, 'grad_norm': 1.5489270687103271, 'learning_rate': 0.00012564550725066708, 'epoch': 3.5454545454545454}\n{'loss': 0.1844, 'grad_norm': 1.0447465181350708, 'learning_rate': 0.00012520464582171738, 'epoch': 3.5681818181818183}\n{'loss': 0.189, 'grad_norm': 1.7492512464523315, 'learning_rate': 0.00012476378439276766, 'epoch': 3.590909090909091}\n{'loss': 0.2524, 'grad_norm': 1.8719950914382935, 'learning_rate': 0.00012432292296381796, 'epoch': 3.6136363636363638}\n{'loss': 0.1724, 'grad_norm': 1.093786597251892, 'learning_rate': 0.00012388206153486826, 'epoch': 3.6363636363636362}\n{'loss': 0.2206, 'grad_norm': 1.6677100658416748, 'learning_rate': 0.00012344120010591853, 'epoch': 3.659090909090909}\n{'loss': 0.1751, 'grad_norm': 0.8891445994377136, 'learning_rate': 0.00012300033867696883, 'epoch': 3.6818181818181817}\n{'loss': 0.1815, 'grad_norm': 1.071810007095337, 'learning_rate': 0.00012255947724801913, 'epoch': 3.7045454545454546}\n{'loss': 0.1989, 'grad_norm': 0.9856374859809875, 'learning_rate': 0.0001221186158190694, 'epoch': 3.7272727272727275}\n{'loss': 0.178, 'grad_norm': 0.8393346071243286, 'learning_rate': 0.00012167775439011972, 'epoch': 3.75}\n{'loss': 0.1472, 'grad_norm': 1.5972291231155396, 'learning_rate': 0.00012123689296116999, 'epoch': 3.7727272727272725}\n{'loss': 0.1869, 'grad_norm': 1.3629568815231323, 'learning_rate': 0.00012079603153222029, 'epoch': 3.7954545454545454}\n{'loss': 0.2074, 'grad_norm': 0.9945088624954224, 'learning_rate': 0.00012035517010327059, 'epoch': 3.8181818181818183}\n{'loss': 0.2205, 'grad_norm': 2.0521159172058105, 'learning_rate': 0.00011991430867432086, 'epoch': 3.840909090909091}\n{'loss': 0.1921, 'grad_norm': 1.8931705951690674, 'learning_rate': 0.00011947344724537116, 'epoch': 3.8636363636363638}\n{'loss': 0.2203, 'grad_norm': 2.298336982727051, 'learning_rate': 0.00011903258581642144, 'epoch': 3.8863636363636362}\n{'loss': 0.1775, 'grad_norm': 1.1705737113952637, 'learning_rate': 0.00011859172438747174, 'epoch': 3.909090909090909}\n{'loss': 0.2153, 'grad_norm': 1.3795326948165894, 'learning_rate': 0.00011815086295852204, 'epoch': 3.9318181818181817}\n{'loss': 0.1986, 'grad_norm': 1.7208331823349, 'learning_rate': 0.00011771000152957231, 'epoch': 3.9545454545454546}\n{'loss': 0.1925, 'grad_norm': 1.1300019025802612, 'learning_rate': 0.00011726914010062261, 'epoch': 3.9772727272727275}\n{'loss': 0.182, 'grad_norm': 3.3300936222076416, 'learning_rate': 0.00011682827867167291, 'epoch': 4.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8556    0.8941    0.8744      3221\n     PERCENT     0.8602    0.9195    0.8889        87\n        TYPE     0.9485    0.9624    0.9554     11501\n      VOLUME     0.8667    0.8814    0.8739        59\n\n   micro avg     0.9271    0.9471    0.9370     14868\n   macro avg     0.8827    0.9144    0.8982     14868\nweighted avg     0.9275    0.9471    0.9372     14868\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.22710338234901428, 'eval_f1_macro': 0.8981766383162628, 'eval_precision': 0.9270524721838173, 'eval_recall': 0.9470675275760021, 'eval_f1': 0.9369531224007719, 'eval_accuracy': 0.9306022788931091, 'eval_runtime': 1.4883, 'eval_samples_per_second': 3702.286, 'eval_steps_per_second': 7.391, 'epoch': 4.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.1658, 'grad_norm': 2.058514356613159, 'learning_rate': 0.0001163874172427232, 'epoch': 4.0227272727272725}\n{'loss': 0.176, 'grad_norm': 1.4155645370483398, 'learning_rate': 0.00011594655581377348, 'epoch': 4.045454545454546}\n{'loss': 0.1388, 'grad_norm': 0.7889969944953918, 'learning_rate': 0.00011550569438482378, 'epoch': 4.068181818181818}\n{'loss': 0.1928, 'grad_norm': 1.0832430124282837, 'learning_rate': 0.00011506483295587407, 'epoch': 4.090909090909091}\n{'loss': 0.1722, 'grad_norm': 0.752537190914154, 'learning_rate': 0.00011462397152692436, 'epoch': 4.113636363636363}\n{'loss': 0.1808, 'grad_norm': 1.5978453159332275, 'learning_rate': 0.00011418311009797466, 'epoch': 4.136363636363637}\n{'loss': 0.138, 'grad_norm': 1.1484190225601196, 'learning_rate': 0.00011374224866902494, 'epoch': 4.159090909090909}\n{'loss': 0.1479, 'grad_norm': 1.0236562490463257, 'learning_rate': 0.00011330138724007523, 'epoch': 4.181818181818182}\n{'loss': 0.1725, 'grad_norm': 0.7333859205245972, 'learning_rate': 0.00011286052581112553, 'epoch': 4.204545454545454}\n{'loss': 0.1532, 'grad_norm': 1.1106083393096924, 'learning_rate': 0.00011241966438217582, 'epoch': 4.2272727272727275}\n{'loss': 0.1395, 'grad_norm': 1.0067877769470215, 'learning_rate': 0.0001119788029532261, 'epoch': 4.25}\n{'loss': 0.1543, 'grad_norm': 1.018361210823059, 'learning_rate': 0.00011153794152427639, 'epoch': 4.2727272727272725}\n{'loss': 0.1777, 'grad_norm': 0.9075608253479004, 'learning_rate': 0.00011109708009532669, 'epoch': 4.295454545454546}\n{'loss': 0.1355, 'grad_norm': 0.6340461373329163, 'learning_rate': 0.00011065621866637699, 'epoch': 4.318181818181818}\n{'loss': 0.1646, 'grad_norm': 1.19170343875885, 'learning_rate': 0.00011021535723742726, 'epoch': 4.340909090909091}\n{'loss': 0.155, 'grad_norm': 0.9791318774223328, 'learning_rate': 0.00010977449580847756, 'epoch': 4.363636363636363}\n{'loss': 0.1657, 'grad_norm': 1.3015141487121582, 'learning_rate': 0.00010933363437952786, 'epoch': 4.386363636363637}\n{'loss': 0.1565, 'grad_norm': 1.0120402574539185, 'learning_rate': 0.00010889277295057814, 'epoch': 4.409090909090909}\n{'loss': 0.1758, 'grad_norm': 1.1289026737213135, 'learning_rate': 0.00010845191152162844, 'epoch': 4.431818181818182}\n{'loss': 0.1743, 'grad_norm': 1.1736308336257935, 'learning_rate': 0.00010801105009267874, 'epoch': 4.454545454545454}\n{'loss': 0.1712, 'grad_norm': 1.079064965248108, 'learning_rate': 0.00010757018866372901, 'epoch': 4.4772727272727275}\n{'loss': 0.1364, 'grad_norm': 0.9453670978546143, 'learning_rate': 0.00010712932723477931, 'epoch': 4.5}\n{'loss': 0.1935, 'grad_norm': 1.3609455823898315, 'learning_rate': 0.00010668846580582961, 'epoch': 4.5227272727272725}\n{'loss': 0.1916, 'grad_norm': 1.349944829940796, 'learning_rate': 0.00010624760437687988, 'epoch': 4.545454545454545}\n{'loss': 0.1328, 'grad_norm': 1.2184947729110718, 'learning_rate': 0.00010580674294793018, 'epoch': 4.568181818181818}\n{'loss': 0.1749, 'grad_norm': 1.324698805809021, 'learning_rate': 0.00010536588151898046, 'epoch': 4.590909090909091}\n{'loss': 0.1497, 'grad_norm': 0.9070959687232971, 'learning_rate': 0.00010492502009003076, 'epoch': 4.613636363636363}\n{'loss': 0.1651, 'grad_norm': 1.6016170978546143, 'learning_rate': 0.00010448415866108106, 'epoch': 4.636363636363637}\n{'loss': 0.1435, 'grad_norm': 0.8906663060188293, 'learning_rate': 0.00010404329723213133, 'epoch': 4.659090909090909}\n{'loss': 0.1753, 'grad_norm': 2.469409227371216, 'learning_rate': 0.00010360243580318163, 'epoch': 4.681818181818182}\n{'loss': 0.1817, 'grad_norm': 1.2487449645996094, 'learning_rate': 0.00010316157437423193, 'epoch': 4.704545454545455}\n{'loss': 0.1326, 'grad_norm': 1.3827905654907227, 'learning_rate': 0.0001027207129452822, 'epoch': 4.7272727272727275}\n{'loss': 0.1397, 'grad_norm': 1.0974689722061157, 'learning_rate': 0.0001022798515163325, 'epoch': 4.75}\n{'loss': 0.147, 'grad_norm': 1.2735981941223145, 'learning_rate': 0.0001018389900873828, 'epoch': 4.7727272727272725}\n{'loss': 0.1658, 'grad_norm': 0.8294415473937988, 'learning_rate': 0.00010139812865843308, 'epoch': 4.795454545454545}\n{'loss': 0.1484, 'grad_norm': 1.1502549648284912, 'learning_rate': 0.00010095726722948338, 'epoch': 4.818181818181818}\n{'loss': 0.1454, 'grad_norm': 1.7292156219482422, 'learning_rate': 0.00010051640580053368, 'epoch': 4.840909090909091}\n{'loss': 0.1264, 'grad_norm': 1.3753169775009155, 'learning_rate': 0.00010007554437158395, 'epoch': 4.863636363636363}\n{'loss': 0.1517, 'grad_norm': 0.8994988203048706, 'learning_rate': 9.963468294263425e-05, 'epoch': 4.886363636363637}\n{'loss': 0.1339, 'grad_norm': 0.7272812128067017, 'learning_rate': 9.919382151368455e-05, 'epoch': 4.909090909090909}\n{'loss': 0.1809, 'grad_norm': 2.008939027786255, 'learning_rate': 9.875296008473482e-05, 'epoch': 4.931818181818182}\n{'loss': 0.1419, 'grad_norm': 0.992429256439209, 'learning_rate': 9.831209865578512e-05, 'epoch': 4.954545454545455}\n{'loss': 0.1987, 'grad_norm': 1.8762538433074951, 'learning_rate': 9.787123722683541e-05, 'epoch': 4.9772727272727275}\n{'loss': 0.1037, 'grad_norm': 2.1364965438842773, 'learning_rate': 9.74303757978857e-05, 'epoch': 5.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8916    0.8857    0.8886      3221\n     PERCENT     0.8511    0.9195    0.8840        87\n        TYPE     0.9483    0.9693    0.9587     11501\n      VOLUME     0.9153    0.9153    0.9153        59\n\n   micro avg     0.9355    0.9507    0.9431     14868\n   macro avg     0.9015    0.9225    0.9116     14868\nweighted avg     0.9353    0.9507    0.9429     14868\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.22242218255996704, 'eval_f1_macro': 0.9116394683642994, 'eval_precision': 0.9355351115229333, 'eval_recall': 0.950699488835082, 'eval_f1': 0.9430563431964507, 'eval_accuracy': 0.9365708084644602, 'eval_runtime': 1.4655, 'eval_samples_per_second': 3759.774, 'eval_steps_per_second': 7.506, 'epoch': 5.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.1025, 'grad_norm': 0.6659309267997742, 'learning_rate': 9.6989514368936e-05, 'epoch': 5.0227272727272725}\n{'loss': 0.1223, 'grad_norm': 1.046621322631836, 'learning_rate': 9.654865293998628e-05, 'epoch': 5.045454545454546}\n{'loss': 0.1315, 'grad_norm': 1.2382094860076904, 'learning_rate': 9.610779151103658e-05, 'epoch': 5.068181818181818}\n{'loss': 0.1578, 'grad_norm': 1.4553406238555908, 'learning_rate': 9.566693008208687e-05, 'epoch': 5.090909090909091}\n{'loss': 0.137, 'grad_norm': 1.3377630710601807, 'learning_rate': 9.522606865313716e-05, 'epoch': 5.113636363636363}\n{'loss': 0.1067, 'grad_norm': 1.1123110055923462, 'learning_rate': 9.478520722418746e-05, 'epoch': 5.136363636363637}\n{'loss': 0.0967, 'grad_norm': 0.8630935549736023, 'learning_rate': 9.434434579523774e-05, 'epoch': 5.159090909090909}\n{'loss': 0.0971, 'grad_norm': 1.338700294494629, 'learning_rate': 9.390348436628803e-05, 'epoch': 5.181818181818182}\n{'loss': 0.1286, 'grad_norm': 1.2774921655654907, 'learning_rate': 9.346262293733833e-05, 'epoch': 5.204545454545454}\n{'loss': 0.1405, 'grad_norm': 1.6522903442382812, 'learning_rate': 9.302176150838862e-05, 'epoch': 5.2272727272727275}\n{'loss': 0.1324, 'grad_norm': 0.7993050813674927, 'learning_rate': 9.25809000794389e-05, 'epoch': 5.25}\n{'loss': 0.1157, 'grad_norm': 0.8781543374061584, 'learning_rate': 9.21400386504892e-05, 'epoch': 5.2727272727272725}\n{'loss': 0.113, 'grad_norm': 1.2213424444198608, 'learning_rate': 9.169917722153949e-05, 'epoch': 5.295454545454546}\n{'loss': 0.1033, 'grad_norm': 1.8079549074172974, 'learning_rate': 9.125831579258978e-05, 'epoch': 5.318181818181818}\n{'loss': 0.1047, 'grad_norm': 0.7558326125144958, 'learning_rate': 9.081745436364008e-05, 'epoch': 5.340909090909091}\n{'loss': 0.1284, 'grad_norm': 1.4323079586029053, 'learning_rate': 9.037659293469035e-05, 'epoch': 5.363636363636363}\n{'loss': 0.1574, 'grad_norm': 1.4877464771270752, 'learning_rate': 8.993573150574065e-05, 'epoch': 5.386363636363637}\n{'loss': 0.1022, 'grad_norm': 0.7535440325737, 'learning_rate': 8.949487007679095e-05, 'epoch': 5.409090909090909}\n{'loss': 0.1475, 'grad_norm': 0.9468399286270142, 'learning_rate': 8.905400864784122e-05, 'epoch': 5.431818181818182}\n{'loss': 0.12, 'grad_norm': 0.8829329013824463, 'learning_rate': 8.861314721889152e-05, 'epoch': 5.454545454545454}\n{'loss': 0.1507, 'grad_norm': 1.142021894454956, 'learning_rate': 8.817228578994182e-05, 'epoch': 5.4772727272727275}\n{'loss': 0.1577, 'grad_norm': 1.074960470199585, 'learning_rate': 8.77314243609921e-05, 'epoch': 5.5}\n{'loss': 0.1241, 'grad_norm': 1.4011099338531494, 'learning_rate': 8.72905629320424e-05, 'epoch': 5.5227272727272725}\n{'loss': 0.0956, 'grad_norm': 1.092245101928711, 'learning_rate': 8.684970150309268e-05, 'epoch': 5.545454545454545}\n{'loss': 0.1546, 'grad_norm': 1.4024502038955688, 'learning_rate': 8.640884007414298e-05, 'epoch': 5.568181818181818}\n{'loss': 0.127, 'grad_norm': 1.0168768167495728, 'learning_rate': 8.596797864519327e-05, 'epoch': 5.590909090909091}\n{'loss': 0.1273, 'grad_norm': 0.8630535006523132, 'learning_rate': 8.552711721624356e-05, 'epoch': 5.613636363636363}\n{'loss': 0.1181, 'grad_norm': 1.1955372095108032, 'learning_rate': 8.508625578729384e-05, 'epoch': 5.636363636363637}\n{'loss': 0.1188, 'grad_norm': 1.681461215019226, 'learning_rate': 8.464539435834414e-05, 'epoch': 5.659090909090909}\n{'loss': 0.1362, 'grad_norm': 2.021127223968506, 'learning_rate': 8.420453292939443e-05, 'epoch': 5.681818181818182}\n{'loss': 0.1325, 'grad_norm': 0.9123927354812622, 'learning_rate': 8.376367150044472e-05, 'epoch': 5.704545454545455}\n{'loss': 0.1587, 'grad_norm': 1.7877675294876099, 'learning_rate': 8.332281007149502e-05, 'epoch': 5.7272727272727275}\n{'loss': 0.1346, 'grad_norm': 0.79017573595047, 'learning_rate': 8.28819486425453e-05, 'epoch': 5.75}\n{'loss': 0.1112, 'grad_norm': 1.1105817556381226, 'learning_rate': 8.244108721359559e-05, 'epoch': 5.7727272727272725}\n{'loss': 0.1381, 'grad_norm': 1.7173556089401245, 'learning_rate': 8.200022578464589e-05, 'epoch': 5.795454545454545}\n{'loss': 0.1123, 'grad_norm': 1.9540116786956787, 'learning_rate': 8.155936435569618e-05, 'epoch': 5.818181818181818}\n{'loss': 0.1313, 'grad_norm': 1.1226245164871216, 'learning_rate': 8.111850292674646e-05, 'epoch': 5.840909090909091}\n{'loss': 0.1376, 'grad_norm': 0.9215132594108582, 'learning_rate': 8.067764149779675e-05, 'epoch': 5.863636363636363}\n{'loss': 0.1703, 'grad_norm': 0.9020353555679321, 'learning_rate': 8.023678006884705e-05, 'epoch': 5.886363636363637}\n{'loss': 0.1217, 'grad_norm': 0.8523699641227722, 'learning_rate': 7.979591863989734e-05, 'epoch': 5.909090909090909}\n{'loss': 0.1185, 'grad_norm': 0.8123972415924072, 'learning_rate': 7.935505721094764e-05, 'epoch': 5.931818181818182}\n{'loss': 0.1225, 'grad_norm': 1.8924133777618408, 'learning_rate': 7.891419578199792e-05, 'epoch': 5.954545454545455}\n{'loss': 0.1552, 'grad_norm': 1.9019358158111572, 'learning_rate': 7.847333435304821e-05, 'epoch': 5.9772727272727275}\n{'loss': 0.1288, 'grad_norm': 3.3976359367370605, 'learning_rate': 7.803247292409851e-05, 'epoch': 6.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.9031    0.8764    0.8896      3221\n     PERCENT     0.8421    0.9195    0.8791        87\n        TYPE     0.9480    0.9698    0.9588     11501\n      VOLUME     0.8361    0.8644    0.8500        59\n\n   micro avg     0.9375    0.9489    0.9432     14868\n   macro avg     0.8823    0.9076    0.8944     14868\nweighted avg     0.9372    0.9489    0.9429     14868\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.23213845491409302, 'eval_f1_macro': 0.894364456159638, 'eval_precision': 0.9375332270069112, 'eval_recall': 0.9488835082055421, 'eval_f1': 0.9431742211525606, 'eval_accuracy': 0.9372761801410744, 'eval_runtime': 1.4598, 'eval_samples_per_second': 3774.413, 'eval_steps_per_second': 7.535, 'epoch': 6.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.1149, 'grad_norm': 1.6387473344802856, 'learning_rate': 7.75916114951488e-05, 'epoch': 6.0227272727272725}\n{'loss': 0.1217, 'grad_norm': 1.2977229356765747, 'learning_rate': 7.71507500661991e-05, 'epoch': 6.045454545454546}\n{'loss': 0.1072, 'grad_norm': 0.9748448133468628, 'learning_rate': 7.670988863724938e-05, 'epoch': 6.068181818181818}\n{'loss': 0.0927, 'grad_norm': 0.7377738356590271, 'learning_rate': 7.626902720829967e-05, 'epoch': 6.090909090909091}\n{'loss': 0.1233, 'grad_norm': 1.0424264669418335, 'learning_rate': 7.582816577934997e-05, 'epoch': 6.113636363636363}\n{'loss': 0.097, 'grad_norm': 0.9699218273162842, 'learning_rate': 7.538730435040026e-05, 'epoch': 6.136363636363637}\n{'loss': 0.1123, 'grad_norm': 0.8874388337135315, 'learning_rate': 7.494644292145054e-05, 'epoch': 6.159090909090909}\n{'loss': 0.101, 'grad_norm': 0.7972594499588013, 'learning_rate': 7.450558149250083e-05, 'epoch': 6.181818181818182}\n{'loss': 0.0973, 'grad_norm': 1.0896798372268677, 'learning_rate': 7.406472006355113e-05, 'epoch': 6.204545454545454}\n{'loss': 0.1044, 'grad_norm': 0.85880446434021, 'learning_rate': 7.362385863460142e-05, 'epoch': 6.2272727272727275}\n{'loss': 0.1264, 'grad_norm': 1.0708256959915161, 'learning_rate': 7.31829972056517e-05, 'epoch': 6.25}\n{'loss': 0.0982, 'grad_norm': 1.1043429374694824, 'learning_rate': 7.2742135776702e-05, 'epoch': 6.2727272727272725}\n{'loss': 0.0987, 'grad_norm': 1.2266429662704468, 'learning_rate': 7.230127434775229e-05, 'epoch': 6.295454545454546}\n{'loss': 0.085, 'grad_norm': 0.7451788783073425, 'learning_rate': 7.186041291880258e-05, 'epoch': 6.318181818181818}\n{'loss': 0.1155, 'grad_norm': 1.4895368814468384, 'learning_rate': 7.141955148985288e-05, 'epoch': 6.340909090909091}\n{'loss': 0.1114, 'grad_norm': 1.1572474241256714, 'learning_rate': 7.097869006090316e-05, 'epoch': 6.363636363636363}\n{'loss': 0.1196, 'grad_norm': 1.0619752407073975, 'learning_rate': 7.053782863195345e-05, 'epoch': 6.386363636363637}\n{'loss': 0.1102, 'grad_norm': 2.1966896057128906, 'learning_rate': 7.009696720300374e-05, 'epoch': 6.409090909090909}\n{'loss': 0.1101, 'grad_norm': 1.3382929563522339, 'learning_rate': 6.965610577405404e-05, 'epoch': 6.431818181818182}\n{'loss': 0.1084, 'grad_norm': 1.23128342628479, 'learning_rate': 6.921524434510432e-05, 'epoch': 6.454545454545454}\n{'loss': 0.0797, 'grad_norm': 1.0891931056976318, 'learning_rate': 6.877438291615461e-05, 'epoch': 6.4772727272727275}\n{'loss': 0.118, 'grad_norm': 1.0655763149261475, 'learning_rate': 6.833352148720491e-05, 'epoch': 6.5}\n{'loss': 0.1102, 'grad_norm': 0.813908576965332, 'learning_rate': 6.78926600582552e-05, 'epoch': 6.5227272727272725}\n{'loss': 0.115, 'grad_norm': 0.8647147417068481, 'learning_rate': 6.745179862930548e-05, 'epoch': 6.545454545454545}\n{'loss': 0.1044, 'grad_norm': 1.1615523099899292, 'learning_rate': 6.701093720035577e-05, 'epoch': 6.568181818181818}\n{'loss': 0.1041, 'grad_norm': 1.3028839826583862, 'learning_rate': 6.657007577140607e-05, 'epoch': 6.590909090909091}\n{'loss': 0.1612, 'grad_norm': 1.0223060846328735, 'learning_rate': 6.612921434245636e-05, 'epoch': 6.613636363636363}\n{'loss': 0.095, 'grad_norm': 1.241584062576294, 'learning_rate': 6.568835291350664e-05, 'epoch': 6.636363636363637}\n{'loss': 0.0908, 'grad_norm': 1.417114019393921, 'learning_rate': 6.524749148455694e-05, 'epoch': 6.659090909090909}\n{'loss': 0.0871, 'grad_norm': 0.8785459399223328, 'learning_rate': 6.480663005560723e-05, 'epoch': 6.681818181818182}\n{'loss': 0.0906, 'grad_norm': 0.8032309412956238, 'learning_rate': 6.436576862665752e-05, 'epoch': 6.704545454545455}\n{'loss': 0.0956, 'grad_norm': 1.9138057231903076, 'learning_rate': 6.392490719770782e-05, 'epoch': 6.7272727272727275}\n{'loss': 0.1174, 'grad_norm': 1.4987126588821411, 'learning_rate': 6.34840457687581e-05, 'epoch': 6.75}\n{'loss': 0.113, 'grad_norm': 1.0225378274917603, 'learning_rate': 6.304318433980839e-05, 'epoch': 6.7727272727272725}\n{'loss': 0.0981, 'grad_norm': 0.8428091406822205, 'learning_rate': 6.260232291085869e-05, 'epoch': 6.795454545454545}\n{'loss': 0.1332, 'grad_norm': 1.082832932472229, 'learning_rate': 6.216146148190898e-05, 'epoch': 6.818181818181818}\n{'loss': 0.0871, 'grad_norm': 1.1032166481018066, 'learning_rate': 6.172060005295926e-05, 'epoch': 6.840909090909091}\n{'loss': 0.1077, 'grad_norm': 1.1213438510894775, 'learning_rate': 6.127973862400956e-05, 'epoch': 6.863636363636363}\n{'loss': 0.1406, 'grad_norm': 1.0494987964630127, 'learning_rate': 6.083887719505986e-05, 'epoch': 6.886363636363637}\n{'loss': 0.1074, 'grad_norm': 0.8197689056396484, 'learning_rate': 6.0398015766110145e-05, 'epoch': 6.909090909090909}\n{'loss': 0.1413, 'grad_norm': 1.574916124343872, 'learning_rate': 5.995715433716043e-05, 'epoch': 6.931818181818182}\n{'loss': 0.0889, 'grad_norm': 0.7229204773902893, 'learning_rate': 5.951629290821072e-05, 'epoch': 6.954545454545455}\n{'loss': 0.1276, 'grad_norm': 0.9592322111129761, 'learning_rate': 5.907543147926102e-05, 'epoch': 6.9772727272727275}\n{'loss': 0.0429, 'grad_norm': 3.5267648696899414, 'learning_rate': 5.8634570050311305e-05, 'epoch': 7.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8937    0.8975    0.8956      3221\n     PERCENT     0.8696    0.9195    0.8939        87\n        TYPE     0.9532    0.9677    0.9604     11501\n      VOLUME     0.8594    0.9322    0.8943        59\n\n   micro avg     0.9395    0.9521    0.9458     14868\n   macro avg     0.8939    0.9293    0.9110     14868\nweighted avg     0.9394    0.9521    0.9457     14868\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.22862954437732697, 'eval_f1_macro': 0.9110395398978233, 'eval_precision': 0.9394743827979825, 'eval_recall': 0.9521119182136132, 'eval_f1': 0.9457509353287012, 'eval_accuracy': 0.9391752577319588, 'eval_runtime': 1.5113, 'eval_samples_per_second': 3645.97, 'eval_steps_per_second': 7.279, 'epoch': 7.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.1003, 'grad_norm': 1.2739453315734863, 'learning_rate': 5.81937086213616e-05, 'epoch': 7.0227272727272725}\n{'loss': 0.1076, 'grad_norm': 1.123036503791809, 'learning_rate': 5.775284719241189e-05, 'epoch': 7.045454545454546}\n{'loss': 0.092, 'grad_norm': 1.1572351455688477, 'learning_rate': 5.731198576346218e-05, 'epoch': 7.068181818181818}\n{'loss': 0.0693, 'grad_norm': 0.6569070219993591, 'learning_rate': 5.687112433451247e-05, 'epoch': 7.090909090909091}\n{'loss': 0.111, 'grad_norm': 1.2494670152664185, 'learning_rate': 5.6430262905562765e-05, 'epoch': 7.113636363636363}\n{'loss': 0.1397, 'grad_norm': 1.1298527717590332, 'learning_rate': 5.598940147661305e-05, 'epoch': 7.136363636363637}\n{'loss': 0.0977, 'grad_norm': 1.4505770206451416, 'learning_rate': 5.5548540047663345e-05, 'epoch': 7.159090909090909}\n{'loss': 0.1043, 'grad_norm': 1.0576353073120117, 'learning_rate': 5.510767861871363e-05, 'epoch': 7.181818181818182}\n{'loss': 0.0909, 'grad_norm': 0.9281038045883179, 'learning_rate': 5.466681718976393e-05, 'epoch': 7.204545454545454}\n{'loss': 0.0995, 'grad_norm': 0.8475931882858276, 'learning_rate': 5.422595576081422e-05, 'epoch': 7.2272727272727275}\n{'loss': 0.1192, 'grad_norm': 0.82334965467453, 'learning_rate': 5.3785094331864505e-05, 'epoch': 7.25}\n{'loss': 0.0952, 'grad_norm': 1.9533065557479858, 'learning_rate': 5.3344232902914805e-05, 'epoch': 7.2727272727272725}\n{'loss': 0.0964, 'grad_norm': 2.324111223220825, 'learning_rate': 5.290337147396509e-05, 'epoch': 7.295454545454546}\n{'loss': 0.0907, 'grad_norm': 1.210298776626587, 'learning_rate': 5.246251004501538e-05, 'epoch': 7.318181818181818}\n{'loss': 0.1061, 'grad_norm': 1.024936556816101, 'learning_rate': 5.2021648616065665e-05, 'epoch': 7.340909090909091}\n{'loss': 0.0925, 'grad_norm': 0.8192159533500671, 'learning_rate': 5.1580787187115965e-05, 'epoch': 7.363636363636363}\n{'loss': 0.0831, 'grad_norm': 0.8951959013938904, 'learning_rate': 5.113992575816625e-05, 'epoch': 7.386363636363637}\n{'loss': 0.0911, 'grad_norm': 1.0222697257995605, 'learning_rate': 5.069906432921654e-05, 'epoch': 7.409090909090909}\n{'loss': 0.1071, 'grad_norm': 0.8580721020698547, 'learning_rate': 5.025820290026684e-05, 'epoch': 7.431818181818182}\n{'loss': 0.0962, 'grad_norm': 0.9202813506126404, 'learning_rate': 4.9817341471317125e-05, 'epoch': 7.454545454545454}\n{'loss': 0.1269, 'grad_norm': 1.3932149410247803, 'learning_rate': 4.937648004236741e-05, 'epoch': 7.4772727272727275}\n{'loss': 0.1436, 'grad_norm': 1.4428541660308838, 'learning_rate': 4.8935618613417705e-05, 'epoch': 7.5}\n{'loss': 0.0821, 'grad_norm': 1.436240315437317, 'learning_rate': 4.8494757184468e-05, 'epoch': 7.5227272727272725}\n{'loss': 0.08, 'grad_norm': 1.4419983625411987, 'learning_rate': 4.805389575551829e-05, 'epoch': 7.545454545454545}\n{'loss': 0.0923, 'grad_norm': 0.7951973676681519, 'learning_rate': 4.761303432656858e-05, 'epoch': 7.568181818181818}\n{'loss': 0.0751, 'grad_norm': 0.6555296182632446, 'learning_rate': 4.717217289761887e-05, 'epoch': 7.590909090909091}\n{'loss': 0.0724, 'grad_norm': 0.7471768260002136, 'learning_rate': 4.6731311468669165e-05, 'epoch': 7.613636363636363}\n{'loss': 0.1255, 'grad_norm': 1.1412757635116577, 'learning_rate': 4.629045003971945e-05, 'epoch': 7.636363636363637}\n{'loss': 0.1004, 'grad_norm': 0.719117283821106, 'learning_rate': 4.5849588610769745e-05, 'epoch': 7.659090909090909}\n{'loss': 0.0605, 'grad_norm': 0.8715918064117432, 'learning_rate': 4.540872718182004e-05, 'epoch': 7.681818181818182}\n{'loss': 0.0844, 'grad_norm': 0.7485896348953247, 'learning_rate': 4.4967865752870325e-05, 'epoch': 7.704545454545455}\n{'loss': 0.07, 'grad_norm': 0.8583906888961792, 'learning_rate': 4.452700432392061e-05, 'epoch': 7.7272727272727275}\n{'loss': 0.0867, 'grad_norm': 0.8753799796104431, 'learning_rate': 4.408614289497091e-05, 'epoch': 7.75}\n{'loss': 0.0998, 'grad_norm': 1.365704894065857, 'learning_rate': 4.36452814660212e-05, 'epoch': 7.7727272727272725}\n{'loss': 0.0978, 'grad_norm': 1.089388132095337, 'learning_rate': 4.320442003707149e-05, 'epoch': 7.795454545454545}\n{'loss': 0.0764, 'grad_norm': 0.6561136841773987, 'learning_rate': 4.276355860812178e-05, 'epoch': 7.818181818181818}\n{'loss': 0.0651, 'grad_norm': 0.9245851039886475, 'learning_rate': 4.232269717917207e-05, 'epoch': 7.840909090909091}\n{'loss': 0.0837, 'grad_norm': 1.048816204071045, 'learning_rate': 4.188183575022236e-05, 'epoch': 7.863636363636363}\n{'loss': 0.0973, 'grad_norm': 1.526364803314209, 'learning_rate': 4.144097432127265e-05, 'epoch': 7.886363636363637}\n{'loss': 0.1046, 'grad_norm': 0.8682225942611694, 'learning_rate': 4.1000112892322945e-05, 'epoch': 7.909090909090909}\n{'loss': 0.0791, 'grad_norm': 1.0257689952850342, 'learning_rate': 4.055925146337323e-05, 'epoch': 7.931818181818182}\n{'loss': 0.081, 'grad_norm': 1.2608318328857422, 'learning_rate': 4.0118390034423525e-05, 'epoch': 7.954545454545455}\n{'loss': 0.0862, 'grad_norm': 0.8339447975158691, 'learning_rate': 3.967752860547382e-05, 'epoch': 7.9772727272727275}\n{'loss': 0.0739, 'grad_norm': 3.257385015487671, 'learning_rate': 3.9236667176524105e-05, 'epoch': 8.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8983    0.8913    0.8948      3221\n     PERCENT     0.8602    0.9195    0.8889        87\n        TYPE     0.9543    0.9673    0.9607     11501\n      VOLUME     0.9167    0.9322    0.9244        59\n\n   micro avg     0.9416    0.9504    0.9460     14868\n   macro avg     0.9074    0.9276    0.9172     14868\nweighted avg     0.9415    0.9504    0.9459     14868\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.22992977499961853, 'eval_f1_macro': 0.9172047241404572, 'eval_precision': 0.9416272406210435, 'eval_recall': 0.9504304546677428, 'eval_f1': 0.9460083682008368, 'eval_accuracy': 0.9383613673358654, 'eval_runtime': 1.8595, 'eval_samples_per_second': 2963.184, 'eval_steps_per_second': 5.916, 'epoch': 8.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.0818, 'grad_norm': 0.8393499851226807, 'learning_rate': 3.87958057475744e-05, 'epoch': 8.022727272727273}\n{'loss': 0.0994, 'grad_norm': 0.9867643117904663, 'learning_rate': 3.835494431862469e-05, 'epoch': 8.045454545454545}\n{'loss': 0.0734, 'grad_norm': 0.9226149320602417, 'learning_rate': 3.7914082889674985e-05, 'epoch': 8.068181818181818}\n{'loss': 0.0726, 'grad_norm': 0.890203595161438, 'learning_rate': 3.747322146072527e-05, 'epoch': 8.090909090909092}\n{'loss': 0.0803, 'grad_norm': 1.4147589206695557, 'learning_rate': 3.7032360031775565e-05, 'epoch': 8.113636363636363}\n{'loss': 0.0782, 'grad_norm': 1.0225651264190674, 'learning_rate': 3.659149860282585e-05, 'epoch': 8.136363636363637}\n{'loss': 0.0719, 'grad_norm': 0.6373351216316223, 'learning_rate': 3.6150637173876145e-05, 'epoch': 8.159090909090908}\n{'loss': 0.0902, 'grad_norm': 1.5866988897323608, 'learning_rate': 3.570977574492644e-05, 'epoch': 8.181818181818182}\n{'loss': 0.0949, 'grad_norm': 1.1825590133666992, 'learning_rate': 3.5268914315976725e-05, 'epoch': 8.204545454545455}\n{'loss': 0.0658, 'grad_norm': 0.9111587405204773, 'learning_rate': 3.482805288702702e-05, 'epoch': 8.227272727272727}\n{'loss': 0.0775, 'grad_norm': 0.9921635985374451, 'learning_rate': 3.4387191458077305e-05, 'epoch': 8.25}\n{'loss': 0.1259, 'grad_norm': 1.4067983627319336, 'learning_rate': 3.39463300291276e-05, 'epoch': 8.272727272727273}\n{'loss': 0.0967, 'grad_norm': 0.8781523704528809, 'learning_rate': 3.3505468600177885e-05, 'epoch': 8.295454545454545}\n{'loss': 0.0843, 'grad_norm': 1.3587443828582764, 'learning_rate': 3.306460717122818e-05, 'epoch': 8.318181818181818}\n{'loss': 0.0814, 'grad_norm': 1.4717440605163574, 'learning_rate': 3.262374574227847e-05, 'epoch': 8.340909090909092}\n{'loss': 0.0701, 'grad_norm': 1.2278343439102173, 'learning_rate': 3.218288431332876e-05, 'epoch': 8.363636363636363}\n{'loss': 0.0702, 'grad_norm': 1.4096800088882446, 'learning_rate': 3.174202288437905e-05, 'epoch': 8.386363636363637}\n{'loss': 0.0968, 'grad_norm': 0.7756890058517456, 'learning_rate': 3.1301161455429346e-05, 'epoch': 8.409090909090908}\n{'loss': 0.0983, 'grad_norm': 0.9105841517448425, 'learning_rate': 3.086030002647963e-05, 'epoch': 8.431818181818182}\n{'loss': 0.0726, 'grad_norm': 1.5170433521270752, 'learning_rate': 3.041943859752993e-05, 'epoch': 8.454545454545455}\n{'loss': 0.091, 'grad_norm': 1.0616137981414795, 'learning_rate': 2.9978577168580216e-05, 'epoch': 8.477272727272727}\n{'loss': 0.0734, 'grad_norm': 0.9789374470710754, 'learning_rate': 2.953771573963051e-05, 'epoch': 8.5}\n{'loss': 0.0763, 'grad_norm': 1.0072485208511353, 'learning_rate': 2.90968543106808e-05, 'epoch': 8.522727272727273}\n{'loss': 0.0988, 'grad_norm': 0.8581865429878235, 'learning_rate': 2.865599288173109e-05, 'epoch': 8.545454545454545}\n{'loss': 0.1011, 'grad_norm': 1.0329747200012207, 'learning_rate': 2.8215131452781382e-05, 'epoch': 8.568181818181818}\n{'loss': 0.0854, 'grad_norm': 0.957147479057312, 'learning_rate': 2.7774270023831672e-05, 'epoch': 8.590909090909092}\n{'loss': 0.1034, 'grad_norm': 0.8718344569206238, 'learning_rate': 2.7333408594881966e-05, 'epoch': 8.613636363636363}\n{'loss': 0.0633, 'grad_norm': 0.5545631647109985, 'learning_rate': 2.6892547165932252e-05, 'epoch': 8.636363636363637}\n{'loss': 0.0842, 'grad_norm': 1.262648344039917, 'learning_rate': 2.6451685736982546e-05, 'epoch': 8.659090909090908}\n{'loss': 0.0996, 'grad_norm': 1.2283275127410889, 'learning_rate': 2.6010824308032832e-05, 'epoch': 8.681818181818182}\n{'loss': 0.08, 'grad_norm': 0.8795614838600159, 'learning_rate': 2.5569962879083126e-05, 'epoch': 8.704545454545455}\n{'loss': 0.0701, 'grad_norm': 0.9142699241638184, 'learning_rate': 2.512910145013342e-05, 'epoch': 8.727272727272727}\n{'loss': 0.0822, 'grad_norm': 1.8206675052642822, 'learning_rate': 2.4688240021183706e-05, 'epoch': 8.75}\n{'loss': 0.0803, 'grad_norm': 0.8363987803459167, 'learning_rate': 2.4247378592234e-05, 'epoch': 8.772727272727273}\n{'loss': 0.0823, 'grad_norm': 0.9086503386497498, 'learning_rate': 2.380651716328429e-05, 'epoch': 8.795454545454545}\n{'loss': 0.0903, 'grad_norm': 1.098980188369751, 'learning_rate': 2.3365655734334583e-05, 'epoch': 8.818181818181818}\n{'loss': 0.1033, 'grad_norm': 0.8567977547645569, 'learning_rate': 2.2924794305384873e-05, 'epoch': 8.840909090909092}\n{'loss': 0.0981, 'grad_norm': 1.0698487758636475, 'learning_rate': 2.2483932876435163e-05, 'epoch': 8.863636363636363}\n{'loss': 0.0762, 'grad_norm': 1.3588252067565918, 'learning_rate': 2.2043071447485456e-05, 'epoch': 8.886363636363637}\n{'loss': 0.0917, 'grad_norm': 1.0635548830032349, 'learning_rate': 2.1602210018535746e-05, 'epoch': 8.909090909090908}\n{'loss': 0.1169, 'grad_norm': 1.127467155456543, 'learning_rate': 2.1161348589586036e-05, 'epoch': 8.931818181818182}\n{'loss': 0.0833, 'grad_norm': 1.2095304727554321, 'learning_rate': 2.0720487160636326e-05, 'epoch': 8.954545454545455}\n{'loss': 0.0798, 'grad_norm': 1.108533501625061, 'learning_rate': 2.0279625731686616e-05, 'epoch': 8.977272727272727}\n{'loss': 0.2116, 'grad_norm': 4.422379016876221, 'learning_rate': 1.983876430273691e-05, 'epoch': 9.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8958    0.8916    0.8937      3221\n     PERCENT     0.8696    0.9195    0.8939        87\n        TYPE     0.9546    0.9670    0.9607     11501\n      VOLUME     0.9016    0.9322    0.9167        59\n\n   micro avg     0.9413    0.9502    0.9457     14868\n   macro avg     0.9054    0.9276    0.9162     14868\nweighted avg     0.9412    0.9502    0.9457     14868\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.2346370816230774, 'eval_f1_macro': 0.9162467577328982, 'eval_precision': 0.9413018855353454, 'eval_recall': 0.9502286790422384, 'eval_f1': 0.945744217960304, 'eval_accuracy': 0.937927292457949, 'eval_runtime': 1.5263, 'eval_samples_per_second': 3610.137, 'eval_steps_per_second': 7.207, 'epoch': 9.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.0988, 'grad_norm': 1.032419204711914, 'learning_rate': 1.93979028737872e-05, 'epoch': 9.022727272727273}\n{'loss': 0.0725, 'grad_norm': 1.0757757425308228, 'learning_rate': 1.8957041444837493e-05, 'epoch': 9.045454545454545}\n{'loss': 0.0637, 'grad_norm': 0.8216179013252258, 'learning_rate': 1.8516180015887783e-05, 'epoch': 9.068181818181818}\n{'loss': 0.0812, 'grad_norm': 0.7829920053482056, 'learning_rate': 1.8075318586938073e-05, 'epoch': 9.090909090909092}\n{'loss': 0.0943, 'grad_norm': 1.2256789207458496, 'learning_rate': 1.7634457157988363e-05, 'epoch': 9.113636363636363}\n{'loss': 0.0731, 'grad_norm': 0.6346463561058044, 'learning_rate': 1.7193595729038653e-05, 'epoch': 9.136363636363637}\n{'loss': 0.0578, 'grad_norm': 0.7666712999343872, 'learning_rate': 1.6752734300088943e-05, 'epoch': 9.159090909090908}\n{'loss': 0.071, 'grad_norm': 0.7931110262870789, 'learning_rate': 1.6311872871139236e-05, 'epoch': 9.181818181818182}\n{'loss': 0.0836, 'grad_norm': 0.8806556463241577, 'learning_rate': 1.5871011442189526e-05, 'epoch': 9.204545454545455}\n{'loss': 0.0585, 'grad_norm': 0.6804632544517517, 'learning_rate': 1.5430150013239816e-05, 'epoch': 9.227272727272727}\n{'loss': 0.0884, 'grad_norm': 0.9759384989738464, 'learning_rate': 1.4989288584290108e-05, 'epoch': 9.25}\n{'loss': 0.0936, 'grad_norm': 1.3467003107070923, 'learning_rate': 1.45484271553404e-05, 'epoch': 9.272727272727273}\n{'loss': 0.0745, 'grad_norm': 0.965067982673645, 'learning_rate': 1.4107565726390691e-05, 'epoch': 9.295454545454545}\n{'loss': 0.066, 'grad_norm': 0.662486732006073, 'learning_rate': 1.3666704297440983e-05, 'epoch': 9.318181818181818}\n{'loss': 0.0588, 'grad_norm': 0.6190209984779358, 'learning_rate': 1.3225842868491273e-05, 'epoch': 9.340909090909092}\n{'loss': 0.072, 'grad_norm': 0.8580191731452942, 'learning_rate': 1.2784981439541563e-05, 'epoch': 9.363636363636363}\n{'loss': 0.0841, 'grad_norm': 1.1267451047897339, 'learning_rate': 1.2344120010591853e-05, 'epoch': 9.386363636363637}\n{'loss': 0.0622, 'grad_norm': 0.8883101344108582, 'learning_rate': 1.1903258581642145e-05, 'epoch': 9.409090909090908}\n{'loss': 0.0775, 'grad_norm': 1.1001726388931274, 'learning_rate': 1.1462397152692436e-05, 'epoch': 9.431818181818182}\n{'loss': 0.0733, 'grad_norm': 0.870248556137085, 'learning_rate': 1.1021535723742728e-05, 'epoch': 9.454545454545455}\n{'loss': 0.0571, 'grad_norm': 1.079230546951294, 'learning_rate': 1.0580674294793018e-05, 'epoch': 9.477272727272727}\n{'loss': 0.0775, 'grad_norm': 0.7018494009971619, 'learning_rate': 1.0139812865843308e-05, 'epoch': 9.5}\n{'loss': 0.1141, 'grad_norm': 1.5560276508331299, 'learning_rate': 9.6989514368936e-06, 'epoch': 9.522727272727273}\n{'loss': 0.0836, 'grad_norm': 1.0809001922607422, 'learning_rate': 9.258090007943891e-06, 'epoch': 9.545454545454545}\n{'loss': 0.0838, 'grad_norm': 0.7638324499130249, 'learning_rate': 8.817228578994181e-06, 'epoch': 9.568181818181818}\n{'loss': 0.0838, 'grad_norm': 0.7845089435577393, 'learning_rate': 8.376367150044471e-06, 'epoch': 9.590909090909092}\n{'loss': 0.0674, 'grad_norm': 0.7654130458831787, 'learning_rate': 7.935505721094763e-06, 'epoch': 9.613636363636363}\n{'loss': 0.0787, 'grad_norm': 0.9192888736724854, 'learning_rate': 7.494644292145054e-06, 'epoch': 9.636363636363637}\n{'loss': 0.0691, 'grad_norm': 0.8487165570259094, 'learning_rate': 7.053782863195346e-06, 'epoch': 9.659090909090908}\n{'loss': 0.0652, 'grad_norm': 0.9704082012176514, 'learning_rate': 6.6129214342456364e-06, 'epoch': 9.681818181818182}\n{'loss': 0.0932, 'grad_norm': 1.0125309228897095, 'learning_rate': 6.1720600052959264e-06, 'epoch': 9.704545454545455}\n{'loss': 0.0896, 'grad_norm': 0.9584550261497498, 'learning_rate': 5.731198576346218e-06, 'epoch': 9.727272727272727}\n{'loss': 0.0548, 'grad_norm': 0.7271965742111206, 'learning_rate': 5.290337147396509e-06, 'epoch': 9.75}\n{'loss': 0.0871, 'grad_norm': 0.8642380833625793, 'learning_rate': 4.8494757184468e-06, 'epoch': 9.772727272727273}\n{'loss': 0.0783, 'grad_norm': 0.7136812806129456, 'learning_rate': 4.408614289497091e-06, 'epoch': 9.795454545454545}\n{'loss': 0.0589, 'grad_norm': 0.7439721822738647, 'learning_rate': 3.9677528605473815e-06, 'epoch': 9.818181818181818}\n{'loss': 0.081, 'grad_norm': 0.8293166756629944, 'learning_rate': 3.526891431597673e-06, 'epoch': 9.840909090909092}\n{'loss': 0.08, 'grad_norm': 0.9177225828170776, 'learning_rate': 3.0860300026479632e-06, 'epoch': 9.863636363636363}\n{'loss': 0.0757, 'grad_norm': 0.936676025390625, 'learning_rate': 2.6451685736982545e-06, 'epoch': 9.886363636363637}\n{'loss': 0.0916, 'grad_norm': 0.8984851241111755, 'learning_rate': 2.2043071447485453e-06, 'epoch': 9.909090909090908}\n{'loss': 0.1021, 'grad_norm': 0.8391938209533691, 'learning_rate': 1.7634457157988364e-06, 'epoch': 9.931818181818182}\n{'loss': 0.0999, 'grad_norm': 0.9838381409645081, 'learning_rate': 1.3225842868491272e-06, 'epoch': 9.954545454545455}\n{'loss': 0.1008, 'grad_norm': 1.0455464124679565, 'learning_rate': 8.817228578994182e-07, 'epoch': 9.977272727272727}\n{'loss': 0.038, 'grad_norm': 2.434706211090088, 'learning_rate': 4.408614289497091e-07, 'epoch': 10.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8962    0.8923    0.8942      3221\n     PERCENT     0.8696    0.9195    0.8939        87\n        TYPE     0.9538    0.9666    0.9602     11501\n      VOLUME     0.8730    0.9322    0.9016        59\n\n   micro avg     0.9407    0.9501    0.9454     14868\n   macro avg     0.8981    0.9277    0.9125     14868\nweighted avg     0.9405    0.9501    0.9453     14868\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.23744060099124908, 'eval_f1_macro': 0.9124725044183415, 'eval_precision': 0.9406672437903709, 'eval_recall': 0.9500941619585688, 'eval_f1': 0.945357202610005, 'eval_accuracy': 0.9378187737384699, 'eval_runtime': 1.4737, 'eval_samples_per_second': 3739.007, 'eval_steps_per_second': 7.464, 'epoch': 10.0}\n{'train_runtime': 62.0707, 'train_samples_per_second': 3551.111, 'train_steps_per_second': 7.089, 'train_loss': 0.28440885765647345, 'epoch': 10.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n[I 2025-09-26 18:23:24,011] Trial 8 finished with value: 0.9075471263735968 and parameters: {'learning_rate': 0.0001745811258640848, 'weight_decay': 0.0005162963078800053, 'num_train_epochs': 10}. Best is trial 0 with value: 0.9174994186455976.\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8983    0.8913    0.8948      3221\n     PERCENT     0.8602    0.9195    0.8889        87\n        TYPE     0.9543    0.9673    0.9607     11501\n      VOLUME     0.9167    0.9322    0.9244        59\n\n   micro avg     0.9416    0.9504    0.9460     14868\n   macro avg     0.9074    0.9276    0.9172     14868\nweighted avg     0.9415    0.9504    0.9459     14868\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.22992977499961853, 'eval_f1_macro': 0.9172047241404572, 'eval_precision': 0.9416272406210435, 'eval_recall': 0.9504304546677428, 'eval_f1': 0.9460083682008368, 'eval_accuracy': 0.9383613673358654, 'eval_runtime': 1.5409, 'eval_samples_per_second': 3575.913, 'eval_steps_per_second': 7.139, 'epoch': 10.0}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/27552 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e67db98886ca4f43ababb0bc57f4b51f"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nThe speedups for torchdynamo mostly come with GPU Ampere or higher and which is not detected here.\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/tmp/ipykernel_36/3364797558.py:66: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 2.2869, 'grad_norm': 6.933525562286377, 'learning_rate': 0.0, 'epoch': 0.022727272727272728}\n{'loss': 2.2967, 'grad_norm': 7.072678089141846, 'learning_rate': 6.282610280107692e-07, 'epoch': 0.045454545454545456}\n{'loss': 2.2838, 'grad_norm': 6.972485065460205, 'learning_rate': 1.2565220560215385e-06, 'epoch': 0.06818181818181818}\n{'loss': 2.2827, 'grad_norm': 7.278472423553467, 'learning_rate': 1.8847830840323077e-06, 'epoch': 0.09090909090909091}\n{'loss': 2.2676, 'grad_norm': 6.895395278930664, 'learning_rate': 2.513044112043077e-06, 'epoch': 0.11363636363636363}\n{'loss': 2.2698, 'grad_norm': 7.0437774658203125, 'learning_rate': 3.141305140053846e-06, 'epoch': 0.13636363636363635}\n{'loss': 2.268, 'grad_norm': 6.807976245880127, 'learning_rate': 3.7695661680646155e-06, 'epoch': 0.1590909090909091}\n{'loss': 2.2522, 'grad_norm': 7.1213059425354, 'learning_rate': 4.397827196075385e-06, 'epoch': 0.18181818181818182}\n{'loss': 2.2494, 'grad_norm': 6.888060569763184, 'learning_rate': 5.026088224086154e-06, 'epoch': 0.20454545454545456}\n{'loss': 2.2482, 'grad_norm': 7.116126537322998, 'learning_rate': 5.654349252096923e-06, 'epoch': 0.22727272727272727}\n{'loss': 2.2291, 'grad_norm': 7.2649688720703125, 'learning_rate': 6.282610280107692e-06, 'epoch': 0.25}\n{'loss': 2.2213, 'grad_norm': 7.236332416534424, 'learning_rate': 6.9108713081184625e-06, 'epoch': 0.2727272727272727}\n{'loss': 2.2018, 'grad_norm': 7.112157821655273, 'learning_rate': 7.539132336129231e-06, 'epoch': 0.29545454545454547}\n{'loss': 2.1794, 'grad_norm': 6.868558883666992, 'learning_rate': 8.16739336414e-06, 'epoch': 0.3181818181818182}\n{'loss': 2.164, 'grad_norm': 6.860518932342529, 'learning_rate': 8.79565439215077e-06, 'epoch': 0.3409090909090909}\n{'loss': 2.1429, 'grad_norm': 6.6637492179870605, 'learning_rate': 9.42391542016154e-06, 'epoch': 0.36363636363636365}\n{'loss': 2.1175, 'grad_norm': 6.734701633453369, 'learning_rate': 1.0052176448172308e-05, 'epoch': 0.38636363636363635}\n{'loss': 2.1039, 'grad_norm': 6.784826278686523, 'learning_rate': 1.0680437476183076e-05, 'epoch': 0.4090909090909091}\n{'loss': 2.0798, 'grad_norm': 6.574251174926758, 'learning_rate': 1.1308698504193846e-05, 'epoch': 0.4318181818181818}\n{'loss': 2.0495, 'grad_norm': 6.698684215545654, 'learning_rate': 1.1237124463028062e-05, 'epoch': 0.45454545454545453}\n{'loss': 2.0287, 'grad_norm': 6.475493431091309, 'learning_rate': 1.116555042186228e-05, 'epoch': 0.4772727272727273}\n{'loss': 2.0117, 'grad_norm': 6.624476432800293, 'learning_rate': 1.1093976380696495e-05, 'epoch': 0.5}\n{'loss': 1.9905, 'grad_norm': 6.27368688583374, 'learning_rate': 1.1022402339530711e-05, 'epoch': 0.5227272727272727}\n{'loss': 1.9567, 'grad_norm': 6.535839557647705, 'learning_rate': 1.0950828298364928e-05, 'epoch': 0.5454545454545454}\n{'loss': 1.9525, 'grad_norm': 6.173325538635254, 'learning_rate': 1.0879254257199144e-05, 'epoch': 0.5681818181818182}\n{'loss': 1.9189, 'grad_norm': 6.174759387969971, 'learning_rate': 1.080768021603336e-05, 'epoch': 0.5909090909090909}\n{'loss': 1.8917, 'grad_norm': 6.210901737213135, 'learning_rate': 1.0736106174867575e-05, 'epoch': 0.6136363636363636}\n{'loss': 1.8777, 'grad_norm': 6.255027770996094, 'learning_rate': 1.0664532133701793e-05, 'epoch': 0.6363636363636364}\n{'loss': 1.8525, 'grad_norm': 6.2158660888671875, 'learning_rate': 1.0592958092536008e-05, 'epoch': 0.6590909090909091}\n{'loss': 1.8316, 'grad_norm': 6.292623996734619, 'learning_rate': 1.0521384051370224e-05, 'epoch': 0.6818181818181818}\n{'loss': 1.8045, 'grad_norm': 6.236103057861328, 'learning_rate': 1.0449810010204442e-05, 'epoch': 0.7045454545454546}\n{'loss': 1.7887, 'grad_norm': 6.111382961273193, 'learning_rate': 1.0378235969038657e-05, 'epoch': 0.7272727272727273}\n{'loss': 1.788, 'grad_norm': 5.61025333404541, 'learning_rate': 1.0306661927872873e-05, 'epoch': 0.75}\n{'loss': 1.7672, 'grad_norm': 5.551419734954834, 'learning_rate': 1.0235087886707089e-05, 'epoch': 0.7727272727272727}\n{'loss': 1.7325, 'grad_norm': 5.994472980499268, 'learning_rate': 1.0163513845541304e-05, 'epoch': 0.7954545454545454}\n{'loss': 1.7078, 'grad_norm': 5.719641208648682, 'learning_rate': 1.0091939804375522e-05, 'epoch': 0.8181818181818182}\n{'loss': 1.6915, 'grad_norm': 5.70468807220459, 'learning_rate': 1.0020365763209737e-05, 'epoch': 0.8409090909090909}\n{'loss': 1.6816, 'grad_norm': 5.492677211761475, 'learning_rate': 9.948791722043953e-06, 'epoch': 0.8636363636363636}\n{'loss': 1.6454, 'grad_norm': 5.698549270629883, 'learning_rate': 9.877217680878169e-06, 'epoch': 0.8863636363636364}\n{'loss': 1.6566, 'grad_norm': 5.265115261077881, 'learning_rate': 9.805643639712385e-06, 'epoch': 0.9090909090909091}\n{'loss': 1.6459, 'grad_norm': 5.1774163246154785, 'learning_rate': 9.734069598546602e-06, 'epoch': 0.9318181818181818}\n{'loss': 1.5993, 'grad_norm': 5.4472551345825195, 'learning_rate': 9.662495557380818e-06, 'epoch': 0.9545454545454546}\n{'loss': 1.6069, 'grad_norm': 5.0778889656066895, 'learning_rate': 9.590921516215033e-06, 'epoch': 0.9772727272727273}\n{'loss': 1.5772, 'grad_norm': 6.019073009490967, 'learning_rate': 9.51934747504925e-06, 'epoch': 1.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.4904    0.0325    0.0609      3142\n     PERCENT     0.0000    0.0000    0.0000        66\n        TYPE     0.5807    0.9163    0.7109     11415\n      VOLUME     0.0000    0.0000    0.0000        70\n\n   micro avg     0.5785    0.7188    0.6410     14693\n   macro avg     0.2678    0.2372    0.1929     14693\nweighted avg     0.5560    0.7188    0.5653     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 1.528655767440796, 'eval_f1_macro': 0.19294690523592745, 'eval_precision': 0.5784630552664731, 'eval_recall': 0.7187776492207173, 'eval_f1': 0.64103186646434, 'eval_accuracy': 0.626618783327892, 'eval_runtime': 1.4762, 'eval_samples_per_second': 3733.16, 'eval_steps_per_second': 7.451, 'epoch': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 1.5328, 'grad_norm': 5.309576034545898, 'learning_rate': 9.447773433883466e-06, 'epoch': 1.0227272727272727}\n{'loss': 1.5304, 'grad_norm': 5.0034027099609375, 'learning_rate': 9.376199392717682e-06, 'epoch': 1.0454545454545454}\n{'loss': 1.5425, 'grad_norm': 4.893497467041016, 'learning_rate': 9.3046253515519e-06, 'epoch': 1.0681818181818181}\n{'loss': 1.5225, 'grad_norm': 4.8308539390563965, 'learning_rate': 9.233051310386115e-06, 'epoch': 1.0909090909090908}\n{'loss': 1.4995, 'grad_norm': 4.754212379455566, 'learning_rate': 9.161477269220331e-06, 'epoch': 1.1136363636363635}\n{'loss': 1.5007, 'grad_norm': 4.48205041885376, 'learning_rate': 9.089903228054547e-06, 'epoch': 1.1363636363636362}\n{'loss': 1.4705, 'grad_norm': 4.503645420074463, 'learning_rate': 9.018329186888764e-06, 'epoch': 1.1590909090909092}\n{'loss': 1.476, 'grad_norm': 4.272598743438721, 'learning_rate': 8.94675514572298e-06, 'epoch': 1.1818181818181819}\n{'loss': 1.4578, 'grad_norm': 4.330286979675293, 'learning_rate': 8.875181104557195e-06, 'epoch': 1.2045454545454546}\n{'loss': 1.4163, 'grad_norm': 4.374111652374268, 'learning_rate': 8.803607063391413e-06, 'epoch': 1.2272727272727273}\n{'loss': 1.4285, 'grad_norm': 4.071231842041016, 'learning_rate': 8.732033022225629e-06, 'epoch': 1.25}\n{'loss': 1.3727, 'grad_norm': 4.3802490234375, 'learning_rate': 8.660458981059844e-06, 'epoch': 1.2727272727272727}\n{'loss': 1.3846, 'grad_norm': 4.005056858062744, 'learning_rate': 8.588884939894062e-06, 'epoch': 1.2954545454545454}\n{'loss': 1.328, 'grad_norm': 4.3297271728515625, 'learning_rate': 8.517310898728277e-06, 'epoch': 1.3181818181818181}\n{'loss': 1.3498, 'grad_norm': 3.9463322162628174, 'learning_rate': 8.445736857562493e-06, 'epoch': 1.3409090909090908}\n{'loss': 1.3792, 'grad_norm': 3.5892369747161865, 'learning_rate': 8.374162816396709e-06, 'epoch': 1.3636363636363638}\n{'loss': 1.3645, 'grad_norm': 3.4816019535064697, 'learning_rate': 8.302588775230926e-06, 'epoch': 1.3863636363636362}\n{'loss': 1.2895, 'grad_norm': 3.8883304595947266, 'learning_rate': 8.231014734065142e-06, 'epoch': 1.4090909090909092}\n{'loss': 1.3129, 'grad_norm': 3.5267510414123535, 'learning_rate': 8.159440692899358e-06, 'epoch': 1.4318181818181819}\n{'loss': 1.3762, 'grad_norm': 3.09601092338562, 'learning_rate': 8.087866651733573e-06, 'epoch': 1.4545454545454546}\n{'loss': 1.3069, 'grad_norm': 3.3321871757507324, 'learning_rate': 8.01629261056779e-06, 'epoch': 1.4772727272727273}\n{'loss': 1.3097, 'grad_norm': 3.1754398345947266, 'learning_rate': 7.944718569402006e-06, 'epoch': 1.5}\n{'loss': 1.3241, 'grad_norm': 2.8971853256225586, 'learning_rate': 7.873144528236222e-06, 'epoch': 1.5227272727272727}\n{'loss': 1.2605, 'grad_norm': 3.0953116416931152, 'learning_rate': 7.801570487070438e-06, 'epoch': 1.5454545454545454}\n{'loss': 1.269, 'grad_norm': 2.9033544063568115, 'learning_rate': 7.729996445904653e-06, 'epoch': 1.5681818181818183}\n{'loss': 1.2477, 'grad_norm': 2.8821661472320557, 'learning_rate': 7.65842240473887e-06, 'epoch': 1.5909090909090908}\n{'loss': 1.3014, 'grad_norm': 2.573099374771118, 'learning_rate': 7.586848363573087e-06, 'epoch': 1.6136363636363638}\n{'loss': 1.2358, 'grad_norm': 2.6929800510406494, 'learning_rate': 7.515274322407303e-06, 'epoch': 1.6363636363636362}\n{'loss': 1.2379, 'grad_norm': 2.5577552318573, 'learning_rate': 7.443700281241519e-06, 'epoch': 1.6590909090909092}\n{'loss': 1.2846, 'grad_norm': 2.3481719493865967, 'learning_rate': 7.372126240075735e-06, 'epoch': 1.6818181818181817}\n{'loss': 1.2456, 'grad_norm': 2.4106905460357666, 'learning_rate': 7.300552198909951e-06, 'epoch': 1.7045454545454546}\n{'loss': 1.2179, 'grad_norm': 2.304994821548462, 'learning_rate': 7.228978157744168e-06, 'epoch': 1.7272727272727273}\n{'loss': 1.2292, 'grad_norm': 2.2379448413848877, 'learning_rate': 7.157404116578384e-06, 'epoch': 1.75}\n{'loss': 1.1881, 'grad_norm': 2.2329318523406982, 'learning_rate': 7.0858300754126e-06, 'epoch': 1.7727272727272727}\n{'loss': 1.1755, 'grad_norm': 2.231999158859253, 'learning_rate': 7.0142560342468155e-06, 'epoch': 1.7954545454545454}\n{'loss': 1.1629, 'grad_norm': 2.2084527015686035, 'learning_rate': 6.942681993081033e-06, 'epoch': 1.8181818181818183}\n{'loss': 1.2033, 'grad_norm': 2.0437254905700684, 'learning_rate': 6.871107951915249e-06, 'epoch': 1.8409090909090908}\n{'loss': 1.1876, 'grad_norm': 2.030447483062744, 'learning_rate': 6.799533910749464e-06, 'epoch': 1.8636363636363638}\n{'loss': 1.147, 'grad_norm': 2.146364212036133, 'learning_rate': 6.72795986958368e-06, 'epoch': 1.8863636363636362}\n{'loss': 1.2148, 'grad_norm': 1.8999087810516357, 'learning_rate': 6.6563858284178974e-06, 'epoch': 1.9090909090909092}\n{'loss': 1.1572, 'grad_norm': 1.952489972114563, 'learning_rate': 6.584811787252113e-06, 'epoch': 1.9318181818181817}\n{'loss': 1.168, 'grad_norm': 1.8102396726608276, 'learning_rate': 6.513237746086329e-06, 'epoch': 1.9545454545454546}\n{'loss': 1.1385, 'grad_norm': 1.8315351009368896, 'learning_rate': 6.441663704920546e-06, 'epoch': 1.9772727272727273}\n{'loss': 1.0462, 'grad_norm': 2.9825847148895264, 'learning_rate': 6.370089663754762e-06, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.6500    0.0041    0.0082      3142\n     PERCENT     0.0000    0.0000    0.0000        66\n        TYPE     0.5740    0.9232    0.7079     11415\n      VOLUME     0.0000    0.0000    0.0000        70\n\n   micro avg     0.5741    0.7181    0.6381     14693\n   macro avg     0.3060    0.2318    0.1790     14693\nweighted avg     0.5850    0.7181    0.5517     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 1.1161677837371826, 'eval_f1_macro': 0.17902808565414632, 'eval_precision': 0.5741103493307215, 'eval_recall': 0.7180970530184442, 'eval_f1': 0.6380817030026307, 'eval_accuracy': 0.6216127979105452, 'eval_runtime': 1.4804, 'eval_samples_per_second': 3722.707, 'eval_steps_per_second': 7.431, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 1.13, 'grad_norm': 1.9231407642364502, 'learning_rate': 6.298515622588978e-06, 'epoch': 2.022727272727273}\n{'loss': 1.1354, 'grad_norm': 1.8604938983917236, 'learning_rate': 6.226941581423194e-06, 'epoch': 2.0454545454545454}\n{'loss': 1.1571, 'grad_norm': 1.6759692430496216, 'learning_rate': 6.15536754025741e-06, 'epoch': 2.0681818181818183}\n{'loss': 1.1076, 'grad_norm': 1.784477949142456, 'learning_rate': 6.083793499091626e-06, 'epoch': 2.090909090909091}\n{'loss': 1.1564, 'grad_norm': 1.6910266876220703, 'learning_rate': 6.012219457925842e-06, 'epoch': 2.1136363636363638}\n{'loss': 1.1107, 'grad_norm': 1.67804753780365, 'learning_rate': 5.940645416760059e-06, 'epoch': 2.1363636363636362}\n{'loss': 1.0975, 'grad_norm': 1.7123647928237915, 'learning_rate': 5.869071375594274e-06, 'epoch': 2.159090909090909}\n{'loss': 1.0881, 'grad_norm': 1.7200177907943726, 'learning_rate': 5.79749733442849e-06, 'epoch': 2.1818181818181817}\n{'loss': 1.1315, 'grad_norm': 1.5994130373001099, 'learning_rate': 5.7259232932627075e-06, 'epoch': 2.2045454545454546}\n{'loss': 1.1068, 'grad_norm': 1.6457163095474243, 'learning_rate': 5.654349252096923e-06, 'epoch': 2.227272727272727}\n{'loss': 1.1091, 'grad_norm': 1.536286473274231, 'learning_rate': 5.58277521093114e-06, 'epoch': 2.25}\n{'loss': 1.0889, 'grad_norm': 1.6103492975234985, 'learning_rate': 5.5112011697653554e-06, 'epoch': 2.2727272727272725}\n{'loss': 1.1527, 'grad_norm': 1.626853585243225, 'learning_rate': 5.439627128599572e-06, 'epoch': 2.2954545454545454}\n{'loss': 1.0494, 'grad_norm': 1.582574725151062, 'learning_rate': 5.368053087433788e-06, 'epoch': 2.3181818181818183}\n{'loss': 1.0929, 'grad_norm': 1.5266185998916626, 'learning_rate': 5.296479046268004e-06, 'epoch': 2.340909090909091}\n{'loss': 1.1065, 'grad_norm': 1.4755381345748901, 'learning_rate': 5.224905005102221e-06, 'epoch': 2.3636363636363638}\n{'loss': 1.0936, 'grad_norm': 1.490660309791565, 'learning_rate': 5.1533309639364365e-06, 'epoch': 2.3863636363636362}\n{'loss': 1.0925, 'grad_norm': 1.4327597618103027, 'learning_rate': 5.081756922770652e-06, 'epoch': 2.409090909090909}\n{'loss': 1.0646, 'grad_norm': 1.473851203918457, 'learning_rate': 5.010182881604869e-06, 'epoch': 2.4318181818181817}\n{'loss': 1.1231, 'grad_norm': 1.496260404586792, 'learning_rate': 4.9386088404390844e-06, 'epoch': 2.4545454545454546}\n{'loss': 1.0537, 'grad_norm': 1.5397217273712158, 'learning_rate': 4.867034799273301e-06, 'epoch': 2.4772727272727275}\n{'loss': 1.0599, 'grad_norm': 1.4215850830078125, 'learning_rate': 4.795460758107517e-06, 'epoch': 2.5}\n{'loss': 1.0648, 'grad_norm': 1.3667455911636353, 'learning_rate': 4.723886716941733e-06, 'epoch': 2.5227272727272725}\n{'loss': 1.0916, 'grad_norm': 1.4000130891799927, 'learning_rate': 4.65231267577595e-06, 'epoch': 2.5454545454545454}\n{'loss': 1.0538, 'grad_norm': 1.5348505973815918, 'learning_rate': 4.5807386346101655e-06, 'epoch': 2.5681818181818183}\n{'loss': 1.1135, 'grad_norm': 1.4397144317626953, 'learning_rate': 4.509164593444382e-06, 'epoch': 2.590909090909091}\n{'loss': 1.0578, 'grad_norm': 1.4737199544906616, 'learning_rate': 4.437590552278598e-06, 'epoch': 2.6136363636363638}\n{'loss': 1.0174, 'grad_norm': 1.471495270729065, 'learning_rate': 4.366016511112814e-06, 'epoch': 2.6363636363636362}\n{'loss': 1.0603, 'grad_norm': 1.4267750978469849, 'learning_rate': 4.294442469947031e-06, 'epoch': 2.659090909090909}\n{'loss': 0.9926, 'grad_norm': 1.5966200828552246, 'learning_rate': 4.2228684287812465e-06, 'epoch': 2.6818181818181817}\n{'loss': 1.0754, 'grad_norm': 1.3765159845352173, 'learning_rate': 4.151294387615463e-06, 'epoch': 2.7045454545454546}\n{'loss': 1.0487, 'grad_norm': 1.4959737062454224, 'learning_rate': 4.079720346449679e-06, 'epoch': 2.7272727272727275}\n{'loss': 1.0589, 'grad_norm': 1.3929615020751953, 'learning_rate': 4.008146305283895e-06, 'epoch': 2.75}\n{'loss': 1.0606, 'grad_norm': 1.3871567249298096, 'learning_rate': 3.936572264118111e-06, 'epoch': 2.7727272727272725}\n{'loss': 1.0607, 'grad_norm': 1.4240697622299194, 'learning_rate': 3.864998222952327e-06, 'epoch': 2.7954545454545454}\n{'loss': 1.0977, 'grad_norm': 1.390518307685852, 'learning_rate': 3.7934241817865437e-06, 'epoch': 2.8181818181818183}\n{'loss': 1.0584, 'grad_norm': 1.4210773706436157, 'learning_rate': 3.7218501406207594e-06, 'epoch': 2.840909090909091}\n{'loss': 1.0125, 'grad_norm': 1.4090219736099243, 'learning_rate': 3.6502760994549755e-06, 'epoch': 2.8636363636363638}\n{'loss': 1.0012, 'grad_norm': 1.4207502603530884, 'learning_rate': 3.578702058289192e-06, 'epoch': 2.8863636363636362}\n{'loss': 1.0353, 'grad_norm': 1.3336834907531738, 'learning_rate': 3.5071280171234078e-06, 'epoch': 2.909090909090909}\n{'loss': 1.0299, 'grad_norm': 1.3190175294876099, 'learning_rate': 3.4355539759576243e-06, 'epoch': 2.9318181818181817}\n{'loss': 1.0455, 'grad_norm': 1.3877731561660767, 'learning_rate': 3.36397993479184e-06, 'epoch': 2.9545454545454546}\n{'loss': 1.0493, 'grad_norm': 1.3819763660430908, 'learning_rate': 3.2924058936260566e-06, 'epoch': 2.9772727272727275}\n{'loss': 1.0519, 'grad_norm': 3.0276706218719482, 'learning_rate': 3.220831852460273e-06, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8899    0.1620    0.2741      3142\n     PERCENT     0.0000    0.0000    0.0000        66\n        TYPE     0.5919    0.9215    0.7208     11415\n      VOLUME     0.0000    0.0000    0.0000        70\n\n   micro avg     0.6012    0.7506    0.6676     14693\n   macro avg     0.3704    0.2709    0.2487     14693\nweighted avg     0.6501    0.7506    0.6186     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.991003155708313, 'eval_f1_macro': 0.24872459096551552, 'eval_precision': 0.6011774967291758, 'eval_recall': 0.7505614918668754, 'eval_f1': 0.6676150982232043, 'eval_accuracy': 0.649200130590924, 'eval_runtime': 1.4924, 'eval_samples_per_second': 3692.737, 'eval_steps_per_second': 7.371, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.9867, 'grad_norm': 1.3373323678970337, 'learning_rate': 3.149257811294489e-06, 'epoch': 3.022727272727273}\n{'loss': 1.0584, 'grad_norm': 1.317168951034546, 'learning_rate': 3.077683770128705e-06, 'epoch': 3.0454545454545454}\n{'loss': 1.0405, 'grad_norm': 1.3378840684890747, 'learning_rate': 3.006109728962921e-06, 'epoch': 3.0681818181818183}\n{'loss': 0.9652, 'grad_norm': 1.597851276397705, 'learning_rate': 2.934535687797137e-06, 'epoch': 3.090909090909091}\n{'loss': 1.0204, 'grad_norm': 1.255275011062622, 'learning_rate': 2.8629616466313537e-06, 'epoch': 3.1136363636363638}\n{'loss': 0.9993, 'grad_norm': 1.3579933643341064, 'learning_rate': 2.79138760546557e-06, 'epoch': 3.1363636363636362}\n{'loss': 1.0254, 'grad_norm': 1.3952455520629883, 'learning_rate': 2.719813564299786e-06, 'epoch': 3.159090909090909}\n{'loss': 0.9939, 'grad_norm': 1.3995882272720337, 'learning_rate': 2.648239523134002e-06, 'epoch': 3.1818181818181817}\n{'loss': 1.0749, 'grad_norm': 1.2768042087554932, 'learning_rate': 2.5766654819682182e-06, 'epoch': 3.2045454545454546}\n{'loss': 1.0041, 'grad_norm': 1.3566970825195312, 'learning_rate': 2.5050914408024344e-06, 'epoch': 3.227272727272727}\n{'loss': 1.0138, 'grad_norm': 1.3082728385925293, 'learning_rate': 2.4335173996366505e-06, 'epoch': 3.25}\n{'loss': 1.0304, 'grad_norm': 1.464716911315918, 'learning_rate': 2.3619433584708666e-06, 'epoch': 3.2727272727272725}\n{'loss': 1.0159, 'grad_norm': 1.249997854232788, 'learning_rate': 2.2903693173050827e-06, 'epoch': 3.2954545454545454}\n{'loss': 1.0143, 'grad_norm': 1.1879578828811646, 'learning_rate': 2.218795276139299e-06, 'epoch': 3.3181818181818183}\n{'loss': 1.0081, 'grad_norm': 1.3043150901794434, 'learning_rate': 2.1472212349735154e-06, 'epoch': 3.340909090909091}\n{'loss': 0.9941, 'grad_norm': 1.3193033933639526, 'learning_rate': 2.0756471938077315e-06, 'epoch': 3.3636363636363638}\n{'loss': 0.9669, 'grad_norm': 1.4091438055038452, 'learning_rate': 2.0040731526419477e-06, 'epoch': 3.3863636363636362}\n{'loss': 0.999, 'grad_norm': 1.326784610748291, 'learning_rate': 1.9324991114761634e-06, 'epoch': 3.409090909090909}\n{'loss': 1.0147, 'grad_norm': 1.2419828176498413, 'learning_rate': 1.8609250703103797e-06, 'epoch': 3.4318181818181817}\n{'loss': 1.037, 'grad_norm': 1.2804044485092163, 'learning_rate': 1.789351029144596e-06, 'epoch': 3.4545454545454546}\n{'loss': 0.9798, 'grad_norm': 1.3639132976531982, 'learning_rate': 1.7177769879788122e-06, 'epoch': 3.4772727272727275}\n{'loss': 1.0046, 'grad_norm': 1.3066413402557373, 'learning_rate': 1.6462029468130283e-06, 'epoch': 3.5}\n{'loss': 1.0034, 'grad_norm': 1.234635353088379, 'learning_rate': 1.5746289056472444e-06, 'epoch': 3.5227272727272725}\n{'loss': 0.9995, 'grad_norm': 1.2597640752792358, 'learning_rate': 1.5030548644814605e-06, 'epoch': 3.5454545454545454}\n{'loss': 1.003, 'grad_norm': 1.2663562297821045, 'learning_rate': 1.4314808233156769e-06, 'epoch': 3.5681818181818183}\n{'loss': 1.0324, 'grad_norm': 1.2826067209243774, 'learning_rate': 1.359906782149893e-06, 'epoch': 3.590909090909091}\n{'loss': 1.0049, 'grad_norm': 1.2223283052444458, 'learning_rate': 1.2883327409841091e-06, 'epoch': 3.6136363636363638}\n{'loss': 1.0097, 'grad_norm': 1.2770072221755981, 'learning_rate': 1.2167586998183252e-06, 'epoch': 3.6363636363636362}\n{'loss': 0.9747, 'grad_norm': 1.3620884418487549, 'learning_rate': 1.1451846586525414e-06, 'epoch': 3.659090909090909}\n{'loss': 0.9636, 'grad_norm': 1.3871970176696777, 'learning_rate': 1.0736106174867577e-06, 'epoch': 3.6818181818181817}\n{'loss': 1.0068, 'grad_norm': 1.2246520519256592, 'learning_rate': 1.0020365763209738e-06, 'epoch': 3.7045454545454546}\n{'loss': 0.9546, 'grad_norm': 1.2787288427352905, 'learning_rate': 9.304625351551898e-07, 'epoch': 3.7272727272727275}\n{'loss': 0.9771, 'grad_norm': 1.3617565631866455, 'learning_rate': 8.588884939894061e-07, 'epoch': 3.75}\n{'loss': 1.0036, 'grad_norm': 1.3711471557617188, 'learning_rate': 7.873144528236222e-07, 'epoch': 3.7727272727272725}\n{'loss': 0.9649, 'grad_norm': 1.3813954591751099, 'learning_rate': 7.157404116578384e-07, 'epoch': 3.7954545454545454}\n{'loss': 0.9802, 'grad_norm': 1.2740802764892578, 'learning_rate': 6.441663704920546e-07, 'epoch': 3.8181818181818183}\n{'loss': 0.9722, 'grad_norm': 1.2816182374954224, 'learning_rate': 5.725923293262707e-07, 'epoch': 3.840909090909091}\n{'loss': 1.0093, 'grad_norm': 1.2463740110397339, 'learning_rate': 5.010182881604869e-07, 'epoch': 3.8636363636363638}\n{'loss': 0.982, 'grad_norm': 1.2689095735549927, 'learning_rate': 4.2944424699470304e-07, 'epoch': 3.8863636363636362}\n{'loss': 0.9971, 'grad_norm': 1.3173394203186035, 'learning_rate': 3.578702058289192e-07, 'epoch': 3.909090909090909}\n{'loss': 0.9664, 'grad_norm': 1.3021435737609863, 'learning_rate': 2.8629616466313534e-07, 'epoch': 3.9318181818181817}\n{'loss': 1.0336, 'grad_norm': 1.2700263261795044, 'learning_rate': 2.1472212349735152e-07, 'epoch': 3.9545454545454546}\n{'loss': 1.0152, 'grad_norm': 1.3126479387283325, 'learning_rate': 1.4314808233156767e-07, 'epoch': 3.9772727272727275}\n{'loss': 1.2198, 'grad_norm': 3.2488136291503906, 'learning_rate': 7.157404116578384e-08, 'epoch': 4.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8762    0.2409    0.3779      3142\n     PERCENT     0.0000    0.0000    0.0000        66\n        TYPE     0.6038    0.9203    0.7292     11415\n      VOLUME     0.0000    0.0000    0.0000        70\n\n   micro avg     0.6167    0.7665    0.6835     14693\n   macro avg     0.3700    0.2903    0.2768     14693\nweighted avg     0.6565    0.7665    0.6473     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.9558020234107971, 'eval_f1_macro': 0.27678578792924036, 'eval_precision': 0.6167241662559553, 'eval_recall': 0.7664874430000681, 'eval_f1': 0.6834982096255386, 'eval_accuracy': 0.6659048862770705, 'eval_runtime': 1.4791, 'eval_samples_per_second': 3725.945, 'eval_steps_per_second': 7.437, 'epoch': 4.0}\n{'train_runtime': 24.6832, 'train_samples_per_second': 3571.825, 'train_steps_per_second': 7.13, 'train_loss': 1.3452344888990575, 'epoch': 4.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nThe speedups for torchdynamo mostly come with GPU Ampere or higher and which is not detected here.\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8762    0.2409    0.3779      3142\n     PERCENT     0.0000    0.0000    0.0000        66\n        TYPE     0.6038    0.9203    0.7292     11415\n      VOLUME     0.0000    0.0000    0.0000        70\n\n   micro avg     0.6167    0.7665    0.6835     14693\n   macro avg     0.3700    0.2903    0.2768     14693\nweighted avg     0.6565    0.7665    0.6473     14693\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.9558020234107971, 'eval_f1_macro': 0.27678578792924036, 'eval_precision': 0.6167241662559553, 'eval_recall': 0.7664874430000681, 'eval_f1': 0.6834982096255386, 'eval_accuracy': 0.6659048862770705, 'eval_runtime': 1.5634, 'eval_samples_per_second': 3525.088, 'eval_steps_per_second': 7.036, 'epoch': 4.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/3364797558.py:66: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 2.1128, 'grad_norm': 6.952020645141602, 'learning_rate': 0.0, 'epoch': 0.022727272727272728}\n{'loss': 2.1098, 'grad_norm': 7.102254390716553, 'learning_rate': 6.282610280107692e-07, 'epoch': 0.045454545454545456}\n{'loss': 2.1111, 'grad_norm': 7.135246753692627, 'learning_rate': 1.2565220560215385e-06, 'epoch': 0.06818181818181818}\n{'loss': 2.1071, 'grad_norm': 6.928664684295654, 'learning_rate': 1.8847830840323077e-06, 'epoch': 0.09090909090909091}\n{'loss': 2.1204, 'grad_norm': 6.912606716156006, 'learning_rate': 2.513044112043077e-06, 'epoch': 0.11363636363636363}\n{'loss': 2.0885, 'grad_norm': 7.009614944458008, 'learning_rate': 3.141305140053846e-06, 'epoch': 0.13636363636363635}\n{'loss': 2.0918, 'grad_norm': 6.908524036407471, 'learning_rate': 3.7695661680646155e-06, 'epoch': 0.1590909090909091}\n{'loss': 2.0971, 'grad_norm': 6.51746940612793, 'learning_rate': 4.397827196075385e-06, 'epoch': 0.18181818181818182}\n{'loss': 2.0778, 'grad_norm': 7.027283668518066, 'learning_rate': 5.026088224086154e-06, 'epoch': 0.20454545454545456}\n{'loss': 2.0772, 'grad_norm': 6.675719738006592, 'learning_rate': 5.654349252096923e-06, 'epoch': 0.22727272727272727}\n{'loss': 2.0519, 'grad_norm': 6.928595066070557, 'learning_rate': 6.282610280107692e-06, 'epoch': 0.25}\n{'loss': 2.052, 'grad_norm': 6.523501396179199, 'learning_rate': 6.9108713081184625e-06, 'epoch': 0.2727272727272727}\n{'loss': 2.0165, 'grad_norm': 6.71820592880249, 'learning_rate': 7.539132336129231e-06, 'epoch': 0.29545454545454547}\n{'loss': 1.9949, 'grad_norm': 6.925177097320557, 'learning_rate': 8.16739336414e-06, 'epoch': 0.3181818181818182}\n{'loss': 2.0098, 'grad_norm': 6.405552864074707, 'learning_rate': 8.79565439215077e-06, 'epoch': 0.3409090909090909}\n{'loss': 1.9803, 'grad_norm': 6.538680553436279, 'learning_rate': 9.42391542016154e-06, 'epoch': 0.36363636363636365}\n{'loss': 1.9493, 'grad_norm': 6.541184425354004, 'learning_rate': 1.0052176448172308e-05, 'epoch': 0.38636363636363635}\n{'loss': 1.9399, 'grad_norm': 6.397544860839844, 'learning_rate': 1.0680437476183076e-05, 'epoch': 0.4090909090909091}\n{'loss': 1.9143, 'grad_norm': 6.4719367027282715, 'learning_rate': 1.1308698504193846e-05, 'epoch': 0.4318181818181818}\n{'loss': 1.9063, 'grad_norm': 6.194000720977783, 'learning_rate': 1.1237124463028062e-05, 'epoch': 0.45454545454545453}\n{'loss': 1.8743, 'grad_norm': 6.219014644622803, 'learning_rate': 1.116555042186228e-05, 'epoch': 0.4772727272727273}\n{'loss': 1.8348, 'grad_norm': 6.380443096160889, 'learning_rate': 1.1093976380696495e-05, 'epoch': 0.5}\n{'loss': 1.8214, 'grad_norm': 6.173537254333496, 'learning_rate': 1.1022402339530711e-05, 'epoch': 0.5227272727272727}\n{'loss': 1.8157, 'grad_norm': 5.744925022125244, 'learning_rate': 1.0950828298364928e-05, 'epoch': 0.5454545454545454}\n{'loss': 1.787, 'grad_norm': 5.831542491912842, 'learning_rate': 1.0879254257199144e-05, 'epoch': 0.5681818181818182}\n{'loss': 1.7557, 'grad_norm': 5.932053089141846, 'learning_rate': 1.080768021603336e-05, 'epoch': 0.5909090909090909}\n{'loss': 1.7527, 'grad_norm': 5.606963157653809, 'learning_rate': 1.0736106174867575e-05, 'epoch': 0.6136363636363636}\n{'loss': 1.7009, 'grad_norm': 5.858755111694336, 'learning_rate': 1.0664532133701793e-05, 'epoch': 0.6363636363636364}\n{'loss': 1.7184, 'grad_norm': 5.522420883178711, 'learning_rate': 1.0592958092536008e-05, 'epoch': 0.6590909090909091}\n{'loss': 1.7007, 'grad_norm': 5.413799285888672, 'learning_rate': 1.0521384051370224e-05, 'epoch': 0.6818181818181818}\n{'loss': 1.6886, 'grad_norm': 5.3429856300354, 'learning_rate': 1.0449810010204442e-05, 'epoch': 0.7045454545454546}\n{'loss': 1.6741, 'grad_norm': 5.3137054443359375, 'learning_rate': 1.0378235969038657e-05, 'epoch': 0.7272727272727273}\n{'loss': 1.6346, 'grad_norm': 5.23324728012085, 'learning_rate': 1.0306661927872873e-05, 'epoch': 0.75}\n{'loss': 1.6171, 'grad_norm': 5.0841965675354, 'learning_rate': 1.0235087886707089e-05, 'epoch': 0.7727272727272727}\n{'loss': 1.6054, 'grad_norm': 5.169312953948975, 'learning_rate': 1.0163513845541304e-05, 'epoch': 0.7954545454545454}\n{'loss': 1.59, 'grad_norm': 5.120639324188232, 'learning_rate': 1.0091939804375522e-05, 'epoch': 0.8181818181818182}\n{'loss': 1.5841, 'grad_norm': 4.888296604156494, 'learning_rate': 1.0020365763209737e-05, 'epoch': 0.8409090909090909}\n{'loss': 1.5694, 'grad_norm': 4.751087188720703, 'learning_rate': 9.948791722043953e-06, 'epoch': 0.8636363636363636}\n{'loss': 1.524, 'grad_norm': 4.786645412445068, 'learning_rate': 9.877217680878169e-06, 'epoch': 0.8863636363636364}\n{'loss': 1.5194, 'grad_norm': 4.7424516677856445, 'learning_rate': 9.805643639712385e-06, 'epoch': 0.9090909090909091}\n{'loss': 1.5373, 'grad_norm': 4.363368034362793, 'learning_rate': 9.734069598546602e-06, 'epoch': 0.9318181818181818}\n{'loss': 1.4722, 'grad_norm': 4.561812877655029, 'learning_rate': 9.662495557380818e-06, 'epoch': 0.9545454545454546}\n{'loss': 1.4953, 'grad_norm': 4.208105564117432, 'learning_rate': 9.590921516215033e-06, 'epoch': 0.9772727272727273}\n{'loss': 1.3993, 'grad_norm': 5.035332202911377, 'learning_rate': 9.51934747504925e-06, 'epoch': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.1818    0.0018    0.0035      3404\n     PERCENT     1.0000    0.0282    0.0548        71\n        TYPE     0.5648    0.9187    0.6996     11194\n      VOLUME     0.0000    0.0000    0.0000        56\n\n   micro avg     0.5642    0.6989    0.6244     14725\n   macro avg     0.4367    0.2372    0.1895     14725\nweighted avg     0.4762    0.6989    0.5329     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 1.4227420091629028, 'eval_f1_macro': 0.18946349484616348, 'eval_precision': 0.5641925227496984, 'eval_recall': 0.6989473684210527, 'eval_f1': 0.6243819577152909, 'eval_accuracy': 0.6131666940744395, 'eval_runtime': 1.4754, 'eval_samples_per_second': 3735.375, 'eval_steps_per_second': 7.456, 'epoch': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 1.4287, 'grad_norm': 4.3157219886779785, 'learning_rate': 9.447773433883466e-06, 'epoch': 1.0227272727272727}\n{'loss': 1.4054, 'grad_norm': 4.359786033630371, 'learning_rate': 9.376199392717682e-06, 'epoch': 1.0454545454545454}\n{'loss': 1.4048, 'grad_norm': 4.0749688148498535, 'learning_rate': 9.3046253515519e-06, 'epoch': 1.0681818181818181}\n{'loss': 1.4151, 'grad_norm': 3.8613858222961426, 'learning_rate': 9.233051310386115e-06, 'epoch': 1.0909090909090908}\n{'loss': 1.437, 'grad_norm': 3.589010000228882, 'learning_rate': 9.161477269220331e-06, 'epoch': 1.1136363636363635}\n{'loss': 1.3752, 'grad_norm': 3.772210121154785, 'learning_rate': 9.089903228054547e-06, 'epoch': 1.1363636363636362}\n{'loss': 1.3536, 'grad_norm': 3.7281782627105713, 'learning_rate': 9.018329186888764e-06, 'epoch': 1.1590909090909092}\n{'loss': 1.3624, 'grad_norm': 3.6366286277770996, 'learning_rate': 8.94675514572298e-06, 'epoch': 1.1818181818181819}\n{'loss': 1.3496, 'grad_norm': 3.5197904109954834, 'learning_rate': 8.875181104557195e-06, 'epoch': 1.2045454545454546}\n{'loss': 1.3555, 'grad_norm': 3.2395646572113037, 'learning_rate': 8.803607063391413e-06, 'epoch': 1.2272727272727273}\n{'loss': 1.3426, 'grad_norm': 3.238224744796753, 'learning_rate': 8.732033022225629e-06, 'epoch': 1.25}\n{'loss': 1.3136, 'grad_norm': 3.1820576190948486, 'learning_rate': 8.660458981059844e-06, 'epoch': 1.2727272727272727}\n{'loss': 1.3255, 'grad_norm': 2.949244976043701, 'learning_rate': 8.588884939894062e-06, 'epoch': 1.2954545454545454}\n{'loss': 1.2995, 'grad_norm': 3.13558030128479, 'learning_rate': 8.517310898728277e-06, 'epoch': 1.3181818181818181}\n{'loss': 1.2484, 'grad_norm': 3.1059799194335938, 'learning_rate': 8.445736857562493e-06, 'epoch': 1.3409090909090908}\n{'loss': 1.2961, 'grad_norm': 2.734297752380371, 'learning_rate': 8.374162816396709e-06, 'epoch': 1.3636363636363638}\n{'loss': 1.2421, 'grad_norm': 2.8156278133392334, 'learning_rate': 8.302588775230926e-06, 'epoch': 1.3863636363636362}\n{'loss': 1.2316, 'grad_norm': 2.8079113960266113, 'learning_rate': 8.231014734065142e-06, 'epoch': 1.4090909090909092}\n{'loss': 1.2126, 'grad_norm': 2.705122470855713, 'learning_rate': 8.159440692899358e-06, 'epoch': 1.4318181818181819}\n{'loss': 1.2509, 'grad_norm': 2.4711062908172607, 'learning_rate': 8.087866651733573e-06, 'epoch': 1.4545454545454546}\n{'loss': 1.1976, 'grad_norm': 2.6460587978363037, 'learning_rate': 8.01629261056779e-06, 'epoch': 1.4772727272727273}\n{'loss': 1.2195, 'grad_norm': 2.3963654041290283, 'learning_rate': 7.944718569402006e-06, 'epoch': 1.5}\n{'loss': 1.2194, 'grad_norm': 2.2420899868011475, 'learning_rate': 7.873144528236222e-06, 'epoch': 1.5227272727272727}\n{'loss': 1.2396, 'grad_norm': 2.1568198204040527, 'learning_rate': 7.801570487070438e-06, 'epoch': 1.5454545454545454}\n{'loss': 1.1969, 'grad_norm': 2.0984206199645996, 'learning_rate': 7.729996445904653e-06, 'epoch': 1.5681818181818183}\n{'loss': 1.2347, 'grad_norm': 1.9923410415649414, 'learning_rate': 7.65842240473887e-06, 'epoch': 1.5909090909090908}\n{'loss': 1.1852, 'grad_norm': 1.9998859167099, 'learning_rate': 7.586848363573087e-06, 'epoch': 1.6136363636363638}\n{'loss': 1.1885, 'grad_norm': 2.0438344478607178, 'learning_rate': 7.515274322407303e-06, 'epoch': 1.6363636363636362}\n{'loss': 1.2142, 'grad_norm': 1.8683645725250244, 'learning_rate': 7.443700281241519e-06, 'epoch': 1.6590909090909092}\n{'loss': 1.1538, 'grad_norm': 1.9237943887710571, 'learning_rate': 7.372126240075735e-06, 'epoch': 1.6818181818181817}\n{'loss': 1.1814, 'grad_norm': 1.826588749885559, 'learning_rate': 7.300552198909951e-06, 'epoch': 1.7045454545454546}\n{'loss': 1.1929, 'grad_norm': 1.7429834604263306, 'learning_rate': 7.228978157744168e-06, 'epoch': 1.7272727272727273}\n{'loss': 1.1457, 'grad_norm': 1.7736155986785889, 'learning_rate': 7.157404116578384e-06, 'epoch': 1.75}\n{'loss': 1.1133, 'grad_norm': 1.8231761455535889, 'learning_rate': 7.0858300754126e-06, 'epoch': 1.7727272727272727}\n{'loss': 1.1707, 'grad_norm': 1.6119158267974854, 'learning_rate': 7.0142560342468155e-06, 'epoch': 1.7954545454545454}\n{'loss': 1.141, 'grad_norm': 1.5850551128387451, 'learning_rate': 6.942681993081033e-06, 'epoch': 1.8181818181818183}\n{'loss': 1.1518, 'grad_norm': 1.5214096307754517, 'learning_rate': 6.871107951915249e-06, 'epoch': 1.8409090909090908}\n{'loss': 1.1258, 'grad_norm': 1.5729631185531616, 'learning_rate': 6.799533910749464e-06, 'epoch': 1.8636363636363638}\n{'loss': 1.1949, 'grad_norm': 1.4479347467422485, 'learning_rate': 6.72795986958368e-06, 'epoch': 1.8863636363636362}\n{'loss': 1.1348, 'grad_norm': 1.5270229578018188, 'learning_rate': 6.6563858284178974e-06, 'epoch': 1.9090909090909092}\n{'loss': 1.0808, 'grad_norm': 1.565596580505371, 'learning_rate': 6.584811787252113e-06, 'epoch': 1.9318181818181817}\n{'loss': 1.045, 'grad_norm': 1.6625617742538452, 'learning_rate': 6.513237746086329e-06, 'epoch': 1.9545454545454546}\n{'loss': 1.1524, 'grad_norm': 1.517045259475708, 'learning_rate': 6.441663704920546e-06, 'epoch': 1.9772727272727273}\n{'loss': 1.0751, 'grad_norm': 2.196195125579834, 'learning_rate': 6.370089663754762e-06, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.2222    0.0006    0.0012      3404\n     PERCENT     0.0000    0.0000    0.0000        71\n        TYPE     0.5642    0.9190    0.6991     11194\n      VOLUME     0.0000    0.0000    0.0000        56\n\n   micro avg     0.5640    0.6987    0.6242     14725\n   macro avg     0.1966    0.2299    0.1751     14725\nweighted avg     0.4803    0.6987    0.5318     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 1.0908294916152954, 'eval_f1_macro': 0.17507551741195673, 'eval_precision': 0.5639971495916242, 'eval_recall': 0.6987436332767403, 'eval_f1': 0.6241810240232953, 'eval_accuracy': 0.6131118785287507, 'eval_runtime': 1.4905, 'eval_samples_per_second': 3697.424, 'eval_steps_per_second': 7.38, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 1.0651, 'grad_norm': 1.4758446216583252, 'learning_rate': 6.298515622588978e-06, 'epoch': 2.022727272727273}\n{'loss': 1.1161, 'grad_norm': 1.37052583694458, 'learning_rate': 6.226941581423194e-06, 'epoch': 2.0454545454545454}\n{'loss': 1.0996, 'grad_norm': 1.3571161031723022, 'learning_rate': 6.15536754025741e-06, 'epoch': 2.0681818181818183}\n{'loss': 1.1029, 'grad_norm': 1.410390019416809, 'learning_rate': 6.083793499091626e-06, 'epoch': 2.090909090909091}\n{'loss': 1.0695, 'grad_norm': 1.3511778116226196, 'learning_rate': 6.012219457925842e-06, 'epoch': 2.1136363636363638}\n{'loss': 1.0967, 'grad_norm': 1.441166877746582, 'learning_rate': 5.940645416760059e-06, 'epoch': 2.1363636363636362}\n{'loss': 1.0942, 'grad_norm': 1.2922166585922241, 'learning_rate': 5.869071375594274e-06, 'epoch': 2.159090909090909}\n{'loss': 1.1184, 'grad_norm': 1.2679314613342285, 'learning_rate': 5.79749733442849e-06, 'epoch': 2.1818181818181817}\n{'loss': 1.0637, 'grad_norm': 1.2998802661895752, 'learning_rate': 5.7259232932627075e-06, 'epoch': 2.2045454545454546}\n{'loss': 1.0607, 'grad_norm': 1.3782609701156616, 'learning_rate': 5.654349252096923e-06, 'epoch': 2.227272727272727}\n{'loss': 1.1119, 'grad_norm': 1.319331169128418, 'learning_rate': 5.58277521093114e-06, 'epoch': 2.25}\n{'loss': 1.0651, 'grad_norm': 1.2884272336959839, 'learning_rate': 5.5112011697653554e-06, 'epoch': 2.2727272727272725}\n{'loss': 1.0292, 'grad_norm': 1.4093472957611084, 'learning_rate': 5.439627128599572e-06, 'epoch': 2.2954545454545454}\n{'loss': 1.0685, 'grad_norm': 1.31492280960083, 'learning_rate': 5.368053087433788e-06, 'epoch': 2.3181818181818183}\n{'loss': 1.0928, 'grad_norm': 1.2270821332931519, 'learning_rate': 5.296479046268004e-06, 'epoch': 2.340909090909091}\n{'loss': 1.056, 'grad_norm': 1.1989963054656982, 'learning_rate': 5.224905005102221e-06, 'epoch': 2.3636363636363638}\n{'loss': 1.0385, 'grad_norm': 1.248823881149292, 'learning_rate': 5.1533309639364365e-06, 'epoch': 2.3863636363636362}\n{'loss': 1.0345, 'grad_norm': 1.3140631914138794, 'learning_rate': 5.081756922770652e-06, 'epoch': 2.409090909090909}\n{'loss': 1.0342, 'grad_norm': 1.323974609375, 'learning_rate': 5.010182881604869e-06, 'epoch': 2.4318181818181817}\n{'loss': 1.0092, 'grad_norm': 1.3569022417068481, 'learning_rate': 4.9386088404390844e-06, 'epoch': 2.4545454545454546}\n{'loss': 1.0382, 'grad_norm': 1.2541357278823853, 'learning_rate': 4.867034799273301e-06, 'epoch': 2.4772727272727275}\n{'loss': 0.9869, 'grad_norm': 1.341149926185608, 'learning_rate': 4.795460758107517e-06, 'epoch': 2.5}\n{'loss': 1.0422, 'grad_norm': 1.2999606132507324, 'learning_rate': 4.723886716941733e-06, 'epoch': 2.5227272727272725}\n{'loss': 1.0515, 'grad_norm': 1.2517706155776978, 'learning_rate': 4.65231267577595e-06, 'epoch': 2.5454545454545454}\n{'loss': 1.0471, 'grad_norm': 1.168426275253296, 'learning_rate': 4.5807386346101655e-06, 'epoch': 2.5681818181818183}\n{'loss': 1.0383, 'grad_norm': 1.1927464008331299, 'learning_rate': 4.509164593444382e-06, 'epoch': 2.590909090909091}\n{'loss': 1.0365, 'grad_norm': 1.1978951692581177, 'learning_rate': 4.437590552278598e-06, 'epoch': 2.6136363636363638}\n{'loss': 0.9712, 'grad_norm': 1.357353687286377, 'learning_rate': 4.366016511112814e-06, 'epoch': 2.6363636363636362}\n{'loss': 1.0023, 'grad_norm': 1.2148911952972412, 'learning_rate': 4.294442469947031e-06, 'epoch': 2.659090909090909}\n{'loss': 1.0062, 'grad_norm': 1.2575665712356567, 'learning_rate': 4.2228684287812465e-06, 'epoch': 2.6818181818181817}\n{'loss': 1.0334, 'grad_norm': 1.2671281099319458, 'learning_rate': 4.151294387615463e-06, 'epoch': 2.7045454545454546}\n{'loss': 0.9619, 'grad_norm': 1.3334969282150269, 'learning_rate': 4.079720346449679e-06, 'epoch': 2.7272727272727275}\n{'loss': 1.0098, 'grad_norm': 1.19021737575531, 'learning_rate': 4.008146305283895e-06, 'epoch': 2.75}\n{'loss': 1.0702, 'grad_norm': 1.1922000646591187, 'learning_rate': 3.936572264118111e-06, 'epoch': 2.7727272727272725}\n{'loss': 0.9934, 'grad_norm': 1.2222234010696411, 'learning_rate': 3.864998222952327e-06, 'epoch': 2.7954545454545454}\n{'loss': 1.0136, 'grad_norm': 1.28850519657135, 'learning_rate': 3.7934241817865437e-06, 'epoch': 2.8181818181818183}\n{'loss': 1.0331, 'grad_norm': 1.2326717376708984, 'learning_rate': 3.7218501406207594e-06, 'epoch': 2.840909090909091}\n{'loss': 1.0557, 'grad_norm': 1.0969561338424683, 'learning_rate': 3.6502760994549755e-06, 'epoch': 2.8636363636363638}\n{'loss': 1.0222, 'grad_norm': 1.226504921913147, 'learning_rate': 3.578702058289192e-06, 'epoch': 2.8863636363636362}\n{'loss': 1.0224, 'grad_norm': 1.1964781284332275, 'learning_rate': 3.5071280171234078e-06, 'epoch': 2.909090909090909}\n{'loss': 0.9951, 'grad_norm': 1.1900681257247925, 'learning_rate': 3.4355539759576243e-06, 'epoch': 2.9318181818181817}\n{'loss': 1.0452, 'grad_norm': 1.208946704864502, 'learning_rate': 3.36397993479184e-06, 'epoch': 2.9545454545454546}\n{'loss': 0.9634, 'grad_norm': 1.2285248041152954, 'learning_rate': 3.2924058936260566e-06, 'epoch': 2.9772727272727275}\n{'loss': 0.9673, 'grad_norm': 2.8978772163391113, 'learning_rate': 3.220831852460273e-06, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.7094    0.0667    0.1219      3404\n     PERCENT     0.0000    0.0000    0.0000        71\n        TYPE     0.5740    0.9185    0.7065     11194\n      VOLUME     0.0000    0.0000    0.0000        56\n\n   micro avg     0.5764    0.7137    0.6377     14725\n   macro avg     0.3209    0.2463    0.2071     14725\nweighted avg     0.6004    0.7137    0.5653     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.9787941575050354, 'eval_f1_macro': 0.20710822872477008, 'eval_precision': 0.5764041246160597, 'eval_recall': 0.7136842105263158, 'eval_f1': 0.6377400855660407, 'eval_accuracy': 0.6257742695828536, 'eval_runtime': 1.4853, 'eval_samples_per_second': 3710.345, 'eval_steps_per_second': 7.406, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 1.009, 'grad_norm': 1.1679151058197021, 'learning_rate': 3.149257811294489e-06, 'epoch': 3.022727272727273}\n{'loss': 1.0124, 'grad_norm': 1.146291732788086, 'learning_rate': 3.077683770128705e-06, 'epoch': 3.0454545454545454}\n{'loss': 1.0179, 'grad_norm': 1.1307077407836914, 'learning_rate': 3.006109728962921e-06, 'epoch': 3.0681818181818183}\n{'loss': 0.9304, 'grad_norm': 1.4213131666183472, 'learning_rate': 2.934535687797137e-06, 'epoch': 3.090909090909091}\n{'loss': 0.9429, 'grad_norm': 1.2643271684646606, 'learning_rate': 2.8629616466313537e-06, 'epoch': 3.1136363636363638}\n{'loss': 1.0151, 'grad_norm': 1.1980737447738647, 'learning_rate': 2.79138760546557e-06, 'epoch': 3.1363636363636362}\n{'loss': 0.9979, 'grad_norm': 1.1236127614974976, 'learning_rate': 2.719813564299786e-06, 'epoch': 3.159090909090909}\n{'loss': 0.993, 'grad_norm': 1.1342346668243408, 'learning_rate': 2.648239523134002e-06, 'epoch': 3.1818181818181817}\n{'loss': 0.9761, 'grad_norm': 1.1633692979812622, 'learning_rate': 2.5766654819682182e-06, 'epoch': 3.2045454545454546}\n{'loss': 1.0196, 'grad_norm': 1.2710450887680054, 'learning_rate': 2.5050914408024344e-06, 'epoch': 3.227272727272727}\n{'loss': 0.9386, 'grad_norm': 1.2818480730056763, 'learning_rate': 2.4335173996366505e-06, 'epoch': 3.25}\n{'loss': 0.9788, 'grad_norm': 1.2410756349563599, 'learning_rate': 2.3619433584708666e-06, 'epoch': 3.2727272727272725}\n{'loss': 1.0403, 'grad_norm': 1.129399299621582, 'learning_rate': 2.2903693173050827e-06, 'epoch': 3.2954545454545454}\n{'loss': 0.9775, 'grad_norm': 1.198320746421814, 'learning_rate': 2.218795276139299e-06, 'epoch': 3.3181818181818183}\n{'loss': 0.9669, 'grad_norm': 1.2046226263046265, 'learning_rate': 2.1472212349735154e-06, 'epoch': 3.340909090909091}\n{'loss': 0.9875, 'grad_norm': 1.2110053300857544, 'learning_rate': 2.0756471938077315e-06, 'epoch': 3.3636363636363638}\n{'loss': 1.0107, 'grad_norm': 1.08348548412323, 'learning_rate': 2.0040731526419477e-06, 'epoch': 3.3863636363636362}\n{'loss': 0.9874, 'grad_norm': 1.1777445077896118, 'learning_rate': 1.9324991114761634e-06, 'epoch': 3.409090909090909}\n{'loss': 0.9847, 'grad_norm': 1.221710205078125, 'learning_rate': 1.8609250703103797e-06, 'epoch': 3.4318181818181817}\n{'loss': 0.9917, 'grad_norm': 1.1224217414855957, 'learning_rate': 1.789351029144596e-06, 'epoch': 3.4545454545454546}\n{'loss': 0.9804, 'grad_norm': 1.0829503536224365, 'learning_rate': 1.7177769879788122e-06, 'epoch': 3.4772727272727275}\n{'loss': 0.9426, 'grad_norm': 1.360007405281067, 'learning_rate': 1.6462029468130283e-06, 'epoch': 3.5}\n{'loss': 0.9767, 'grad_norm': 1.1258363723754883, 'learning_rate': 1.5746289056472444e-06, 'epoch': 3.5227272727272725}\n{'loss': 0.9907, 'grad_norm': 1.2456682920455933, 'learning_rate': 1.5030548644814605e-06, 'epoch': 3.5454545454545454}\n{'loss': 1.0089, 'grad_norm': 1.211729884147644, 'learning_rate': 1.4314808233156769e-06, 'epoch': 3.5681818181818183}\n{'loss': 0.9484, 'grad_norm': 1.1814876794815063, 'learning_rate': 1.359906782149893e-06, 'epoch': 3.590909090909091}\n{'loss': 0.9259, 'grad_norm': 1.2271161079406738, 'learning_rate': 1.2883327409841091e-06, 'epoch': 3.6136363636363638}\n{'loss': 0.957, 'grad_norm': 1.1157383918762207, 'learning_rate': 1.2167586998183252e-06, 'epoch': 3.6363636363636362}\n{'loss': 0.9008, 'grad_norm': 1.292130708694458, 'learning_rate': 1.1451846586525414e-06, 'epoch': 3.659090909090909}\n{'loss': 0.9737, 'grad_norm': 1.1887803077697754, 'learning_rate': 1.0736106174867577e-06, 'epoch': 3.6818181818181817}\n{'loss': 1.0115, 'grad_norm': 1.0798745155334473, 'learning_rate': 1.0020365763209738e-06, 'epoch': 3.7045454545454546}\n{'loss': 0.9628, 'grad_norm': 1.1522222757339478, 'learning_rate': 9.304625351551898e-07, 'epoch': 3.7272727272727275}\n{'loss': 0.9345, 'grad_norm': 1.1850017309188843, 'learning_rate': 8.588884939894061e-07, 'epoch': 3.75}\n{'loss': 0.9293, 'grad_norm': 1.215934157371521, 'learning_rate': 7.873144528236222e-07, 'epoch': 3.7727272727272725}\n{'loss': 0.957, 'grad_norm': 1.163694143295288, 'learning_rate': 7.157404116578384e-07, 'epoch': 3.7954545454545454}\n{'loss': 0.9253, 'grad_norm': 1.2246332168579102, 'learning_rate': 6.441663704920546e-07, 'epoch': 3.8181818181818183}\n{'loss': 0.9486, 'grad_norm': 1.207979679107666, 'learning_rate': 5.725923293262707e-07, 'epoch': 3.840909090909091}\n{'loss': 0.9744, 'grad_norm': 1.1211541891098022, 'learning_rate': 5.010182881604869e-07, 'epoch': 3.8636363636363638}\n{'loss': 0.9967, 'grad_norm': 1.1110398769378662, 'learning_rate': 4.2944424699470304e-07, 'epoch': 3.8863636363636362}\n{'loss': 0.9728, 'grad_norm': 1.1535061597824097, 'learning_rate': 3.578702058289192e-07, 'epoch': 3.909090909090909}\n{'loss': 0.9258, 'grad_norm': 1.23804771900177, 'learning_rate': 2.8629616466313534e-07, 'epoch': 3.9318181818181817}\n{'loss': 0.9619, 'grad_norm': 1.1518027782440186, 'learning_rate': 2.1472212349735152e-07, 'epoch': 3.9545454545454546}\n{'loss': 0.9293, 'grad_norm': 1.192086100578308, 'learning_rate': 1.4314808233156767e-07, 'epoch': 3.9772727272727275}\n{'loss': 1.1435, 'grad_norm': 2.5933964252471924, 'learning_rate': 7.157404116578384e-08, 'epoch': 4.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.6992    0.1263    0.2140      3404\n     PERCENT     0.0000    0.0000    0.0000        71\n        TYPE     0.5849    0.9161    0.7140     11194\n      VOLUME     0.0000    0.0000    0.0000        56\n\n   micro avg     0.5888    0.7256    0.6501     14725\n   macro avg     0.3210    0.2606    0.2320     14725\nweighted avg     0.6063    0.7256    0.5922     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.9449236392974854, 'eval_f1_macro': 0.23198651308294166, 'eval_precision': 0.5887701124090808, 'eval_recall': 0.7256366723259763, 'eval_f1': 0.650077571259088, 'eval_accuracy': 0.6389848160938442, 'eval_runtime': 1.5511, 'eval_samples_per_second': 3552.966, 'eval_steps_per_second': 7.092, 'epoch': 4.0}\n{'train_runtime': 24.81, 'train_samples_per_second': 3553.567, 'train_steps_per_second': 7.094, 'train_loss': 1.2720305987379767, 'epoch': 4.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nThe speedups for torchdynamo mostly come with GPU Ampere or higher and which is not detected here.\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.6992    0.1263    0.2140      3404\n     PERCENT     0.0000    0.0000    0.0000        71\n        TYPE     0.5849    0.9161    0.7140     11194\n      VOLUME     0.0000    0.0000    0.0000        56\n\n   micro avg     0.5888    0.7256    0.6501     14725\n   macro avg     0.3210    0.2606    0.2320     14725\nweighted avg     0.6063    0.7256    0.5922     14725\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.9449236392974854, 'eval_f1_macro': 0.23198651308294166, 'eval_precision': 0.5887701124090808, 'eval_recall': 0.7256366723259763, 'eval_f1': 0.650077571259088, 'eval_accuracy': 0.6389848160938442, 'eval_runtime': 1.5453, 'eval_samples_per_second': 3566.379, 'eval_steps_per_second': 7.119, 'epoch': 4.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/3364797558.py:66: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 2.1859, 'grad_norm': 6.638328552246094, 'learning_rate': 0.0, 'epoch': 0.022727272727272728}\n{'loss': 2.193, 'grad_norm': 6.709334850311279, 'learning_rate': 6.282610280107692e-07, 'epoch': 0.045454545454545456}\n{'loss': 2.1962, 'grad_norm': 6.732530117034912, 'learning_rate': 1.2565220560215385e-06, 'epoch': 0.06818181818181818}\n{'loss': 2.1809, 'grad_norm': 6.530272483825684, 'learning_rate': 1.8847830840323077e-06, 'epoch': 0.09090909090909091}\n{'loss': 2.1834, 'grad_norm': 6.609808444976807, 'learning_rate': 2.513044112043077e-06, 'epoch': 0.11363636363636363}\n{'loss': 2.1751, 'grad_norm': 6.977482318878174, 'learning_rate': 3.141305140053846e-06, 'epoch': 0.13636363636363635}\n{'loss': 2.1734, 'grad_norm': 6.3482441902160645, 'learning_rate': 3.7695661680646155e-06, 'epoch': 0.1590909090909091}\n{'loss': 2.1785, 'grad_norm': 6.648825645446777, 'learning_rate': 4.397827196075385e-06, 'epoch': 0.18181818181818182}\n{'loss': 2.1674, 'grad_norm': 6.607244491577148, 'learning_rate': 5.026088224086154e-06, 'epoch': 0.20454545454545456}\n{'loss': 2.155, 'grad_norm': 6.581453800201416, 'learning_rate': 5.654349252096923e-06, 'epoch': 0.22727272727272727}\n{'loss': 2.1489, 'grad_norm': 6.283424377441406, 'learning_rate': 6.282610280107692e-06, 'epoch': 0.25}\n{'loss': 2.1366, 'grad_norm': 6.365634918212891, 'learning_rate': 6.9108713081184625e-06, 'epoch': 0.2727272727272727}\n{'loss': 2.1081, 'grad_norm': 6.452038288116455, 'learning_rate': 7.539132336129231e-06, 'epoch': 0.29545454545454547}\n{'loss': 2.0921, 'grad_norm': 6.296431064605713, 'learning_rate': 8.16739336414e-06, 'epoch': 0.3181818181818182}\n{'loss': 2.0828, 'grad_norm': 6.117300510406494, 'learning_rate': 8.79565439215077e-06, 'epoch': 0.3409090909090909}\n{'loss': 2.0677, 'grad_norm': 6.439466953277588, 'learning_rate': 9.42391542016154e-06, 'epoch': 0.36363636363636365}\n{'loss': 2.0394, 'grad_norm': 6.222761631011963, 'learning_rate': 1.0052176448172308e-05, 'epoch': 0.38636363636363635}\n{'loss': 2.0257, 'grad_norm': 6.384652614593506, 'learning_rate': 1.0680437476183076e-05, 'epoch': 0.4090909090909091}\n{'loss': 2.0048, 'grad_norm': 6.3725666999816895, 'learning_rate': 1.1308698504193846e-05, 'epoch': 0.4318181818181818}\n{'loss': 1.9801, 'grad_norm': 6.018496513366699, 'learning_rate': 1.1237124463028062e-05, 'epoch': 0.45454545454545453}\n{'loss': 1.961, 'grad_norm': 6.283790111541748, 'learning_rate': 1.116555042186228e-05, 'epoch': 0.4772727272727273}\n{'loss': 1.9476, 'grad_norm': 6.121443748474121, 'learning_rate': 1.1093976380696495e-05, 'epoch': 0.5}\n{'loss': 1.9273, 'grad_norm': 6.042662620544434, 'learning_rate': 1.1022402339530711e-05, 'epoch': 0.5227272727272727}\n{'loss': 1.9044, 'grad_norm': 6.250328063964844, 'learning_rate': 1.0950828298364928e-05, 'epoch': 0.5454545454545454}\n{'loss': 1.8893, 'grad_norm': 5.946518421173096, 'learning_rate': 1.0879254257199144e-05, 'epoch': 0.5681818181818182}\n{'loss': 1.8809, 'grad_norm': 5.817107200622559, 'learning_rate': 1.080768021603336e-05, 'epoch': 0.5909090909090909}\n{'loss': 1.8712, 'grad_norm': 5.575562953948975, 'learning_rate': 1.0736106174867575e-05, 'epoch': 0.6136363636363636}\n{'loss': 1.8324, 'grad_norm': 5.755128383636475, 'learning_rate': 1.0664532133701793e-05, 'epoch': 0.6363636363636364}\n{'loss': 1.8046, 'grad_norm': 5.751474857330322, 'learning_rate': 1.0592958092536008e-05, 'epoch': 0.6590909090909091}\n{'loss': 1.7884, 'grad_norm': 5.790631294250488, 'learning_rate': 1.0521384051370224e-05, 'epoch': 0.6818181818181818}\n{'loss': 1.7863, 'grad_norm': 5.6315999031066895, 'learning_rate': 1.0449810010204442e-05, 'epoch': 0.7045454545454546}\n{'loss': 1.8022, 'grad_norm': 5.154465198516846, 'learning_rate': 1.0378235969038657e-05, 'epoch': 0.7272727272727273}\n{'loss': 1.7441, 'grad_norm': 5.569584846496582, 'learning_rate': 1.0306661927872873e-05, 'epoch': 0.75}\n{'loss': 1.7318, 'grad_norm': 5.367273330688477, 'learning_rate': 1.0235087886707089e-05, 'epoch': 0.7727272727272727}\n{'loss': 1.7544, 'grad_norm': 4.949596405029297, 'learning_rate': 1.0163513845541304e-05, 'epoch': 0.7954545454545454}\n{'loss': 1.7082, 'grad_norm': 5.288121700286865, 'learning_rate': 1.0091939804375522e-05, 'epoch': 0.8181818181818182}\n{'loss': 1.6644, 'grad_norm': 5.365711688995361, 'learning_rate': 1.0020365763209737e-05, 'epoch': 0.8409090909090909}\n{'loss': 1.6699, 'grad_norm': 5.128235816955566, 'learning_rate': 9.948791722043953e-06, 'epoch': 0.8636363636363636}\n{'loss': 1.6675, 'grad_norm': 5.1123247146606445, 'learning_rate': 9.877217680878169e-06, 'epoch': 0.8863636363636364}\n{'loss': 1.6273, 'grad_norm': 5.298433780670166, 'learning_rate': 9.805643639712385e-06, 'epoch': 0.9090909090909091}\n{'loss': 1.6155, 'grad_norm': 4.981107711791992, 'learning_rate': 9.734069598546602e-06, 'epoch': 0.9318181818181818}\n{'loss': 1.6156, 'grad_norm': 4.929608345031738, 'learning_rate': 9.662495557380818e-06, 'epoch': 0.9545454545454546}\n{'loss': 1.6018, 'grad_norm': 4.816102504730225, 'learning_rate': 9.590921516215033e-06, 'epoch': 0.9772727272727273}\n{'loss': 1.6992, 'grad_norm': 4.812218189239502, 'learning_rate': 9.51934747504925e-06, 'epoch': 1.0}\n=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.6078    0.0094    0.0184      3311\n     PERCENT     0.0000    0.0000    0.0000        86\n        TYPE     0.5707    0.9175    0.7037     11299\n      VOLUME     0.0000    0.0000    0.0000        42\n\n   micro avg     0.5706    0.7055    0.6309     14738\n   macro avg     0.2946    0.2317    0.1805     14738\nweighted avg     0.5741    0.7055    0.5436     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 1.5411556959152222, 'eval_f1_macro': 0.18053690542087436, 'eval_precision': 0.5706289101086599, 'eval_recall': 0.7055231374677704, 'eval_f1': 0.6309466019417476, 'eval_accuracy': 0.6189825692585105, 'eval_runtime': 1.5047, 'eval_samples_per_second': 3661.857, 'eval_steps_per_second': 7.31, 'epoch': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 1.5893, 'grad_norm': 4.587990760803223, 'learning_rate': 9.447773433883466e-06, 'epoch': 1.0227272727272727}\n{'loss': 1.5576, 'grad_norm': 4.708609104156494, 'learning_rate': 9.376199392717682e-06, 'epoch': 1.0454545454545454}\n{'loss': 1.5445, 'grad_norm': 4.6373467445373535, 'learning_rate': 9.3046253515519e-06, 'epoch': 1.0681818181818181}\n{'loss': 1.517, 'grad_norm': 4.657010078430176, 'learning_rate': 9.233051310386115e-06, 'epoch': 1.0909090909090908}\n{'loss': 1.5171, 'grad_norm': 4.553560256958008, 'learning_rate': 9.161477269220331e-06, 'epoch': 1.1136363636363635}\n{'loss': 1.5111, 'grad_norm': 4.209017276763916, 'learning_rate': 9.089903228054547e-06, 'epoch': 1.1363636363636362}\n{'loss': 1.4444, 'grad_norm': 4.732595920562744, 'learning_rate': 9.018329186888764e-06, 'epoch': 1.1590909090909092}\n{'loss': 1.4907, 'grad_norm': 4.053438663482666, 'learning_rate': 8.94675514572298e-06, 'epoch': 1.1818181818181819}\n{'loss': 1.4732, 'grad_norm': 4.132763385772705, 'learning_rate': 8.875181104557195e-06, 'epoch': 1.2045454545454546}\n{'loss': 1.4593, 'grad_norm': 4.053229808807373, 'learning_rate': 8.803607063391413e-06, 'epoch': 1.2272727272727273}\n{'loss': 1.4483, 'grad_norm': 3.9723165035247803, 'learning_rate': 8.732033022225629e-06, 'epoch': 1.25}\n{'loss': 1.4338, 'grad_norm': 4.001101493835449, 'learning_rate': 8.660458981059844e-06, 'epoch': 1.2727272727272727}\n{'loss': 1.4142, 'grad_norm': 3.9206485748291016, 'learning_rate': 8.588884939894062e-06, 'epoch': 1.2954545454545454}\n{'loss': 1.4219, 'grad_norm': 3.7572450637817383, 'learning_rate': 8.517310898728277e-06, 'epoch': 1.3181818181818181}\n{'loss': 1.3957, 'grad_norm': 3.8620975017547607, 'learning_rate': 8.445736857562493e-06, 'epoch': 1.3409090909090908}\n{'loss': 1.3874, 'grad_norm': 3.606532573699951, 'learning_rate': 8.374162816396709e-06, 'epoch': 1.3636363636363638}\n{'loss': 1.4126, 'grad_norm': 3.345811128616333, 'learning_rate': 8.302588775230926e-06, 'epoch': 1.3863636363636362}\n{'loss': 1.3491, 'grad_norm': 3.655531167984009, 'learning_rate': 8.231014734065142e-06, 'epoch': 1.4090909090909092}\n{'loss': 1.3848, 'grad_norm': 3.3024940490722656, 'learning_rate': 8.159440692899358e-06, 'epoch': 1.4318181818181819}\n{'loss': 1.3067, 'grad_norm': 3.598168134689331, 'learning_rate': 8.087866651733573e-06, 'epoch': 1.4545454545454546}\n{'loss': 1.3563, 'grad_norm': 3.271141290664673, 'learning_rate': 8.01629261056779e-06, 'epoch': 1.4772727272727273}\n{'loss': 1.3164, 'grad_norm': 3.45906138420105, 'learning_rate': 7.944718569402006e-06, 'epoch': 1.5}\n{'loss': 1.3284, 'grad_norm': 3.2456345558166504, 'learning_rate': 7.873144528236222e-06, 'epoch': 1.5227272727272727}\n{'loss': 1.3221, 'grad_norm': 3.0887908935546875, 'learning_rate': 7.801570487070438e-06, 'epoch': 1.5454545454545454}\n{'loss': 1.3324, 'grad_norm': 2.948319911956787, 'learning_rate': 7.729996445904653e-06, 'epoch': 1.5681818181818183}\n{'loss': 1.2947, 'grad_norm': 3.023061513900757, 'learning_rate': 7.65842240473887e-06, 'epoch': 1.5909090909090908}\n{'loss': 1.3037, 'grad_norm': 2.8527820110321045, 'learning_rate': 7.586848363573087e-06, 'epoch': 1.6136363636363638}\n{'loss': 1.2996, 'grad_norm': 2.7197487354278564, 'learning_rate': 7.515274322407303e-06, 'epoch': 1.6363636363636362}\n{'loss': 1.2663, 'grad_norm': 2.9372901916503906, 'learning_rate': 7.443700281241519e-06, 'epoch': 1.6590909090909092}\n{'loss': 1.3003, 'grad_norm': 2.6435065269470215, 'learning_rate': 7.372126240075735e-06, 'epoch': 1.6818181818181817}\n{'loss': 1.2816, 'grad_norm': 2.701111316680908, 'learning_rate': 7.300552198909951e-06, 'epoch': 1.7045454545454546}\n{'loss': 1.2775, 'grad_norm': 2.5845487117767334, 'learning_rate': 7.228978157744168e-06, 'epoch': 1.7272727272727273}\n{'loss': 1.2632, 'grad_norm': 2.5037546157836914, 'learning_rate': 7.157404116578384e-06, 'epoch': 1.75}\n{'loss': 1.2659, 'grad_norm': 2.3998818397521973, 'learning_rate': 7.0858300754126e-06, 'epoch': 1.7727272727272727}\n{'loss': 1.1748, 'grad_norm': 2.7243945598602295, 'learning_rate': 7.0142560342468155e-06, 'epoch': 1.7954545454545454}\n{'loss': 1.236, 'grad_norm': 2.369589328765869, 'learning_rate': 6.942681993081033e-06, 'epoch': 1.8181818181818183}\n{'loss': 1.1757, 'grad_norm': 2.5797879695892334, 'learning_rate': 6.871107951915249e-06, 'epoch': 1.8409090909090908}\n{'loss': 1.2, 'grad_norm': 2.302297592163086, 'learning_rate': 6.799533910749464e-06, 'epoch': 1.8636363636363638}\n{'loss': 1.1902, 'grad_norm': 2.314183473587036, 'learning_rate': 6.72795986958368e-06, 'epoch': 1.8863636363636362}\n{'loss': 1.2703, 'grad_norm': 2.0632591247558594, 'learning_rate': 6.6563858284178974e-06, 'epoch': 1.9090909090909092}\n{'loss': 1.2052, 'grad_norm': 2.1250860691070557, 'learning_rate': 6.584811787252113e-06, 'epoch': 1.9318181818181817}\n{'loss': 1.2022, 'grad_norm': 2.105027437210083, 'learning_rate': 6.513237746086329e-06, 'epoch': 1.9545454545454546}\n{'loss': 1.1257, 'grad_norm': 2.232611656188965, 'learning_rate': 6.441663704920546e-06, 'epoch': 1.9772727272727273}\n{'loss': 1.0859, 'grad_norm': 3.1615607738494873, 'learning_rate': 6.370089663754762e-06, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8182    0.0082    0.0161      3311\n     PERCENT     0.0000    0.0000    0.0000        86\n        TYPE     0.5703    0.9220    0.7048     11299\n      VOLUME     0.0000    0.0000    0.0000        42\n\n   micro avg     0.5708    0.7087    0.6323     14738\n   macro avg     0.3471    0.2325    0.1802     14738\nweighted avg     0.6211    0.7087    0.5439     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 1.144400954246521, 'eval_f1_macro': 0.18022514154603797, 'eval_precision': 0.5707962183725886, 'eval_recall': 0.7087121726150089, 'eval_f1': 0.6323213366831129, 'eval_accuracy': 0.6181083000928911, 'eval_runtime': 1.4757, 'eval_samples_per_second': 3733.711, 'eval_steps_per_second': 7.454, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 1.1629, 'grad_norm': 2.047724962234497, 'learning_rate': 6.298515622588978e-06, 'epoch': 2.022727272727273}\n{'loss': 1.1911, 'grad_norm': 1.917173147201538, 'learning_rate': 6.226941581423194e-06, 'epoch': 2.0454545454545454}\n{'loss': 1.1637, 'grad_norm': 1.949661135673523, 'learning_rate': 6.15536754025741e-06, 'epoch': 2.0681818181818183}\n{'loss': 1.2277, 'grad_norm': 1.7962757349014282, 'learning_rate': 6.083793499091626e-06, 'epoch': 2.090909090909091}\n{'loss': 1.1682, 'grad_norm': 1.8384639024734497, 'learning_rate': 6.012219457925842e-06, 'epoch': 2.1136363636363638}\n{'loss': 1.1257, 'grad_norm': 1.95204758644104, 'learning_rate': 5.940645416760059e-06, 'epoch': 2.1363636363636362}\n{'loss': 1.1582, 'grad_norm': 1.7953013181686401, 'learning_rate': 5.869071375594274e-06, 'epoch': 2.159090909090909}\n{'loss': 1.1455, 'grad_norm': 1.7541462182998657, 'learning_rate': 5.79749733442849e-06, 'epoch': 2.1818181818181817}\n{'loss': 1.1361, 'grad_norm': 1.7560491561889648, 'learning_rate': 5.7259232932627075e-06, 'epoch': 2.2045454545454546}\n{'loss': 1.1615, 'grad_norm': 1.720212697982788, 'learning_rate': 5.654349252096923e-06, 'epoch': 2.227272727272727}\n{'loss': 1.1563, 'grad_norm': 1.7442442178726196, 'learning_rate': 5.58277521093114e-06, 'epoch': 2.25}\n{'loss': 1.1591, 'grad_norm': 1.7229247093200684, 'learning_rate': 5.5112011697653554e-06, 'epoch': 2.2727272727272725}\n{'loss': 1.1332, 'grad_norm': 1.6068758964538574, 'learning_rate': 5.439627128599572e-06, 'epoch': 2.2954545454545454}\n{'loss': 1.1277, 'grad_norm': 1.597482442855835, 'learning_rate': 5.368053087433788e-06, 'epoch': 2.3181818181818183}\n{'loss': 1.0635, 'grad_norm': 1.8180243968963623, 'learning_rate': 5.296479046268004e-06, 'epoch': 2.340909090909091}\n{'loss': 1.1154, 'grad_norm': 1.6914889812469482, 'learning_rate': 5.224905005102221e-06, 'epoch': 2.3636363636363638}\n{'loss': 1.0938, 'grad_norm': 1.6786218881607056, 'learning_rate': 5.1533309639364365e-06, 'epoch': 2.3863636363636362}\n{'loss': 1.1365, 'grad_norm': 1.6047111749649048, 'learning_rate': 5.081756922770652e-06, 'epoch': 2.409090909090909}\n{'loss': 1.1604, 'grad_norm': 1.5132535696029663, 'learning_rate': 5.010182881604869e-06, 'epoch': 2.4318181818181817}\n{'loss': 1.0575, 'grad_norm': 1.815501093864441, 'learning_rate': 4.9386088404390844e-06, 'epoch': 2.4545454545454546}\n{'loss': 1.087, 'grad_norm': 1.578349232673645, 'learning_rate': 4.867034799273301e-06, 'epoch': 2.4772727272727275}\n{'loss': 1.1527, 'grad_norm': 1.4497978687286377, 'learning_rate': 4.795460758107517e-06, 'epoch': 2.5}\n{'loss': 1.1173, 'grad_norm': 1.4703890085220337, 'learning_rate': 4.723886716941733e-06, 'epoch': 2.5227272727272725}\n{'loss': 1.1185, 'grad_norm': 1.4854024648666382, 'learning_rate': 4.65231267577595e-06, 'epoch': 2.5454545454545454}\n{'loss': 1.0889, 'grad_norm': 1.6751080751419067, 'learning_rate': 4.5807386346101655e-06, 'epoch': 2.5681818181818183}\n{'loss': 1.0498, 'grad_norm': 1.6008073091506958, 'learning_rate': 4.509164593444382e-06, 'epoch': 2.590909090909091}\n{'loss': 1.078, 'grad_norm': 1.5206719636917114, 'learning_rate': 4.437590552278598e-06, 'epoch': 2.6136363636363638}\n{'loss': 1.0725, 'grad_norm': 1.512196660041809, 'learning_rate': 4.366016511112814e-06, 'epoch': 2.6363636363636362}\n{'loss': 1.0736, 'grad_norm': 1.5160311460494995, 'learning_rate': 4.294442469947031e-06, 'epoch': 2.659090909090909}\n{'loss': 1.006, 'grad_norm': 1.6944730281829834, 'learning_rate': 4.2228684287812465e-06, 'epoch': 2.6818181818181817}\n{'loss': 1.0376, 'grad_norm': 1.4917062520980835, 'learning_rate': 4.151294387615463e-06, 'epoch': 2.7045454545454546}\n{'loss': 1.0716, 'grad_norm': 1.4618964195251465, 'learning_rate': 4.079720346449679e-06, 'epoch': 2.7272727272727275}\n{'loss': 1.0377, 'grad_norm': 1.5143741369247437, 'learning_rate': 4.008146305283895e-06, 'epoch': 2.75}\n{'loss': 1.0969, 'grad_norm': 1.4713006019592285, 'learning_rate': 3.936572264118111e-06, 'epoch': 2.7727272727272725}\n{'loss': 1.001, 'grad_norm': 1.8286594152450562, 'learning_rate': 3.864998222952327e-06, 'epoch': 2.7954545454545454}\n{'loss': 1.0887, 'grad_norm': 1.4124337434768677, 'learning_rate': 3.7934241817865437e-06, 'epoch': 2.8181818181818183}\n{'loss': 1.0643, 'grad_norm': 1.4818758964538574, 'learning_rate': 3.7218501406207594e-06, 'epoch': 2.840909090909091}\n{'loss': 1.0127, 'grad_norm': 1.5168564319610596, 'learning_rate': 3.6502760994549755e-06, 'epoch': 2.8636363636363638}\n{'loss': 1.0335, 'grad_norm': 1.5024620294570923, 'learning_rate': 3.578702058289192e-06, 'epoch': 2.8863636363636362}\n{'loss': 1.0443, 'grad_norm': 1.4203590154647827, 'learning_rate': 3.5071280171234078e-06, 'epoch': 2.909090909090909}\n{'loss': 1.0379, 'grad_norm': 1.3855324983596802, 'learning_rate': 3.4355539759576243e-06, 'epoch': 2.9318181818181817}\n{'loss': 1.0593, 'grad_norm': 1.3264693021774292, 'learning_rate': 3.36397993479184e-06, 'epoch': 2.9545454545454546}\n{'loss': 1.0335, 'grad_norm': 1.3628190755844116, 'learning_rate': 3.2924058936260566e-06, 'epoch': 2.9772727272727275}\n{'loss': 1.0521, 'grad_norm': 3.2217888832092285, 'learning_rate': 3.220831852460273e-06, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8652    0.1571    0.2658      3311\n     PERCENT     0.0000    0.0000    0.0000        86\n        TYPE     0.5882    0.9191    0.7173     11299\n      VOLUME     0.0000    0.0000    0.0000        42\n\n   micro avg     0.5973    0.7399    0.6610     14738\n   macro avg     0.3634    0.2690    0.2458     14738\nweighted avg     0.6453    0.7399    0.6097     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 1.0015735626220703, 'eval_f1_macro': 0.24579835612599238, 'eval_precision': 0.5973378615249781, 'eval_recall': 0.7399240059709594, 'eval_f1': 0.6610292780505547, 'eval_accuracy': 0.6458663461013059, 'eval_runtime': 1.4884, 'eval_samples_per_second': 3701.955, 'eval_steps_per_second': 7.39, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 1.0481, 'grad_norm': 1.3634649515151978, 'learning_rate': 3.149257811294489e-06, 'epoch': 3.022727272727273}\n{'loss': 1.0061, 'grad_norm': 1.507120966911316, 'learning_rate': 3.077683770128705e-06, 'epoch': 3.0454545454545454}\n{'loss': 1.0298, 'grad_norm': 1.384630560874939, 'learning_rate': 3.006109728962921e-06, 'epoch': 3.0681818181818183}\n{'loss': 1.0253, 'grad_norm': 1.3590863943099976, 'learning_rate': 2.934535687797137e-06, 'epoch': 3.090909090909091}\n{'loss': 1.044, 'grad_norm': 1.271601676940918, 'learning_rate': 2.8629616466313537e-06, 'epoch': 3.1136363636363638}\n{'loss': 1.0168, 'grad_norm': 1.3768301010131836, 'learning_rate': 2.79138760546557e-06, 'epoch': 3.1363636363636362}\n{'loss': 1.045, 'grad_norm': 1.270020604133606, 'learning_rate': 2.719813564299786e-06, 'epoch': 3.159090909090909}\n{'loss': 0.9907, 'grad_norm': 1.3973199129104614, 'learning_rate': 2.648239523134002e-06, 'epoch': 3.1818181818181817}\n{'loss': 1.0546, 'grad_norm': 1.229644536972046, 'learning_rate': 2.5766654819682182e-06, 'epoch': 3.2045454545454546}\n{'loss': 1.0096, 'grad_norm': 1.3093678951263428, 'learning_rate': 2.5050914408024344e-06, 'epoch': 3.227272727272727}\n{'loss': 1.0171, 'grad_norm': 1.298926830291748, 'learning_rate': 2.4335173996366505e-06, 'epoch': 3.25}\n{'loss': 1.0433, 'grad_norm': 1.2696884870529175, 'learning_rate': 2.3619433584708666e-06, 'epoch': 3.2727272727272725}\n{'loss': 1.0038, 'grad_norm': 1.3388564586639404, 'learning_rate': 2.2903693173050827e-06, 'epoch': 3.2954545454545454}\n{'loss': 1.0602, 'grad_norm': 1.2293821573257446, 'learning_rate': 2.218795276139299e-06, 'epoch': 3.3181818181818183}\n{'loss': 1.0233, 'grad_norm': 1.3781166076660156, 'learning_rate': 2.1472212349735154e-06, 'epoch': 3.340909090909091}\n{'loss': 1.0068, 'grad_norm': 1.3013169765472412, 'learning_rate': 2.0756471938077315e-06, 'epoch': 3.3636363636363638}\n{'loss': 1.0507, 'grad_norm': 1.298409104347229, 'learning_rate': 2.0040731526419477e-06, 'epoch': 3.3863636363636362}\n{'loss': 1.0464, 'grad_norm': 1.2018312215805054, 'learning_rate': 1.9324991114761634e-06, 'epoch': 3.409090909090909}\n{'loss': 1.0141, 'grad_norm': 1.3454821109771729, 'learning_rate': 1.8609250703103797e-06, 'epoch': 3.4318181818181817}\n{'loss': 1.0621, 'grad_norm': 1.3073893785476685, 'learning_rate': 1.789351029144596e-06, 'epoch': 3.4545454545454546}\n{'loss': 0.9755, 'grad_norm': 1.3862446546554565, 'learning_rate': 1.7177769879788122e-06, 'epoch': 3.4772727272727275}\n{'loss': 1.0195, 'grad_norm': 1.3178479671478271, 'learning_rate': 1.6462029468130283e-06, 'epoch': 3.5}\n{'loss': 1.0323, 'grad_norm': 1.2522071599960327, 'learning_rate': 1.5746289056472444e-06, 'epoch': 3.5227272727272725}\n{'loss': 1.0221, 'grad_norm': 1.322687029838562, 'learning_rate': 1.5030548644814605e-06, 'epoch': 3.5454545454545454}\n{'loss': 0.9707, 'grad_norm': 1.3663275241851807, 'learning_rate': 1.4314808233156769e-06, 'epoch': 3.5681818181818183}\n{'loss': 1.0233, 'grad_norm': 1.3423179388046265, 'learning_rate': 1.359906782149893e-06, 'epoch': 3.590909090909091}\n{'loss': 0.9991, 'grad_norm': 1.275644063949585, 'learning_rate': 1.2883327409841091e-06, 'epoch': 3.6136363636363638}\n{'loss': 0.9675, 'grad_norm': 1.3756405115127563, 'learning_rate': 1.2167586998183252e-06, 'epoch': 3.6363636363636362}\n{'loss': 0.9742, 'grad_norm': 1.367084264755249, 'learning_rate': 1.1451846586525414e-06, 'epoch': 3.659090909090909}\n{'loss': 0.9987, 'grad_norm': 1.3286465406417847, 'learning_rate': 1.0736106174867577e-06, 'epoch': 3.6818181818181817}\n{'loss': 1.0215, 'grad_norm': 1.356196403503418, 'learning_rate': 1.0020365763209738e-06, 'epoch': 3.7045454545454546}\n{'loss': 0.9866, 'grad_norm': 1.2940700054168701, 'learning_rate': 9.304625351551898e-07, 'epoch': 3.7272727272727275}\n{'loss': 1.0202, 'grad_norm': 1.1949204206466675, 'learning_rate': 8.588884939894061e-07, 'epoch': 3.75}\n{'loss': 0.9956, 'grad_norm': 1.330744743347168, 'learning_rate': 7.873144528236222e-07, 'epoch': 3.7727272727272725}\n{'loss': 0.9894, 'grad_norm': 1.286833643913269, 'learning_rate': 7.157404116578384e-07, 'epoch': 3.7954545454545454}\n{'loss': 0.9936, 'grad_norm': 1.283610463142395, 'learning_rate': 6.441663704920546e-07, 'epoch': 3.8181818181818183}\n{'loss': 1.0409, 'grad_norm': 1.2526077032089233, 'learning_rate': 5.725923293262707e-07, 'epoch': 3.840909090909091}\n{'loss': 1.0106, 'grad_norm': 1.276254415512085, 'learning_rate': 5.010182881604869e-07, 'epoch': 3.8636363636363638}\n{'loss': 0.9597, 'grad_norm': 1.4845385551452637, 'learning_rate': 4.2944424699470304e-07, 'epoch': 3.8863636363636362}\n{'loss': 1.0129, 'grad_norm': 1.2295325994491577, 'learning_rate': 3.578702058289192e-07, 'epoch': 3.909090909090909}\n{'loss': 1.0182, 'grad_norm': 1.281311273574829, 'learning_rate': 2.8629616466313534e-07, 'epoch': 3.9318181818181817}\n{'loss': 1.0084, 'grad_norm': 1.2574572563171387, 'learning_rate': 2.1472212349735152e-07, 'epoch': 3.9545454545454546}\n{'loss': 1.0431, 'grad_norm': 1.2234196662902832, 'learning_rate': 1.4314808233156767e-07, 'epoch': 3.9772727272727275}\n{'loss': 0.7601, 'grad_norm': 3.5464389324188232, 'learning_rate': 7.157404116578384e-08, 'epoch': 4.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8584    0.2344    0.3682      3311\n     PERCENT     0.0000    0.0000    0.0000        86\n        TYPE     0.6017    0.9173    0.7267     11299\n      VOLUME     0.0000    0.0000    0.0000        42\n\n   micro avg     0.6145    0.7559    0.6779     14738\n   macro avg     0.3650    0.2879    0.2737     14738\nweighted avg     0.6541    0.7559    0.6399     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.9619789123535156, 'eval_f1_macro': 0.2737285599643172, 'eval_precision': 0.6144724504991451, 'eval_recall': 0.7559370335187949, 'eval_f1': 0.677903191456996, 'eval_accuracy': 0.6654281186820392, 'eval_runtime': 1.5205, 'eval_samples_per_second': 3623.734, 'eval_steps_per_second': 7.234, 'epoch': 4.0}\n{'train_runtime': 24.85, 'train_samples_per_second': 3548.003, 'train_steps_per_second': 7.082, 'train_loss': 1.3460459069094874, 'epoch': 4.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n[I 2025-09-26 18:24:46,353] Trial 9 pruned. \n","output_type":"stream"},{"name":"stdout","text":"=== seqeval classification_report ===\n              precision    recall  f1-score   support\n\n       BRAND     0.8584    0.2344    0.3682      3311\n     PERCENT     0.0000    0.0000    0.0000        86\n        TYPE     0.6017    0.9173    0.7267     11299\n      VOLUME     0.0000    0.0000    0.0000        42\n\n   micro avg     0.6145    0.7559    0.6779     14738\n   macro avg     0.3650    0.2879    0.2737     14738\nweighted avg     0.6541    0.7559    0.6399     14738\n\nWarning: failed to write classification report to /content/logs/last_classification_report.txt: [Errno 2] No such file or directory: '/content/logs/last_classification_report.txt'\n{'eval_loss': 0.9619789123535156, 'eval_f1_macro': 0.2737285599643172, 'eval_precision': 0.6144724504991451, 'eval_recall': 0.7559370335187949, 'eval_f1': 0.677903191456996, 'eval_accuracy': 0.6654281186820392, 'eval_runtime': 1.5428, 'eval_samples_per_second': 3571.521, 'eval_steps_per_second': 7.13, 'epoch': 4.0}\nBest trial:\n  Value: 0.9175\n  Params:\n    learning_rate: 0.00037710330953711225\n    weight_decay: 0.040411977783360725\n    num_train_epochs: 10\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"doc[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T10:14:48.037216Z","iopub.execute_input":"2025-09-26T10:14:48.037490Z","iopub.status.idle":"2025-09-26T10:14:48.042412Z","shell.execute_reply.started":"2025-09-26T10:14:48.037472Z","shell.execute_reply":"2025-09-26T10:14:48.041544Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"'abon'"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"df_all_valid_docs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T10:11:58.486328Z","iopub.execute_input":"2025-09-26T10:11:58.486620Z","iopub.status.idle":"2025-09-26T10:11:58.518843Z","shell.execute_reply.started":"2025-09-26T10:11:58.486600Z","shell.execute_reply":"2025-09-26T10:11:58.518096Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"         id            tokens ner_tags  \\\n0         3            [abon]      [0]   \n1         6  [abtoys, игрушк]   [1, 3]   \n2        17          [active]      [1]   \n3        29           [agata]      [1]   \n4        30     [agnesi, пше]   [1, 3]   \n...     ...               ...      ...   \n5445  27232             [яыц]      [3]   \n5446  27233            [яыца]      [3]   \n5447  27240        [№1, газе]   [1, 3]   \n5448  27247    [№1, кофейник]   [1, 3]   \n5449  27249          [№1, са]   [1, 3]   \n\n                                         input_ids  \\\n0                               [8, 4877, 1146, 2]   \n1     [8, 4877, 2769, 448, 125, 5112, 319, 189, 2]   \n2                               [8, 4128, 2496, 2]   \n3                              [8, 6084, 12625, 2]   \n4               [8, 6084, 16805, 633, 454, 682, 2]   \n...                                            ...   \n5445                              [35, 19, 557, 2]   \n5446                              [35, 19, 518, 2]   \n5447                       [545, 471, 3762, 13, 2]   \n5448                     [545, 471, 18465, 588, 2]   \n5449                          [545, 471, 11, 7, 2]   \n\n                   attention_mask                          labels  \\\n0                    [1, 1, 1, 1]                 [0, 0, 0, -100]   \n1     [1, 1, 1, 1, 1, 1, 1, 1, 1]  [1, 1, 1, 1, 1, 3, 3, 3, -100]   \n2                    [1, 1, 1, 1]                 [1, 1, 1, -100]   \n3                    [1, 1, 1, 1]                 [1, 1, 1, -100]   \n4           [1, 1, 1, 1, 1, 1, 1]        [1, 1, 1, 1, 3, 3, -100]   \n...                           ...                             ...   \n5445                 [1, 1, 1, 1]                 [3, 3, 3, -100]   \n5446                 [1, 1, 1, 1]                 [3, 3, 3, -100]   \n5447              [1, 1, 1, 1, 1]              [1, 1, 3, 3, -100]   \n5448              [1, 1, 1, 1, 1]              [1, 1, 3, 3, -100]   \n5449              [1, 1, 1, 1, 1]              [1, 1, 3, 3, -100]   \n\n                                     predicted_ner_tags  \n0     [(0, 3, B-TYPE), (4, 10, I-TYPE), (11, 20, I-T...  \n1     [(0, 3, B-TYPE), (4, 10, I-TYPE), (11, 20, I-T...  \n2     [(0, 3, B-TYPE), (4, 10, I-TYPE), (11, 20, I-T...  \n3     [(0, 3, B-TYPE), (4, 10, I-TYPE), (11, 20, I-T...  \n4     [(0, 3, B-TYPE), (4, 10, I-TYPE), (11, 20, I-T...  \n...                                                 ...  \n5445  [(0, 3, B-TYPE), (4, 10, B-BRAND), (11, 20, I-...  \n5446  [(0, 3, B-TYPE), (4, 10, B-BRAND), (11, 20, I-...  \n5447  [(0, 3, B-TYPE), (4, 10, B-BRAND), (11, 20, I-...  \n5448  [(0, 3, B-TYPE), (4, 10, B-BRAND), (11, 20, I-...  \n5449  [(0, 3, B-TYPE), (4, 10, B-BRAND), (11, 20, I-...  \n\n[27251 rows x 7 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>tokens</th>\n      <th>ner_tags</th>\n      <th>input_ids</th>\n      <th>attention_mask</th>\n      <th>labels</th>\n      <th>predicted_ner_tags</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3</td>\n      <td>[abon]</td>\n      <td>[0]</td>\n      <td>[8, 4877, 1146, 2]</td>\n      <td>[1, 1, 1, 1]</td>\n      <td>[0, 0, 0, -100]</td>\n      <td>[(0, 3, B-TYPE), (4, 10, I-TYPE), (11, 20, I-T...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6</td>\n      <td>[abtoys, игрушк]</td>\n      <td>[1, 3]</td>\n      <td>[8, 4877, 2769, 448, 125, 5112, 319, 189, 2]</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n      <td>[1, 1, 1, 1, 1, 3, 3, 3, -100]</td>\n      <td>[(0, 3, B-TYPE), (4, 10, I-TYPE), (11, 20, I-T...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>17</td>\n      <td>[active]</td>\n      <td>[1]</td>\n      <td>[8, 4128, 2496, 2]</td>\n      <td>[1, 1, 1, 1]</td>\n      <td>[1, 1, 1, -100]</td>\n      <td>[(0, 3, B-TYPE), (4, 10, I-TYPE), (11, 20, I-T...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>29</td>\n      <td>[agata]</td>\n      <td>[1]</td>\n      <td>[8, 6084, 12625, 2]</td>\n      <td>[1, 1, 1, 1]</td>\n      <td>[1, 1, 1, -100]</td>\n      <td>[(0, 3, B-TYPE), (4, 10, I-TYPE), (11, 20, I-T...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>30</td>\n      <td>[agnesi, пше]</td>\n      <td>[1, 3]</td>\n      <td>[8, 6084, 16805, 633, 454, 682, 2]</td>\n      <td>[1, 1, 1, 1, 1, 1, 1]</td>\n      <td>[1, 1, 1, 1, 3, 3, -100]</td>\n      <td>[(0, 3, B-TYPE), (4, 10, I-TYPE), (11, 20, I-T...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5445</th>\n      <td>27232</td>\n      <td>[яыц]</td>\n      <td>[3]</td>\n      <td>[35, 19, 557, 2]</td>\n      <td>[1, 1, 1, 1]</td>\n      <td>[3, 3, 3, -100]</td>\n      <td>[(0, 3, B-TYPE), (4, 10, B-BRAND), (11, 20, I-...</td>\n    </tr>\n    <tr>\n      <th>5446</th>\n      <td>27233</td>\n      <td>[яыца]</td>\n      <td>[3]</td>\n      <td>[35, 19, 518, 2]</td>\n      <td>[1, 1, 1, 1]</td>\n      <td>[3, 3, 3, -100]</td>\n      <td>[(0, 3, B-TYPE), (4, 10, B-BRAND), (11, 20, I-...</td>\n    </tr>\n    <tr>\n      <th>5447</th>\n      <td>27240</td>\n      <td>[№1, газе]</td>\n      <td>[1, 3]</td>\n      <td>[545, 471, 3762, 13, 2]</td>\n      <td>[1, 1, 1, 1, 1]</td>\n      <td>[1, 1, 3, 3, -100]</td>\n      <td>[(0, 3, B-TYPE), (4, 10, B-BRAND), (11, 20, I-...</td>\n    </tr>\n    <tr>\n      <th>5448</th>\n      <td>27247</td>\n      <td>[№1, кофейник]</td>\n      <td>[1, 3]</td>\n      <td>[545, 471, 18465, 588, 2]</td>\n      <td>[1, 1, 1, 1, 1]</td>\n      <td>[1, 1, 3, 3, -100]</td>\n      <td>[(0, 3, B-TYPE), (4, 10, B-BRAND), (11, 20, I-...</td>\n    </tr>\n    <tr>\n      <th>5449</th>\n      <td>27249</td>\n      <td>[№1, са]</td>\n      <td>[1, 3]</td>\n      <td>[545, 471, 11, 7, 2]</td>\n      <td>[1, 1, 1, 1, 1]</td>\n      <td>[1, 1, 3, 3, -100]</td>\n      <td>[(0, 3, B-TYPE), (4, 10, B-BRAND), (11, 20, I-...</td>\n    </tr>\n  </tbody>\n</table>\n<p>27251 rows × 7 columns</p>\n</div>"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"df_all_valid_docs.iloc[-1]['predicted_ner_tags']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T10:12:48.842684Z","iopub.execute_input":"2025-09-26T10:12:48.842951Z","iopub.status.idle":"2025-09-26T10:12:48.848452Z","shell.execute_reply.started":"2025-09-26T10:12:48.842933Z","shell.execute_reply":"2025-09-26T10:12:48.847869Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"[(0, 3, 'B-TYPE'), (4, 10, 'I-TYPE'), (11, 20, 'I-TYPE')]"},"metadata":{}}],"execution_count":19},{"cell_type":"markdown","source":"## где ошибка в трейне","metadata":{"id":"PBNShRveLSnW"}},{"cell_type":"code","source":"df_valid_final[df_valid_final.annotation.astype(str) != df_valid_final.ann.astype(str)].to_csv('comparison_valid.csv', index=False)","metadata":{"id":"I4FzZLmhh-ek"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"folds[0]","metadata":{"id":"ytXg6f7xiaOK","colab":{"base_uri":"https://localhost:8080/"},"outputId":"370ae089-5651-4d3e-c8a7-b815c2333643"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['id', 'tokens', 'ner_tags'],\n","        num_rows: 21800\n","    })\n","    validation: Dataset({\n","        features: ['id', 'tokens', 'ner_tags'],\n","        num_rows: 5451\n","    })\n","})"]},"metadata":{},"execution_count":21}],"execution_count":null},{"cell_type":"code","source":"dd = pd.DataFrame()\ndd.empty","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2ZSjgCa0NkKY","outputId":"c93044c0-7d81-4222-8641-97ae78411c76"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":23}],"execution_count":null},{"cell_type":"markdown","source":"## итоговая тренировка","metadata":{}},{"cell_type":"code","source":"data_collator = DataCollatorForTokenClassification(tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T17:39:59.473402Z","iopub.execute_input":"2025-09-26T17:39:59.473722Z","iopub.status.idle":"2025-09-26T17:39:59.478431Z","shell.execute_reply.started":"2025-09-26T17:39:59.473698Z","shell.execute_reply":"2025-09-26T17:39:59.477418Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained('cointegrated/rubert-tiny2',\n                                          use_fast=True,\n                                          add_prefix_space=True)\n\nmodel = AutoModelForTokenClassification.from_pretrained('cointegrated/rubert-tiny2',\n                                                        num_labels=len(lbls_in_dataset),\n                                                        id2label=id2label,\n                                                        label2id=label2id).to(\"cuda\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T18:35:34.062206Z","iopub.execute_input":"2025-09-26T18:35:34.062982Z","iopub.status.idle":"2025-09-26T18:35:34.533901Z","shell.execute_reply.started":"2025-09-26T18:35:34.062947Z","shell.execute_reply":"2025-09-26T18:35:34.533329Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"\nds_train = ds.map(tokenize_and_align_labels,\n                                          batched=True,\n                                          fn_kwargs={'tokenizer': tokenizer})\n\ntraining_args = TrainingArguments(\n    eval_strategy=\"no\",  # No evaluation during training\n    torch_compile=True,\n    per_device_train_batch_size=256,\n    learning_rate=0.00037710330953711225,          # fixed value (was suggested by Optuna before)\n    weight_decay=0.040411977783360725,\n    num_train_epochs=6,          # fixed\n    seed=42,\n    data_seed=24,\n    gradient_accumulation_steps=1,\n    warmup_ratio=0.1,\n    report_to=None,\n    logging_dir=\"./logs\",\n    logging_steps=20,\n    )\n\n# Initialize Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=ds_train,\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n\n)\n\n# Start training\nprint(\"Starting training without evaluation...\")\ntrainer.train()\ntrained_model = trainer.model\n\n\n","metadata":{"id":"wwzsP16fPLTe","trusted":true,"execution":{"iopub.status.busy":"2025-09-26T18:35:37.317210Z","iopub.execute_input":"2025-09-26T18:35:37.317483Z","iopub.status.idle":"2025-09-26T18:36:05.364853Z","shell.execute_reply.started":"2025-09-26T18:35:37.317462Z","shell.execute_reply":"2025-09-26T18:36:05.364293Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/27552 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"006f998ec16147a6b1cdbfd391f2963b"}},"metadata":{}},{"name":"stderr","text":"The speedups for torchdynamo mostly come with GPU Ampere or higher and which is not detected here.\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/tmp/ipykernel_36/1467123832.py:22: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"name":"stdout","text":"Starting training without evaluation...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='324' max='324' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [324/324 00:26, Epoch 6/6]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>1.578600</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.612800</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.375000</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.290900</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.239900</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.194600</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.163800</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.164500</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.127400</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.105300</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.109600</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.077900</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.085200</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.079800</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.063900</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>0.063500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":35},{"cell_type":"code","source":"ds_train.select(range(5)).to_pandas()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T14:44:49.025512Z","iopub.execute_input":"2025-09-26T14:44:49.026363Z","iopub.status.idle":"2025-09-26T14:44:49.060771Z","shell.execute_reply.started":"2025-09-26T14:44:49.026326Z","shell.execute_reply":"2025-09-26T14:44:49.060064Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"   id    tokens ner_tags                  input_ids   attention_mask  \\\n0   0      [aa]      [0]              [351, 500, 2]        [1, 1, 1]   \n1   1    [aala]      [0]         [351, 700, 500, 2]     [1, 1, 1, 1]   \n2   2  [aarcca]      [0]  [351, 1228, 679, 4701, 2]  [1, 1, 1, 1, 1]   \n3   3    [abon]      [0]         [8, 4877, 1146, 2]     [1, 1, 1, 1]   \n4   4    [abso]      [1]        [8, 4877, 12364, 2]     [1, 1, 1, 1]   \n\n               labels  \n0        [0, 0, -100]  \n1     [0, 0, 0, -100]  \n2  [0, 0, 0, 0, -100]  \n3     [0, 0, 0, -100]  \n4     [1, 1, 1, -100]  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>tokens</th>\n      <th>ner_tags</th>\n      <th>input_ids</th>\n      <th>attention_mask</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>[aa]</td>\n      <td>[0]</td>\n      <td>[351, 500, 2]</td>\n      <td>[1, 1, 1]</td>\n      <td>[0, 0, -100]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>[aala]</td>\n      <td>[0]</td>\n      <td>[351, 700, 500, 2]</td>\n      <td>[1, 1, 1, 1]</td>\n      <td>[0, 0, 0, -100]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>[aarcca]</td>\n      <td>[0]</td>\n      <td>[351, 1228, 679, 4701, 2]</td>\n      <td>[1, 1, 1, 1, 1]</td>\n      <td>[0, 0, 0, 0, -100]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>[abon]</td>\n      <td>[0]</td>\n      <td>[8, 4877, 1146, 2]</td>\n      <td>[1, 1, 1, 1]</td>\n      <td>[0, 0, 0, -100]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>[abso]</td>\n      <td>[1]</td>\n      <td>[8, 4877, 12364, 2]</td>\n      <td>[1, 1, 1, 1]</td>\n      <td>[1, 1, 1, -100]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nimport re\n\ndef predict_all_entities(text: str, model, tokenizer, id2label, device=None, debug=False):\n    \"\"\"\n    Word-level inference with original character spans (start_idx, end_idx, ENTITY).\n    Returns a list of tuples for each word (including 'O').\n    \"\"\"\n\n    model.eval()\n    if device is None:\n        device = next(model.parameters()).device\n\n    # --- find words and their char spans in original text ---\n    words = []\n    spans = []\n    for match in re.finditer(r\"\\S+\", text):\n        words.append(match.group())\n        spans.append(match.span())  # (start_idx, end_idx)\n\n    # encode with word-level info\n    enc = tokenizer(\n        words,\n        is_split_into_words=True,\n        return_tensors=\"pt\",\n        truncation=True\n    )\n\n    input_ids = enc[\"input_ids\"].to(device)\n    attention_mask = enc[\"attention_mask\"].to(device)\n    word_ids = enc.word_ids(batch_index=0)\n\n    with torch.no_grad():\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        logits = outputs.logits[0]               # (seq_len, num_labels)\n        probs = F.softmax(logits, dim=-1)        # (seq_len, num_labels)\n    # print()\n    # print(probs)\n    results = []\n    prev_word_idx = None\n    # print(word_ids)\n    for token_idx, word_idx in enumerate(word_ids):\n        if debug:\n            print(token_idx, word_idx, logits[token_idx])\n        if word_idx is None:\n            prev_word_idx = None\n            continue\n\n        # only take first subtoken per word\n        if word_idx != prev_word_idx:\n            label_id = int(torch.argmax(logits[token_idx]).cpu().numpy())\n            label = id2label[label_id]\n\n            start_idx, end_idx = spans[word_idx]\n            results.append((start_idx, end_idx, label))\n\n        prev_word_idx = word_idx\n\n    return results\n\n\n# -------------------------\n# Example usage\n# -------------------------\ns = \"сыр натура сливочный\"\n\n# Example id2label (replace with your mapping)\n# id2label = {0: \"O\", 1: \"B-TYPE\", 2: \"I-TYPE\", ...}\n\nres = predict_all_entities(s, trained_model, tokenizer, id2label)\nprint(res)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T18:36:41.456627Z","iopub.execute_input":"2025-09-26T18:36:41.456895Z","iopub.status.idle":"2025-09-26T18:36:41.470244Z","shell.execute_reply.started":"2025-09-26T18:36:41.456874Z","shell.execute_reply":"2025-09-26T18:36:41.469671Z"}},"outputs":[{"name":"stdout","text":"[(0, 3, 'B-TYPE'), (4, 10, 'O'), (11, 20, 'O')]\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"df_validation = pd.read_csv(\"/kaggle/input/submission/submission.csv\", sep=\";\", usecols=['sample'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T18:36:43.833948Z","iopub.execute_input":"2025-09-26T18:36:43.834468Z","iopub.status.idle":"2025-09-26T18:36:43.847634Z","shell.execute_reply.started":"2025-09-26T18:36:43.834448Z","shell.execute_reply":"2025-09-26T18:36:43.847014Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"predicted_ner_tags = []\nfor doc in tqdm(df_validation['sample'].tolist()):\n    predicted_doc_ner = predict_all_entities(doc, trained_model, tokenizer, id2label)\n    predicted_ner_tags.append(predicted_doc_ner)\ndf_validation['annotation'] = predicted_ner_tags","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T18:36:45.724785Z","iopub.execute_input":"2025-09-26T18:36:45.724996Z","iopub.status.idle":"2025-09-26T18:36:59.251947Z","shell.execute_reply.started":"2025-09-26T18:36:45.724981Z","shell.execute_reply":"2025-09-26T18:36:59.251406Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 5000/5000 [00:13<00:00, 369.85it/s]\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"df_validation.to_csv('submission_rubert_tiny2_ft_augmented_new_6_epochs_260925.csv', index=False, sep=';')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T18:37:04.649320Z","iopub.execute_input":"2025-09-26T18:37:04.649892Z","iopub.status.idle":"2025-09-26T18:37:04.669589Z","shell.execute_reply.started":"2025-09-26T18:37:04.649870Z","shell.execute_reply":"2025-09-26T18:37:04.669053Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"predict_all_entities(\"garner ructi\", trained_model, tokenizer, id2label, debug=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T14:45:16.688508Z","iopub.execute_input":"2025-09-26T14:45:16.689115Z","iopub.status.idle":"2025-09-26T14:45:16.796599Z","shell.execute_reply.started":"2025-09-26T14:45:16.689093Z","shell.execute_reply":"2025-09-26T14:45:16.795957Z"}},"outputs":[{"name":"stdout","text":"0 0 tensor([ 6.5906, 10.1643, -2.2721,  3.4366, -4.5278,  2.5592, -2.8466, -6.9580,\n        -3.9529], device='cuda:0')\n1 0 tensor([ 7.7309, 10.9714, -3.8946,  3.1987, -4.7314,  3.2854, -1.5130, -5.1659,\n        -4.1112], device='cuda:0')\n2 0 tensor([ 7.2921, 10.1729, -5.3683,  2.7412, -4.5518,  4.3491, -3.4186, -4.6163,\n        -5.3117], device='cuda:0')\n3 1 tensor([ 5.6538,  3.8720, -0.9463,  3.5795, -3.4112,  5.8417, -3.4994, -3.0971,\n        -2.3954], device='cuda:0')\n4 1 tensor([ 6.2706,  6.4004, -1.8007,  6.0896, -2.7667,  3.7546, -2.8255, -1.3013,\n        -5.2245], device='cuda:0')\n5 1 tensor([ 6.1525,  4.2112, -3.2930,  6.1238, -4.6684,  3.9089, -2.6030, -0.5535,\n        -2.8735], device='cuda:0')\n6 1 tensor([ 6.2605,  5.1263, -3.0759,  4.7641, -2.6424,  4.7713, -2.8157, -2.3314,\n        -4.1695], device='cuda:0')\n7 None tensor([ 1.7911, -0.1701,  0.5656, -0.5731,  1.6106, -0.8561,  2.0464,  2.4553,\n         1.5692], device='cuda:0')\n","output_type":"stream"},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"[(0, 6, 'B-BRAND'), (7, 12, 'I-BRAND')]"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"id2label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T13:18:11.560728Z","iopub.execute_input":"2025-09-26T13:18:11.561004Z","iopub.status.idle":"2025-09-26T13:18:11.565846Z","shell.execute_reply.started":"2025-09-26T13:18:11.560984Z","shell.execute_reply":"2025-09-26T13:18:11.565089Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"{0: 'O',\n 1: 'B-BRAND',\n 2: 'B-PERCENT',\n 3: 'B-TYPE',\n 4: 'B-VOLUME',\n 5: 'I-BRAND',\n 6: 'I-PERCENT',\n 7: 'I-TYPE',\n 8: 'I-VOLUME'}"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"tokenizer.tokenize(\" garner ructi\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T13:18:15.784375Z","iopub.execute_input":"2025-09-26T13:18:15.784632Z","iopub.status.idle":"2025-09-26T13:18:15.789648Z","shell.execute_reply.started":"2025-09-26T13:18:15.784613Z","shell.execute_reply":"2025-09-26T13:18:15.789081Z"}},"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"['▁', 'gar', 'ner', '▁', 'ru', 'c', 'ti']"},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"# молоко 1,5 % -- не находит процент\n# стейк говядина --  проверить усреднение\n# сок 2 литра яблочный -- не находит volume ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T10:55:52.952042Z","iopub.execute_input":"2025-09-26T10:55:52.952738Z","iopub.status.idle":"2025-09-26T10:55:52.956844Z","shell.execute_reply.started":"2025-09-26T10:55:52.952706Z","shell.execute_reply":"2025-09-26T10:55:52.955806Z"}},"outputs":[],"execution_count":82},{"cell_type":"code","source":"predict_all_entities(\"schwartau со\", trained_model, tokenizer, id2label, debug=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T13:20:55.738312Z","iopub.execute_input":"2025-09-26T13:20:55.739171Z","iopub.status.idle":"2025-09-26T13:20:55.786446Z","shell.execute_reply.started":"2025-09-26T13:20:55.739138Z","shell.execute_reply":"2025-09-26T13:20:55.785714Z"}},"outputs":[{"name":"stdout","text":"0 0 tensor([ 2.4230e+00,  1.3040e+01, -6.8861e-03,  9.9319e-01, -1.6785e+00,\n        -1.4810e+00, -8.7815e-01, -2.5692e-01, -1.3998e+00], device='cuda:0')\n1 0 tensor([ 2.2394, 12.3609,  0.4930,  1.3229, -1.7823, -0.0829, -1.3356, -0.2234,\n        -0.8064], device='cuda:0')\n2 0 tensor([ 2.6557e+00,  1.3508e+01,  1.0873e-02,  1.7207e+00, -2.1920e+00,\n         4.2342e-01, -7.9274e-01, -9.1873e-02, -1.4111e+00], device='cuda:0')\n3 0 tensor([ 2.6026, 13.6531, -0.9042,  1.6521, -2.1215,  0.8590, -0.8216, -1.2197,\n        -0.8692], device='cuda:0')\n4 0 tensor([ 3.3646, 14.1260,  0.9424,  1.7963, -0.5018,  1.6216, -1.6133, -2.3228,\n        -1.5213], device='cuda:0')\n5 1 tensor([ 6.6762, -1.6012, -2.8019, 11.6132, -3.1620,  1.1287, -1.9783,  0.8047,\n        -2.5529], device='cuda:0')\n6 None tensor([ 6.3755, -2.4741, -3.1129, 14.5177, -3.3633, -0.2019, -3.0747,  1.3072,\n        -4.7479], device='cuda:0')\n","output_type":"stream"},{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"[(0, 9, 'B-BRAND'), (10, 12, 'B-TYPE')]"},"metadata":{}}],"execution_count":37},{"cell_type":"code","source":"tokenizer.tokenize(\"крем jundo\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T13:19:50.266235Z","iopub.execute_input":"2025-09-26T13:19:50.266535Z","iopub.status.idle":"2025-09-26T13:19:50.271674Z","shell.execute_reply.started":"2025-09-26T13:19:50.266508Z","shell.execute_reply":"2025-09-26T13:19:50.271150Z"}},"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"['▁крем', '▁j', 'und', 'o']"},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer2 = AutoTokenizer.from_pretrained('ai-forever/ruT5-large',\n                                          use_fast=True,\n                                          add_prefix_space=True)\n\nmodel2 = AutoModel.from_pretrained('ai-forever/ruT5-large').to(\"cuda\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T14:53:47.663258Z","iopub.execute_input":"2025-09-26T14:53:47.663911Z","iopub.status.idle":"2025-09-26T14:53:55.954353Z","shell.execute_reply.started":"2025-09-26T14:53:47.663890Z","shell.execute_reply":"2025-09-26T14:53:55.953505Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"model2.encoder","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T14:54:41.455613Z","iopub.execute_input":"2025-09-26T14:54:41.456355Z","iopub.status.idle":"2025-09-26T14:54:41.462526Z","shell.execute_reply.started":"2025-09-26T14:54:41.456329Z","shell.execute_reply":"2025-09-26T14:54:41.461765Z"}},"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"T5Stack(\n  (embed_tokens): Embedding(32128, 1024)\n  (block): ModuleList(\n    (0): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=1024, out_features=1024, bias=False)\n            (k): Linear(in_features=1024, out_features=1024, bias=False)\n            (v): Linear(in_features=1024, out_features=1024, bias=False)\n            (o): Linear(in_features=1024, out_features=1024, bias=False)\n            (relative_attention_bias): Embedding(32, 16)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerFF(\n          (DenseReluDense): T5DenseActDense(\n            (wi): Linear(in_features=1024, out_features=4096, bias=False)\n            (wo): Linear(in_features=4096, out_features=1024, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (act): ReLU()\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (1-23): 23 x T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=1024, out_features=1024, bias=False)\n            (k): Linear(in_features=1024, out_features=1024, bias=False)\n            (v): Linear(in_features=1024, out_features=1024, bias=False)\n            (o): Linear(in_features=1024, out_features=1024, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerFF(\n          (DenseReluDense): T5DenseActDense(\n            (wi): Linear(in_features=1024, out_features=4096, bias=False)\n            (wo): Linear(in_features=4096, out_features=1024, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (act): ReLU()\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (final_layer_norm): T5LayerNorm()\n  (dropout): Dropout(p=0.1, inplace=False)\n)"},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"trained_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T14:55:05.587619Z","iopub.execute_input":"2025-09-26T14:55:05.588331Z","iopub.status.idle":"2025-09-26T14:55:05.594419Z","shell.execute_reply.started":"2025-09-26T14:55:05.588307Z","shell.execute_reply":"2025-09-26T14:55:05.593668Z"}},"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"T5ForTokenClassification(\n  (transformer): T5EncoderModel(\n    (shared): Embedding(32128, 1024)\n    (encoder): T5Stack(\n      (embed_tokens): Embedding(32128, 1024)\n      (block): ModuleList(\n        (0): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n                (relative_attention_bias): Embedding(32, 16)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=1024, out_features=4096, bias=False)\n                (wo): Linear(in_features=4096, out_features=1024, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (1-23): 23 x T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=1024, out_features=4096, bias=False)\n                (wo): Linear(in_features=4096, out_features=1024, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n      (final_layer_norm): T5LayerNorm()\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n  )\n  (dropout): Dropout(p=0.0, inplace=False)\n  (classifier): Linear(in_features=1024, out_features=9, bias=True)\n)"},"metadata":{}}],"execution_count":39},{"cell_type":"code","source":"trainer.save_model(\"/kaggle/working/rubert_tiny2_ft_optuna_260925\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T18:45:33.674912Z","iopub.execute_input":"2025-09-26T18:45:33.675602Z","iopub.status.idle":"2025-09-26T18:45:34.033492Z","shell.execute_reply.started":"2025-09-26T18:45:33.675572Z","shell.execute_reply":"2025-09-26T18:45:34.032672Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"tr","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}