{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ef710e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import math\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, trainers\n",
    "from transformers import (\n",
    "AutoConfig,\n",
    "AutoModelForMaskedLM,\n",
    "AutoTokenizer,\n",
    "PreTrainedTokenizerFast,\n",
    "DataCollatorForLanguageModeling,\n",
    "DataCollatorForWholeWordMask,\n",
    "Trainer,\n",
    "TrainingArguments,\n",
    ")\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a76b210",
   "metadata": {},
   "source": [
    "### собираем данные"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a588dce",
   "metadata": {},
   "source": [
    "#### пятерочка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b21958e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def parse_product_jsons(folder_path):\n",
    "    all_data = []\n",
    "    \n",
    "    # Find all JSON files in the folder\n",
    "    json_files = glob.glob(os.path.join(folder_path, \"*.json\"))\n",
    "    \n",
    "    print(f\"Found {len(json_files)} JSON files\")\n",
    "    \n",
    "    for file_path in json_files:\n",
    "        if 'products_list' in file_path:\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    data = json.load(f)\n",
    "                \n",
    "                # Extract category information\n",
    "                category_id = data.get('category_id', '')\n",
    "                category_name = data.get('category_name', '')\n",
    "                \n",
    "                # Extract products list\n",
    "                products_list = data.get('products_list', [])\n",
    "                \n",
    "                for product in products_list:\n",
    "                    product_info = product.get('product_info', {})\n",
    "                    \n",
    "                    # Extract product details\n",
    "                    plu = product_info.get('plu', '')\n",
    "                    name = product_info.get('name', '')\n",
    "                    property = product_info.get('property', '')\n",
    "                    \n",
    "                    # Append to our data list\n",
    "                    all_data.append({\n",
    "                        'plu': plu,\n",
    "                        'name': name,\n",
    "                        'property': property,\n",
    "                        'category_name': category_name,\n",
    "                        'category_id': category_id\n",
    "                    })\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file_path}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(all_data)\n",
    "    \n",
    "    print(f\"Successfully parsed {len(df)} products\")\n",
    "    print(f\"DataFrame shape: {df.shape}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "41b1ee9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 206 JSON files\n",
      "Successfully parsed 25253 products\n",
      "DataFrame shape: (25253, 5)\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"/home/mikhail/Documents/Хакатоны/X5_ner_MiLky_way/parser/parsed_catalog_5ka\"\n",
    "\n",
    "# Parse the JSON files\n",
    "df_5ka = parse_product_jsons(folder_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a598faff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25253, 5)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_5ka.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ecf024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "plu",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "property",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "category_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "category_id",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "518c0b54-95f8-4037-b136-030e3206e5da",
       "rows": [
        [
         "0",
         "4304864",
         "Нектар Rich вишня 900мл",
         "900 мл",
         "Для особых случаев",
         "251C42818"
        ],
        [
         "1",
         "4360696",
         "Чай черный Rich Персик холодный 1.5л",
         "1.5 л",
         "Для особых случаев",
         "251C42818"
        ],
        [
         "2",
         "4378400",
         "Напиток Добрый Апельсин-мандарин для детского питания сокосодержащий 1.45л",
         "1.45 л",
         "Для особых случаев",
         "251C42818"
        ],
        [
         "3",
         "58053",
         "Нектар Добрый мультифрукт 1л",
         "1 л",
         "Для особых случаев",
         "251C42818"
        ],
        [
         "4",
         "4274867",
         "Напиток Добрый Cola без сахара газированный 1.5л",
         "1.5 л",
         "Для особых случаев",
         "251C42818"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>plu</th>\n",
       "      <th>name</th>\n",
       "      <th>property</th>\n",
       "      <th>category_name</th>\n",
       "      <th>category_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4304864</td>\n",
       "      <td>Нектар Rich вишня 900мл</td>\n",
       "      <td>900 мл</td>\n",
       "      <td>Для особых случаев</td>\n",
       "      <td>251C42818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4360696</td>\n",
       "      <td>Чай черный Rich Персик холодный 1.5л</td>\n",
       "      <td>1.5 л</td>\n",
       "      <td>Для особых случаев</td>\n",
       "      <td>251C42818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4378400</td>\n",
       "      <td>Напиток Добрый Апельсин-мандарин для детского ...</td>\n",
       "      <td>1.45 л</td>\n",
       "      <td>Для особых случаев</td>\n",
       "      <td>251C42818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58053</td>\n",
       "      <td>Нектар Добрый мультифрукт 1л</td>\n",
       "      <td>1 л</td>\n",
       "      <td>Для особых случаев</td>\n",
       "      <td>251C42818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4274867</td>\n",
       "      <td>Напиток Добрый Cola без сахара газированный 1.5л</td>\n",
       "      <td>1.5 л</td>\n",
       "      <td>Для особых случаев</td>\n",
       "      <td>251C42818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       plu                                               name property  \\\n",
       "0  4304864                            Нектар Rich вишня 900мл   900 мл   \n",
       "1  4360696               Чай черный Rich Персик холодный 1.5л    1.5 л   \n",
       "2  4378400  Напиток Добрый Апельсин-мандарин для детского ...   1.45 л   \n",
       "3    58053                       Нектар Добрый мультифрукт 1л      1 л   \n",
       "4  4274867   Напиток Добрый Cola без сахара газированный 1.5л    1.5 л   \n",
       "\n",
       "        category_name category_id  \n",
       "0  Для особых случаев   251C42818  \n",
       "1  Для особых случаев   251C42818  \n",
       "2  Для особых случаев   251C42818  \n",
       "3  Для особых случаев   251C42818  \n",
       "4  Для особых случаев   251C42818  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_5ka.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c7072872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "property",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "82af9e80-69dd-4ed8-a99c-60f8a1afa21e",
       "rows": [
        [
         "1 шт",
         "1992"
        ],
        [
         "200 г",
         "1146"
        ],
        [
         "100 г",
         "925"
        ],
        [
         "250 г",
         "894"
        ],
        [
         "300 г",
         "802"
        ],
        [
         "400 г",
         "742"
        ],
        [
         "500 мл",
         "661"
        ],
        [
         "500 г",
         "592"
        ],
        [
         "1 л",
         "574"
        ],
        [
         "150 г",
         "559"
        ],
        [
         "180 г",
         "404"
        ],
        [
         "200 мл",
         "388"
        ],
        [
         "70 г",
         "356"
        ],
        [
         "90 г",
         "353"
        ],
        [
         "75 г",
         "344"
        ],
        [
         "Цена за 1 кг",
         "338"
        ],
        [
         "80 г",
         "325"
        ],
        [
         "450 г",
         "319"
        ],
        [
         "250 мл",
         "313"
        ],
        [
         "350 г",
         "292"
        ],
        [
         "50 г",
         "291"
        ],
        [
         "40 г",
         "281"
        ],
        [
         "Цена за 100 г",
         "268"
        ],
        [
         "120 г",
         "243"
        ],
        [
         "130 г",
         "238"
        ],
        [
         "450 мл",
         "232"
        ],
        [
         "1 кг",
         "226"
        ],
        [
         "10 шт",
         "217"
        ],
        [
         "300 мл",
         "208"
        ],
        [
         "85 г",
         "207"
        ],
        [
         "400 мл",
         "202"
        ],
        [
         "60 г",
         "199"
        ],
        [
         "1.5 л",
         "198"
        ],
        [
         "750 мл",
         "196"
        ],
        [
         "240 г",
         "187"
        ],
        [
         "330 мл",
         "187"
        ],
        [
         "160 г",
         "157"
        ],
        [
         "140 г",
         "155"
        ],
        [
         "30 г",
         "153"
        ],
        [
         "270 г",
         "145"
        ],
        [
         "20 шт",
         "145"
        ],
        [
         "190 г",
         "142"
        ],
        [
         "2 шт",
         "136"
        ],
        [
         "125 г",
         "134"
        ],
        [
         "220 г",
         "131"
        ],
        [
         "110 г",
         "124"
        ],
        [
         "20 г",
         "123"
        ],
        [
         "600 г",
         "121"
        ],
        [
         "800 г",
         "110"
        ],
        [
         "230 г",
         "108"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 713
       }
      },
      "text/plain": [
       "property\n",
       "1 шт       1992\n",
       "200 г      1146\n",
       "100 г       925\n",
       "250 г       894\n",
       "300 г       802\n",
       "           ... \n",
       "4.75 кг       1\n",
       "2.3 л         1\n",
       "35 шт         1\n",
       "3.78 л        1\n",
       "7 мл          1\n",
       "Name: count, Length: 713, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_5ka.property.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2cab43fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5ka['name_processed'] = df_5ka['name'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cde1c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "names_processed_5ka = list(set(df_5ka['name_processed']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a1a9f651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19660"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(names_processed_5ka)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6de485",
   "metadata": {},
   "source": [
    "#### перекресток"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3751de2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_product_jsons_v2(folder_path):\n",
    "    \"\"\"\n",
    "    Parse multiple JSON files with the new structure into a pandas DataFrame\n",
    "    \n",
    "    Args:\n",
    "        folder_path (str): Path to folder containing JSON files\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with columns: plu, name, en_name, category_name, \n",
    "                     category_id, unit_name, weight, volume\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    \n",
    "    # Find all JSON files in the folder\n",
    "    json_files = glob.glob(os.path.join(folder_path, \"*.json\"))\n",
    "    \n",
    "    print(f\"Found {len(json_files)} JSON files\")\n",
    "    \n",
    "    for file_path in json_files:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # The structure has a top-level category name as key\n",
    "            for cat_key, category_data in data.items():\n",
    "                # Extract category information from the nested structure\n",
    "                cat_id = category_data.get('cat_id', '')\n",
    "                \n",
    "                # Extract items list\n",
    "                items_list = category_data.get('items', [])\n",
    "                \n",
    "                for item in items_list:\n",
    "                    # Extract basic item info\n",
    "                    title = item.get('title', '')\n",
    "                    \n",
    "                    # Extract masterData\n",
    "                    master_data = item.get('masterData', {})\n",
    "                    plu = master_data.get('plu', '')\n",
    "                    slug = master_data.get('slug', '')\n",
    "                    unit_name = master_data.get('unitName', '')\n",
    "                    weight = master_data.get('weight', '')\n",
    "                    volume = master_data.get('volume', '')\n",
    "                    \n",
    "                    # Extract primaryCategory\n",
    "                    primary_category = item.get('primaryCategory', {})\n",
    "                    primary_cat_name = primary_category.get('title', '')\n",
    "                    primary_cat_id = primary_category.get('id', '')\n",
    "                    \n",
    "                    # Append to our data list\n",
    "                    all_data.append({\n",
    "                        'plu': plu,\n",
    "                        'name': title,\n",
    "                        'en_name': slug,  # Using slug as English name\n",
    "                        'category_name': primary_cat_name,\n",
    "                        'category_id': primary_cat_id,\n",
    "                        'unit_name': unit_name,\n",
    "                        'weight': weight,\n",
    "                        'volume': volume\n",
    "                    })\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(all_data)\n",
    "    \n",
    "    print(f\"Successfully parsed {len(df)} products\")\n",
    "    print(f\"DataFrame shape: {df.shape}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a586eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30 JSON files\n",
      "Successfully parsed 21025 products\n",
      "DataFrame shape: (21025, 8)\n"
     ]
    }
   ],
   "source": [
    "df_perek = parse_product_jsons_v2(\"/home/mikhail/Documents/Хакатоны/X5_ner_MiLky_way/parser/parsed_catalog_perek\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c828513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "plu",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "en_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "category_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "category_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "unit_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "weight",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "volume",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "en_name_raw",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "e90ac32c-7280-4b7c-8495-9b60a2c3b4bd",
       "rows": [
        [
         "0",
         "4166310",
         "Соус SanBonsai Ореховый, 250мл",
         "sous-sanbonsai-orehovyj-250ml",
         "Майонез, соусы",
         "394",
         "шт",
         "250.0",
         "250.0",
         "sous sanbonsai orehovyj 250ml"
        ],
        [
         "1",
         "3691298",
         "Аджика Буздякский Кавказская, 350мл",
         "adzika-buzdakskij-kavkazskaa-350ml",
         "Майонез, соусы",
         "394",
         "шт",
         "350.0",
         null,
         "adzika buzdakskij kavkazskaa 350ml"
        ],
        [
         "2",
         "3361376",
         "Соус Tabasco Хабанеро перечный, 60мл",
         "sous-tabasco-habanero-perecnyj-60ml",
         "Майонез, соусы",
         "394",
         "шт",
         "100.0",
         "60.0",
         "sous tabasco habanero perecnyj 60ml"
        ],
        [
         "3",
         "4013003",
         "Крем бальзамический Casa Rinaldi со вкусом имбиря и яблока, 250мл",
         "krem-balzamiceskij-casa-rinaldi-so-vkusom-imbira-i-abloka-250ml",
         "Майонез, соусы",
         "394",
         "шт",
         "250.0",
         "250.0",
         "krem balzamiceskij casa rinaldi so vkusom imbira i abloka 250ml"
        ],
        [
         "4",
         "3494860",
         "Кетчуп Балтимор Томатный, 260г",
         "ketcup-baltimor-tomatnyj-260g",
         "Майонез, соусы",
         "394",
         "шт",
         "260.0",
         null,
         "ketcup baltimor tomatnyj 260g"
        ],
        [
         "5",
         "4332842",
         "Соус Итальянский, 350мл",
         "sous-italanskij-350ml",
         "Майонез, соусы",
         "394",
         "шт",
         "350.0",
         "350.0",
         "sous italanskij 350ml"
        ],
        [
         "6",
         "4350285",
         "Соус Barilla томатный с чесноком и травами, 400г",
         "sous-barilla-tomatnyj-s-cesnokom-i-travami-400g",
         "Майонез, соусы",
         "394",
         "шт",
         "400.0",
         "375.0",
         "sous barilla tomatnyj s cesnokom i travami 400g"
        ],
        [
         "7",
         "4262571",
         "Майонез Махеевъ Провансаль 50,5%, 630мл",
         "majonez-maheev-provansal-50-5-630ml",
         "Майонез, соусы",
         "394",
         "шт",
         "630.0",
         "655.0",
         "majonez maheev provansal 50 5 630ml"
        ],
        [
         "8",
         "3659170",
         "Основа для супа Sen Soy Premium Фо 5%, 80мл",
         "osnova-dla-supa-sen-soy-premium-fo-5-80ml",
         "Майонез, соусы",
         "394",
         "шт",
         "80.0",
         null,
         "osnova dla supa sen soy premium fo 5 80ml"
        ],
        [
         "9",
         "3495095",
         "Соус-крем бальзамический СП Мирный сливочный, 220мл",
         "sous-krem-balzamiceskij-sp-mirnyj-slivocnyj-220ml",
         "Майонез, соусы",
         "394",
         "шт",
         "220.0",
         "220.0",
         "sous krem balzamiceskij sp mirnyj slivocnyj 220ml"
        ],
        [
         "10",
         "4236370",
         "Аджика Славянский Дар Кавказская, 170мл",
         "adzika-slavanskij-dar-kavkazskaa-170ml",
         "Майонез, соусы",
         "394",
         "шт",
         "170.0",
         "160.0",
         "adzika slavanskij dar kavkazskaa 170ml"
        ],
        [
         "11",
         "4236355",
         "Соус облепиховый Tamaki, 470мл",
         "sous-oblepihovyj-tamaki-470ml",
         "Майонез, соусы",
         "394",
         "шт",
         "494.0",
         "470.0",
         "sous oblepihovyj tamaki 470ml"
        ],
        [
         "12",
         "3619553",
         "Соус бальзамический Filippo Berio классический, 250мл",
         "sous-balzamiceskij-filippo-berio-klassiceskij-250ml",
         "Майонез, соусы",
         "394",
         "шт",
         "250.0",
         "250.0",
         "sous balzamiceskij filippo berio klassiceskij 250ml"
        ],
        [
         "13",
         "50567",
         "Уксус Kuhne бальзамический белый 5%, 250мл",
         "uksus-kuhne-balzamiceskij-belyj-5-250ml",
         "Масло, уксус",
         "393",
         "шт",
         "250.0",
         "250.0",
         "uksus kuhne balzamiceskij belyj 5 250ml"
        ],
        [
         "14",
         "4321938",
         "Приправа Iori Аджика Абхазская из зеленого перца, 120мл",
         "priprava-iori-adzika-abhazskaa-iz-zelenogo-perca-120ml",
         "Майонез, соусы",
         "394",
         "шт",
         "120.0",
         "120.0",
         "priprava iori adzika abhazskaa iz zelenogo perca 120ml"
        ],
        [
         "15",
         "4371212",
         "Майонез Иволга Провансаль 67%, 200мл",
         "majonez-ivolga-provansal-67-200ml",
         "Майонез, соусы",
         "394",
         "шт",
         "200.0",
         "200.0",
         "majonez ivolga provansal 67 200ml"
        ],
        [
         "16",
         "4265904",
         "Соус Sen Soy Premium для рыбы и морепродуктов, 120мл",
         "sous-sen-soy-premium-dla-ryby-i-moreproduktov-120ml",
         "Майонез, соусы",
         "394",
         "шт",
         "120.0",
         null,
         "sous sen soy premium dla ryby i moreproduktov 120ml"
        ],
        [
         "17",
         "3054047",
         "Кетчуп Кухмастер Острый по-грузински, 260г",
         "ketcup-kuhmaster-ostryj-po-gruzinski-260g",
         "Майонез, соусы",
         "394",
         "шт",
         "260.0",
         null,
         "ketcup kuhmaster ostryj po gruzinski 260g"
        ],
        [
         "18",
         "4273281",
         "Хрен Столовый Маркет, 100г",
         "hren-stolovyj-market-100g",
         "Майонез, соусы",
         "394",
         "шт",
         "100.0",
         null,
         "hren stolovyj market 100g"
        ],
        [
         "19",
         "4100327",
         "Соус Sen Soy Premium Якитори для курицы, 120мл",
         "sous-sen-soy-premium-akitori-dla-kuricy-120ml",
         "Майонез, соусы",
         "394",
         "шт",
         "120.0",
         null,
         "sous sen soy premium akitori dla kuricy 120ml"
        ],
        [
         "20",
         "4276654",
         "Уксус Кинто бальзамический 5%, 500мл",
         "uksus-kinto-balzamiceskij-5-500ml",
         "Масло, уксус",
         "393",
         "шт",
         "550.0",
         "500.0",
         "uksus kinto balzamiceskij 5 500ml"
        ],
        [
         "21",
         "4358643",
         "Соус Pervafood Pomodoro сливочно-овощной итальянский, 300мл",
         "sous-pervafood-pomodoro-slivocno-ovosnoj-italanskij-300ml",
         "Майонез, соусы",
         "394",
         "шт",
         "300.0",
         null,
         "sous pervafood pomodoro slivocno ovosnoj italanskij 300ml"
        ],
        [
         "22",
         "4276808",
         "Горчица Gustoff Cook Profi русская, 150г",
         "gorcica-gustoff-cook-profi-russkaa-150g",
         "Майонез, соусы",
         "394",
         "шт",
         "150.0",
         null,
         "gorcica gustoff cook profi russkaa 150g"
        ],
        [
         "23",
         "3636364",
         "Соус Кинто Абхазский перечный, 195мл",
         "sous-kinto-abhazskij-perecnyj-195ml",
         "Майонез, соусы",
         "394",
         "шт",
         "195.0",
         "195.0",
         "sous kinto abhazskij perecnyj 195ml"
        ],
        [
         "24",
         "4397609",
         "Заправка Вкуснее Норвежская, 15мл",
         "zapravka-vkusnee-norvezskaa-15ml",
         "Майонез, соусы",
         "394",
         "шт",
         "14.0",
         "15.0",
         "zapravka vkusnee norvezskaa 15ml"
        ],
        [
         "25",
         "4361055",
         "Горчица byJam.me фруктовая с инжиром, 100г",
         "gorcica-byjam-me-fruktovaa-s-inzirom-100g",
         "Майонез, соусы",
         "394",
         "шт",
         "100.0",
         null,
         "gorcica byjam me fruktovaa s inzirom 100g"
        ],
        [
         "26",
         "4352032",
         "Соус Andrea Milano на основе винного уксуса с перцем чили, 150мл",
         "sous-andrea-milano-na-osnove-vinnogo-uksusa-s-percem-cili-150ml",
         "Майонез, соусы",
         "394",
         "шт",
         "184.0",
         "150.0",
         "sous andrea milano na osnove vinnogo uksusa s percem cili 150ml"
        ],
        [
         "27",
         "4336613",
         "Соус Tamaki Чёрный Чеснок, 470мл",
         "sous-tamaki-cernyj-cesnok-470ml",
         "Майонез, соусы",
         "394",
         "шт",
         "527.0",
         "470.0",
         "sous tamaki cernyj cesnok 470ml"
        ],
        [
         "28",
         "3466439",
         "Соус Кинто Зогалшараб кизиловый, 370мл",
         "sous-kinto-zogalsarab-kizilovyj-370ml",
         "Майонез, соусы",
         "394",
         "шт",
         "370.0",
         "370.0",
         "sous kinto zogalsarab kizilovyj 370ml"
        ],
        [
         "29",
         "4377818",
         "Аджика Bio Georgia зелёная, 200мл",
         "adzika-bio-georgia-zelenaa-200ml",
         "Майонез, соусы",
         "394",
         "шт",
         "200.0",
         "200.0",
         "adzika bio georgia zelenaa 200ml"
        ],
        [
         "30",
         "4331802",
         "Соус Mr. Djemius Том Ям на основе растительных масел пониженной калорийности, 270мл",
         "sous-mr-djemius-tom-am-na-osnove-rastitelnyh-masel-ponizennoj-kalorijnosti-270ml",
         "Майонез, соусы",
         "394",
         "шт",
         "270.0",
         null,
         "sous mr djemius tom am na osnove rastitelnyh masel ponizennoj kalorijnosti 270ml"
        ],
        [
         "31",
         "4276653",
         "Уксус Кинто винный из белого вина 6%, 500мл",
         "uksus-kinto-vinnyj-iz-belogo-vina-6-500ml",
         "Масло, уксус",
         "393",
         "шт",
         "520.0",
         "500.0",
         "uksus kinto vinnyj iz belogo vina 6 500ml"
        ],
        [
         "32",
         "4255266",
         "Соус Filippo Berio Olive томатный, 340г",
         "sous-filippo-berio-olive-tomatnyj-340g",
         "Майонез, соусы",
         "394",
         "шт",
         "340.0",
         null,
         "sous filippo berio olive tomatnyj 340g"
        ],
        [
         "33",
         "3438198",
         "Соус Чим Чим Корейская заправка на основе растительного масла для капусты, 60мл",
         "sous-cim-cim-korejskaa-zapravka-na-osnove-rastitelnogo-masla-dla-kapusty-60ml",
         "Майонез, соусы",
         "394",
         "шт",
         "60.0",
         "60.0",
         "sous cim cim korejskaa zapravka na osnove rastitelnogo masla dla kapusty 60ml"
        ],
        [
         "34",
         "3408180",
         "Соус Sen Soy чёрный перец столовый, 120мл",
         "sous-sen-soy-cernyj-perec-stolovyj-120ml",
         "Майонез, соусы",
         "394",
         "шт",
         "120.0",
         "120.0",
         "sous sen soy cernyj perec stolovyj 120ml"
        ],
        [
         "35",
         "4392758",
         "Соус Жар-соус Острая клюква, 110г",
         "sous-zar-sous-ostraa-klukva-110g",
         "Майонез, соусы",
         "394",
         "шт",
         "110.0",
         null,
         "sous zar sous ostraa klukva 110g"
        ],
        [
         "36",
         "4236382",
         "Соус Takemura Унаги для угря, 215мл",
         "sous-takemura-unagi-dla-ugra-215ml",
         "Майонез, соусы",
         "394",
         "шт",
         "262.0",
         "215.0",
         "sous takemura unagi dla ugra 215ml"
        ],
        [
         "37",
         "4331801",
         "Соус Mr. Djemius с оливками на основе растительных масел пониженной калорийности, 270мл",
         "sous-mr-djemius-s-olivkami-na-osnove-rastitelnyh-masel-ponizennoj-kalorijnosti-270ml",
         "Майонез, соусы",
         "394",
         "шт",
         "270.0",
         "270.0",
         "sous mr djemius s olivkami na osnove rastitelnyh masel ponizennoj kalorijnosti 270ml"
        ],
        [
         "38",
         "4351806",
         "Соус Tamaki Чёрная шрирача, 470мл",
         "sous-tamaki-cernaa-sriraca-470ml",
         "Майонез, соусы",
         "394",
         "шт",
         "503.0",
         "470.0",
         "sous tamaki cernaa sriraca 470ml"
        ],
        [
         "39",
         "4397238",
         "Соус Philosophia de Natura Грибной майонезный, 150мл",
         "sous-philosophia-de-natura-gribnoj-majoneznyj-150ml",
         "Майонез, соусы",
         "394",
         "шт",
         "150.0",
         "150.0",
         "sous philosophia de natura gribnoj majoneznyj 150ml"
        ],
        [
         "40",
         "4391793",
         "Соус Lavka Lavka Болоньезе для пасты охлажденный, 270г",
         "sous-lavka-lavka-boloneze-dla-pasty-ohlazdennyj-270g",
         "Майонез, соусы",
         "394",
         "шт",
         "270.0",
         null,
         "sous lavka lavka boloneze dla pasty ohlazdennyj 270g"
        ],
        [
         "41",
         "4236425",
         "Соус Tamaki Поке, 470мл",
         "sous-tamaki-poke-470ml",
         "Майонез, соусы",
         "394",
         "шт",
         "565.0",
         "470.0",
         "sous tamaki poke 470ml"
        ],
        [
         "42",
         "4397613",
         "Заправка Вкуснее Испанская, 15мл",
         "zapravka-vkusnee-ispanskaa-15ml",
         "Майонез, соусы",
         "394",
         "шт",
         "14.0",
         "15.0",
         "zapravka vkusnee ispanskaa 15ml"
        ],
        [
         "43",
         "3384959",
         "Аджика Стоевъ Грузинская перечная, 120мл",
         "adzika-stoev-gruzinskaa-perecnaa-120ml",
         "Майонез, соусы",
         "394",
         "шт",
         "120.0",
         null,
         "adzika stoev gruzinskaa perecnaa 120ml"
        ],
        [
         "44",
         "4327497",
         "Соус Madli Ткемали Ежевичный фруктово-ягодный кисло-пряный, 270мл",
         "sous-madli-tkemali-ezevicnyj-fruktovo-agodnyj-kislo-pranyj-270ml",
         "Майонез, соусы",
         "394",
         "шт",
         "270.0",
         null,
         "sous madli tkemali ezevicnyj fruktovo agodnyj kislo pranyj 270ml"
        ],
        [
         "45",
         "4321929",
         "Соус Costa Caliente Scorpion перечный, 90мл",
         "sous-costa-caliente-scorpion-perecnyj-90ml",
         "Майонез, соусы",
         "394",
         "шт",
         "95.0",
         "90.0",
         "sous costa caliente scorpion perecnyj 90ml"
        ],
        [
         "46",
         "3648819",
         "Соус устричный Чим Чим для приготовления фунчозы, 90мл",
         "sous-ustricnyj-cim-cim-dla-prigotovlenia-funcozy-90ml",
         "Майонез, соусы",
         "394",
         "шт",
         "90.0",
         "90.0",
         "sous ustricnyj cim cim dla prigotovlenia funcozy 90ml"
        ],
        [
         "47",
         "4255263",
         "Соус Filippo Berio Verdure grigliate томатный, 340г",
         "sous-filippo-berio-verdure-grigliate-tomatnyj-340g",
         "Майонез, соусы",
         "394",
         "шт",
         "340.0",
         null,
         "sous filippo berio verdure grigliate tomatnyj 340g"
        ],
        [
         "48",
         "4331804",
         "Соус Mr. Djemius Провансальский на основе растительных масел пониженной калорийности, 270мл",
         "sous-mr-djemius-provansalskij-na-osnove-rastitelnyh-masel-ponizennoj-kalorijnosti-270ml",
         "Майонез, соусы",
         "394",
         "шт",
         "270.0",
         null,
         "sous mr djemius provansalskij na osnove rastitelnyh masel ponizennoj kalorijnosti 270ml"
        ],
        [
         "49",
         "4149867",
         "Кетчуп Помидорка острый пастеризованный высшей категории, 350г",
         "ketcup-pomidorka-ostryj-pasterizovannyj-vyssej-kategorii-350g",
         "Майонез, соусы",
         "394",
         "шт",
         "350.0",
         null,
         "ketcup pomidorka ostryj pasterizovannyj vyssej kategorii 350g"
        ]
       ],
       "shape": {
        "columns": 9,
        "rows": 21025
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>plu</th>\n",
       "      <th>name</th>\n",
       "      <th>en_name</th>\n",
       "      <th>category_name</th>\n",
       "      <th>category_id</th>\n",
       "      <th>unit_name</th>\n",
       "      <th>weight</th>\n",
       "      <th>volume</th>\n",
       "      <th>en_name_raw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4166310</td>\n",
       "      <td>Соус SanBonsai Ореховый, 250мл</td>\n",
       "      <td>sous-sanbonsai-orehovyj-250ml</td>\n",
       "      <td>Майонез, соусы</td>\n",
       "      <td>394</td>\n",
       "      <td>шт</td>\n",
       "      <td>250.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>sous sanbonsai orehovyj 250ml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3691298</td>\n",
       "      <td>Аджика Буздякский Кавказская, 350мл</td>\n",
       "      <td>adzika-buzdakskij-kavkazskaa-350ml</td>\n",
       "      <td>Майонез, соусы</td>\n",
       "      <td>394</td>\n",
       "      <td>шт</td>\n",
       "      <td>350.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adzika buzdakskij kavkazskaa 350ml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3361376</td>\n",
       "      <td>Соус Tabasco Хабанеро перечный, 60мл</td>\n",
       "      <td>sous-tabasco-habanero-perecnyj-60ml</td>\n",
       "      <td>Майонез, соусы</td>\n",
       "      <td>394</td>\n",
       "      <td>шт</td>\n",
       "      <td>100.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>sous tabasco habanero perecnyj 60ml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4013003</td>\n",
       "      <td>Крем бальзамический Casa Rinaldi со вкусом имб...</td>\n",
       "      <td>krem-balzamiceskij-casa-rinaldi-so-vkusom-imbi...</td>\n",
       "      <td>Майонез, соусы</td>\n",
       "      <td>394</td>\n",
       "      <td>шт</td>\n",
       "      <td>250.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>krem balzamiceskij casa rinaldi so vkusom imbi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3494860</td>\n",
       "      <td>Кетчуп Балтимор Томатный, 260г</td>\n",
       "      <td>ketcup-baltimor-tomatnyj-260g</td>\n",
       "      <td>Майонез, соусы</td>\n",
       "      <td>394</td>\n",
       "      <td>шт</td>\n",
       "      <td>260.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ketcup baltimor tomatnyj 260g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21020</th>\n",
       "      <td>4274867</td>\n",
       "      <td>Напиток газированный Добрый Cola без сахара, 1.5л</td>\n",
       "      <td>napitok-gazirovannyj-dobryj-cola-bez-sahara-1-5l</td>\n",
       "      <td>Газировка</td>\n",
       "      <td>423</td>\n",
       "      <td>шт</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>napitok gazirovannyj dobryj cola bez sahara 1 5l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21021</th>\n",
       "      <td>1863</td>\n",
       "      <td>Напиток газированный Coca-Cola, 330мл</td>\n",
       "      <td>napitok-gazirovannyj-coca-cola-330ml</td>\n",
       "      <td>Газировка</td>\n",
       "      <td>423</td>\n",
       "      <td>шт</td>\n",
       "      <td>330.0</td>\n",
       "      <td>330.0</td>\n",
       "      <td>napitok gazirovannyj coca cola 330ml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21022</th>\n",
       "      <td>3503711</td>\n",
       "      <td>Вода Малаховская №1 питьевая 1 категории негаз...</td>\n",
       "      <td>voda-malahovskaa-no1-pitevaa-1-kategorii-negaz...</td>\n",
       "      <td>Вода</td>\n",
       "      <td>424</td>\n",
       "      <td>шт</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>voda malahovskaa no1 pitevaa 1 kategorii negaz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21023</th>\n",
       "      <td>3173468</td>\n",
       "      <td>Энергетический напиток Red Bull, 473мл</td>\n",
       "      <td>energeticeskij-napitok-red-bull-473ml</td>\n",
       "      <td>Энергетик</td>\n",
       "      <td>426</td>\n",
       "      <td>шт</td>\n",
       "      <td>473.0</td>\n",
       "      <td>473.0</td>\n",
       "      <td>energeticeskij napitok red bull 473ml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21024</th>\n",
       "      <td>3931836</td>\n",
       "      <td>Энергетический напиток Adrenaline Rush, 449мл</td>\n",
       "      <td>energeticeskij-napitok-adrenaline-rush-449ml</td>\n",
       "      <td>Энергетик</td>\n",
       "      <td>426</td>\n",
       "      <td>шт</td>\n",
       "      <td>449.0</td>\n",
       "      <td>449.0</td>\n",
       "      <td>energeticeskij napitok adrenaline rush 449ml</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21025 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           plu                                               name  \\\n",
       "0      4166310                     Соус SanBonsai Ореховый, 250мл   \n",
       "1      3691298                Аджика Буздякский Кавказская, 350мл   \n",
       "2      3361376               Соус Tabasco Хабанеро перечный, 60мл   \n",
       "3      4013003  Крем бальзамический Casa Rinaldi со вкусом имб...   \n",
       "4      3494860                     Кетчуп Балтимор Томатный, 260г   \n",
       "...        ...                                                ...   \n",
       "21020  4274867  Напиток газированный Добрый Cola без сахара, 1.5л   \n",
       "21021     1863              Напиток газированный Coca-Cola, 330мл   \n",
       "21022  3503711  Вода Малаховская №1 питьевая 1 категории негаз...   \n",
       "21023  3173468             Энергетический напиток Red Bull, 473мл   \n",
       "21024  3931836      Энергетический напиток Adrenaline Rush, 449мл   \n",
       "\n",
       "                                                 en_name   category_name  \\\n",
       "0                          sous-sanbonsai-orehovyj-250ml  Майонез, соусы   \n",
       "1                     adzika-buzdakskij-kavkazskaa-350ml  Майонез, соусы   \n",
       "2                    sous-tabasco-habanero-perecnyj-60ml  Майонез, соусы   \n",
       "3      krem-balzamiceskij-casa-rinaldi-so-vkusom-imbi...  Майонез, соусы   \n",
       "4                          ketcup-baltimor-tomatnyj-260g  Майонез, соусы   \n",
       "...                                                  ...             ...   \n",
       "21020   napitok-gazirovannyj-dobryj-cola-bez-sahara-1-5l       Газировка   \n",
       "21021               napitok-gazirovannyj-coca-cola-330ml       Газировка   \n",
       "21022  voda-malahovskaa-no1-pitevaa-1-kategorii-negaz...            Вода   \n",
       "21023              energeticeskij-napitok-red-bull-473ml       Энергетик   \n",
       "21024       energeticeskij-napitok-adrenaline-rush-449ml       Энергетик   \n",
       "\n",
       "       category_id unit_name  weight  volume  \\\n",
       "0              394        шт   250.0   250.0   \n",
       "1              394        шт   350.0     NaN   \n",
       "2              394        шт   100.0    60.0   \n",
       "3              394        шт   250.0   250.0   \n",
       "4              394        шт   260.0     NaN   \n",
       "...            ...       ...     ...     ...   \n",
       "21020          423        шт  1500.0  1500.0   \n",
       "21021          423        шт   330.0   330.0   \n",
       "21022          424        шт  5000.0  5000.0   \n",
       "21023          426        шт   473.0   473.0   \n",
       "21024          426        шт   449.0   449.0   \n",
       "\n",
       "                                             en_name_raw  \n",
       "0                          sous sanbonsai orehovyj 250ml  \n",
       "1                     adzika buzdakskij kavkazskaa 350ml  \n",
       "2                    sous tabasco habanero perecnyj 60ml  \n",
       "3      krem balzamiceskij casa rinaldi so vkusom imbi...  \n",
       "4                          ketcup baltimor tomatnyj 260g  \n",
       "...                                                  ...  \n",
       "21020   napitok gazirovannyj dobryj cola bez sahara 1 5l  \n",
       "21021               napitok gazirovannyj coca cola 330ml  \n",
       "21022  voda malahovskaa no1 pitevaa 1 kategorii negaz...  \n",
       "21023              energeticeskij napitok red bull 473ml  \n",
       "21024       energeticeskij napitok adrenaline rush 449ml  \n",
       "\n",
       "[21025 rows x 9 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_perek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ebfb267c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_perek['en_name_raw'] = df_perek['en_name'].str.replace(\"-\", \" \")\n",
    "df_perek[\"name_processed\"] = df_perek[\"name\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "579af486",
   "metadata": {},
   "outputs": [],
   "source": [
    "names_processed_perek = list(set(df_perek['name_processed']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "92036e45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17942"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(names_processed_perek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cdd58271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['огурцы по-дижонски с мёдом и горчицей целые маринованные маркет перекрёсток, 680г',\n",
       " 'напиток газированный coca-cola, 330мл',\n",
       " 'пиво балтика крепкое легендарное №9 светлое пастеризованное 8%, 450мл',\n",
       " 'йогурт epica питьевой киви-виноград 2.5%, 260г',\n",
       " 'торт черёмушки наполеон слоёный, 310г',\n",
       " 'грудка цыплёнка-бройлера копчёно-варёная маркет',\n",
       " 'пиво prazacka злата светлое 4.9%, 500мл',\n",
       " 'конфитюр ратибор грушевый, 350г',\n",
       " 'удобрение fertika leaf power универсальное водорастворимое, 50г',\n",
       " 'настойка веда малина и базилик горькая 38%, 500мл']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names_processed_perek[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5636ec",
   "metadata": {},
   "source": [
    "#### combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "37970cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = names_processed_5ka + names_processed_perek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7c54bad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "plu",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "en_name",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "category_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "category_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "unit_name",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "weight",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "volume",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "en_name_raw",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "name_processed",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "property",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "2b19ff6d-f134-41d5-b497-869da264f7cb",
       "rows": [
        [
         "25248",
         "4119675",
         "Подставка Лакарт Дизайн для телефона в ассортименте 1шт",
         null,
         "Хозтовары",
         "251C12944",
         null,
         null,
         null,
         null,
         "подставка лакарт дизайн для телефона в ассортименте 1шт",
         "1 шт"
        ],
        [
         "25249",
         "4120583",
         "Корзинка EcoNova универсальная 170х120х75мм в ассортименте 1шт.",
         null,
         "Хозтовары",
         "251C12944",
         null,
         null,
         null,
         null,
         "корзинка econova универсальная 170х120х75мм в ассортименте 1шт.",
         "1 шт"
        ],
        [
         "25250",
         "4007117",
         "Сумка Zhejiang Senmiao Trade в ассортименте 1шт.",
         null,
         "Хозтовары",
         "251C12944",
         null,
         null,
         null,
         null,
         "сумка zhejiang senmiao trade в ассортименте 1шт.",
         "1 шт"
        ],
        [
         "25251",
         "4118948",
         "Удлинитель Эра UX-3-1.5m без заземления 1300Вт 3 гнезда 1.5м 1шт",
         null,
         "Хозтовары",
         "251C12944",
         null,
         null,
         null,
         null,
         "удлинитель эра ux-3-1.5m без заземления 1300вт 3 гнезда 1.5м 1шт",
         "1 шт"
        ],
        [
         "25252",
         "4150382",
         "Чехол для наушников 5.7х5х2.2см в ассортименте 1шт.",
         null,
         "Хозтовары",
         "251C12944",
         null,
         null,
         null,
         null,
         "чехол для наушников 5.7х5х2.2см в ассортименте 1шт.",
         "1 шт"
        ]
       ],
       "shape": {
        "columns": 11,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>plu</th>\n",
       "      <th>name</th>\n",
       "      <th>en_name</th>\n",
       "      <th>category_name</th>\n",
       "      <th>category_id</th>\n",
       "      <th>unit_name</th>\n",
       "      <th>weight</th>\n",
       "      <th>volume</th>\n",
       "      <th>en_name_raw</th>\n",
       "      <th>name_processed</th>\n",
       "      <th>property</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25248</th>\n",
       "      <td>4119675</td>\n",
       "      <td>Подставка Лакарт Дизайн для телефона в ассорти...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Хозтовары</td>\n",
       "      <td>251C12944</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>подставка лакарт дизайн для телефона в ассорти...</td>\n",
       "      <td>1 шт</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25249</th>\n",
       "      <td>4120583</td>\n",
       "      <td>Корзинка EcoNova универсальная 170х120х75мм в ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Хозтовары</td>\n",
       "      <td>251C12944</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>корзинка econova универсальная 170х120х75мм в ...</td>\n",
       "      <td>1 шт</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25250</th>\n",
       "      <td>4007117</td>\n",
       "      <td>Сумка Zhejiang Senmiao Trade в ассортименте 1шт.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Хозтовары</td>\n",
       "      <td>251C12944</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>сумка zhejiang senmiao trade в ассортименте 1шт.</td>\n",
       "      <td>1 шт</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25251</th>\n",
       "      <td>4118948</td>\n",
       "      <td>Удлинитель Эра UX-3-1.5m без заземления 1300Вт...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Хозтовары</td>\n",
       "      <td>251C12944</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>удлинитель эра ux-3-1.5m без заземления 1300вт...</td>\n",
       "      <td>1 шт</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25252</th>\n",
       "      <td>4150382</td>\n",
       "      <td>Чехол для наушников 5.7х5х2.2см в ассортименте...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Хозтовары</td>\n",
       "      <td>251C12944</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>чехол для наушников 5.7х5х2.2см в ассортименте...</td>\n",
       "      <td>1 шт</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           plu                                               name en_name  \\\n",
       "25248  4119675  Подставка Лакарт Дизайн для телефона в ассорти...     NaN   \n",
       "25249  4120583  Корзинка EcoNova универсальная 170х120х75мм в ...     NaN   \n",
       "25250  4007117   Сумка Zhejiang Senmiao Trade в ассортименте 1шт.     NaN   \n",
       "25251  4118948  Удлинитель Эра UX-3-1.5m без заземления 1300Вт...     NaN   \n",
       "25252  4150382  Чехол для наушников 5.7х5х2.2см в ассортименте...     NaN   \n",
       "\n",
       "      category_name category_id unit_name  weight  volume en_name_raw  \\\n",
       "25248     Хозтовары   251C12944       NaN     NaN     NaN         NaN   \n",
       "25249     Хозтовары   251C12944       NaN     NaN     NaN         NaN   \n",
       "25250     Хозтовары   251C12944       NaN     NaN     NaN         NaN   \n",
       "25251     Хозтовары   251C12944       NaN     NaN     NaN         NaN   \n",
       "25252     Хозтовары   251C12944       NaN     NaN     NaN         NaN   \n",
       "\n",
       "                                          name_processed property  \n",
       "25248  подставка лакарт дизайн для телефона в ассорти...     1 шт  \n",
       "25249  корзинка econova универсальная 170х120х75мм в ...     1 шт  \n",
       "25250   сумка zhejiang senmiao trade в ассортименте 1шт.     1 шт  \n",
       "25251  удлинитель эра ux-3-1.5m без заземления 1300вт...     1 шт  \n",
       "25252  чехол для наушников 5.7х5х2.2см в ассортименте...     1 шт  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([df_perek, df_5ka])\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ca0768",
   "metadata": {},
   "source": [
    "### создаем аугментированный датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81a6419",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37602/37602 [00:04<00:00, 9361.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample augmented variants (first 30 rows):\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "source",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "original",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "variant",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "e04245ab-4cb2-4786-bce9-71c23ba3176a",
       "rows": [
        [
         "0",
         "query",
         "узвар великая русь из боярышника 1л",
         "узвар великая русь из боярышника 1л"
        ],
        [
         "1",
         "query",
         "узвар великая русь из боярышника 1л",
         "извар веидлая русх из боярышнгка 1л"
        ],
        [
         "2",
         "query",
         "узвар великая русь из боярышника 1л",
         "узвар великая русь из боярышника"
        ],
        [
         "3",
         "query",
         "узвар великая русь из боярышника 1л",
         "увае великая русь изю бярышника 1л"
        ],
        [
         "4",
         "query",
         "узвар великая русь из боярышника 1л",
         "узвар великая русь з боярышhика 1л"
        ],
        [
         "5",
         "query",
         "свинина по-тирольски 4 сезона 600г",
         "свинина по-тирольски 4 сезона 600г"
        ],
        [
         "6",
         "query",
         "свинина по-тирольски 4 сезона 600г",
         "всинина по-трольски 4 сезона 600г"
        ],
        [
         "7",
         "query",
         "свинина по-тирольски 4 сезона 600г",
         "свиина п-тирольсыки 4 сезона 600г"
        ],
        [
         "8",
         "query",
         "свинина по-тирольски 4 сезона 600г",
         "свининх по-тирольски 4 сзепна 600г"
        ],
        [
         "9",
         "query",
         "свинина по-тирольски 4 сезона 600г",
         "свинина по-тирольски 4 сезона 600г"
        ],
        [
         "10",
         "query",
         "сок ideas томатный с кинзой, перцем и чесноком 1л",
         "сок ideas томатный с кинзой, перцем и чесноком 1л"
        ],
        [
         "11",
         "query",
         "сок ideas томатный с кинзой, перцем и чесноком 1л",
         "сок ideas томтаный с кинзой, перцем и 1л"
        ],
        [
         "12",
         "query",
         "сок ideas томатный с кинзой, перцем и чесноком 1л",
         "сок ideas томаонй с линзой, перцем и чесhокoм 1л"
        ],
        [
         "13",
         "query",
         "сок ideas томатный с кинзой, перцем и чесноком 1л",
         "сок ides томатный с кинзой, перцем и чесноком 1л"
        ],
        [
         "14",
         "query",
         "сок ideas томатный с кинзой, перцем и чесноком 1л",
         "сок ideas тосматный с кнзой, перйцем и чесноком 1л"
        ],
        [
         "15",
         "query",
         "мармелад белевская пастильная мануфактура белевский ассорти 260г",
         "мармелад белевская пастильная мануфактура белевский ассорти 260г"
        ],
        [
         "16",
         "query",
         "мармелад белевская пастильная мануфактура белевский ассорти 260г",
         "mармлеад белевсакя псатилньаря мануфаkтура белевсkий ассорти 260г;;"
        ],
        [
         "17",
         "query",
         "мармелад белевская пастильная мануфактура белевский ассорти 260г",
         "мармезлал белевскфя пастильная мануфактура белевский ассортиё"
        ],
        [
         "18",
         "query",
         "мармелад белевская пастильная мануфактура белевский ассорти 260г",
         "мармелад белевская пстилкьhая мануфавкёtура белевский есёорти 260г"
        ],
        [
         "19",
         "query",
         "мармелад белевская пастильная мануфактура белевский ассорти 260г",
         "мaрмелад белевская пастильная амнуфакйура боеюлеский ассот 260г"
        ],
        [
         "20",
         "query",
         "салфетки влажные pampers fresh clean детские  52шт.",
         "салфетки влажные pampers fresh clean детские  52шт."
        ],
        [
         "21",
         "query",
         "салфетки влажные pampers fresh clean детские  52шт.",
         "салфетки влажные pаmpers fresh clean детские 52пт."
        ],
        [
         "22",
         "query",
         "салфетки влажные pampers fresh clean детские  52шт.",
         "салафетки влсжные pamрers fresh clean детске 52шт."
        ],
        [
         "23",
         "query",
         "салфетки влажные pampers fresh clean детские  52шт.",
         "салфетки влажhые pampёrs fres clean детские 52шт."
        ],
        [
         "24",
         "query",
         "салфетки влажные pampers fresh clean детские  52шт.",
         "салфетлив влажные pamper fresh clean дтескице 52шт."
        ],
        [
         "25",
         "query",
         "напиток berg ice tea черный чай со вкусом лимона 450мл",
         "напиток berg ice tea черный чай со вкусом лимона 450мл"
        ],
        [
         "26",
         "query",
         "напиток berg ice tea черный чай со вкусом лимона 450мл",
         "напиток begr ice tёа черный чай со вцксуом лимона 450мл"
        ],
        [
         "27",
         "query",
         "напиток berg ice tea черный чай со вкусом лимона 450мл",
         "напитщок berg ice tea чернйы чай со вкюсоыm липта 450мл"
        ],
        [
         "28",
         "query",
         "напиток berg ice tea черный чай со вкусом лимона 450мл",
         "наптиок ebrg ice tea черный чай со вкфсмо лимна 450мл"
        ],
        [
         "29",
         "query",
         "напиток berg ice tea черный чай со вкусом лимона 450мл",
         "напиток вerр ice tea ерный чай cо вчусом либмона 450мл"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 30
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>original</th>\n",
       "      <th>variant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>query</td>\n",
       "      <td>узвар великая русь из боярышника 1л</td>\n",
       "      <td>узвар великая русь из боярышника 1л</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>query</td>\n",
       "      <td>узвар великая русь из боярышника 1л</td>\n",
       "      <td>извар веидлая русх из боярышнгка 1л</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>query</td>\n",
       "      <td>узвар великая русь из боярышника 1л</td>\n",
       "      <td>узвар великая русь из боярышника</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>query</td>\n",
       "      <td>узвар великая русь из боярышника 1л</td>\n",
       "      <td>увае великая русь изю бярышника 1л</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>query</td>\n",
       "      <td>узвар великая русь из боярышника 1л</td>\n",
       "      <td>узвар великая русь з боярышhика 1л</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>query</td>\n",
       "      <td>свинина по-тирольски 4 сезона 600г</td>\n",
       "      <td>свинина по-тирольски 4 сезона 600г</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>query</td>\n",
       "      <td>свинина по-тирольски 4 сезона 600г</td>\n",
       "      <td>всинина по-трольски 4 сезона 600г</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>query</td>\n",
       "      <td>свинина по-тирольски 4 сезона 600г</td>\n",
       "      <td>свиина п-тирольсыки 4 сезона 600г</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>query</td>\n",
       "      <td>свинина по-тирольски 4 сезона 600г</td>\n",
       "      <td>свининх по-тирольски 4 сзепна 600г</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>query</td>\n",
       "      <td>свинина по-тирольски 4 сезона 600г</td>\n",
       "      <td>свинина по-тирольски 4 сезона 600г</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>query</td>\n",
       "      <td>сок ideas томатный с кинзой, перцем и чесноком 1л</td>\n",
       "      <td>сок ideas томатный с кинзой, перцем и чесноком 1л</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>query</td>\n",
       "      <td>сок ideas томатный с кинзой, перцем и чесноком 1л</td>\n",
       "      <td>сок ideas томтаный с кинзой, перцем и 1л</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>query</td>\n",
       "      <td>сок ideas томатный с кинзой, перцем и чесноком 1л</td>\n",
       "      <td>сок ideas томаонй с линзой, перцем и чесhокoм 1л</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>query</td>\n",
       "      <td>сок ideas томатный с кинзой, перцем и чесноком 1л</td>\n",
       "      <td>сок ides томатный с кинзой, перцем и чесноком 1л</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>query</td>\n",
       "      <td>сок ideas томатный с кинзой, перцем и чесноком 1л</td>\n",
       "      <td>сок ideas тосматный с кнзой, перйцем и чесноко...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>query</td>\n",
       "      <td>мармелад белевская пастильная мануфактура беле...</td>\n",
       "      <td>мармелад белевская пастильная мануфактура беле...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>query</td>\n",
       "      <td>мармелад белевская пастильная мануфактура беле...</td>\n",
       "      <td>mармлеад белевсакя псатилньаря мануфаkтура бел...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>query</td>\n",
       "      <td>мармелад белевская пастильная мануфактура беле...</td>\n",
       "      <td>мармезлал белевскфя пастильная мануфактура бел...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>query</td>\n",
       "      <td>мармелад белевская пастильная мануфактура беле...</td>\n",
       "      <td>мармелад белевская пстилкьhая мануфавкёtура бе...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>query</td>\n",
       "      <td>мармелад белевская пастильная мануфактура беле...</td>\n",
       "      <td>мaрмелад белевская пастильная амнуфакйура боею...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>query</td>\n",
       "      <td>салфетки влажные pampers fresh clean детские  ...</td>\n",
       "      <td>салфетки влажные pampers fresh clean детские  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>query</td>\n",
       "      <td>салфетки влажные pampers fresh clean детские  ...</td>\n",
       "      <td>салфетки влажные pаmpers fresh clean детские 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>query</td>\n",
       "      <td>салфетки влажные pampers fresh clean детские  ...</td>\n",
       "      <td>салафетки влсжные pamрers fresh clean детске 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>query</td>\n",
       "      <td>салфетки влажные pampers fresh clean детские  ...</td>\n",
       "      <td>салфетки влажhые pampёrs fres clean детские 52шт.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>query</td>\n",
       "      <td>салфетки влажные pampers fresh clean детские  ...</td>\n",
       "      <td>салфетлив влажные pamper fresh clean дтескице ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>query</td>\n",
       "      <td>напиток berg ice tea черный чай со вкусом лимо...</td>\n",
       "      <td>напиток berg ice tea черный чай со вкусом лимо...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>query</td>\n",
       "      <td>напиток berg ice tea черный чай со вкусом лимо...</td>\n",
       "      <td>напиток begr ice tёа черный чай со вцксуом лим...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>query</td>\n",
       "      <td>напиток berg ice tea черный чай со вкусом лимо...</td>\n",
       "      <td>напитщок berg ice tea чернйы чай со вкюсоыm ли...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>query</td>\n",
       "      <td>напиток berg ice tea черный чай со вкусом лимо...</td>\n",
       "      <td>наптиок ebrg ice tea черный чай со вкфсмо лимн...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>query</td>\n",
       "      <td>напиток berg ice tea черный чай со вкусом лимо...</td>\n",
       "      <td>напиток вerр ice tea ерный чай cо вчусом либмо...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   source                                           original  \\\n",
       "0   query                узвар великая русь из боярышника 1л   \n",
       "1   query                узвар великая русь из боярышника 1л   \n",
       "2   query                узвар великая русь из боярышника 1л   \n",
       "3   query                узвар великая русь из боярышника 1л   \n",
       "4   query                узвар великая русь из боярышника 1л   \n",
       "5   query                 свинина по-тирольски 4 сезона 600г   \n",
       "6   query                 свинина по-тирольски 4 сезона 600г   \n",
       "7   query                 свинина по-тирольски 4 сезона 600г   \n",
       "8   query                 свинина по-тирольски 4 сезона 600г   \n",
       "9   query                 свинина по-тирольски 4 сезона 600г   \n",
       "10  query  сок ideas томатный с кинзой, перцем и чесноком 1л   \n",
       "11  query  сок ideas томатный с кинзой, перцем и чесноком 1л   \n",
       "12  query  сок ideas томатный с кинзой, перцем и чесноком 1л   \n",
       "13  query  сок ideas томатный с кинзой, перцем и чесноком 1л   \n",
       "14  query  сок ideas томатный с кинзой, перцем и чесноком 1л   \n",
       "15  query  мармелад белевская пастильная мануфактура беле...   \n",
       "16  query  мармелад белевская пастильная мануфактура беле...   \n",
       "17  query  мармелад белевская пастильная мануфактура беле...   \n",
       "18  query  мармелад белевская пастильная мануфактура беле...   \n",
       "19  query  мармелад белевская пастильная мануфактура беле...   \n",
       "20  query  салфетки влажные pampers fresh clean детские  ...   \n",
       "21  query  салфетки влажные pampers fresh clean детские  ...   \n",
       "22  query  салфетки влажные pampers fresh clean детские  ...   \n",
       "23  query  салфетки влажные pampers fresh clean детские  ...   \n",
       "24  query  салфетки влажные pampers fresh clean детские  ...   \n",
       "25  query  напиток berg ice tea черный чай со вкусом лимо...   \n",
       "26  query  напиток berg ice tea черный чай со вкусом лимо...   \n",
       "27  query  напиток berg ice tea черный чай со вкусом лимо...   \n",
       "28  query  напиток berg ice tea черный чай со вкусом лимо...   \n",
       "29  query  напиток berg ice tea черный чай со вкусом лимо...   \n",
       "\n",
       "                                              variant  \n",
       "0                 узвар великая русь из боярышника 1л  \n",
       "1                 извар веидлая русх из боярышнгка 1л  \n",
       "2                    узвар великая русь из боярышника  \n",
       "3                  увае великая русь изю бярышника 1л  \n",
       "4                  узвар великая русь з боярышhика 1л  \n",
       "5                  свинина по-тирольски 4 сезона 600г  \n",
       "6                   всинина по-трольски 4 сезона 600г  \n",
       "7                   свиина п-тирольсыки 4 сезона 600г  \n",
       "8                  свининх по-тирольски 4 сзепна 600г  \n",
       "9                  свинина по-тирольски 4 сезона 600г  \n",
       "10  сок ideas томатный с кинзой, перцем и чесноком 1л  \n",
       "11           сок ideas томтаный с кинзой, перцем и 1л  \n",
       "12   сок ideas томаонй с линзой, перцем и чесhокoм 1л  \n",
       "13   сок ides томатный с кинзой, перцем и чесноком 1л  \n",
       "14  сок ideas тосматный с кнзой, перйцем и чесноко...  \n",
       "15  мармелад белевская пастильная мануфактура беле...  \n",
       "16  mармлеад белевсакя псатилньаря мануфаkтура бел...  \n",
       "17  мармезлал белевскфя пастильная мануфактура бел...  \n",
       "18  мармелад белевская пстилкьhая мануфавкёtура бе...  \n",
       "19  мaрмелад белевская пастильная амнуфакйура боею...  \n",
       "20  салфетки влажные pampers fresh clean детские  ...  \n",
       "21  салфетки влажные pаmpers fresh clean детские 5...  \n",
       "22  салафетки влсжные pamрers fresh clean детске 5...  \n",
       "23  салфетки влажhые pampёrs fres clean детские 52шт.  \n",
       "24  салфетлив влажные pamper fresh clean дтескице ...  \n",
       "25  напиток berg ice tea черный чай со вкусом лимо...  \n",
       "26  напиток begr ice tёа черный чай со вцксуом лим...  \n",
       "27  напитщок berg ice tea чернйы чай со вкюсоыm ли...  \n",
       "28  наптиок ebrg ice tea черный чай со вкфсмо лимн...  \n",
       "29  напиток вerр ice tea ерный чай cо вчусом либмо...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Visual/keyboard confusable map (common Cyrillic <-> Latin substitutes)\n",
    "confusable_map = {\n",
    "    'а':'a','в':'b','е':'e','ё':'e','к':'k','м':'m','н':'h','о':'o','р':'p','с':'c','т':'t','у':'y','х':'x',\n",
    "    'A':'A','B':'B','E':'E','K':'K','M':'M','H':'H','O':'O','P':'P','C':'C','T':'T','Y':'Y','X':'X'\n",
    "}\n",
    "# Reverse mapping (Latin->Cyrillic where visually confusable)\n",
    "confusable_map_rev = {v:k for k,v in confusable_map.items()}\n",
    "\n",
    "# Simple Russian keyboard adjacency (partial, common neighbors)\n",
    "# (this is an approximation for common typos)\n",
    "keyboard_neighbors = {\n",
    "    'й':'ц','ц':'й','у':'и','и':'у','е':'р','р':'е','т':'ь','ь':'т','о':'п','п':'о',\n",
    "    'а':'ф','ф':'а','ы':'в','в':'ы','с':'м','м':'с','д':'л','л':'д','ж':'э','э':'ж','я':'ш','ш':'я',\n",
    "    'ч':'с','ю':'б','б':'ю','н':'т','г':'ш','к':'л'\n",
    "}\n",
    "\n",
    "# --- Augmentation functions ---\n",
    "def substitute_chars(s, prob=0.04):\n",
    "    \"\"\"Randomly substitute letters: confusable or keyboard neighbor or random cyrillic letter.\"\"\"\n",
    "    out = []\n",
    "    for c in s:\n",
    "        if c.isalpha() and random.random() < prob:\n",
    "            r = random.random()\n",
    "            if r < 0.4 and c.lower() in keyboard_neighbors:  # neighbor\n",
    "                rep = keyboard_neighbors.get(c.lower(), c)\n",
    "                # Preserve case\n",
    "                rep = rep.upper() if c.isupper() else rep\n",
    "                out.append(rep)\n",
    "            elif r < 0.7 and c in confusable_map:  # visual swap to Latin\n",
    "                out.append(confusable_map[c])\n",
    "            elif r < 0.85 and c.lower() in confusable_map_rev:  # Latin->Cyrillic\n",
    "                rep = confusable_map_rev.get(c.lower(), c)\n",
    "                rep = rep.upper() if c.isupper() else rep\n",
    "                out.append(rep)\n",
    "            else:\n",
    "                # random Cyrillic letter substitute (common set)\n",
    "                pool = 'абвгдеёжзийклмнопрстуфхцчшщыьюя'\n",
    "                out.append(random.choice(pool))\n",
    "        else:\n",
    "            out.append(c)\n",
    "    return ''.join(out)\n",
    "\n",
    "def delete_chars(s, prob=0.03):\n",
    "    return ''.join(c for c in s if not (c.isalpha() and random.random() < prob))\n",
    "\n",
    "def insert_chars(s, prob=0.02):\n",
    "    out = []\n",
    "    pool = 'абвгдеёжзийклмнопрстуфхцчшщыьюя'\n",
    "    for c in s:\n",
    "        out.append(c)\n",
    "        if c.isalpha() and random.random() < prob:\n",
    "            out.append(random.choice(pool))\n",
    "    return ''.join(out)\n",
    "\n",
    "def transpose_chars(s, prob=0.02):\n",
    "    s = list(s)\n",
    "    i = 0\n",
    "    while i < len(s)-1:\n",
    "        if s[i].isalpha() and s[i+1].isalpha() and random.random() < prob:\n",
    "            s[i], s[i+1] = s[i+1], s[i]\n",
    "            i += 2\n",
    "        else:\n",
    "            i += 1\n",
    "    return ''.join(s)\n",
    "\n",
    "def remove_spaces_or_merge(s, prob=0.06):\n",
    "    if random.random() < prob:\n",
    "        # either remove all spaces or replace one space randomly\n",
    "        if random.random() < 0.5:\n",
    "            return s.replace(' ', '')\n",
    "        else:\n",
    "            parts = s.split()\n",
    "            if len(parts) > 1:\n",
    "                i = random.randrange(len(parts)-1)\n",
    "                parts[i] = parts[i] + parts[i+1]\n",
    "                del parts[i+1]\n",
    "                return ' '.join(parts)\n",
    "    return s\n",
    "\n",
    "def truncation(s, prob=0.15):\n",
    "    if random.random() < prob:\n",
    "        parts = s.split()\n",
    "        if len(parts) > 1:\n",
    "            # drop last token sometimes or drop random token\n",
    "            if random.random() < 0.8:\n",
    "                return ' '.join(parts[:-1])\n",
    "            else:\n",
    "                idx = random.randrange(len(parts))\n",
    "                del parts[idx]\n",
    "                return ' '.join(parts)\n",
    "        else:\n",
    "            # drop last few chars\n",
    "            cut = max(1, int(len(s)*0.3))\n",
    "            return s[:-cut]\n",
    "    return s\n",
    "\n",
    "def add_template(s, prob=0.12):\n",
    "    if random.random() < prob:\n",
    "        templ = random.choice(templates)\n",
    "        return templ.format(s)\n",
    "    return s\n",
    "\n",
    "def random_case_and_punct(s):\n",
    "    s = s.lower()\n",
    "    # optionally add punctuation or trailing dots/commas\n",
    "    if random.random() < 0.06:\n",
    "        s = s + random.choice(['.', '..', ',', ';;', '!'])\n",
    "    return s\n",
    "\n",
    "def swap_cyr_lat(s, prob=0.05):\n",
    "    # randomly replace some chars with confusables (Cyr->Lat or Lat->Cyr)\n",
    "    out = []\n",
    "    for c in s:\n",
    "        if c.isalpha() and random.random() < prob:\n",
    "            if c in confusable_map and random.random() < 0.6:\n",
    "                out.append(confusable_map[c])\n",
    "                continue\n",
    "            if c.lower() in confusable_map_rev and random.random() < 0.6:\n",
    "                rep = confusable_map_rev.get(c.lower(), c)\n",
    "                rep = rep.upper() if c.isupper() else rep\n",
    "                out.append(rep)\n",
    "                continue\n",
    "        out.append(c)\n",
    "    return ''.join(out)\n",
    "\n",
    "# Compose simple augmentation pipeline\n",
    "def augment_once(s):\n",
    "    s = s.strip()\n",
    "    s = random_case_and_punct(s)\n",
    "    s = substitute_chars(s, prob=0.045)\n",
    "    s = delete_chars(s, prob=0.03)\n",
    "    s = insert_chars(s, prob=0.02)\n",
    "    s = transpose_chars(s, prob=0.02)\n",
    "    s = swap_cyr_lat(s, prob=0.04)\n",
    "    s = remove_spaces_or_merge(s, prob=0.06)\n",
    "    s = truncation(s, prob=0.12)\n",
    "    # s = add_template(s, prob=0.10)\n",
    "    # final cleanup: collapse multiple spaces\n",
    "    s = ' '.join(s.split())\n",
    "    return s\n",
    "\n",
    "def generate_variants(item, n_variants=4, include_original=True):\n",
    "    variants = []\n",
    "    if include_original:\n",
    "        variants.append(item)\n",
    "    # produce variants with at least one correction attempt to canonical sometimes\n",
    "    for _ in range(n_variants - (1 if include_original else 0)):\n",
    "        v = augment_once(item)\n",
    "        # ensure not identical to original unless random chance\n",
    "        if v == item and random.random() < 0.6:\n",
    "            v = augment_once(item)\n",
    "        variants.append(v)\n",
    "    return variants\n",
    "\n",
    "# --- Example usage on provided query samples and parsed items ---\n",
    "queries = [\n",
    "\"кечуп для гриля\",\"кешью\",\"кеыир\",\"кеыирн\",\"кзамороженнные\",\"киа\",\"кив\",\"кивих\",\"кидька\",\"киевский\",\n",
    "\"кизель\",\"кизинаки\",\"кильк\",\"килька\",\"килька в т\",\"килька в то\",\"килька в томате\",\"килька в томатной\",\n",
    "\"килька с овощами\",\"кильки\",\"ким носк\",\"ким носков\",\"кимч\",\"кимчи\",\"кин9а\",\"кинги\",\"киндер\",\n",
    "\"киндер макми кинг\",\"киндер молочный ломтик\",\"киндер пингви\",\"киндер сюрприз\",\"киндер яйцо большое\",\n",
    "\"киндеры\",\"киндза\",\"кинза\",\"кинзу\",\"кино\",\"киноа\",\"киноя\",\"кинто\",\"кинто еще\",\"кинуа\",\"киприно\",\n",
    "\"киреешки\",\"кирешки\",\"кирие\",\"кириеш\",\"кисе\",\"киселб\",\"киселт\",\"кисель dr.oetke\",\"кисель dr.oetker\",\n",
    "\"кисель айдиг\",\"кисель айдиго\",\"кисель детский\",\"кисель фрутоня\",\"кисель фрутонян\",\"кисеь\",\"кискель\",\n",
    "\"кисл\",\"кислая\",\"кислая капуста\",\"кислель\",\"кислец\",\"кислин\",\"кислла\",\"кисло слад\",\"кисло сладк\",\n",
    "\"кисло сладки\",\"кисло сладкий\",\"кисло-\",\"кисло-сладк\",\"кисло-сладки\",\"кисло-сладкий\",\"кисловодская\",\n",
    "\"кислом\",\"кисломикс\",\"кисломоло\",\"кисломолочка\",\"кисломолочн\"\n",
    "]\n",
    "\n",
    "parsed_items = [\n",
    "'огурцы по-дижонски с мёдом и горчицей целые маринованные маркет перекрёсток, 680г',\n",
    "'напиток газированный coca-cola, 330мл',\n",
    "'пиво балтика крепкое легендарное №9 светлое пастеризованное 8%, 450мл',\n",
    "'йогурт epica питьевой киви-виноград 2.5%, 260г',\n",
    "'торт черёмушки наполеон слоёный, 310г',\n",
    "'грудка цыплёнка-бройлера копчёно-варёная маркет',\n",
    "'пиво prazacka злата светлое 4.9%, 500мл',\n",
    "'конфитюр ратибор грушевый, 350г',\n",
    "'удобрение fertika leaf power универсальное водорастворимое, 50г',\n",
    "'настойка веда малина и базилик горькая 38%, 500мл'\n",
    "]\n",
    "\n",
    "# Generate a small table: for each of first 30 queries and first 5 parsed items generate variants\n",
    "rows = []\n",
    "for q in tqdm(names):\n",
    "    variants = generate_variants(q, n_variants=5, include_original=True)\n",
    "    for v in variants:\n",
    "        rows.append({'source':'query', 'original':q, 'variant':v})\n",
    "\n",
    "for p in parsed_items:\n",
    "    variants = generate_variants(p, n_variants=5, include_original=True)\n",
    "    for v in variants:\n",
    "        rows.append({'source':'parsed_item', 'original':p, 'variant':v})\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# Also print a short textual sample for quick view:\n",
    "print(\"Sample augmented variants (first 30 rows):\")\n",
    "df.head(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e47ca7f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'печенье американо сдобное маркет, 400г'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample().original.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "530f7390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(188060, 3)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "016d5587",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.tokenize(df.variant.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70eaa0c",
   "metadata": {},
   "source": [
    "### rubert tiny 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a6ae1d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"cointegrated/rubert-tiny2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0f3ee6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0daf4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> чипсы картофельные и maxx куриные крылышки барбекю 110г'\n",
      "'>>> чипсы картофельные , maxx куриные крылышки барбекю 110г'\n",
      "'>>> чипсы картофельные изделия maxx куриные крылышки барбекю 110г'\n",
      "'>>> чипсы картофельные или maxx куриные крылышки барбекю 110г'\n",
      "'>>> чипсы картофельные блюда maxx куриные крылышки барбекю 110г'\n"
     ]
    }
   ],
   "source": [
    "maska = \"[MASK]\"\n",
    "text = f\"чипсы картофельные {maska} maxx куриные крылышки барбекю 110г\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "token_logits = model(**inputs).logits\n",
    "# Find the location of [MASK] and extract its logits\n",
    "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "# Pick the [MASK] candidates with the highest logits\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "for token in top_5_tokens:\n",
    "    print(f\"'>>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c9b2c312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='cointegrated/rubert-tiny2', vocab_size=83828, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t4: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b3626630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total tokens: 545983\n",
      "unk tokens: 56 unk rate: 0.00010256729605134226\n",
      "avg subwords per word: 1.9642078527589706\n",
      "median subwords per word: 2\n"
     ]
    }
   ],
   "source": [
    "texts = [...]  # your combined corpus, list of strings\n",
    "\n",
    "vocab = tokenizer.get_vocab()\n",
    "unk_id = tokenizer.unk_token_id\n",
    "\n",
    "total_tokens = 0\n",
    "unk_tokens = 0\n",
    "subword_lengths = []\n",
    "\n",
    "for t in names:\n",
    "    enc = tokenizer(t, add_special_tokens=False)\n",
    "    ids = enc[\"input_ids\"]\n",
    "    total_tokens += len(ids)\n",
    "    unk_tokens += sum(1 for i in ids if i == unk_id)\n",
    "    # measure average tokens per word\n",
    "    words = t.split()\n",
    "    if words:\n",
    "        subword_lengths.extend([len(tokenizer(w, add_special_tokens=False)[\"input_ids\"]) for w in words])\n",
    "\n",
    "print(\"total tokens:\", total_tokens)\n",
    "print(\"unk tokens:\", unk_tokens, \"unk rate:\", unk_tokens/total_tokens if total_tokens else 0)\n",
    "print(\"avg subwords per word:\", sum(subword_lengths)/len(subword_lengths))\n",
    "print(\"median subwords per word:\", sorted(subword_lengths)[len(subword_lengths)//2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545b3191",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_text_stats(lines: List[str]) -> dict:\n",
    "stats = {}\n",
    "lens = [len(l) for l in lines]\n",
    "toks = [l.split() for l in lines]\n",
    "token_counts = [len(t) for t in toks]\n",
    "\n",
    "\n",
    "def has_latin(s):\n",
    "return bool(re.search(r'[A-Za-z]', s))\n",
    "\n",
    "\n",
    "def has_cyr(s):\n",
    "return bool(re.search(r'[А-Яа-яЁё]', s))\n",
    "\n",
    "\n",
    "latin_frac = sum(1 for l in lines if has_latin(l)) / max(1, len(lines))\n",
    "cyr_frac = sum(1 for l in lines if has_cyr(l)) / max(1, len(lines))\n",
    "digits_frac = sum(1 for l in lines if re.search(r'\\d', l)) / max(1, len(lines))\n",
    "punct_frac = sum(1 for l in lines if re.search(r'[.,;:!\\-\\/\\\\()]', l)) / max(1, len(lines))\n",
    "\n",
    "\n",
    "stats['n_lines'] = len(lines)\n",
    "stats['avg_len_chars'] = float(np.mean(lens))\n",
    "stats['median_len_chars'] = int(np.median(lens))\n",
    "stats['avg_tokens'] = float(np.mean(token_counts))\n",
    "stats['median_tokens'] = int(np.median(token_counts))\n",
    "stats['latin_frac'] = float(latin_frac)\n",
    "stats['cyrillic_frac'] = float(cyr_frac)\n",
    "stats['digits_frac'] = float(digits_frac)\n",
    "stats['punct_frac'] = float(punct_frac)\n",
    "return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "678f55a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lines_from_file(path: Path) -> List[str]:\n",
    "    lines = []\n",
    "    with path.open('r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            s = line.strip()\n",
    "            if s:\n",
    "                lines.append(s)\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ace7666a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_and_subsample(originals: List[str], augmented: List[str], mix_ratio: float = 0.75, seed: int = 42) -> List[str]:\n",
    "    # mix_ratio = fraction of examples from originals\n",
    "    random.seed(seed)\n",
    "    n_total = len(originals) + len(augmented)\n",
    "    n_from_orig = int(n_total * mix_ratio)\n",
    "    n_from_aug = n_total - n_from_orig\n",
    "\n",
    "    chosen_orig = random.choices(originals, k=max(1, n_from_orig)) if originals else []\n",
    "    chosen_aug = random.choices(augmented, k=max(1, n_from_aug)) if augmented else []\n",
    "    combined = chosen_orig + chosen_aug\n",
    "    random.shuffle(combined)\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "afdf76c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_group_texts(lines: List[str], tokenizer: PreTrainedTokenizerFast, block_size: int = 128, use_wwm: bool = False):\n",
    "    \"\"\"Tokenize lines and group into blocks of `block_size`. If use_wwm=True we use is_split_into_words approach.\n",
    "\n",
    "    Returns a HuggingFace Dataset with columns 'input_ids' and 'labels'\n",
    "    \"\"\"\n",
    "    # Build dataset\n",
    "    ds = Dataset.from_dict({'text': lines})\n",
    "\n",
    "    def tokenize_func(examples):\n",
    "        if use_wwm:\n",
    "            # split into whitespace tokens for whole-word masking\n",
    "            words = [t.split() for t in examples['text']]\n",
    "            return tokenizer(words, is_split_into_words=True, add_special_tokens=True)\n",
    "        else:\n",
    "            return tokenizer(examples['text'], add_special_tokens=True)\n",
    "\n",
    "    tokenized = ds.map(tokenize_func, batched=True, remove_columns=['text'])\n",
    "\n",
    "    # concatenate and group\n",
    "    def group_texts(examples):\n",
    "        concatenated = sum(examples['input_ids'], [])\n",
    "        total_length = len(concatenated)\n",
    "        if total_length >= block_size:\n",
    "            total_length = (total_length // block_size) * block_size\n",
    "        else:\n",
    "            total_length = 0\n",
    "        result = {}\n",
    "        if total_length == 0:\n",
    "            result['input_ids'] = []\n",
    "            result['labels'] = []\n",
    "            return result\n",
    "        result['input_ids'] = [concatenated[i:i+block_size] for i in range(0, total_length, block_size)]\n",
    "        result['labels'] = [list(ids) for ids in result['input_ids']]\n",
    "        return result\n",
    "\n",
    "    lm_dataset = tokenized.map(group_texts, batched=True, remove_columns=tokenized.column_names)\n",
    "    # drop empty rows\n",
    "    lm_dataset = lm_dataset.filter(lambda ex: len(ex['input_ids']) > 0)\n",
    "\n",
    "    # train/val split\n",
    "    split = lm_dataset.train_test_split(test_size=0.01)\n",
    "    return DatasetDict({'train': split['train'], 'validation': split['test']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "caa458cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data_collator(tokenizer: PreTrainedTokenizerFast, use_wwm: bool = False, mlm_probability: float = 0.15):\n",
    "    if use_wwm:\n",
    "        # DataCollatorForWholeWordMask expects tokenizer with word_ids support\n",
    "        return DataCollatorForWholeWordMask(tokenizer=tokenizer, mlm=True, mlm_probability=mlm_probability)\n",
    "    else:\n",
    "        return DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=mlm_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cbbc0941",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(\n",
    "    output_dir: str = './here/',\n",
    "    model_name_or_path: str = 'cointegrated/rubert-tiny2',\n",
    "    train_tokenizer: bool = False,\n",
    "    tokenizer_vocab_size: int = 30000,\n",
    "    block_size: int = 128,\n",
    "    use_wwm: bool = False,\n",
    "    mix_ratio: float = 0.75,\n",
    "    num_epochs: int = 3,\n",
    "    per_device_train_batch_size: int = 32,\n",
    "    gradient_accumulation_steps: int = 1,\n",
    "    learning_rate: float = 5e-5,\n",
    "    weight_decay: float = 0.01,\n",
    "    seed: int = 42,\n",
    "    save_steps: int = 2000,        # DataCollatorForWholeWordMask expects tokenizer with word_ids support\n",
    "\n",
    "    logging_steps: int = 200,\n",
    "    fp16: bool = False,\n",
    "):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    random.seed(seed)\n",
    "\n",
    "    originals = df['original'].tolist()\n",
    "    augmented = df['variant'].tolist()\n",
    "\n",
    "\n",
    "    # # Basic exploration\n",
    "    # print('Exploring data distributions...')\n",
    "    # s_orig = simple_text_stats(originals) if originals else {}\n",
    "    # s_aug = simple_text_stats(augmented) if augmented else {}\n",
    "    # print('Originals stats:', json.dumps(s_orig, ensure_ascii=False, indent=2))\n",
    "    # print('Augmented stats:', json.dumps(s_aug, ensure_ascii=False, indent=2))\n",
    "\n",
    "    # Combine corpus according to mix_ratio\n",
    "    combined = mix_and_subsample(originals, augmented, mix_ratio=mix_ratio, seed=seed)\n",
    "    print(f'Combined corpus size: {len(combined)}')\n",
    "\n",
    "    # Tokenizer\n",
    "    tokenizer = None\n",
    "    tokenizer_path = os.path.join(output_dir, 'tokenizer.json')\n",
    "    print(f'Loading tokenizer from pretrained model: {model_name_or_path}')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, do_lower_case=True)\n",
    "    # ensure mask token exists\n",
    "    # if tokenizer.mask_token is None:\n",
    "    #     tokenizer.add_special_tokens({'mask_token': '[MASK]'})\n",
    "\n",
    "    # Tokenize and group\n",
    "    datasets = tokenize_and_group_texts(combined, tokenizer, block_size=block_size, use_wwm=use_wwm)\n",
    "    print('Train examples (blocks):', len(datasets['train']))\n",
    "    print('Validation examples (blocks):', len(datasets['validation']))\n",
    "\n",
    "    # Model\n",
    "    print('Loading model...')\n",
    "    model = AutoModelForMaskedLM.from_pretrained(model_name_or_path)\n",
    "    # # if tokenizer added tokens, resize\n",
    "    # try:\n",
    "    #     model.resize_token_embeddings(len(tokenizer))\n",
    "    # except Exception:\n",
    "    #     pass\n",
    "\n",
    "    # Data collator\n",
    "    data_collator = build_data_collator(tokenizer, use_wwm=use_wwm, mlm_probability=0.15)\n",
    "\n",
    "    # Training args\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=num_epochs,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        eval_strategy='steps',\n",
    "        eval_steps=save_steps,\n",
    "        save_steps=save_steps,\n",
    "        save_total_limit=3,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        logging_steps=logging_steps,\n",
    "        seed=seed,\n",
    "        fp16=fp16,\n",
    "        dataloader_num_workers=4,\n",
    "        report_to='none',\n",
    "    )\n",
    "\n",
    "    # Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=datasets['train'],\n",
    "        eval_dataset=datasets['validation'],\n",
    "    )\n",
    "\n",
    "    # Train\n",
    "    print('Starting training...')\n",
    "    trainer.train()\n",
    "\n",
    "    # Eval\n",
    "    print('Running final evaluation...')\n",
    "    metrics = trainer.evaluate()\n",
    "    loss = metrics.get('eval_loss')\n",
    "    if loss is not None:\n",
    "        try:\n",
    "            ppl = math.exp(loss)\n",
    "        except OverflowError:\n",
    "            ppl = float('inf')\n",
    "        metrics['perplexity'] = ppl\n",
    "    print('Eval metrics:', metrics)\n",
    "\n",
    "    # Save\n",
    "    print('Saving tokenizer and model...')\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    print('Done. Model saved to', output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "925819d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined corpus size: 376120\n",
      "Loading tokenizer from pretrained model: cointegrated/rubert-tiny2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 376120/376120 [00:12<00:00, 29609.84 examples/s]\n",
      "Map: 100%|██████████| 376120/376120 [00:15<00:00, 24682.72 examples/s]\n",
      "Filter: 100%|██████████| 53465/53465 [00:05<00:00, 10495.15 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train examples (blocks): 52930\n",
      "Validation examples (blocks): 535\n",
      "Loading model...\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mikhail/Documents/Хакатоны/X5_ner_MiLky_way/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='4965' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  13/4965 00:57 < 7:11:53, 0.19 it/s, Epoch 0.01/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[81]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mrun_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[80]\u001b[39m\u001b[32m, line 94\u001b[39m, in \u001b[36mrun_pipeline\u001b[39m\u001b[34m(output_dir, model_name_or_path, train_tokenizer, tokenizer_vocab_size, block_size, use_wwm, mix_ratio, num_epochs, per_device_train_batch_size, gradient_accumulation_steps, learning_rate, weight_decay, seed, save_steps, logging_steps, fp16)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m     93\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mStarting training...\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[38;5;66;03m# Eval\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mRunning final evaluation...\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Хакатоны/X5_ner_MiLky_way/.venv/lib/python3.12/site-packages/transformers/trainer.py:2245\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2243\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Хакатоны/X5_ner_MiLky_way/.venv/lib/python3.12/site-packages/transformers/trainer.py:2560\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2553\u001b[39m context = (\n\u001b[32m   2554\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2555\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2556\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2557\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2558\u001b[39m )\n\u001b[32m   2559\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2560\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2562\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2563\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2564\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2565\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2566\u001b[39m ):\n\u001b[32m   2567\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2568\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Хакатоны/X5_ner_MiLky_way/.venv/lib/python3.12/site-packages/transformers/trainer.py:3782\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   3779\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type == DistributedType.DEEPSPEED:\n\u001b[32m   3780\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mscale_wrt_gas\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3782\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3784\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.detach()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Хакатоны/X5_ner_MiLky_way/.venv/lib/python3.12/site-packages/accelerate/accelerator.py:2734\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2732\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n\u001b[32m   2733\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2734\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Хакатоны/X5_ner_MiLky_way/.venv/lib/python3.12/site-packages/torch/_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Хакатоны/X5_ner_MiLky_way/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Хакатоны/X5_ner_MiLky_way/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "run_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b0fefd3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "чай императорский чай травяной в пакетиках, 20х1.2г\n"
     ]
    }
   ],
   "source": [
    "print(df.original.sample().iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a522b655",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm_model_path = \"/home/mikhail/Documents/Хакатоны/X5_ner_MiLky_way/models/rubert_tiny2_mlm\"\n",
    "model = AutoModelForMaskedLM.from_pretrained(mlm_model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(mlm_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "3f8f1bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> хлеб императорский травяной в пакетиках, 20х1.2г'\n",
      "'>>> напиток императорский травяной в пакетиках, 20х1.2г'\n",
      "'>>> соус императорский травяной в пакетиках, 20х1.2г'\n",
      "'>>> торт императорский травяной в пакетиках, 20х1.2г'\n",
      "'>>> салат императорский травяной в пакетиках, 20х1.2г'\n",
      "'>>> кофе императорский травяной в пакетиках, 20х1.2г'\n",
      "'>>> перец императорский травяной в пакетиках, 20х1.2г'\n",
      "'>>> завтрак императорский травяной в пакетиках, 20х1.2г'\n",
      "'>>> сыр императорский травяной в пакетиках, 20х1.2г'\n",
      "'>>> лук императорский травяной в пакетиках, 20х1.2г'\n"
     ]
    }
   ],
   "source": [
    "maska = \"[MASK]\"\n",
    "text = f'{maska} императорский травяной в пакетиках, 20х1.2г'\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "token_logits = model(**inputs).logits\n",
    "# Find the location of [MASK] and extract its logits\n",
    "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "# Pick the [MASK] candidates with the highest logits\n",
    "top_5_tokens = torch.topk(mask_token_logits, 10, dim=1).indices[0].tolist()\n",
    "\n",
    "for token in top_5_tokens:\n",
    "    print(f\"'>>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8672ba54",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "Invalid user token.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Хакатоны/X5_ner_MiLky_way/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:409\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Хакатоны/X5_ner_MiLky_way/.venv/lib/python3.12/site-packages/requests/models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 401 Client Error: Unauthorized for url: https://huggingface.co/api/whoami-v2",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mHfHubHTTPError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Хакатоны/X5_ner_MiLky_way/.venv/lib/python3.12/site-packages/huggingface_hub/hf_api.py:1782\u001b[39m, in \u001b[36mHfApi.whoami\u001b[39m\u001b[34m(self, token)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1782\u001b[39m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Хакатоны/X5_ner_MiLky_way/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:482\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    480\u001b[39m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[32m    481\u001b[39m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m482\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, \u001b[38;5;28mstr\u001b[39m(e), response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mHfHubHTTPError\u001b[39m: 401 Client Error: Unauthorized for url: https://huggingface.co/api/whoami-v2 (Request ID: Root=1-68da9b6b-53a2f56549e3efd02802c73f;bfb2bcf5-583d-42de-a152-933868e860b2)\n\nInvalid credentials in Authorization header",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[32m/tmp/ipykernel_1652232/3585241253.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m huggingface_hub \u001b[38;5;28;01mimport\u001b[39;00m login\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m login(token=\u001b[33m\"your_huggingface_token\"\u001b[39m)\n",
      "\u001b[32m~/Documents/Хакатоны/X5_ner_MiLky_way/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_deprecation.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     97\u001b[39m                 )\n\u001b[32m     98\u001b[39m                 \u001b[38;5;28;01mif\u001b[39;00m custom_message \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     99\u001b[39m                     message += \u001b[33m\"\\n\\n\"\u001b[39m + custom_message\n\u001b[32m    100\u001b[39m                 warnings.warn(message, FutureWarning)\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m f(*args, **kwargs)\n",
      "\u001b[32m~/Documents/Хакатоны/X5_ner_MiLky_way/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_deprecation.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     27\u001b[39m         @wraps(f)\n\u001b[32m     28\u001b[39m         \u001b[38;5;28;01mdef\u001b[39;00m inner_f(*args, **kwargs):\n\u001b[32m     29\u001b[39m             extra_args = len(args) - len(all_args)\n\u001b[32m     30\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m extra_args <= \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m f(*args, **kwargs)\n\u001b[32m     32\u001b[39m             \u001b[38;5;66;03m# extra_args > 0\u001b[39;00m\n\u001b[32m     33\u001b[39m             args_msg = [\n\u001b[32m     34\u001b[39m                 f\"{name}='{arg}'\" \u001b[38;5;28;01mif\u001b[39;00m isinstance(arg, str) \u001b[38;5;28;01melse\u001b[39;00m f\"{name}={arg}\"\n",
      "\u001b[32m~/Documents/Хакатоны/X5_ner_MiLky_way/.venv/lib/python3.12/site-packages/huggingface_hub/_login.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(token, add_to_git_credential, new_session, write_permission)\u001b[39m\n\u001b[32m    122\u001b[39m                 \u001b[33m\"`add_to_git_credential=True` in this function directly or \"\u001b[39m\n\u001b[32m    123\u001b[39m                 \u001b[33m\"`--add-to-git-credential` if using via `hf`CLI if \"\u001b[39m\n\u001b[32m    124\u001b[39m                 \u001b[33m\"you want to set the git credential as well.\"\u001b[39m\n\u001b[32m    125\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m         _login(token, add_to_git_credential=add_to_git_credential)\n\u001b[32m    127\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m is_notebook():\n\u001b[32m    128\u001b[39m         notebook_login(new_session=new_session)\n\u001b[32m    129\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m~/Documents/Хакатоны/X5_ner_MiLky_way/.venv/lib/python3.12/site-packages/huggingface_hub/_login.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(token, add_to_git_credential)\u001b[39m\n\u001b[32m    400\u001b[39m \n\u001b[32m    401\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m token.startswith(\u001b[33m\"api_org\"\u001b[39m):\n\u001b[32m    402\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ValueError(\u001b[33m\"You must use your personal account token, not an organization token.\"\u001b[39m)\n\u001b[32m    403\u001b[39m \n\u001b[32m--> \u001b[39m\u001b[32m404\u001b[39m     token_info = whoami(token)\n\u001b[32m    405\u001b[39m     permission = token_info[\u001b[33m\"auth\"\u001b[39m][\u001b[33m\"accessToken\"\u001b[39m][\u001b[33m\"role\"\u001b[39m]\n\u001b[32m    406\u001b[39m     logger.info(f\"Token is valid (permission: {permission}).\")\n\u001b[32m    407\u001b[39m \n",
      "\u001b[32m~/Documents/Хакатоны/X5_ner_MiLky_way/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    110\u001b[39m \n\u001b[32m    111\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m             kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.__name__, has_token=has_token, kwargs=kwargs)\n\u001b[32m    113\u001b[39m \n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m fn(*args, **kwargs)\n",
      "\u001b[32m~/Documents/Хакатоны/X5_ner_MiLky_way/.venv/lib/python3.12/site-packages/huggingface_hub/hf_api.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, token)\u001b[39m\n\u001b[32m   1792\u001b[39m                         \u001b[33m\"Note that HF_TOKEN takes precedence over `hf auth login`.\"\u001b[39m\n\u001b[32m   1793\u001b[39m                     )\n\u001b[32m   1794\u001b[39m                 \u001b[38;5;28;01melif\u001b[39;00m effective_token == _get_token_from_file():\n\u001b[32m   1795\u001b[39m                     error_message += \u001b[33m\" The token stored is invalid. Please run `hf auth login` to update it.\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1796\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(error_message, request=e.request, response=e.response) \u001b[38;5;28;01mfrom\u001b[39;00m e\n\u001b[32m   1797\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m   1798\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m r.json()\n",
      "\u001b[31mHTTPError\u001b[39m: Invalid user token."
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"your_huggingface_token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ecc39b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
