{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T13:23:24.811022Z",
     "iopub.status.busy": "2025-10-01T13:23:24.810230Z",
     "iopub.status.idle": "2025-10-01T13:24:04.342485Z",
     "shell.execute_reply": "2025-10-01T13:24:04.341852Z",
     "shell.execute_reply.started": "2025-10-01T13:23:24.810988Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-01 13:23:44.913616: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1759325025.251002      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1759325025.350887      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import datasets\n",
    "from datasets import (Dataset, Features, Sequence, Value, ClassLabel, load_dataset,\n",
    "                    load_from_disk, concatenate_datasets, DatasetDict)\n",
    "from sklearn.model_selection import KFold\n",
    "from transformers import (AutoTokenizer, AutoModel, AutoModelForTokenClassification,\n",
    "                         pipeline, PreTrainedTokenizerFast, TrainingArguments, Trainer,AutoModelForMaskedLM,\n",
    "                         DataCollatorForTokenClassification, EarlyStoppingCallback,\n",
    "                        DataCollatorForLanguageModeling, DataCollatorForWholeWordMask)\n",
    "import torch\n",
    "import optuna\n",
    "import os\n",
    "os.environ['WANDB_DISABLED'] = 'true'\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from typing import List, Optional\n",
    "import random\n",
    "seed=42\n",
    "random.seed(seed)\n",
    "\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка модели и датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T13:24:04.344031Z",
     "iopub.status.busy": "2025-10-01T13:24:04.343463Z",
     "iopub.status.idle": "2025-10-01T13:24:05.347699Z",
     "shell.execute_reply": "2025-10-01T13:24:05.347014Z",
     "shell.execute_reply.started": "2025-10-01T13:24:04.344011Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(188060, 3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mlm = pd.read_csv('/kaggle/input/mlm-v1/augmented_dataset_mlm.csv')\n",
    "df_mlm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T13:24:05.348807Z",
     "iopub.status.busy": "2025-10-01T13:24:05.348520Z",
     "iopub.status.idle": "2025-10-01T13:24:05.353784Z",
     "shell.execute_reply": "2025-10-01T13:24:05.353195Z",
     "shell.execute_reply.started": "2025-10-01T13:24:05.348780Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(188060, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_mlm.iloc[::]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T13:24:05.355607Z",
     "iopub.status.busy": "2025-10-01T13:24:05.355151Z",
     "iopub.status.idle": "2025-10-01T13:24:11.042228Z",
     "shell.execute_reply": "2025-10-01T13:24:11.041295Z",
     "shell.execute_reply.started": "2025-10-01T13:24:05.355584Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_name_or_path = \"DeepPavlov/distilrubert-base-cased-conversational\"\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name_or_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T13:24:11.043357Z",
     "iopub.status.busy": "2025-10-01T13:24:11.043031Z",
     "iopub.status.idle": "2025-10-01T13:24:11.054360Z",
     "shell.execute_reply": "2025-10-01T13:24:11.052004Z",
     "shell.execute_reply.started": "2025-10-01T13:24:11.043320Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def mix_and_subsample(originals: List[str], augmented: List[str], mix_ratio: float = 0.75, seed: int = 42) -> List[str]:\n",
    "    random.seed(seed)\n",
    "    n_total = len(originals) + len(augmented)\n",
    "    n_from_orig = int(n_total * mix_ratio)\n",
    "    n_from_aug = n_total - n_from_orig\n",
    "\n",
    "    chosen_orig = random.choices(originals, k=max(1, n_from_orig)) if originals else []\n",
    "    chosen_aug = random.choices(augmented, k=max(1, n_from_aug)) if augmented else []\n",
    "    combined = chosen_orig + chosen_aug\n",
    "    random.shuffle(combined)\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T13:24:11.056705Z",
     "iopub.status.busy": "2025-10-01T13:24:11.056313Z",
     "iopub.status.idle": "2025-10-01T13:24:11.092332Z",
     "shell.execute_reply": "2025-10-01T13:24:11.091320Z",
     "shell.execute_reply.started": "2025-10-01T13:24:11.056676Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def tokenize_and_group_texts(lines: List[str], tokenizer: PreTrainedTokenizerFast, block_size: int = 32, use_wwm: bool = False, seed=42):\n",
    "\n",
    "    ds = Dataset.from_dict({'text': lines})\n",
    "\n",
    "    def tokenize_func(examples):\n",
    "        if use_wwm:\n",
    "            words = [t.split() for t in examples['text']]\n",
    "            return tokenizer(words, is_split_into_words=True, add_special_tokens=True)\n",
    "        else:\n",
    "            return tokenizer(examples['text'], add_special_tokens=True)\n",
    "\n",
    "    tokenized = ds.map(tokenize_func, batched=True, remove_columns=['text'])\n",
    "\n",
    "    def group_texts(examples):\n",
    "        concatenated = sum(examples['input_ids'], [])\n",
    "        total_length = len(concatenated)\n",
    "        if total_length >= block_size:\n",
    "            total_length = (total_length // block_size) * block_size\n",
    "        else:\n",
    "            total_length = 0\n",
    "        result = {}\n",
    "        if total_length == 0:\n",
    "            result['input_ids'] = []\n",
    "            result['labels'] = []\n",
    "            return result\n",
    "        result['input_ids'] = [concatenated[i:i+block_size] for i in range(0, total_length, block_size)]\n",
    "        result['labels'] = [list(ids) for ids in result['input_ids']]\n",
    "        return result\n",
    "\n",
    "    lm_dataset = tokenized.map(group_texts, batched=True, remove_columns=tokenized.column_names)\n",
    "    lm_dataset = lm_dataset.filter(lambda ex: len(ex['input_ids']) > 0)\n",
    "\n",
    "    split = lm_dataset.train_test_split(test_size=0.01, seed=seed)\n",
    "    return DatasetDict({'train': split['train'], 'validation': split['test']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T13:24:11.093657Z",
     "iopub.status.busy": "2025-10-01T13:24:11.093372Z",
     "iopub.status.idle": "2025-10-01T13:24:11.122729Z",
     "shell.execute_reply": "2025-10-01T13:24:11.119542Z",
     "shell.execute_reply.started": "2025-10-01T13:24:11.093630Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def build_data_collator(tokenizer: PreTrainedTokenizerFast, use_wwm: bool = False, mlm_probability: float = 0.15):\n",
    "    if use_wwm:\n",
    "        return DataCollatorForWholeWordMask(tokenizer=tokenizer, mlm=True, mlm_probability=mlm_probability)\n",
    "    else:\n",
    "        return DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=mlm_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T13:25:03.695429Z",
     "iopub.status.busy": "2025-10-01T13:25:03.695080Z",
     "iopub.status.idle": "2025-10-01T13:25:03.699144Z",
     "shell.execute_reply": "2025-10-01T13:25:03.698531Z",
     "shell.execute_reply.started": "2025-10-01T13:25:03.695404Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "use_wwm = True\n",
    "output_dir = \"./results_ft\"\n",
    "num_epochs=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T13:24:11.164710Z",
     "iopub.status.busy": "2025-10-01T13:24:11.164400Z",
     "iopub.status.idle": "2025-10-01T13:24:54.004404Z",
     "shell.execute_reply": "2025-10-01T13:24:54.003777Z",
     "shell.execute_reply.started": "2025-10-01T13:24:11.164683Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined corpus size: 376120\n",
      "Loading tokenizer from pretrained model: \"DeepPavlov/distilrubert-base-cased-conversational\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f688ffe1ef214bfda673a0d6ede1a31d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/376120 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a78461c98df545c1959e3b7dcc2fdee0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/376120 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a47c747dd30d4186b850164399edfa39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/206706 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train examples (blocks): 204638\n",
      "Validation examples (blocks): 2068\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(output_dir, exist_ok=True)\n",
    "random.seed(seed)\n",
    "\n",
    "originals = df['original'].tolist()\n",
    "augmented = df['variant'].tolist()\n",
    "\n",
    "\n",
    "combined = mix_and_subsample(originals, augmented, mix_ratio=0.75, seed=seed)\n",
    "print(f'Combined corpus size: {len(combined)}')\n",
    "\n",
    "print(f'Loading tokenizer from pretrained model: \"{model_name_or_path}\"')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, do_lower_case=True)\n",
    "\n",
    "tokenized_ds = tokenize_and_group_texts(combined, tokenizer, use_wwm=use_wwm)\n",
    "print('Train examples (blocks):', len(tokenized_ds['train']))\n",
    "print('Validation examples (blocks):', len(tokenized_ds['validation']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T12:43:24.036319Z",
     "iopub.status.busy": "2025-10-01T12:43:24.035721Z",
     "iopub.status.idle": "2025-10-01T12:43:24.064167Z",
     "shell.execute_reply": "2025-10-01T12:43:24.063596Z",
     "shell.execute_reply.started": "2025-10-01T12:43:24.036294Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[102, 101, 6359, 130, 30396, 949, 26051, 34206...</td>\n",
       "      <td>[102, 101, 6359, 130, 30396, 949, 26051, 34206...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[336, 58124, 1455, 949, 15752, 22963, 128, 530...</td>\n",
       "      <td>[336, 58124, 1455, 949, 15752, 22963, 128, 530...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[101, 10206, 28826, 14422, 96300, 1405, 1638, ...</td>\n",
       "      <td>[101, 10206, 28826, 14422, 96300, 1405, 1638, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[4242, 828, 32948, 21595, 17230, 36324, 102, 1...</td>\n",
       "      <td>[4242, 828, 32948, 21595, 17230, 36324, 102, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[102, 101, 93065, 244, 14932, 12039, 241, 2376...</td>\n",
       "      <td>[102, 101, 93065, 244, 14932, 12039, 241, 2376...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[1454, 106, 963, 128, 8210, 10906, 102, 101, 3...</td>\n",
       "      <td>[1454, 106, 963, 128, 8210, 10906, 102, 101, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[58398, 294, 17230, 36324, 102, 101, 60699, 69...</td>\n",
       "      <td>[58398, 294, 17230, 36324, 102, 101, 60699, 69...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[367, 255, 56986, 333, 39934, 3459, 297, 850, ...</td>\n",
       "      <td>[367, 255, 56986, 333, 39934, 3459, 297, 850, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[11854, 22382, 261, 7386, 355, 323, 14362, 323...</td>\n",
       "      <td>[11854, 22382, 261, 7386, 355, 323, 14362, 323...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[949, 21595, 20669, 7751, 76870, 141, 102, 101...</td>\n",
       "      <td>[949, 21595, 20669, 7751, 76870, 141, 102, 101...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           input_ids  \\\n",
       "0  [102, 101, 6359, 130, 30396, 949, 26051, 34206...   \n",
       "1  [336, 58124, 1455, 949, 15752, 22963, 128, 530...   \n",
       "2  [101, 10206, 28826, 14422, 96300, 1405, 1638, ...   \n",
       "3  [4242, 828, 32948, 21595, 17230, 36324, 102, 1...   \n",
       "4  [102, 101, 93065, 244, 14932, 12039, 241, 2376...   \n",
       "5  [1454, 106, 963, 128, 8210, 10906, 102, 101, 3...   \n",
       "6  [58398, 294, 17230, 36324, 102, 101, 60699, 69...   \n",
       "7  [367, 255, 56986, 333, 39934, 3459, 297, 850, ...   \n",
       "8  [11854, 22382, 261, 7386, 355, 323, 14362, 323...   \n",
       "9  [949, 21595, 20669, 7751, 76870, 141, 102, 101...   \n",
       "\n",
       "                                              labels  \n",
       "0  [102, 101, 6359, 130, 30396, 949, 26051, 34206...  \n",
       "1  [336, 58124, 1455, 949, 15752, 22963, 128, 530...  \n",
       "2  [101, 10206, 28826, 14422, 96300, 1405, 1638, ...  \n",
       "3  [4242, 828, 32948, 21595, 17230, 36324, 102, 1...  \n",
       "4  [102, 101, 93065, 244, 14932, 12039, 241, 2376...  \n",
       "5  [1454, 106, 963, 128, 8210, 10906, 102, 101, 3...  \n",
       "6  [58398, 294, 17230, 36324, 102, 101, 60699, 69...  \n",
       "7  [367, 255, 56986, 333, 39934, 3459, 297, 850, ...  \n",
       "8  [11854, 22382, 261, 7386, 355, 323, 14362, 323...  \n",
       "9  [949, 21595, 20669, 7751, 76870, 141, 102, 101...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_ds[\"train\"].select(range(10)).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T12:43:29.177803Z",
     "iopub.status.busy": "2025-10-01T12:43:29.177067Z",
     "iopub.status.idle": "2025-10-01T12:43:29.182431Z",
     "shell.execute_reply": "2025-10-01T12:43:29.181899Z",
     "shell.execute_reply.started": "2025-10-01T12:43:29.177779Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['как', 'делать', '[MASK]']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"как делать [MASK]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T12:43:26.741726Z",
     "iopub.status.busy": "2025-10-01T12:43:26.741095Z",
     "iopub.status.idle": "2025-10-01T12:43:26.746738Z",
     "shell.execute_reply": "2025-10-01T12:43:26.746011Z",
     "shell.execute_reply.started": "2025-10-01T12:43:26.741698Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 879, 1634, 103, 102]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"как делать [MASK]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T07:20:23.693309Z",
     "iopub.status.busy": "2025-09-30T07:20:23.692722Z",
     "iopub.status.idle": "2025-09-30T07:20:23.703400Z",
     "shell.execute_reply": "2025-09-30T07:20:23.702774Z",
     "shell.execute_reply.started": "2025-09-30T07:20:23.693285Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'##азированныи без сахар, 50г [SEP] [CLS] хлебцы take a bieт ккууризhо - рсиоы'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_ds[\"train\"].select(range(10)).to_pandas()['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T13:25:07.652725Z",
     "iopub.status.busy": "2025-10-01T13:25:07.652456Z",
     "iopub.status.idle": "2025-10-01T13:25:08.434434Z",
     "shell.execute_reply": "2025-10-01T13:25:08.433788Z",
     "shell.execute_reply.started": "2025-10-01T13:25:07.652706Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "print('Loading model...')\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name_or_path)\n",
    "data_collator = build_data_collator(tokenizer, use_wwm=use_wwm, mlm_probability=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T13:25:12.905661Z",
     "iopub.status.busy": "2025-10-01T13:25:12.905379Z",
     "iopub.status.idle": "2025-10-01T14:58:47.158699Z",
     "shell.execute_reply": "2025-10-01T14:58:47.157996Z",
     "shell.execute_reply.started": "2025-10-01T13:25:12.905643Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/data/data_collator.py:1325: UserWarning: DataCollatorForWholeWordMask is only suitable for BertTokenizer-like tokenizers. Please refer to the documentation for more information.\n",
      "  warnings.warn(\n",
      "DistilBertSdpaAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3200' max='3200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3200/3200 1:33:13, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>4.235800</td>\n",
       "      <td>3.551408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.407600</td>\n",
       "      <td>3.140821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.060900</td>\n",
       "      <td>2.883120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.859600</td>\n",
       "      <td>2.725362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.653700</td>\n",
       "      <td>2.616949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.548800</td>\n",
       "      <td>2.398414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.434900</td>\n",
       "      <td>2.391071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.337700</td>\n",
       "      <td>2.256440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.224200</td>\n",
       "      <td>2.191037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.161700</td>\n",
       "      <td>2.148824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>2.115100</td>\n",
       "      <td>2.053026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>2.066000</td>\n",
       "      <td>2.070741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>1.984100</td>\n",
       "      <td>1.964442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>1.966900</td>\n",
       "      <td>1.969701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.926100</td>\n",
       "      <td>1.903529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>1.906000</td>\n",
       "      <td>1.940244</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/data/data_collator.py:1325: UserWarning: DataCollatorForWholeWordMask is only suitable for BertTokenizer-like tokenizers. Please refer to the documentation for more information.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/data/data_collator.py:1325: UserWarning: DataCollatorForWholeWordMask is only suitable for BertTokenizer-like tokenizers. Please refer to the documentation for more information.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/data/data_collator.py:1325: UserWarning: DataCollatorForWholeWordMask is only suitable for BertTokenizer-like tokenizers. Please refer to the documentation for more information.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running final evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/data/data_collator.py:1325: UserWarning: DataCollatorForWholeWordMask is only suitable for BertTokenizer-like tokenizers. Please refer to the documentation for more information.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='130' max='130' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [130/130 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval metrics: {'eval_loss': 1.9294145107269287, 'eval_runtime': 15.218, 'eval_samples_per_second': 135.891, 'eval_steps_per_second': 8.542, 'epoch': 4.0, 'perplexity': 6.885477687861801}\n",
      "Saving tokenizer and model...\n",
      "Done. Model saved to ./results_ft\n"
     ]
    }
   ],
   "source": [
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=128,\n",
    "    gradient_accumulation_steps=1,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,   \n",
    "\n",
    "    logging_strategy=\"steps\",  \n",
    "    logging_steps=200,\n",
    "\n",
    "    save_strategy=\"epoch\",   \n",
    "    save_total_limit=num_epochs,   \n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.01,\n",
    "    seed=seed,\n",
    "    fp16=False,\n",
    "    report_to='none',\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_ds['train'],\n",
    "    eval_dataset=tokenized_ds['validation'],\n",
    ")\n",
    "\n",
    "# Train\n",
    "print('Starting training...')\n",
    "trainer.train()\n",
    "\n",
    "# Eval\n",
    "print('Running final evaluation...')\n",
    "metrics = trainer.evaluate()\n",
    "loss = metrics.get('eval_loss')\n",
    "if loss is not None:\n",
    "    try:\n",
    "        ppl = math.exp(loss)\n",
    "    except OverflowError:\n",
    "        ppl = float('inf')\n",
    "    metrics['perplexity'] = ppl\n",
    "print('Eval metrics:', metrics)\n",
    "\n",
    "# Save\n",
    "print('Saving tokenizer and model...')\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print('Done. Model saved to', output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T15:01:55.330651Z",
     "iopub.status.busy": "2025-10-01T15:01:55.330393Z",
     "iopub.status.idle": "2025-10-01T15:01:55.335620Z",
     "shell.execute_reply": "2025-10-01T15:01:55.334912Z",
     "shell.execute_reply.started": "2025-10-01T15:01:55.330636Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model == trainer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T15:02:00.021181Z",
     "iopub.status.busy": "2025-10-01T15:02:00.020350Z",
     "iopub.status.idle": "2025-10-01T15:02:00.045578Z",
     "shell.execute_reply": "2025-10-01T15:02:00.045026Z",
     "shell.execute_reply.started": "2025-10-01T15:02:00.021156Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'яблоки джерамин 4шт.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mlm.sample().original.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T15:02:37.711776Z",
     "iopub.status.busy": "2025-10-01T15:02:37.711529Z",
     "iopub.status.idle": "2025-10-01T15:02:37.730247Z",
     "shell.execute_reply": "2025-10-01T15:02:37.729548Z",
     "shell.execute_reply.started": "2025-10-01T15:02:37.711759Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> яблоки джерамин 4шт.'\n",
      "'>>> спички джерамин 4шт.'\n",
      "'>>> груши джерамин 4шт.'\n",
      "'>>> губки джерамин 4шт.'\n",
      "'>>> подгузники джерамин 4шт.'\n"
     ]
    }
   ],
   "source": [
    "maska = \"[MASK]\"\n",
    "text = f'{maska} джерамин 4шт.'\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "token_logits = model(**inputs).logits\n",
    "# Find the location of [MASK] and extract its logits\n",
    "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "# Pick the [MASK] candidates with the highest logits\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "for token in top_5_tokens:\n",
    "    print(f\"'>>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T15:03:09.027496Z",
     "iopub.status.busy": "2025-10-01T15:03:09.026760Z",
     "iopub.status.idle": "2025-10-01T15:03:09.277090Z",
     "shell.execute_reply": "2025-10-01T15:03:09.276331Z",
     "shell.execute_reply.started": "2025-10-01T15:03:09.027472Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T15:05:47.059674Z",
     "iopub.status.busy": "2025-10-01T15:05:47.059382Z",
     "iopub.status.idle": "2025-10-01T15:06:15.562929Z",
     "shell.execute_reply": "2025-10-01T15:06:15.562359Z",
     "shell.execute_reply.started": "2025-10-01T15:05:47.059642Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a638338e84646bf9497465df0afdb8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading...:   0%|          | 0.00/542M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Dersty/results_ft/commit/12c0960e78472fa3bc6d51676702b6cc0f8b2baf', commit_message='Dersty/distilbert_rubert_X5_ner_MLM', commit_description='', oid='12c0960e78472fa3bc6d51676702b6cc0f8b2baf', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Dersty/results_ft', endpoint='https://huggingface.co', repo_type='model', repo_id='Dersty/results_ft'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub(\"Dersty/distilbert_rubert_X5_ner_MLM\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8370192,
     "sourceId": 13206554,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
